{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef350b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID (this is not a feature)</th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>355053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>132601</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>Private</td>\n",
       "      <td>63814</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>112507</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>126850</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID (this is not a feature)  age      workclass  fnlwgt      education  \\\n",
       "0                           1   36        Private  355053        HS-grad   \n",
       "1                           2   30   Self-emp-inc  132601      Bachelors   \n",
       "2                           3   19        Private   63814   Some-college   \n",
       "3                           4   44        Private  112507   Some-college   \n",
       "4                           5   51   Self-emp-inc  126850        HS-grad   \n",
       "\n",
       "   education-num       marital-status         occupation    relationship  \\\n",
       "0              9            Separated      Other-service       Unmarried   \n",
       "1             13   Married-civ-spouse       Craft-repair         Husband   \n",
       "2             10        Never-married       Adm-clerical   Not-in-family   \n",
       "3             10   Married-civ-spouse              Sales         Husband   \n",
       "4              9   Married-civ-spouse   Transport-moving         Husband   \n",
       "\n",
       "     race      sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0   Black   Female             0             0              28   \n",
       "1   White     Male             0             0              40   \n",
       "2   White   Female             0             0              18   \n",
       "3   White     Male             0             0              40   \n",
       "4   White     Male             0             0              65   \n",
       "\n",
       "   native-country salary  \n",
       "0   United-States  <=50K  \n",
       "1   United-States   >50K  \n",
       "2   United-States  <=50K  \n",
       "3   United-States  <=50K  \n",
       "4   United-States  <=50K  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "train_df = pd.read_csv('train.csv', na_values=' ?')\n",
    "\n",
    "test_df = pd.read_csv('test.csv', na_values=' ?')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f719ce2",
   "metadata": {},
   "source": [
    "## Data Cleaning -- For Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed067cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID (this is not a feature)    0\n",
       "age                           0\n",
       "workclass                     0\n",
       "fnlwgt                        0\n",
       "education                     0\n",
       "education-num                 0\n",
       "marital-status                0\n",
       "occupation                    0\n",
       "relationship                  0\n",
       "race                          0\n",
       "sex                           0\n",
       "capital-gain                  0\n",
       "capital-loss                  0\n",
       "hours-per-week                0\n",
       "native-country                0\n",
       "salary                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding the special characters in the data frame \n",
    "train_df.isin([' ?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb9ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will replace the special character to nan and then drop the columns \n",
    "train_df['workclass'] = train_df['workclass'].replace(' ?',np.nan)\n",
    "train_df['occupation'] = train_df['occupation'].replace(' ?',np.nan)\n",
    "train_df['native-country'] = train_df['native-country'].replace(' ?',np.nan)\n",
    "\n",
    "\n",
    "#dropping the NaN rows now \n",
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf26929f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID (this is not a feature)    0\n",
       "age                           0\n",
       "workclass                     0\n",
       "fnlwgt                        0\n",
       "education                     0\n",
       "education-num                 0\n",
       "marital-status                0\n",
       "occupation                    0\n",
       "relationship                  0\n",
       "race                          0\n",
       "sex                           0\n",
       "capital-gain                  0\n",
       "capital-loss                  0\n",
       "hours-per-week                0\n",
       "native-country                0\n",
       "salary                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isin([' ?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f17e94",
   "metadata": {},
   "source": [
    "## Data Cleaning -- For testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e0bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code will replace the special character to nan and then drop the columns \n",
    "test_df['workclass'] = test_df['workclass'].replace(' ?',np.nan)\n",
    "test_df['occupation'] = test_df['occupation'].replace(' ?',np.nan)\n",
    "test_df['native-country'] = test_df['native-country'].replace(' ?',np.nan)\n",
    "\n",
    "\n",
    "#dropping the NaN rows now \n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede27003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID (this is not a feature)    0\n",
       "age                           0\n",
       "workclass                     0\n",
       "fnlwgt                        0\n",
       "education                     0\n",
       "education-num                 0\n",
       "marital-status                0\n",
       "occupation                    0\n",
       "relationship                  0\n",
       "race                          0\n",
       "sex                           0\n",
       "capital-gain                  0\n",
       "capital-loss                  0\n",
       "hours-per-week                0\n",
       "native-country                0\n",
       "salary                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isin([' ?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa98878",
   "metadata": {},
   "source": [
    "## Feature Engineering -- For training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf457a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        1\n",
      "25911    1\n",
      "25905    1\n",
      "25906    1\n",
      "25907    1\n",
      "        ..\n",
      "12986    1\n",
      "12985    1\n",
      "12984    1\n",
      "12983    1\n",
      "38842    1\n",
      "Name: ID (this is not a feature), Length: 35976, dtype: int64\n",
      "31    1017\n",
      "33    1017\n",
      "23    1015\n",
      "36    1007\n",
      "30     993\n",
      "      ... \n",
      "88       5\n",
      "85       4\n",
      "86       1\n",
      "87       1\n",
      "89       1\n",
      "Name: age, Length: 74, dtype: int64\n",
      " Private             26489\n",
      " Self-emp-not-inc     3022\n",
      " Local-gov            2483\n",
      " State-gov            1526\n",
      " Self-emp-inc         1312\n",
      " Federal-gov          1129\n",
      " Without-pay            15\n",
      "Name: workclass, dtype: int64\n",
      "203488    17\n",
      "120277    14\n",
      "99185     14\n",
      "125892    13\n",
      "113364    13\n",
      "          ..\n",
      "316120     1\n",
      "160472     1\n",
      "147314     1\n",
      "135285     1\n",
      "210217     1\n",
      "Name: fnlwgt, Length: 22936, dtype: int64\n",
      " HS-grad         11807\n",
      " Some-college     7864\n",
      " Bachelors        5990\n",
      " Masters          2030\n",
      " Assoc-voc        1554\n",
      " 11th             1286\n",
      " Assoc-acdm       1198\n",
      " 10th              986\n",
      " 7th-8th           640\n",
      " Prof-school       620\n",
      " 9th               533\n",
      " 12th              442\n",
      " Doctorate         436\n",
      " 5th-6th           357\n",
      " 1st-4th           181\n",
      " Preschool          52\n",
      "Name: education, dtype: int64\n",
      "9     11807\n",
      "10     7864\n",
      "13     5990\n",
      "14     2030\n",
      "11     1554\n",
      "7      1286\n",
      "12     1198\n",
      "6       986\n",
      "4       640\n",
      "15      620\n",
      "5       533\n",
      "8       442\n",
      "16      436\n",
      "3       357\n",
      "2       181\n",
      "1        52\n",
      "Name: education-num, dtype: int64\n",
      " Married-civ-spouse       16742\n",
      " Never-married            11607\n",
      " Divorced                  5014\n",
      " Separated                 1130\n",
      " Widowed                   1032\n",
      " Married-spouse-absent      423\n",
      " Married-AF-spouse           28\n",
      "Name: marital-status, dtype: int64\n",
      " Craft-repair         4795\n",
      " Prof-specialty       4790\n",
      " Exec-managerial      4779\n",
      " Adm-clerical         4442\n",
      " Sales                4270\n",
      " Other-service        3821\n",
      " Machine-op-inspct    2392\n",
      " Transport-moving     1808\n",
      " Handlers-cleaners    1632\n",
      " Farming-fishing      1154\n",
      " Tech-support         1124\n",
      " Protective-serv       769\n",
      " Priv-house-serv       188\n",
      " Armed-Forces           12\n",
      "Name: occupation, dtype: int64\n",
      " Husband           14847\n",
      " Not-in-family      9305\n",
      " Own-child          5265\n",
      " Unmarried          3821\n",
      " Wife               1674\n",
      " Other-relative     1064\n",
      "Name: relationship, dtype: int64\n",
      " White                 30957\n",
      " Black                  3354\n",
      " Asian-Pac-Islander     1034\n",
      " Amer-Indian-Eskimo      357\n",
      " Other                   274\n",
      "Name: race, dtype: int64\n",
      " Male      24272\n",
      " Female    11704\n",
      "Name: sex, dtype: int64\n",
      "0        32956\n",
      "15024      406\n",
      "7688       296\n",
      "7298       281\n",
      "99999      191\n",
      "         ...  \n",
      "2387         1\n",
      "5060         1\n",
      "22040        1\n",
      "6097         1\n",
      "7262         1\n",
      "Name: capital-gain, Length: 118, dtype: int64\n",
      "0       34251\n",
      "1902      247\n",
      "1977      199\n",
      "1887      184\n",
      "1485       54\n",
      "        ...  \n",
      "4356        1\n",
      "1844        1\n",
      "1911        1\n",
      "2163        1\n",
      "3900        1\n",
      "Name: capital-loss, Length: 97, dtype: int64\n",
      "40    16932\n",
      "50     3245\n",
      "45     2104\n",
      "60     1651\n",
      "35     1442\n",
      "      ...  \n",
      "73        2\n",
      "89        2\n",
      "79        1\n",
      "94        1\n",
      "69        1\n",
      "Name: hours-per-week, Length: 94, dtype: int64\n",
      " United-States                 32848\n",
      " Mexico                          730\n",
      " Philippines                     234\n",
      " Germany                         153\n",
      " Puerto-Rico                     132\n",
      " Canada                          126\n",
      " El-Salvador                     125\n",
      " India                           111\n",
      " Cuba                            100\n",
      " China                            93\n",
      " England                          92\n",
      " Jamaica                          89\n",
      " Italy                            80\n",
      " South                            77\n",
      " Japan                            73\n",
      " Guatemala                        70\n",
      " Columbia                         70\n",
      " Dominican-Republic               69\n",
      " Vietnam                          68\n",
      " Poland                           63\n",
      " Haiti                            53\n",
      " Portugal                         46\n",
      " Iran                             42\n",
      " Greece                           41\n",
      " Taiwan                           40\n",
      " Nicaragua                        38\n",
      " Peru                             36\n",
      " France                           31\n",
      " Ecuador                          28\n",
      " Ireland                          27\n",
      " Hong                             23\n",
      " Cambodia                         21\n",
      " Thailand                         21\n",
      " Outlying-US(Guam-USVI-etc)       20\n",
      " Yugoslavia                       19\n",
      " Scotland                         18\n",
      " Hungary                          17\n",
      " Laos                             17\n",
      " Trinadad&Tobago                  17\n",
      " Honduras                         17\n",
      " Holand-Netherlands                1\n",
      "Name: native-country, dtype: int64\n",
      "<=50K    27037\n",
      ">50K      8939\n",
      "Name: salary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# converting the categorical data to numerical\n",
    "\n",
    "for c in train_df.columns:\n",
    "    print (train_df[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32852c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       workclass      education       marital-status         occupation  \\\n",
       "0        Private        HS-grad            Separated      Other-service   \n",
       "1   Self-emp-inc      Bachelors   Married-civ-spouse       Craft-repair   \n",
       "2        Private   Some-college        Never-married       Adm-clerical   \n",
       "3        Private   Some-college   Married-civ-spouse              Sales   \n",
       "4   Self-emp-inc        HS-grad   Married-civ-spouse   Transport-moving   \n",
       "\n",
       "     relationship    race      sex salary  \n",
       "0       Unmarried   Black   Female  <=50K  \n",
       "1         Husband   White     Male   >50K  \n",
       "2   Not-in-family   White   Female  <=50K  \n",
       "3         Husband   White     Male  <=50K  \n",
       "4         Husband   White     Male  <=50K  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing noisy data based on the unique values\n",
    "# to standardize the data\n",
    "train_df.drop(['ID (this is not a feature)', 'education-num','age', 'hours-per-week', 'fnlwgt', 'capital-gain','capital-loss', 'native-country'], axis=1, inplace = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30b3ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the data into numerical data using map function\n",
    "train_df['salary'] = train_df['salary'].map({'<=50K': 0, '>50K': 1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09849486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Female', ' Male'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change for every categorical variable\n",
    "\n",
    "train_df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23940882",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['workclass'] = train_df['workclass'].map({' Private':0, ' Self-emp-inc':1, ' Local-gov': 2, ' Self-emp-not-inc': 3,\n",
    "                                                  ' State-gov': 4, ' Federal-gov': 5, ' Without-pay': 6}).astype(int)\n",
    "\n",
    "train_df['education'] = train_df['education'].map({' HS-grad': 0, ' Bachelors': 1, ' Some-college':2, ' Prof-school': 3, ' 10th':4, \n",
    "                                                   ' 12th':5, ' 7th-8th':6, ' Masters':7, ' Assoc-acdm':8, ' Assoc-voc':9, ' 11th':10,\n",
    "                                                   ' 9th':11, ' 1st-4th':12, ' 5th-6th':13, ' Doctorate':14, ' Preschool':15}).astype(int)\n",
    "\n",
    "train_df['marital-status'] = train_df['marital-status'].map({' Separated':0, ' Married-civ-spouse':1, ' Never-married':2, \n",
    "                                                             ' Divorced':3, ' Widowed':4, ' Married-spouse-absent':5, \n",
    "                                                             ' Married-AF-spouse':6}).astype(int)\n",
    "\n",
    "train_df['occupation'] = train_df['occupation'].map({' Other-service':0, ' Craft-repair':1, ' Adm-clerical':2, ' Sales':3,\n",
    "                                                     ' Transport-moving':4, ' Farming-fishing':5, ' Tech-support':6,\n",
    "                                                     ' Prof-specialty':7, ' Exec-managerial':8, ' Machine-op-inspct':9,\n",
    "                                                     ' Handlers-cleaners':10, ' Armed-Forces':11, ' Priv-house-serv':12,\n",
    "                                                     ' Protective-serv':13}).astype(int)\n",
    "\n",
    "train_df['relationship'] = train_df['relationship'].map({' Unmarried':0, ' Husband':1, ' Not-in-family':2, ' Own-child':3,\n",
    "                                                         ' Wife':4,' Other-relative':5}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93c84a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['race'] = train_df['race'].map({' Black':0, ' White':1, ' Other':2, ' Asian-Pac-Islander':3,\n",
    "                                         ' Amer-Indian-Eskimo':4}).astype(int)\n",
    "\n",
    "train_df['sex'] = train_df['sex'].map({' Female':0, ' Male':1}).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85b9f486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass  education  marital-status  occupation  relationship  race  sex  \\\n",
       "0          0          0               0           0             0     0    0   \n",
       "1          1          1               1           1             1     1    1   \n",
       "2          0          2               2           2             2     1    0   \n",
       "3          0          2               1           3             1     1    1   \n",
       "4          1          0               1           4             1     1    1   \n",
       "\n",
       "   salary  \n",
       "0       0  \n",
       "1       1  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eecfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25babd7b",
   "metadata": {},
   "source": [
    "## Feature engineering -- testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1e717f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        1\n",
      "6581     1\n",
      "6678     1\n",
      "6679     1\n",
      "6680     1\n",
      "        ..\n",
      "3341     1\n",
      "3342     1\n",
      "3343     1\n",
      "3344     1\n",
      "10000    1\n",
      "Name: ID (this is not a feature), Length: 9246, dtype: int64\n",
      "35    289\n",
      "36    276\n",
      "34    264\n",
      "33    262\n",
      "41    257\n",
      "     ... \n",
      "83      3\n",
      "78      2\n",
      "82      2\n",
      "84      1\n",
      "85      1\n",
      "Name: age, Length: 70, dtype: int64\n",
      " Private             6818\n",
      " Self-emp-not-inc     774\n",
      " Local-gov            617\n",
      " State-gov            420\n",
      " Self-emp-inc         334\n",
      " Federal-gov          277\n",
      " Without-pay            6\n",
      "Name: workclass, dtype: int64\n",
      "149102    7\n",
      "177675    6\n",
      "132879    6\n",
      "143062    6\n",
      "216129    5\n",
      "         ..\n",
      "152924    1\n",
      "178310    1\n",
      "71221     1\n",
      "220187    1\n",
      "203392    1\n",
      "Name: fnlwgt, Length: 7973, dtype: int64\n",
      " HS-grad         2976\n",
      " Some-college    2035\n",
      " Bachelors       1580\n",
      " Masters          484\n",
      " Assoc-voc        405\n",
      " 11th             333\n",
      " Assoc-acdm       309\n",
      " 10th             237\n",
      " 7th-8th          183\n",
      " Prof-school      165\n",
      " 9th              143\n",
      " 12th             135\n",
      " Doctorate        108\n",
      " 5th-6th           92\n",
      " 1st-4th           41\n",
      " Preschool         20\n",
      "Name: education, dtype: int64\n",
      "9     2976\n",
      "10    2035\n",
      "13    1580\n",
      "14     484\n",
      "11     405\n",
      "7      333\n",
      "12     309\n",
      "6      237\n",
      "4      183\n",
      "15     165\n",
      "5      143\n",
      "8      135\n",
      "16     108\n",
      "3       92\n",
      "2       41\n",
      "1       20\n",
      "Name: education-num, dtype: int64\n",
      " Married-civ-spouse       4313\n",
      " Never-married            2991\n",
      " Divorced                 1283\n",
      " Separated                 281\n",
      " Widowed                   245\n",
      " Married-spouse-absent     129\n",
      " Married-AF-spouse           4\n",
      "Name: marital-status, dtype: int64\n",
      " Craft-repair         1225\n",
      " Prof-specialty       1218\n",
      " Exec-managerial      1205\n",
      " Sales                1138\n",
      " Adm-clerical         1098\n",
      " Other-service         987\n",
      " Machine-op-inspct     578\n",
      " Transport-moving      508\n",
      " Handlers-cleaners     414\n",
      " Farming-fishing       326\n",
      " Tech-support          296\n",
      " Protective-serv       207\n",
      " Priv-house-serv        44\n",
      " Armed-Forces            2\n",
      "Name: occupation, dtype: int64\n",
      " Husband           3819\n",
      " Not-in-family     2397\n",
      " Own-child         1361\n",
      " Unmarried          967\n",
      " Wife               417\n",
      " Other-relative     285\n",
      "Name: relationship, dtype: int64\n",
      " White                 7946\n",
      " Black                  874\n",
      " Asian-Pac-Islander     269\n",
      " Other                   79\n",
      " Amer-Indian-Eskimo      78\n",
      "Name: race, dtype: int64\n",
      " Male      6255\n",
      " Female    2991\n",
      "Name: sex, dtype: int64\n",
      "0        8476\n",
      "7688       95\n",
      "15024      92\n",
      "7298       70\n",
      "99999      38\n",
      "         ... \n",
      "4931        1\n",
      "5455        1\n",
      "34095       1\n",
      "6360        1\n",
      "5060        1\n",
      "Name: capital-gain, Length: 94, dtype: int64\n",
      "0       8831\n",
      "1902      47\n",
      "1977      47\n",
      "1887      44\n",
      "2415      17\n",
      "        ... \n",
      "1944       1\n",
      "3900       1\n",
      "1762       1\n",
      "1825       1\n",
      "1092       1\n",
      "Name: capital-loss, Length: 71, dtype: int64\n",
      "40    4426\n",
      "50     849\n",
      "45     498\n",
      "60     434\n",
      "35     334\n",
      "      ... \n",
      "88       1\n",
      "87       1\n",
      "77       1\n",
      "89       1\n",
      "74       1\n",
      "Name: hours-per-week, Length: 85, dtype: int64\n",
      " United-States                 8444\n",
      " Mexico                         173\n",
      " Philippines                     49\n",
      " Puerto-Rico                     43\n",
      " Germany                         40\n",
      " Canada                          37\n",
      " India                           36\n",
      " Cuba                            33\n",
      " Dominican-Republic              28\n",
      " England                         27\n",
      " South                           24\n",
      " El-Salvador                     22\n",
      " Italy                           20\n",
      " China                           20\n",
      " Poland                          18\n",
      " Haiti                           16\n",
      " Japan                           16\n",
      " Portugal                        16\n",
      " Guatemala                       16\n",
      " Taiwan                          15\n",
      " Ecuador                         15\n",
      " Vietnam                         15\n",
      " Iran                            14\n",
      " Jamaica                         14\n",
      " Columbia                        12\n",
      " Nicaragua                       10\n",
      " Ireland                          9\n",
      " Peru                             9\n",
      " Trinadad&Tobago                  9\n",
      " Thailand                         8\n",
      " Greece                           8\n",
      " Cambodia                         5\n",
      " Hong                             5\n",
      " France                           5\n",
      " Yugoslavia                       4\n",
      " Laos                             4\n",
      " Outlying-US(Guam-USVI-etc)       2\n",
      " Scotland                         2\n",
      " Honduras                         2\n",
      " Hungary                          1\n",
      "Name: native-country, dtype: int64\n",
      "<=50K    6977\n",
      ">50K     2269\n",
      "Name: salary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# converting the categorical data to numerical\n",
    "\n",
    "for c in test_df.columns:\n",
    "    print (test_df[c].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5db6a35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>11th</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Transport-moving</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           workclass      education       marital-status          occupation  \\\n",
       "0   Self-emp-not-inc        HS-grad   Married-civ-spouse     Farming-fishing   \n",
       "1   Self-emp-not-inc           11th             Divorced     Exec-managerial   \n",
       "2            Private   Some-college   Married-civ-spouse        Craft-repair   \n",
       "3            Private        HS-grad        Never-married    Transport-moving   \n",
       "4            Private   Some-college        Never-married   Machine-op-inspct   \n",
       "\n",
       "     relationship    race    sex salary  \n",
       "0         Husband   White   Male  <=50K  \n",
       "1   Not-in-family   White   Male  <=50K  \n",
       "2         Husband   Black   Male  <=50K  \n",
       "3       Own-child   White   Male   >50K  \n",
       "4       Unmarried   White   Male  <=50K  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing noisy data based on the unique values\n",
    "test_df.drop(['ID (this is not a feature)', 'education-num','age', 'hours-per-week', 'fnlwgt', 'capital-gain','capital-loss', 'native-country'], axis=1, inplace = True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f225906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping the data into numerical data using map function\n",
    "test_df['salary'] = test_df['salary'].map({'<=50K': 0, '>50K': 1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c48cd86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Male', ' Female'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change for every categorical variable\n",
    "\n",
    "test_df['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcb9bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['workclass'] = test_df['workclass'].map({' Private':0, ' Self-emp-inc':1, ' Local-gov': 2, ' Self-emp-not-inc': 3,\n",
    "                                                  ' State-gov': 4, ' Federal-gov': 5, ' Without-pay': 6}).astype(int)\n",
    "\n",
    "test_df['education'] = test_df['education'].map({' HS-grad': 0, ' Bachelors': 1, ' Some-college':2, ' Prof-school': 3, ' 10th':4, \n",
    "                                                   ' 12th':5, ' 7th-8th':6, ' Masters':7, ' Assoc-acdm':8, ' Assoc-voc':9, ' 11th':10,\n",
    "                                                   ' 9th':11, ' 1st-4th':12, ' 5th-6th':13, ' Doctorate':14, ' Preschool':15}).astype(int)\n",
    "\n",
    "test_df['marital-status'] = test_df['marital-status'].map({' Separated':0, ' Married-civ-spouse':1, ' Never-married':2, \n",
    "                                                             ' Divorced':3, ' Widowed':4, ' Married-spouse-absent':5, \n",
    "                                                             ' Married-AF-spouse':6}).astype(int)\n",
    "\n",
    "test_df['occupation'] = test_df['occupation'].map({' Other-service':0, ' Craft-repair':1, ' Adm-clerical':2, ' Sales':3,\n",
    "                                                     ' Transport-moving':4, ' Farming-fishing':5, ' Tech-support':6,\n",
    "                                                     ' Prof-specialty':7, ' Exec-managerial':8, ' Machine-op-inspct':9,\n",
    "                                                     ' Handlers-cleaners':10, ' Armed-Forces':11, ' Priv-house-serv':12,\n",
    "                                                     ' Protective-serv':13}).astype(int)\n",
    "\n",
    "test_df['relationship'] = test_df['relationship'].map({' Unmarried':0, ' Husband':1, ' Not-in-family':2, ' Own-child':3,\n",
    "                                                         ' Wife':4,' Other-relative':5}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55239b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['race'] = test_df['race'].map({' Black':0, ' White':1, ' Other':2, ' Asian-Pac-Islander':3,\n",
    "                                         ' Amer-Indian-Eskimo':4}).astype(int)\n",
    "\n",
    "test_df['sex'] = test_df['sex'].map({' Female':0, ' Male':1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff0f7aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   workclass  education  marital-status  occupation  relationship  race  sex  \\\n",
       "0          3          0               1           5             1     1    1   \n",
       "1          3         10               3           8             2     1    1   \n",
       "2          0          2               1           1             1     0    1   \n",
       "3          0          0               2           4             3     1    1   \n",
       "4          0          2               2           9             0     1    1   \n",
       "\n",
       "   salary  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d597825",
   "metadata": {},
   "source": [
    "## Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "428af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(['salary'], axis=1)\n",
    "x_test = test_df.drop(['salary'], axis=1)\n",
    "\n",
    "y_train = pd.DataFrame(train_df['salary'])\n",
    "y_test = pd.DataFrame(test_df['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c03f3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "percep = Perceptron()\n",
    "dectreeclf = DecisionTreeClassifier()\n",
    "knnclf = KNeighborsClassifier(n_neighbors=5) \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(6, 5),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    alpha=0.0001,\n",
    "                    batch_size=min(200, len(train_df)),\n",
    "                    learning_rate_init=0.01)\n",
    "linsvc = SVC(kernel = 'linear')\n",
    "nonsvc = SVC(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f24ae6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameteric Grid Search\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid = [\n",
    "#     {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "#     'C' : np.logspace(-4, 4, 20),\n",
    "#     'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#     'max_iter' : [100, 1000, 2500, 5000]\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9edbad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_clf = GridSearchCV(reg, param_grid = param_grid, cv = 3, verbose = True, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "608e4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48435777\n",
      "Iteration 2, loss = 0.45934945\n",
      "Iteration 3, loss = 0.44654267\n",
      "Iteration 4, loss = 0.44203009\n",
      "Iteration 5, loss = 0.43460491\n",
      "Iteration 6, loss = 0.42620607\n",
      "Iteration 7, loss = 0.42330444\n",
      "Iteration 8, loss = 0.41995095\n",
      "Iteration 9, loss = 0.41938583\n",
      "Iteration 10, loss = 0.41838168\n",
      "Iteration 11, loss = 0.41740305\n",
      "Iteration 12, loss = 0.41469058\n",
      "Iteration 13, loss = 0.41569296\n",
      "Iteration 14, loss = 0.41400463\n",
      "Iteration 15, loss = 0.41275461\n",
      "Iteration 16, loss = 0.41281690\n",
      "Iteration 17, loss = 0.41298990\n",
      "Iteration 18, loss = 0.41217493\n",
      "Iteration 19, loss = 0.41284386\n",
      "Iteration 20, loss = 0.41200543\n",
      "Iteration 21, loss = 0.41197736\n",
      "Iteration 22, loss = 0.41282731\n",
      "Iteration 23, loss = 0.41075197\n",
      "Iteration 24, loss = 0.41145982\n",
      "Iteration 25, loss = 0.41169003\n",
      "Iteration 26, loss = 0.41159859\n",
      "Iteration 27, loss = 0.41092165\n",
      "Iteration 28, loss = 0.41045724\n",
      "Iteration 29, loss = 0.41133145\n",
      "Iteration 30, loss = 0.40943422\n",
      "Iteration 31, loss = 0.40967992\n",
      "Iteration 32, loss = 0.41070450\n",
      "Iteration 33, loss = 0.40946253\n",
      "Iteration 34, loss = 0.40927612\n",
      "Iteration 35, loss = 0.40923142\n",
      "Iteration 36, loss = 0.40840164\n",
      "Iteration 37, loss = 0.40829416\n",
      "Iteration 38, loss = 0.40865273\n",
      "Iteration 39, loss = 0.40772268\n",
      "Iteration 40, loss = 0.40679051\n",
      "Iteration 41, loss = 0.40765037\n",
      "Iteration 42, loss = 0.40613734\n",
      "Iteration 43, loss = 0.40622819\n",
      "Iteration 44, loss = 0.40610276\n",
      "Iteration 45, loss = 0.40609048\n",
      "Iteration 46, loss = 0.40641408\n",
      "Iteration 47, loss = 0.40648829\n",
      "Iteration 48, loss = 0.40708609\n",
      "Iteration 49, loss = 0.40641927\n",
      "Iteration 50, loss = 0.40498476\n",
      "Iteration 51, loss = 0.40580086\n",
      "Iteration 52, loss = 0.40596964\n",
      "Iteration 53, loss = 0.40511796\n",
      "Iteration 54, loss = 0.40500813\n",
      "Iteration 55, loss = 0.40523958\n",
      "Iteration 56, loss = 0.40458406\n",
      "Iteration 57, loss = 0.40428776\n",
      "Iteration 58, loss = 0.40447194\n",
      "Iteration 59, loss = 0.40440039\n",
      "Iteration 60, loss = 0.40489490\n",
      "Iteration 61, loss = 0.40488525\n",
      "Iteration 62, loss = 0.40510112\n",
      "Iteration 63, loss = 0.40441786\n",
      "Iteration 64, loss = 0.40455083\n",
      "Iteration 65, loss = 0.40494326\n",
      "Iteration 66, loss = 0.40384586\n",
      "Iteration 67, loss = 0.40478572\n",
      "Iteration 68, loss = 0.40646385\n",
      "Iteration 69, loss = 0.40360589\n",
      "Iteration 70, loss = 0.40414379\n",
      "Iteration 71, loss = 0.40515288\n",
      "Iteration 72, loss = 0.40368512\n",
      "Iteration 73, loss = 0.40383480\n",
      "Iteration 74, loss = 0.40368834\n",
      "Iteration 75, loss = 0.40463723\n",
      "Iteration 76, loss = 0.40407493\n",
      "Iteration 77, loss = 0.40340136\n",
      "Iteration 78, loss = 0.40340045\n",
      "Iteration 79, loss = 0.40375061\n",
      "Iteration 80, loss = 0.40315799\n",
      "Iteration 81, loss = 0.40374805\n",
      "Iteration 82, loss = 0.40339557\n",
      "Iteration 83, loss = 0.40330998\n",
      "Iteration 84, loss = 0.40309123\n",
      "Iteration 85, loss = 0.40437337\n",
      "Iteration 86, loss = 0.40304227\n",
      "Iteration 87, loss = 0.40309371\n",
      "Iteration 88, loss = 0.40271446\n",
      "Iteration 89, loss = 0.40276278\n",
      "Iteration 90, loss = 0.40254841\n",
      "Iteration 91, loss = 0.40310162\n",
      "Iteration 92, loss = 0.40254861\n",
      "Iteration 93, loss = 0.40305074\n",
      "Iteration 94, loss = 0.40298663\n",
      "Iteration 95, loss = 0.40235171\n",
      "Iteration 96, loss = 0.40221810\n",
      "Iteration 97, loss = 0.40284135\n",
      "Iteration 98, loss = 0.40177983\n",
      "Iteration 99, loss = 0.40279762\n",
      "Iteration 100, loss = 0.40195696\n",
      "Iteration 101, loss = 0.40122933\n",
      "Iteration 102, loss = 0.40198614\n",
      "Iteration 103, loss = 0.40300721\n",
      "Iteration 104, loss = 0.40308588\n",
      "Iteration 105, loss = 0.40134460\n",
      "Iteration 106, loss = 0.40127156\n",
      "Iteration 107, loss = 0.40063283\n",
      "Iteration 108, loss = 0.40285077\n",
      "Iteration 109, loss = 0.40269717\n",
      "Iteration 110, loss = 0.40245214\n",
      "Iteration 111, loss = 0.40122003\n",
      "Iteration 112, loss = 0.40159077\n",
      "Iteration 113, loss = 0.40211451\n",
      "Iteration 114, loss = 0.40214441\n",
      "Iteration 115, loss = 0.40131173\n",
      "Iteration 116, loss = 0.40126024\n",
      "Iteration 117, loss = 0.40075220\n",
      "Iteration 118, loss = 0.40079636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# train our model using training data\n",
    "logreg.fit(x_train, y_train)\n",
    "# predict the salary on test data\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "\n",
    "# For perceptron\n",
    "percep.fit(x_train, y_train)\n",
    "y_pred_1 = percep.predict(x_test)\n",
    "\n",
    "\n",
    "# For decision tree classifier\n",
    "dectreeclf.fit(x_train, y_train)\n",
    "y_pred_2 = dectreeclf.predict(x_test)\n",
    "\n",
    "\n",
    "# KNN for classifier\n",
    "knnclf.fit(x_train, y_train)\n",
    "y_pred_3 = knnclf.predict(x_test)\n",
    "\n",
    "\n",
    "# MLP\n",
    "mlp.fit(x_train, y_train)\n",
    "y_pred_4 = mlp.predict(x_test)\n",
    "\n",
    "\n",
    "# Linear SVC\n",
    "linsvc.fit(x_train, y_train)\n",
    "y_pred_5 = linsvc.predict(x_test)\n",
    "\n",
    "\n",
    "# Non-linear SVC\n",
    "nonsvc.fit(x_train, y_train)\n",
    "y_pred_6 = nonsvc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e047d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5044925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but Perceptron was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percep.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a7fcc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dectreeclf.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dfe0e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnclf.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f714faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "707b6439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC\n",
    "linsvc.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "921ed6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Non-linear SVC\n",
    "nonsvc.predict([[0,0,2,4,3,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "798724e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7407527579493836\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "062b92de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7336145360155742\n"
     ]
    }
   ],
   "source": [
    "# Perceptron\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c48564d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.818191650443435\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9a83b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = list(map(str, dectreeclf.classes_))\n",
    "\n",
    "plt.figure(figsize=(32, 16))\n",
    "\n",
    "tree.plot_tree(\n",
    "    decision_tree=dectreeclf, \n",
    "    max_depth=3, \n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    filled=True,\n",
    "    fontsize=12,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd80fd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.799480856586632\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "640aca89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8119186675319057\n"
     ]
    }
   ],
   "source": [
    "# MLP without PCA\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "feb5c8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'education'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9ad7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61dd3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d024459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "adac4e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.754596582305862\n"
     ]
    }
   ],
   "source": [
    "# Linear SVC\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9503da61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8011031797534068\n"
     ]
    }
   ],
   "source": [
    " # Non-linear SVC\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c6819",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af615763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "train_df['salary'] = le.fit_transform(train_df['salary'])\n",
    "train_df['workclass'] = le.fit_transform(train_df['workclass'])\n",
    "train_df['education'] = le.fit_transform(train_df['education'])\n",
    "train_df['marital-status'] = le.fit_transform(train_df['marital-status'])\n",
    "train_df['occupation'] = le.fit_transform(train_df['occupation'])\n",
    "train_df['relationship'] = le.fit_transform(train_df['relationship'])\n",
    "train_df['race'] = le.fit_transform(train_df['race'])\n",
    "train_df['sex'] = le.fit_transform(train_df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aec17f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "test_df['salary']=le.fit_transform(test_df['salary'])\n",
    "test_df['workclass']=le.fit_transform(test_df['workclass'])\n",
    "test_df['education']=le.fit_transform(test_df['education'])\n",
    "test_df['marital-status']=le.fit_transform(test_df['marital-status'])\n",
    "test_df['occupation']=le.fit_transform(test_df['occupation'])\n",
    "test_df['relationship']=le.fit_transform(test_df['relationship'])\n",
    "test_df['race']=le.fit_transform(test_df['race'])\n",
    "test_df['sex']=le.fit_transform(test_df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa41a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9246 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      workclass  education  marital-status  occupation  relationship  race  \\\n",
       "0             3          0               1           5             1     1   \n",
       "1             3         10               3           8             2     1   \n",
       "2             0          2               1           1             1     0   \n",
       "3             0          0               2           4             3     1   \n",
       "4             0          2               2           9             0     1   \n",
       "...         ...        ...             ...         ...           ...   ...   \n",
       "9995          0          1               1           7             1     1   \n",
       "9996          0         10               2           2             3     1   \n",
       "9997          0          0               0           8             0     1   \n",
       "9998          0          2               1           7             1     0   \n",
       "9999          0          2               1           1             1     1   \n",
       "\n",
       "      sex  salary  \n",
       "0       1       0  \n",
       "1       1       0  \n",
       "2       1       0  \n",
       "3       1       1  \n",
       "4       1       0  \n",
       "...   ...     ...  \n",
       "9995    1       1  \n",
       "9996    0       0  \n",
       "9997    0       0  \n",
       "9998    1       1  \n",
       "9999    1       0  \n",
       "\n",
       "[9246 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2cd4c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(['salary'], axis=1)\n",
    "x_test = test_df.drop(['salary'], axis=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_test = scaler.fit_transform(x_test)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "y_train = pd.DataFrame(train_df['salary'])\n",
    "y_test = pd.DataFrame(test_df['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "092cc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dectreereg = DecisionTreeRegressor()\n",
    "knnreg = KNeighborsRegressor(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "da30172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For decision tree regressor\n",
    "dectreereg.fit(x_train, y_train)\n",
    "y_pred_22 = dectreereg.predict(x_test)\n",
    "\n",
    "# KNN for regression\n",
    "knnreg.fit(x_train, y_train)\n",
    "y_pred_33 = knnreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c79a216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Regressor\n",
    "# Calculating root mean squared error of our predictions\n",
    "mse = metrics.mean_squared_error(y_test, y_pred_22)\n",
    "rmse = sqrt(mse)\n",
    "print(round(rmse,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91c29b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(32, 16))\n",
    "\n",
    "tree.plot_tree(\n",
    "    decision_tree=dectreereg, \n",
    "    max_depth=3, \n",
    "    feature_names=feature_names, \n",
    "    filled=True, \n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f01530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# KNN Regressor\n",
    "#root mean squared error of our predictions\n",
    "mse = metrics.mean_squared_error(y_test, y_pred_33)\n",
    "rmse = sqrt(mse)\n",
    "print(round(rmse,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4390df",
   "metadata": {},
   "source": [
    "## MLP classification model (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "57ef1acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix: \n",
      "[[ 5.48867863e-02  1.15704419e-03 -1.37351992e-03  4.02045196e-03\n",
      "  -3.97990074e-03  1.89275381e-04  4.44934412e-03]\n",
      " [ 1.15704419e-03  5.83400068e-02 -1.13023779e-04  3.60295433e-03\n",
      "  -5.73076216e-04  8.24098108e-04  1.07470345e-03]\n",
      " [-1.37351992e-03 -1.13023779e-04  2.43759598e-02 -2.67390389e-03\n",
      "   4.46790322e-03 -2.34780713e-04 -2.62679317e-02]\n",
      " [ 4.02045196e-03  3.60295433e-03 -2.67390389e-03  7.04903414e-02\n",
      "  -2.77704287e-03  4.46413531e-04  1.33830734e-02]\n",
      " [-3.97990074e-03 -5.73076216e-04  4.46790322e-03 -2.77704287e-03\n",
      "   5.39209017e-02  8.84672092e-04 -2.68175361e-02]\n",
      " [ 1.89275381e-04  8.24098108e-04 -2.34780713e-04  4.46413531e-04\n",
      "   8.84672092e-04  1.90703670e-02  3.35089890e-03]\n",
      " [ 4.44934412e-03  1.07470345e-03 -2.62679317e-02  1.33830734e-02\n",
      "  -2.68175361e-02  3.35089890e-03  2.19495792e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Computing Eigenvectors and Eigenvalues: Covariance matrix\n",
    "print('Covariance matrix: \\n%s' % np.cov(x_train.T))\n",
    "\n",
    "cov_matrix = np.cov(x_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ea8d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors \n",
      "[[ 0.03164928  0.00781896  0.01770828 -0.23219844  0.45997868 -0.85453599\n",
      "  -0.05380831]\n",
      " [ 0.00891587  0.02007729 -0.00511561 -0.27346048 -0.00193094  0.13363786\n",
      "  -0.95228557]\n",
      " [-0.13008962  0.08645952  0.98756472  0.00733745  0.00140761  0.01549382\n",
      "  -0.00463584]\n",
      " [ 0.08842239  0.00476767  0.01629428 -0.92616403 -0.05117218  0.20911855\n",
      "   0.29625038]\n",
      " [-0.15496057  0.04083576 -0.03244531  0.05231108  0.87600035  0.44840193\n",
      "   0.04571234]\n",
      " [ 0.01532416 -0.99463276  0.08869179 -0.00558771  0.04365697  0.01939714\n",
      "  -0.01706495]\n",
      " [ 0.97464321  0.03280101  0.12325477  0.10345016  0.12850201  0.08061016\n",
      "  -0.00950058]]\n",
      "\n",
      "Eigenvalues \n",
      "[0.22868681 0.01892369 0.02086151 0.07124799 0.04811194 0.05541124\n",
      " 0.05733698]\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print('Eigenvectors \\n%s' % eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' % eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b33a8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues in descending order:\n",
      "0.2286868062313153\n",
      "0.07124799489176642\n",
      "0.057336975993204045\n",
      "0.05541124231108514\n",
      "0.04811194097989665\n",
      "0.020861509355326765\n",
      "0.018923685653034768\n"
     ]
    }
   ],
   "source": [
    "## Selecting Principal Component Analysis\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "358e0ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# how many principal components are we going to choose for our new feature subspace?\n",
    "# ANSWER: explained variance tells us how much information (variance) can be attributed to each of the principal components\n",
    "\n",
    "cols = train_df.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('salary')))\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(cols[1:])]\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.barh(x_pos, var_exp, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.xlabel('Explained variance ratio')\n",
    "    plt.ylabel('Principal components')\n",
    "    plt.yticks(x_pos, cols[1:])\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33bcfd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative explained variance')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA().fit(x_train)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,20,1)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "581076b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50829542\n",
      "Iteration 2, loss = 0.46527595\n",
      "Iteration 3, loss = 0.44724934\n",
      "Iteration 4, loss = 0.43778680\n",
      "Iteration 5, loss = 0.43436489\n",
      "Iteration 6, loss = 0.42962400\n",
      "Iteration 7, loss = 0.42748204\n",
      "Iteration 8, loss = 0.42664769\n",
      "Iteration 9, loss = 0.42394373\n",
      "Iteration 10, loss = 0.42281796\n",
      "Iteration 11, loss = 0.42019068\n",
      "Iteration 12, loss = 0.41942905\n",
      "Iteration 13, loss = 0.41974518\n",
      "Iteration 14, loss = 0.41927473\n",
      "Iteration 15, loss = 0.41675928\n",
      "Iteration 16, loss = 0.41557161\n",
      "Iteration 17, loss = 0.41563676\n",
      "Iteration 18, loss = 0.41444705\n",
      "Iteration 19, loss = 0.41467558\n",
      "Iteration 20, loss = 0.41435039\n",
      "Iteration 21, loss = 0.41405278\n",
      "Iteration 22, loss = 0.41431703\n",
      "Iteration 23, loss = 0.41375598\n",
      "Iteration 24, loss = 0.41324976\n",
      "Iteration 25, loss = 0.41325086\n",
      "Iteration 26, loss = 0.41274529\n",
      "Iteration 27, loss = 0.41419438\n",
      "Iteration 28, loss = 0.41346768\n",
      "Iteration 29, loss = 0.41126652\n",
      "Iteration 30, loss = 0.41202021\n",
      "Iteration 31, loss = 0.41129515\n",
      "Iteration 32, loss = 0.41159080\n",
      "Iteration 33, loss = 0.41122172\n",
      "Iteration 34, loss = 0.41077893\n",
      "Iteration 35, loss = 0.41122798\n",
      "Iteration 36, loss = 0.40999017\n",
      "Iteration 37, loss = 0.41090850\n",
      "Iteration 38, loss = 0.41039130\n",
      "Iteration 39, loss = 0.41058933\n",
      "Iteration 40, loss = 0.41035040\n",
      "Iteration 41, loss = 0.41057635\n",
      "Iteration 42, loss = 0.41189661\n",
      "Iteration 43, loss = 0.40978253\n",
      "Iteration 44, loss = 0.40958154\n",
      "Iteration 45, loss = 0.40952148\n",
      "Iteration 46, loss = 0.41011227\n",
      "Iteration 47, loss = 0.41019686\n",
      "Iteration 48, loss = 0.40926065\n",
      "Iteration 49, loss = 0.40944756\n",
      "Iteration 50, loss = 0.40918015\n",
      "Iteration 51, loss = 0.40883373\n",
      "Iteration 52, loss = 0.40805576\n",
      "Iteration 53, loss = 0.41125366\n",
      "Iteration 54, loss = 0.40905932\n",
      "Iteration 55, loss = 0.40901861\n",
      "Iteration 56, loss = 0.40856181\n",
      "Iteration 57, loss = 0.40966253\n",
      "Iteration 58, loss = 0.40920848\n",
      "Iteration 59, loss = 0.40930038\n",
      "Iteration 60, loss = 0.40863568\n",
      "Iteration 61, loss = 0.40741057\n",
      "Iteration 62, loss = 0.40792910\n",
      "Iteration 63, loss = 0.40867722\n",
      "Iteration 64, loss = 0.40867126\n",
      "Iteration 65, loss = 0.40718512\n",
      "Iteration 66, loss = 0.40850798\n",
      "Iteration 67, loss = 0.40817471\n",
      "Iteration 68, loss = 0.40755855\n",
      "Iteration 69, loss = 0.40830843\n",
      "Iteration 70, loss = 0.40817951\n",
      "Iteration 71, loss = 0.40944393\n",
      "Iteration 72, loss = 0.40867412\n",
      "Iteration 73, loss = 0.40816762\n",
      "Iteration 74, loss = 0.40892878\n",
      "Iteration 75, loss = 0.40793259\n",
      "Iteration 76, loss = 0.40823860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(6, 5), learning_rate_init=0.01,\n",
       "              random_state=5, verbose=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=6)\n",
    "\n",
    "# Create model object\n",
    "pcamlp = MLPClassifier(hidden_layer_sizes=(6,5),\n",
    "                    random_state=5,\n",
    "                    verbose=True,\n",
    "                    learning_rate_init=0.01)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)\n",
    "\n",
    "pcamlp.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74e1c9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22156259, 0.15193462, 0.14549537, 0.13823179, 0.13287955,\n",
       "       0.12305259])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb88ecc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.809322950465066"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction on test dataset\n",
    "ypred=pcamlp.predict(x_test)\n",
    "\n",
    "# Calcuate accuracy\n",
    "accuracy_score(y_test,ypred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1dc0935a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class A       0.84      0.92      0.88      6977\n",
      "     class B       0.66      0.47      0.55      2269\n",
      "\n",
      "    accuracy                           0.81      9246\n",
      "   macro avg       0.75      0.69      0.71      9246\n",
      "weighted avg       0.80      0.81      0.80      9246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, ypred, target_names=['class A', 'class B']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22691e4",
   "metadata": {},
   "source": [
    "## Find Best subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "12f99cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop('salary', axis=1)\n",
    "y = train_df['salary']\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)  # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "339c726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best feature subset\n",
    "\n",
    "# params\n",
    "# X: feature values\n",
    "# y: labels\n",
    "# model: fitted classifier object\n",
    "# subset_len: how many features the desired feature subset will have\n",
    "# cv: number of folds for cross validation scoring\n",
    "def best_subset(X, y, model, subset_len, cv=3, verbose=False):\n",
    "    \n",
    "    # for elapsed time\n",
    "    start = time.time()\n",
    "    \n",
    "    # getting number of features\n",
    "    l = X.shape[1]\n",
    "    \n",
    "    # getting all the different possible combinations of features for the given\n",
    "    # subset length\n",
    "    subsets = list(combinations(range(l), subset_len))\n",
    "    print(f'{len(subsets)} total combinations for subset length {subset_len}.')\n",
    "    \n",
    "    # setting baseline values for best score and subset to update later\n",
    "    best_score = -np.inf\n",
    "    best_subset = None\n",
    "    \n",
    "    # iterating through possible combinations\n",
    "    for subset in subsets:\n",
    "        \n",
    "        # subset is currently an index, need to get feature names to select df\n",
    "        subset_nm = [X.columns[i] for i in subset]\n",
    "        \n",
    "        # getting CV score from model with subsetted features\n",
    "        score = cross_val_score(model, X[subset_nm], y, cv=cv).mean()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Current subset: {subset_nm}\\nScore: {score}')\n",
    "        \n",
    "        # updating best score and best subset if a new best score happens\n",
    "        if score > best_score:\n",
    "            best_score, best_subset = score, subset\n",
    "            \n",
    "    # for elapsed time\n",
    "    end = time.time()\n",
    "    \n",
    "    # printing and returning metrics\n",
    "    print(f'Elapsed time: {int((end - start)/60)} min. and {(end - start)%60} sec.\\n')\n",
    "    return [[X.columns[i] for i in best_subset], best_score]\n",
    "\n",
    "# Function to iterate through multiple subset lengths and return a df of\n",
    "# associated features and CV scores\n",
    "#\n",
    "# params\n",
    "# X: feature values\n",
    "# y: labels\n",
    "# model: fitted classifier object\n",
    "# min_subset_len: lowest number of features for a subset\n",
    "# max_subset_len: greatest number of features for a subset\n",
    "# cv: number of folds for cross validation scoring\n",
    "# verbose: prints each combination\n",
    "def best_subsets(X, y, model, min_subset_len, max_subset_len, cv=3, verbose=False):\n",
    "    \n",
    "    # lists to later create df with\n",
    "    subset_length = list()\n",
    "    subset_features = list()\n",
    "    subset_cv_score = list()\n",
    "    \n",
    "    # iterating through desired subset lengths\n",
    "    for i in range(min_subset_len, max_subset_len+1):\n",
    "        best = best_subset(X, y, mlp, i, cv=cv, verbose=verbose)\n",
    "        \n",
    "        # adding to lists to create df with later\n",
    "        subset_length.append(i)\n",
    "        subset_features.append(best[0])\n",
    "        subset_cv_score.append(best[1])\n",
    "        \n",
    "    # creating and returning dataframe with length, feature list, and score for\n",
    "    # each desired subset length\n",
    "    df = pd.DataFrame(data={'length': subset_length, 'features': subset_features, 'cv_score': subset_cv_score})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "083e9d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 total combinations for subset length 6.\n",
      "Iteration 1, loss = 0.50981445\n",
      "Iteration 2, loss = 0.46996391\n",
      "Iteration 3, loss = 0.46322208\n",
      "Iteration 4, loss = 0.45578035\n",
      "Iteration 5, loss = 0.44740781\n",
      "Iteration 6, loss = 0.43808272\n",
      "Iteration 7, loss = 0.43048889\n",
      "Iteration 8, loss = 0.42771964\n",
      "Iteration 9, loss = 0.42235701\n",
      "Iteration 10, loss = 0.41974894\n",
      "Iteration 11, loss = 0.41768971\n",
      "Iteration 12, loss = 0.41615884\n",
      "Iteration 13, loss = 0.41457331\n",
      "Iteration 14, loss = 0.41321344\n",
      "Iteration 15, loss = 0.41425357\n",
      "Iteration 16, loss = 0.41249397\n",
      "Iteration 17, loss = 0.41197627\n",
      "Iteration 18, loss = 0.41097464\n",
      "Iteration 19, loss = 0.41137739\n",
      "Iteration 20, loss = 0.41039229\n",
      "Iteration 21, loss = 0.40923464\n",
      "Iteration 22, loss = 0.40804509\n",
      "Iteration 23, loss = 0.40808211\n",
      "Iteration 24, loss = 0.40849730\n",
      "Iteration 25, loss = 0.40760666\n",
      "Iteration 26, loss = 0.40886357\n",
      "Iteration 27, loss = 0.40871064\n",
      "Iteration 28, loss = 0.40816876\n",
      "Iteration 29, loss = 0.40808112\n",
      "Iteration 30, loss = 0.40709947\n",
      "Iteration 31, loss = 0.40728756\n",
      "Iteration 32, loss = 0.40685009\n",
      "Iteration 33, loss = 0.40695766\n",
      "Iteration 34, loss = 0.40765764\n",
      "Iteration 35, loss = 0.40773046\n",
      "Iteration 36, loss = 0.40522999\n",
      "Iteration 37, loss = 0.40634574\n",
      "Iteration 38, loss = 0.40630475\n",
      "Iteration 39, loss = 0.40517537\n",
      "Iteration 40, loss = 0.40549429\n",
      "Iteration 41, loss = 0.40539003\n",
      "Iteration 42, loss = 0.40624021\n",
      "Iteration 43, loss = 0.40555931\n",
      "Iteration 44, loss = 0.40664611\n",
      "Iteration 45, loss = 0.40544268\n",
      "Iteration 46, loss = 0.40576479\n",
      "Iteration 47, loss = 0.40643375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51133737\n",
      "Iteration 2, loss = 0.47764328\n",
      "Iteration 3, loss = 0.47093504\n",
      "Iteration 4, loss = 0.46293714\n",
      "Iteration 5, loss = 0.45737200\n",
      "Iteration 6, loss = 0.44756651\n",
      "Iteration 7, loss = 0.44141106\n",
      "Iteration 8, loss = 0.43779031\n",
      "Iteration 9, loss = 0.43117609\n",
      "Iteration 10, loss = 0.42843219\n",
      "Iteration 11, loss = 0.42524967\n",
      "Iteration 12, loss = 0.42283875\n",
      "Iteration 13, loss = 0.42320733\n",
      "Iteration 14, loss = 0.42159214\n",
      "Iteration 15, loss = 0.42129168\n",
      "Iteration 16, loss = 0.42005058\n",
      "Iteration 17, loss = 0.41913271\n",
      "Iteration 18, loss = 0.41791657\n",
      "Iteration 19, loss = 0.41756255\n",
      "Iteration 20, loss = 0.41680224\n",
      "Iteration 21, loss = 0.41752405\n",
      "Iteration 22, loss = 0.41679026\n",
      "Iteration 23, loss = 0.41474610\n",
      "Iteration 24, loss = 0.41954059\n",
      "Iteration 25, loss = 0.41469516\n",
      "Iteration 26, loss = 0.41340946\n",
      "Iteration 27, loss = 0.41367725\n",
      "Iteration 28, loss = 0.41244974\n",
      "Iteration 29, loss = 0.41151708\n",
      "Iteration 30, loss = 0.41189464\n",
      "Iteration 31, loss = 0.41165968\n",
      "Iteration 32, loss = 0.41370996\n",
      "Iteration 33, loss = 0.41258440\n",
      "Iteration 34, loss = 0.41278643\n",
      "Iteration 35, loss = 0.41071656\n",
      "Iteration 36, loss = 0.40987358\n",
      "Iteration 37, loss = 0.40949608\n",
      "Iteration 38, loss = 0.41146167\n",
      "Iteration 39, loss = 0.41032095\n",
      "Iteration 40, loss = 0.41202273\n",
      "Iteration 41, loss = 0.41022969\n",
      "Iteration 42, loss = 0.41017468\n",
      "Iteration 43, loss = 0.41021407\n",
      "Iteration 44, loss = 0.41188107\n",
      "Iteration 45, loss = 0.40957706\n",
      "Iteration 46, loss = 0.40854230\n",
      "Iteration 47, loss = 0.40818215\n",
      "Iteration 48, loss = 0.40935405\n",
      "Iteration 49, loss = 0.40817648\n",
      "Iteration 50, loss = 0.40820018\n",
      "Iteration 51, loss = 0.40805983\n",
      "Iteration 52, loss = 0.40837889\n",
      "Iteration 53, loss = 0.40686704\n",
      "Iteration 54, loss = 0.40756854\n",
      "Iteration 55, loss = 0.40861381\n",
      "Iteration 56, loss = 0.40772681\n",
      "Iteration 57, loss = 0.40658336\n",
      "Iteration 58, loss = 0.40767091\n",
      "Iteration 59, loss = 0.40736956\n",
      "Iteration 60, loss = 0.40699018\n",
      "Iteration 61, loss = 0.40693674\n",
      "Iteration 62, loss = 0.40837846\n",
      "Iteration 63, loss = 0.40665336\n",
      "Iteration 64, loss = 0.40722750\n",
      "Iteration 65, loss = 0.40638989\n",
      "Iteration 66, loss = 0.40966583\n",
      "Iteration 67, loss = 0.40723127\n",
      "Iteration 68, loss = 0.40857334\n",
      "Iteration 69, loss = 0.40753601\n",
      "Iteration 70, loss = 0.40698441\n",
      "Iteration 71, loss = 0.40733324\n",
      "Iteration 72, loss = 0.40770140\n",
      "Iteration 73, loss = 0.40735700\n",
      "Iteration 74, loss = 0.40621229\n",
      "Iteration 75, loss = 0.40652891\n",
      "Iteration 76, loss = 0.40553967\n",
      "Iteration 77, loss = 0.40633134\n",
      "Iteration 78, loss = 0.40586484\n",
      "Iteration 79, loss = 0.40530330\n",
      "Iteration 80, loss = 0.40538751\n",
      "Iteration 81, loss = 0.40644941\n",
      "Iteration 82, loss = 0.40695615\n",
      "Iteration 83, loss = 0.40502612\n",
      "Iteration 84, loss = 0.40736247\n",
      "Iteration 85, loss = 0.40574368\n",
      "Iteration 86, loss = 0.40561106\n",
      "Iteration 87, loss = 0.40511963\n",
      "Iteration 88, loss = 0.40599505\n",
      "Iteration 89, loss = 0.40612354\n",
      "Iteration 90, loss = 0.40491197\n",
      "Iteration 91, loss = 0.40692392\n",
      "Iteration 92, loss = 0.40668767\n",
      "Iteration 93, loss = 0.40551743\n",
      "Iteration 94, loss = 0.40689373\n",
      "Iteration 95, loss = 0.40719969\n",
      "Iteration 96, loss = 0.40608510\n",
      "Iteration 97, loss = 0.40508715\n",
      "Iteration 98, loss = 0.40599036\n",
      "Iteration 99, loss = 0.40507714\n",
      "Iteration 100, loss = 0.40500648\n",
      "Iteration 101, loss = 0.40464513\n",
      "Iteration 102, loss = 0.40573647\n",
      "Iteration 103, loss = 0.40552853\n",
      "Iteration 104, loss = 0.40631272\n",
      "Iteration 105, loss = 0.40444042\n",
      "Iteration 106, loss = 0.40492211\n",
      "Iteration 107, loss = 0.40516926\n",
      "Iteration 108, loss = 0.40794325\n",
      "Iteration 109, loss = 0.40487940\n",
      "Iteration 110, loss = 0.40414198\n",
      "Iteration 111, loss = 0.40419500\n",
      "Iteration 112, loss = 0.40434735\n",
      "Iteration 113, loss = 0.40528466\n",
      "Iteration 114, loss = 0.40468182\n",
      "Iteration 115, loss = 0.40390749\n",
      "Iteration 116, loss = 0.40299920\n",
      "Iteration 117, loss = 0.40522152\n",
      "Iteration 118, loss = 0.40213024\n",
      "Iteration 119, loss = 0.40291868\n",
      "Iteration 120, loss = 0.40277372\n",
      "Iteration 121, loss = 0.40414541\n",
      "Iteration 122, loss = 0.40264388\n",
      "Iteration 123, loss = 0.40296518\n",
      "Iteration 124, loss = 0.40300220\n",
      "Iteration 125, loss = 0.40185763\n",
      "Iteration 126, loss = 0.40284556\n",
      "Iteration 127, loss = 0.40311462\n",
      "Iteration 128, loss = 0.40295717\n",
      "Iteration 129, loss = 0.40403521\n",
      "Iteration 130, loss = 0.40288262\n",
      "Iteration 131, loss = 0.40291632\n",
      "Iteration 132, loss = 0.40138108\n",
      "Iteration 133, loss = 0.40184459\n",
      "Iteration 134, loss = 0.40318970\n",
      "Iteration 135, loss = 0.40248648\n",
      "Iteration 136, loss = 0.40309598\n",
      "Iteration 137, loss = 0.40397656\n",
      "Iteration 138, loss = 0.40221064\n",
      "Iteration 139, loss = 0.40394507\n",
      "Iteration 140, loss = 0.40185214\n",
      "Iteration 141, loss = 0.40253515\n",
      "Iteration 142, loss = 0.40209243\n",
      "Iteration 143, loss = 0.40158147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51273472\n",
      "Iteration 2, loss = 0.47908145\n",
      "Iteration 3, loss = 0.47000101\n",
      "Iteration 4, loss = 0.46154746\n",
      "Iteration 5, loss = 0.45122452\n",
      "Iteration 6, loss = 0.44574802\n",
      "Iteration 7, loss = 0.43740946\n",
      "Iteration 8, loss = 0.43437068\n",
      "Iteration 9, loss = 0.43197000\n",
      "Iteration 10, loss = 0.42866200\n",
      "Iteration 11, loss = 0.42740222\n",
      "Iteration 12, loss = 0.42534785\n",
      "Iteration 13, loss = 0.42096741\n",
      "Iteration 14, loss = 0.41893646\n",
      "Iteration 15, loss = 0.41835519\n",
      "Iteration 16, loss = 0.41605144\n",
      "Iteration 17, loss = 0.41409209\n",
      "Iteration 18, loss = 0.41461300\n",
      "Iteration 19, loss = 0.41515457\n",
      "Iteration 20, loss = 0.41408528\n",
      "Iteration 21, loss = 0.41472759\n",
      "Iteration 22, loss = 0.41333750\n",
      "Iteration 23, loss = 0.41428934\n",
      "Iteration 24, loss = 0.41331061\n",
      "Iteration 25, loss = 0.41266729\n",
      "Iteration 26, loss = 0.41431343\n",
      "Iteration 27, loss = 0.41296776\n",
      "Iteration 28, loss = 0.41423440\n",
      "Iteration 29, loss = 0.41393473\n",
      "Iteration 30, loss = 0.41236399\n",
      "Iteration 31, loss = 0.41235938\n",
      "Iteration 32, loss = 0.41350858\n",
      "Iteration 33, loss = 0.41174546\n",
      "Iteration 34, loss = 0.41156487\n",
      "Iteration 35, loss = 0.41154223\n",
      "Iteration 36, loss = 0.41228738\n",
      "Iteration 37, loss = 0.41121119\n",
      "Iteration 38, loss = 0.41087624\n",
      "Iteration 39, loss = 0.41410070\n",
      "Iteration 40, loss = 0.41121867\n",
      "Iteration 41, loss = 0.41055480\n",
      "Iteration 42, loss = 0.41028482\n",
      "Iteration 43, loss = 0.41087616\n",
      "Iteration 44, loss = 0.41412338\n",
      "Iteration 45, loss = 0.40998640\n",
      "Iteration 46, loss = 0.41011925\n",
      "Iteration 47, loss = 0.41070112\n",
      "Iteration 48, loss = 0.41182085\n",
      "Iteration 49, loss = 0.41104385\n",
      "Iteration 50, loss = 0.41108833\n",
      "Iteration 51, loss = 0.41065936\n",
      "Iteration 52, loss = 0.41086476\n",
      "Iteration 53, loss = 0.40922104\n",
      "Iteration 54, loss = 0.41025284\n",
      "Iteration 55, loss = 0.41058488\n",
      "Iteration 56, loss = 0.41124072\n",
      "Iteration 57, loss = 0.41154194\n",
      "Iteration 58, loss = 0.41067488\n",
      "Iteration 59, loss = 0.41166053\n",
      "Iteration 60, loss = 0.40986871\n",
      "Iteration 61, loss = 0.41019279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 62, loss = 0.41138912\n",
      "Iteration 63, loss = 0.41208220\n",
      "Iteration 64, loss = 0.41015964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
      "Score: 0.8019513008672448\n",
      "Iteration 1, loss = 0.50952437\n",
      "Iteration 2, loss = 0.47134533\n",
      "Iteration 3, loss = 0.45724266\n",
      "Iteration 4, loss = 0.44823973\n",
      "Iteration 5, loss = 0.44207392\n",
      "Iteration 6, loss = 0.43712499\n",
      "Iteration 7, loss = 0.42870823\n",
      "Iteration 8, loss = 0.42184197\n",
      "Iteration 9, loss = 0.41682855\n",
      "Iteration 10, loss = 0.41452821\n",
      "Iteration 11, loss = 0.41317767\n",
      "Iteration 12, loss = 0.40986103\n",
      "Iteration 13, loss = 0.40881996\n",
      "Iteration 14, loss = 0.40762752\n",
      "Iteration 15, loss = 0.40747972\n",
      "Iteration 16, loss = 0.40792011\n",
      "Iteration 17, loss = 0.40566526\n",
      "Iteration 18, loss = 0.40542361\n",
      "Iteration 19, loss = 0.40432708\n",
      "Iteration 20, loss = 0.40471189\n",
      "Iteration 21, loss = 0.40441162\n",
      "Iteration 22, loss = 0.40362603\n",
      "Iteration 23, loss = 0.40356299\n",
      "Iteration 24, loss = 0.40359806\n",
      "Iteration 25, loss = 0.40357789\n",
      "Iteration 26, loss = 0.40172246\n",
      "Iteration 27, loss = 0.40376275\n",
      "Iteration 28, loss = 0.40161691\n",
      "Iteration 29, loss = 0.40289706\n",
      "Iteration 30, loss = 0.40249761\n",
      "Iteration 31, loss = 0.40175617\n",
      "Iteration 32, loss = 0.40184564\n",
      "Iteration 33, loss = 0.40157558\n",
      "Iteration 34, loss = 0.40092266\n",
      "Iteration 35, loss = 0.40246537\n",
      "Iteration 36, loss = 0.40115129\n",
      "Iteration 37, loss = 0.40103543\n",
      "Iteration 38, loss = 0.39987117\n",
      "Iteration 39, loss = 0.40042493\n",
      "Iteration 40, loss = 0.40048928\n",
      "Iteration 41, loss = 0.39996890\n",
      "Iteration 42, loss = 0.40113317\n",
      "Iteration 43, loss = 0.40143968\n",
      "Iteration 44, loss = 0.40254171\n",
      "Iteration 45, loss = 0.40011441\n",
      "Iteration 46, loss = 0.40000894\n",
      "Iteration 47, loss = 0.40049200\n",
      "Iteration 48, loss = 0.39918687\n",
      "Iteration 49, loss = 0.40080403\n",
      "Iteration 50, loss = 0.40003935\n",
      "Iteration 51, loss = 0.39880270\n",
      "Iteration 52, loss = 0.40021485\n",
      "Iteration 53, loss = 0.39994985\n",
      "Iteration 54, loss = 0.40028291\n",
      "Iteration 55, loss = 0.39944414\n",
      "Iteration 56, loss = 0.39939181\n",
      "Iteration 57, loss = 0.39903943\n",
      "Iteration 58, loss = 0.39949939\n",
      "Iteration 59, loss = 0.40001369\n",
      "Iteration 60, loss = 0.39949471\n",
      "Iteration 61, loss = 0.39758017\n",
      "Iteration 62, loss = 0.39927257\n",
      "Iteration 63, loss = 0.39882874\n",
      "Iteration 64, loss = 0.40009453\n",
      "Iteration 65, loss = 0.39994551\n",
      "Iteration 66, loss = 0.39935711\n",
      "Iteration 67, loss = 0.39766115\n",
      "Iteration 68, loss = 0.40004252\n",
      "Iteration 69, loss = 0.40292782\n",
      "Iteration 70, loss = 0.39819526\n",
      "Iteration 71, loss = 0.39847797\n",
      "Iteration 72, loss = 0.40284988\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51074424\n",
      "Iteration 2, loss = 0.47701487\n",
      "Iteration 3, loss = 0.47286128\n",
      "Iteration 4, loss = 0.46299276\n",
      "Iteration 5, loss = 0.45517464\n",
      "Iteration 6, loss = 0.44415643\n",
      "Iteration 7, loss = 0.43886253\n",
      "Iteration 8, loss = 0.43452372\n",
      "Iteration 9, loss = 0.43165194\n",
      "Iteration 10, loss = 0.42980039\n",
      "Iteration 11, loss = 0.42739813\n",
      "Iteration 12, loss = 0.42413355\n",
      "Iteration 13, loss = 0.42138915\n",
      "Iteration 14, loss = 0.42008591\n",
      "Iteration 15, loss = 0.41998056\n",
      "Iteration 16, loss = 0.41703993\n",
      "Iteration 17, loss = 0.41633681\n",
      "Iteration 18, loss = 0.41361508\n",
      "Iteration 19, loss = 0.41492152\n",
      "Iteration 20, loss = 0.41302067\n",
      "Iteration 21, loss = 0.41303363\n",
      "Iteration 22, loss = 0.41165634\n",
      "Iteration 23, loss = 0.41095712\n",
      "Iteration 24, loss = 0.41188750\n",
      "Iteration 25, loss = 0.41030508\n",
      "Iteration 26, loss = 0.41130156\n",
      "Iteration 27, loss = 0.41122463\n",
      "Iteration 28, loss = 0.40938522\n",
      "Iteration 29, loss = 0.40940834\n",
      "Iteration 30, loss = 0.40713473\n",
      "Iteration 31, loss = 0.40833032\n",
      "Iteration 32, loss = 0.41006735\n",
      "Iteration 33, loss = 0.40810080\n",
      "Iteration 34, loss = 0.40771708\n",
      "Iteration 35, loss = 0.40727939\n",
      "Iteration 36, loss = 0.40787480\n",
      "Iteration 37, loss = 0.40618181\n",
      "Iteration 38, loss = 0.40671687\n",
      "Iteration 39, loss = 0.40584973\n",
      "Iteration 40, loss = 0.40697377\n",
      "Iteration 41, loss = 0.40653678\n",
      "Iteration 42, loss = 0.40697805\n",
      "Iteration 43, loss = 0.40588578\n",
      "Iteration 44, loss = 0.40705462\n",
      "Iteration 45, loss = 0.40634623\n",
      "Iteration 46, loss = 0.40538612\n",
      "Iteration 47, loss = 0.40579569\n",
      "Iteration 48, loss = 0.40633348\n",
      "Iteration 49, loss = 0.40659947\n",
      "Iteration 50, loss = 0.40631878\n",
      "Iteration 51, loss = 0.40630388\n",
      "Iteration 52, loss = 0.40617579\n",
      "Iteration 53, loss = 0.40479343\n",
      "Iteration 54, loss = 0.40580694\n",
      "Iteration 55, loss = 0.40714615\n",
      "Iteration 56, loss = 0.40535345\n",
      "Iteration 57, loss = 0.40487893\n",
      "Iteration 58, loss = 0.40516830\n",
      "Iteration 59, loss = 0.40515466\n",
      "Iteration 60, loss = 0.40490139\n",
      "Iteration 61, loss = 0.40550851\n",
      "Iteration 62, loss = 0.40652000\n",
      "Iteration 63, loss = 0.40390906\n",
      "Iteration 64, loss = 0.40504929\n",
      "Iteration 65, loss = 0.40433252\n",
      "Iteration 66, loss = 0.40601048\n",
      "Iteration 67, loss = 0.40544164\n",
      "Iteration 68, loss = 0.40356573\n",
      "Iteration 69, loss = 0.40350673\n",
      "Iteration 70, loss = 0.40314735\n",
      "Iteration 71, loss = 0.40452639\n",
      "Iteration 72, loss = 0.40429590\n",
      "Iteration 73, loss = 0.40237481\n",
      "Iteration 74, loss = 0.40270814\n",
      "Iteration 75, loss = 0.40223083\n",
      "Iteration 76, loss = 0.40112441\n",
      "Iteration 77, loss = 0.40274097\n",
      "Iteration 78, loss = 0.40162459\n",
      "Iteration 79, loss = 0.40160091\n",
      "Iteration 80, loss = 0.40316346\n",
      "Iteration 81, loss = 0.40228577\n",
      "Iteration 82, loss = 0.40363588\n",
      "Iteration 83, loss = 0.40278759\n",
      "Iteration 84, loss = 0.40400066\n",
      "Iteration 85, loss = 0.40155609\n",
      "Iteration 86, loss = 0.40186954\n",
      "Iteration 87, loss = 0.40142839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51341801\n",
      "Iteration 2, loss = 0.47752228\n",
      "Iteration 3, loss = 0.46588530\n",
      "Iteration 4, loss = 0.45667910\n",
      "Iteration 5, loss = 0.45124791\n",
      "Iteration 6, loss = 0.44716760\n",
      "Iteration 7, loss = 0.44136674\n",
      "Iteration 8, loss = 0.43806713\n",
      "Iteration 9, loss = 0.43165974\n",
      "Iteration 10, loss = 0.42571575\n",
      "Iteration 11, loss = 0.42390653\n",
      "Iteration 12, loss = 0.42219862\n",
      "Iteration 13, loss = 0.42199606\n",
      "Iteration 14, loss = 0.42083375\n",
      "Iteration 15, loss = 0.41965407\n",
      "Iteration 16, loss = 0.41886024\n",
      "Iteration 17, loss = 0.41720861\n",
      "Iteration 18, loss = 0.41770411\n",
      "Iteration 19, loss = 0.41694539\n",
      "Iteration 20, loss = 0.41683915\n",
      "Iteration 21, loss = 0.41566464\n",
      "Iteration 22, loss = 0.41635976\n",
      "Iteration 23, loss = 0.41518788\n",
      "Iteration 24, loss = 0.41398957\n",
      "Iteration 25, loss = 0.41420796\n",
      "Iteration 26, loss = 0.41477527\n",
      "Iteration 27, loss = 0.41397238\n",
      "Iteration 28, loss = 0.41510074\n",
      "Iteration 29, loss = 0.41275505\n",
      "Iteration 30, loss = 0.41324385\n",
      "Iteration 31, loss = 0.41344128\n",
      "Iteration 32, loss = 0.41352758\n",
      "Iteration 33, loss = 0.41179561\n",
      "Iteration 34, loss = 0.41284868\n",
      "Iteration 35, loss = 0.41223800\n",
      "Iteration 36, loss = 0.41136049\n",
      "Iteration 37, loss = 0.41166598\n",
      "Iteration 38, loss = 0.41180709\n",
      "Iteration 39, loss = 0.41251492\n",
      "Iteration 40, loss = 0.41131599\n",
      "Iteration 41, loss = 0.41176926\n",
      "Iteration 42, loss = 0.41208701\n",
      "Iteration 43, loss = 0.41071088\n",
      "Iteration 44, loss = 0.41157765\n",
      "Iteration 45, loss = 0.41142798\n",
      "Iteration 46, loss = 0.41095844\n",
      "Iteration 47, loss = 0.41137052\n",
      "Iteration 48, loss = 0.41079717\n",
      "Iteration 49, loss = 0.41055223\n",
      "Iteration 50, loss = 0.41131284\n",
      "Iteration 51, loss = 0.41084439\n",
      "Iteration 52, loss = 0.41181203\n",
      "Iteration 53, loss = 0.41050918\n",
      "Iteration 54, loss = 0.40988381\n",
      "Iteration 55, loss = 0.41126017\n",
      "Iteration 56, loss = 0.41055664\n",
      "Iteration 57, loss = 0.41221105\n",
      "Iteration 58, loss = 0.41057610\n",
      "Iteration 59, loss = 0.41031728\n",
      "Iteration 60, loss = 0.40963466\n",
      "Iteration 61, loss = 0.40983344\n",
      "Iteration 62, loss = 0.41058542\n",
      "Iteration 63, loss = 0.41057928\n",
      "Iteration 64, loss = 0.41022248\n",
      "Iteration 65, loss = 0.41028508\n",
      "Iteration 66, loss = 0.41078367\n",
      "Iteration 67, loss = 0.40962289\n",
      "Iteration 68, loss = 0.41058757\n",
      "Iteration 69, loss = 0.41039402\n",
      "Iteration 70, loss = 0.41063538\n",
      "Iteration 71, loss = 0.40955703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex']\n",
      "Score: 0.8118745830553703\n",
      "Iteration 1, loss = 0.50667151\n",
      "Iteration 2, loss = 0.47283882\n",
      "Iteration 3, loss = 0.46879978\n",
      "Iteration 4, loss = 0.46508568\n",
      "Iteration 5, loss = 0.46145228\n",
      "Iteration 6, loss = 0.45539335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.45133643\n",
      "Iteration 8, loss = 0.44761488\n",
      "Iteration 9, loss = 0.43591512\n",
      "Iteration 10, loss = 0.43028076\n",
      "Iteration 11, loss = 0.42759213\n",
      "Iteration 12, loss = 0.42152609\n",
      "Iteration 13, loss = 0.41985285\n",
      "Iteration 14, loss = 0.41785106\n",
      "Iteration 15, loss = 0.41642405\n",
      "Iteration 16, loss = 0.41499414\n",
      "Iteration 17, loss = 0.41517609\n",
      "Iteration 18, loss = 0.41442574\n",
      "Iteration 19, loss = 0.41263410\n",
      "Iteration 20, loss = 0.41458453\n",
      "Iteration 21, loss = 0.41139771\n",
      "Iteration 22, loss = 0.41147939\n",
      "Iteration 23, loss = 0.41134413\n",
      "Iteration 24, loss = 0.41379544\n",
      "Iteration 25, loss = 0.41098389\n",
      "Iteration 26, loss = 0.40947686\n",
      "Iteration 27, loss = 0.40934919\n",
      "Iteration 28, loss = 0.40935370\n",
      "Iteration 29, loss = 0.40893761\n",
      "Iteration 30, loss = 0.40813253\n",
      "Iteration 31, loss = 0.40793164\n",
      "Iteration 32, loss = 0.40755837\n",
      "Iteration 33, loss = 0.40618673\n",
      "Iteration 34, loss = 0.40561614\n",
      "Iteration 35, loss = 0.40713425\n",
      "Iteration 36, loss = 0.40487892\n",
      "Iteration 37, loss = 0.40489908\n",
      "Iteration 38, loss = 0.40507862\n",
      "Iteration 39, loss = 0.40380887\n",
      "Iteration 40, loss = 0.40400846\n",
      "Iteration 41, loss = 0.40285851\n",
      "Iteration 42, loss = 0.40208425\n",
      "Iteration 43, loss = 0.40218524\n",
      "Iteration 44, loss = 0.40199200\n",
      "Iteration 45, loss = 0.40081840\n",
      "Iteration 46, loss = 0.40128952\n",
      "Iteration 47, loss = 0.40050260\n",
      "Iteration 48, loss = 0.39959576\n",
      "Iteration 49, loss = 0.40264265\n",
      "Iteration 50, loss = 0.40000932\n",
      "Iteration 51, loss = 0.39871801\n",
      "Iteration 52, loss = 0.40190460\n",
      "Iteration 53, loss = 0.40041010\n",
      "Iteration 54, loss = 0.39897248\n",
      "Iteration 55, loss = 0.40155432\n",
      "Iteration 56, loss = 0.40067361\n",
      "Iteration 57, loss = 0.39987002\n",
      "Iteration 58, loss = 0.39944711\n",
      "Iteration 59, loss = 0.40007761\n",
      "Iteration 60, loss = 0.39963323\n",
      "Iteration 61, loss = 0.40083930\n",
      "Iteration 62, loss = 0.39987826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50923663\n",
      "Iteration 2, loss = 0.47725701\n",
      "Iteration 3, loss = 0.47125098\n",
      "Iteration 4, loss = 0.46996489\n",
      "Iteration 5, loss = 0.46885941\n",
      "Iteration 6, loss = 0.46742294\n",
      "Iteration 7, loss = 0.46573026\n",
      "Iteration 8, loss = 0.46421735\n",
      "Iteration 9, loss = 0.46318206\n",
      "Iteration 10, loss = 0.46278099\n",
      "Iteration 11, loss = 0.46183547\n",
      "Iteration 12, loss = 0.46050400\n",
      "Iteration 13, loss = 0.45707440\n",
      "Iteration 14, loss = 0.45394666\n",
      "Iteration 15, loss = 0.45198613\n",
      "Iteration 16, loss = 0.44947421\n",
      "Iteration 17, loss = 0.44946329\n",
      "Iteration 18, loss = 0.44666917\n",
      "Iteration 19, loss = 0.43939058\n",
      "Iteration 20, loss = 0.42841798\n",
      "Iteration 21, loss = 0.42550776\n",
      "Iteration 22, loss = 0.42322865\n",
      "Iteration 23, loss = 0.42264213\n",
      "Iteration 24, loss = 0.42231173\n",
      "Iteration 25, loss = 0.41943837\n",
      "Iteration 26, loss = 0.41983815\n",
      "Iteration 27, loss = 0.41985054\n",
      "Iteration 28, loss = 0.41737437\n",
      "Iteration 29, loss = 0.41868939\n",
      "Iteration 30, loss = 0.41666619\n",
      "Iteration 31, loss = 0.41469735\n",
      "Iteration 32, loss = 0.41624895\n",
      "Iteration 33, loss = 0.41683687\n",
      "Iteration 34, loss = 0.41653698\n",
      "Iteration 35, loss = 0.41569398\n",
      "Iteration 36, loss = 0.41497668\n",
      "Iteration 37, loss = 0.41579000\n",
      "Iteration 38, loss = 0.41596400\n",
      "Iteration 39, loss = 0.41395971\n",
      "Iteration 40, loss = 0.41361018\n",
      "Iteration 41, loss = 0.41387701\n",
      "Iteration 42, loss = 0.41460804\n",
      "Iteration 43, loss = 0.41399926\n",
      "Iteration 44, loss = 0.41303246\n",
      "Iteration 45, loss = 0.41224825\n",
      "Iteration 46, loss = 0.41201250\n",
      "Iteration 47, loss = 0.41484552\n",
      "Iteration 48, loss = 0.41253264\n",
      "Iteration 49, loss = 0.41247265\n",
      "Iteration 50, loss = 0.41300540\n",
      "Iteration 51, loss = 0.41180565\n",
      "Iteration 52, loss = 0.41193095\n",
      "Iteration 53, loss = 0.41123380\n",
      "Iteration 54, loss = 0.41092069\n",
      "Iteration 55, loss = 0.41317832\n",
      "Iteration 56, loss = 0.41092062\n",
      "Iteration 57, loss = 0.41037522\n",
      "Iteration 58, loss = 0.41174414\n",
      "Iteration 59, loss = 0.41099605\n",
      "Iteration 60, loss = 0.41062859\n",
      "Iteration 61, loss = 0.41115513\n",
      "Iteration 62, loss = 0.41185022\n",
      "Iteration 63, loss = 0.41158730\n",
      "Iteration 64, loss = 0.41081975\n",
      "Iteration 65, loss = 0.41011797\n",
      "Iteration 66, loss = 0.41091697\n",
      "Iteration 67, loss = 0.40925485\n",
      "Iteration 68, loss = 0.41145235\n",
      "Iteration 69, loss = 0.41200039\n",
      "Iteration 70, loss = 0.40989897\n",
      "Iteration 71, loss = 0.41182182\n",
      "Iteration 72, loss = 0.41128224\n",
      "Iteration 73, loss = 0.41036208\n",
      "Iteration 74, loss = 0.40968775\n",
      "Iteration 75, loss = 0.40857227\n",
      "Iteration 76, loss = 0.41018449\n",
      "Iteration 77, loss = 0.40989682\n",
      "Iteration 78, loss = 0.41013944\n",
      "Iteration 79, loss = 0.41072270\n",
      "Iteration 80, loss = 0.40965079\n",
      "Iteration 81, loss = 0.40881101\n",
      "Iteration 82, loss = 0.40933279\n",
      "Iteration 83, loss = 0.40962179\n",
      "Iteration 84, loss = 0.41017455\n",
      "Iteration 85, loss = 0.40936430\n",
      "Iteration 86, loss = 0.40860831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51002143\n",
      "Iteration 2, loss = 0.47754119\n",
      "Iteration 3, loss = 0.47601607\n",
      "Iteration 4, loss = 0.47328198\n",
      "Iteration 5, loss = 0.47078999\n",
      "Iteration 6, loss = 0.46868813\n",
      "Iteration 7, loss = 0.46503131\n",
      "Iteration 8, loss = 0.45313229\n",
      "Iteration 9, loss = 0.44166933\n",
      "Iteration 10, loss = 0.43181259\n",
      "Iteration 11, loss = 0.42596629\n",
      "Iteration 12, loss = 0.42131350\n",
      "Iteration 13, loss = 0.41842482\n",
      "Iteration 14, loss = 0.41746704\n",
      "Iteration 15, loss = 0.41597446\n",
      "Iteration 16, loss = 0.41454615\n",
      "Iteration 17, loss = 0.41285519\n",
      "Iteration 18, loss = 0.41305774\n",
      "Iteration 19, loss = 0.41328137\n",
      "Iteration 20, loss = 0.41172992\n",
      "Iteration 21, loss = 0.41118849\n",
      "Iteration 22, loss = 0.40979389\n",
      "Iteration 23, loss = 0.40912572\n",
      "Iteration 24, loss = 0.41007418\n",
      "Iteration 25, loss = 0.40981792\n",
      "Iteration 26, loss = 0.40853762\n",
      "Iteration 27, loss = 0.40764932\n",
      "Iteration 28, loss = 0.40695563\n",
      "Iteration 29, loss = 0.40758780\n",
      "Iteration 30, loss = 0.40703303\n",
      "Iteration 31, loss = 0.40775840\n",
      "Iteration 32, loss = 0.40875570\n",
      "Iteration 33, loss = 0.40792049\n",
      "Iteration 34, loss = 0.40706023\n",
      "Iteration 35, loss = 0.40528235\n",
      "Iteration 36, loss = 0.40553222\n",
      "Iteration 37, loss = 0.40718714\n",
      "Iteration 38, loss = 0.40554732\n",
      "Iteration 39, loss = 0.40433660\n",
      "Iteration 40, loss = 0.40454474\n",
      "Iteration 41, loss = 0.40530853\n",
      "Iteration 42, loss = 0.40485260\n",
      "Iteration 43, loss = 0.40547983\n",
      "Iteration 44, loss = 0.40503103\n",
      "Iteration 45, loss = 0.40618578\n",
      "Iteration 46, loss = 0.40468811\n",
      "Iteration 47, loss = 0.40429905\n",
      "Iteration 48, loss = 0.40725475\n",
      "Iteration 49, loss = 0.40430315\n",
      "Iteration 50, loss = 0.40553008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex']\n",
      "Score: 0.8071214142761841\n",
      "Iteration 1, loss = 0.50943381\n",
      "Iteration 2, loss = 0.48158326\n",
      "Iteration 3, loss = 0.47603915\n",
      "Iteration 4, loss = 0.46999361\n",
      "Iteration 5, loss = 0.45756460\n",
      "Iteration 6, loss = 0.45109766\n",
      "Iteration 7, loss = 0.44632065\n",
      "Iteration 8, loss = 0.43853518\n",
      "Iteration 9, loss = 0.43336945\n",
      "Iteration 10, loss = 0.43161826\n",
      "Iteration 11, loss = 0.43055560\n",
      "Iteration 12, loss = 0.42850041\n",
      "Iteration 13, loss = 0.42857596\n",
      "Iteration 14, loss = 0.42934248\n",
      "Iteration 15, loss = 0.42743748\n",
      "Iteration 16, loss = 0.42849075\n",
      "Iteration 17, loss = 0.42825450\n",
      "Iteration 18, loss = 0.42826391\n",
      "Iteration 19, loss = 0.42822388\n",
      "Iteration 20, loss = 0.42744402\n",
      "Iteration 21, loss = 0.42743718\n",
      "Iteration 22, loss = 0.42790028\n",
      "Iteration 23, loss = 0.42761482\n",
      "Iteration 24, loss = 0.42662365\n",
      "Iteration 25, loss = 0.42743212\n",
      "Iteration 26, loss = 0.42740040\n",
      "Iteration 27, loss = 0.42737037\n",
      "Iteration 28, loss = 0.42736183\n",
      "Iteration 29, loss = 0.42817362\n",
      "Iteration 30, loss = 0.42776296\n",
      "Iteration 31, loss = 0.42773055\n",
      "Iteration 32, loss = 0.42634061\n",
      "Iteration 33, loss = 0.42650089\n",
      "Iteration 34, loss = 0.42824050\n",
      "Iteration 35, loss = 0.42718033\n",
      "Iteration 36, loss = 0.42592964\n",
      "Iteration 37, loss = 0.42685822\n",
      "Iteration 38, loss = 0.42650308\n",
      "Iteration 39, loss = 0.42576683\n",
      "Iteration 40, loss = 0.42745941\n",
      "Iteration 41, loss = 0.42668341\n",
      "Iteration 42, loss = 0.42674831\n",
      "Iteration 43, loss = 0.42727736\n",
      "Iteration 44, loss = 0.42662258\n",
      "Iteration 45, loss = 0.42540763\n",
      "Iteration 46, loss = 0.42540738\n",
      "Iteration 47, loss = 0.42573526\n",
      "Iteration 48, loss = 0.42592505\n",
      "Iteration 49, loss = 0.42590507\n",
      "Iteration 50, loss = 0.42503269\n",
      "Iteration 51, loss = 0.42518663\n",
      "Iteration 52, loss = 0.42582954\n",
      "Iteration 53, loss = 0.42462606\n",
      "Iteration 54, loss = 0.42583242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.42516082\n",
      "Iteration 56, loss = 0.42515019\n",
      "Iteration 57, loss = 0.42507744\n",
      "Iteration 58, loss = 0.42519164\n",
      "Iteration 59, loss = 0.42592284\n",
      "Iteration 60, loss = 0.42512388\n",
      "Iteration 61, loss = 0.42360824\n",
      "Iteration 62, loss = 0.42467854\n",
      "Iteration 63, loss = 0.42449882\n",
      "Iteration 64, loss = 0.42482068\n",
      "Iteration 65, loss = 0.42520090\n",
      "Iteration 66, loss = 0.42550267\n",
      "Iteration 67, loss = 0.42408068\n",
      "Iteration 68, loss = 0.42446240\n",
      "Iteration 69, loss = 0.42506506\n",
      "Iteration 70, loss = 0.42432568\n",
      "Iteration 71, loss = 0.42405658\n",
      "Iteration 72, loss = 0.42618397\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51188113\n",
      "Iteration 2, loss = 0.48403399\n",
      "Iteration 3, loss = 0.47763487\n",
      "Iteration 4, loss = 0.47246042\n",
      "Iteration 5, loss = 0.46765060\n",
      "Iteration 6, loss = 0.46084145\n",
      "Iteration 7, loss = 0.45343119\n",
      "Iteration 8, loss = 0.44557273\n",
      "Iteration 9, loss = 0.43967128\n",
      "Iteration 10, loss = 0.43585736\n",
      "Iteration 11, loss = 0.43436366\n",
      "Iteration 12, loss = 0.43255314\n",
      "Iteration 13, loss = 0.43142565\n",
      "Iteration 14, loss = 0.43080860\n",
      "Iteration 15, loss = 0.43128168\n",
      "Iteration 16, loss = 0.43053339\n",
      "Iteration 17, loss = 0.43049449\n",
      "Iteration 18, loss = 0.42997627\n",
      "Iteration 19, loss = 0.42982150\n",
      "Iteration 20, loss = 0.43013711\n",
      "Iteration 21, loss = 0.42997250\n",
      "Iteration 22, loss = 0.42949303\n",
      "Iteration 23, loss = 0.42899646\n",
      "Iteration 24, loss = 0.42939035\n",
      "Iteration 25, loss = 0.42887810\n",
      "Iteration 26, loss = 0.42901613\n",
      "Iteration 27, loss = 0.42941213\n",
      "Iteration 28, loss = 0.42887963\n",
      "Iteration 29, loss = 0.42856162\n",
      "Iteration 30, loss = 0.42989286\n",
      "Iteration 31, loss = 0.42847920\n",
      "Iteration 32, loss = 0.42816297\n",
      "Iteration 33, loss = 0.42875319\n",
      "Iteration 34, loss = 0.42884544\n",
      "Iteration 35, loss = 0.42801767\n",
      "Iteration 36, loss = 0.42846117\n",
      "Iteration 37, loss = 0.42929820\n",
      "Iteration 38, loss = 0.42966907\n",
      "Iteration 39, loss = 0.42852359\n",
      "Iteration 40, loss = 0.42894456\n",
      "Iteration 41, loss = 0.42866031\n",
      "Iteration 42, loss = 0.42836276\n",
      "Iteration 43, loss = 0.42887214\n",
      "Iteration 44, loss = 0.42942389\n",
      "Iteration 45, loss = 0.42941022\n",
      "Iteration 46, loss = 0.42787699\n",
      "Iteration 47, loss = 0.42806759\n",
      "Iteration 48, loss = 0.42817462\n",
      "Iteration 49, loss = 0.42855539\n",
      "Iteration 50, loss = 0.42787292\n",
      "Iteration 51, loss = 0.42822292\n",
      "Iteration 52, loss = 0.42806361\n",
      "Iteration 53, loss = 0.42827633\n",
      "Iteration 54, loss = 0.42834431\n",
      "Iteration 55, loss = 0.42803161\n",
      "Iteration 56, loss = 0.42744104\n",
      "Iteration 57, loss = 0.42878466\n",
      "Iteration 58, loss = 0.42832851\n",
      "Iteration 59, loss = 0.42962275\n",
      "Iteration 60, loss = 0.42789542\n",
      "Iteration 61, loss = 0.42749519\n",
      "Iteration 62, loss = 0.42784811\n",
      "Iteration 63, loss = 0.42816480\n",
      "Iteration 64, loss = 0.42848486\n",
      "Iteration 65, loss = 0.42806370\n",
      "Iteration 66, loss = 0.42887891\n",
      "Iteration 67, loss = 0.42766917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51559654\n",
      "Iteration 2, loss = 0.48765826\n",
      "Iteration 3, loss = 0.48286456\n",
      "Iteration 4, loss = 0.47862305\n",
      "Iteration 5, loss = 0.47125596\n",
      "Iteration 6, loss = 0.45453748\n",
      "Iteration 7, loss = 0.43967881\n",
      "Iteration 8, loss = 0.43266926\n",
      "Iteration 9, loss = 0.42883859\n",
      "Iteration 10, loss = 0.42732991\n",
      "Iteration 11, loss = 0.42696746\n",
      "Iteration 12, loss = 0.42751939\n",
      "Iteration 13, loss = 0.42651608\n",
      "Iteration 14, loss = 0.42579883\n",
      "Iteration 15, loss = 0.42737521\n",
      "Iteration 16, loss = 0.42611267\n",
      "Iteration 17, loss = 0.42484614\n",
      "Iteration 18, loss = 0.42620012\n",
      "Iteration 19, loss = 0.42558579\n",
      "Iteration 20, loss = 0.42596825\n",
      "Iteration 21, loss = 0.42613622\n",
      "Iteration 22, loss = 0.42381552\n",
      "Iteration 23, loss = 0.42425119\n",
      "Iteration 24, loss = 0.42515390\n",
      "Iteration 25, loss = 0.42409143\n",
      "Iteration 26, loss = 0.42571771\n",
      "Iteration 27, loss = 0.42438564\n",
      "Iteration 28, loss = 0.42657685\n",
      "Iteration 29, loss = 0.42430362\n",
      "Iteration 30, loss = 0.42334538\n",
      "Iteration 31, loss = 0.42466846\n",
      "Iteration 32, loss = 0.42399156\n",
      "Iteration 33, loss = 0.42627744\n",
      "Iteration 34, loss = 0.42487156\n",
      "Iteration 35, loss = 0.42396316\n",
      "Iteration 36, loss = 0.42291983\n",
      "Iteration 37, loss = 0.42305971\n",
      "Iteration 38, loss = 0.42327210\n",
      "Iteration 39, loss = 0.42338396\n",
      "Iteration 40, loss = 0.42507993\n",
      "Iteration 41, loss = 0.42573596\n",
      "Iteration 42, loss = 0.42691980\n",
      "Iteration 43, loss = 0.42627042\n",
      "Iteration 44, loss = 0.42648949\n",
      "Iteration 45, loss = 0.42588414\n",
      "Iteration 46, loss = 0.42559727\n",
      "Iteration 47, loss = 0.42651589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'relationship', 'race', 'sex']\n",
      "Score: 0.7760451412052479\n",
      "Iteration 1, loss = 0.55045391\n",
      "Iteration 2, loss = 0.51812325\n",
      "Iteration 3, loss = 0.50793914\n",
      "Iteration 4, loss = 0.48460217\n",
      "Iteration 5, loss = 0.46140257\n",
      "Iteration 6, loss = 0.45200271\n",
      "Iteration 7, loss = 0.44893441\n",
      "Iteration 8, loss = 0.44872968\n",
      "Iteration 9, loss = 0.44671632\n",
      "Iteration 10, loss = 0.44459766\n",
      "Iteration 11, loss = 0.44028917\n",
      "Iteration 12, loss = 0.44009327\n",
      "Iteration 13, loss = 0.43917026\n",
      "Iteration 14, loss = 0.43876013\n",
      "Iteration 15, loss = 0.43818704\n",
      "Iteration 16, loss = 0.43740129\n",
      "Iteration 17, loss = 0.43785316\n",
      "Iteration 18, loss = 0.43796228\n",
      "Iteration 19, loss = 0.43779870\n",
      "Iteration 20, loss = 0.43483721\n",
      "Iteration 21, loss = 0.43230945\n",
      "Iteration 22, loss = 0.43216165\n",
      "Iteration 23, loss = 0.43196081\n",
      "Iteration 24, loss = 0.43203101\n",
      "Iteration 25, loss = 0.43133839\n",
      "Iteration 26, loss = 0.43039850\n",
      "Iteration 27, loss = 0.43066419\n",
      "Iteration 28, loss = 0.43155269\n",
      "Iteration 29, loss = 0.43047703\n",
      "Iteration 30, loss = 0.43051989\n",
      "Iteration 31, loss = 0.43016376\n",
      "Iteration 32, loss = 0.42961030\n",
      "Iteration 33, loss = 0.42941915\n",
      "Iteration 34, loss = 0.42993901\n",
      "Iteration 35, loss = 0.43042105\n",
      "Iteration 36, loss = 0.42975647\n",
      "Iteration 37, loss = 0.42998769\n",
      "Iteration 38, loss = 0.42964437\n",
      "Iteration 39, loss = 0.42954507\n",
      "Iteration 40, loss = 0.42967230\n",
      "Iteration 41, loss = 0.42955938\n",
      "Iteration 42, loss = 0.43000646\n",
      "Iteration 43, loss = 0.42975605\n",
      "Iteration 44, loss = 0.42981163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54911761\n",
      "Iteration 2, loss = 0.51574885\n",
      "Iteration 3, loss = 0.50634402\n",
      "Iteration 4, loss = 0.49120577\n",
      "Iteration 5, loss = 0.46450853\n",
      "Iteration 6, loss = 0.44568519\n",
      "Iteration 7, loss = 0.43916632\n",
      "Iteration 8, loss = 0.43647167\n",
      "Iteration 9, loss = 0.43674008\n",
      "Iteration 10, loss = 0.43730864\n",
      "Iteration 11, loss = 0.43643034\n",
      "Iteration 12, loss = 0.43626235\n",
      "Iteration 13, loss = 0.43573246\n",
      "Iteration 14, loss = 0.43473337\n",
      "Iteration 15, loss = 0.43512397\n",
      "Iteration 16, loss = 0.43452304\n",
      "Iteration 17, loss = 0.43448395\n",
      "Iteration 18, loss = 0.43410692\n",
      "Iteration 19, loss = 0.43544784\n",
      "Iteration 20, loss = 0.43400958\n",
      "Iteration 21, loss = 0.43469422\n",
      "Iteration 22, loss = 0.43384117\n",
      "Iteration 23, loss = 0.43410650\n",
      "Iteration 24, loss = 0.43524525\n",
      "Iteration 25, loss = 0.43374453\n",
      "Iteration 26, loss = 0.43383252\n",
      "Iteration 27, loss = 0.43473975\n",
      "Iteration 28, loss = 0.43313464\n",
      "Iteration 29, loss = 0.43433416\n",
      "Iteration 30, loss = 0.43393817\n",
      "Iteration 31, loss = 0.43332800\n",
      "Iteration 32, loss = 0.43260124\n",
      "Iteration 33, loss = 0.43295935\n",
      "Iteration 34, loss = 0.43315634\n",
      "Iteration 35, loss = 0.43175922\n",
      "Iteration 36, loss = 0.43252224\n",
      "Iteration 37, loss = 0.43357350\n",
      "Iteration 38, loss = 0.43497893\n",
      "Iteration 39, loss = 0.43257024\n",
      "Iteration 40, loss = 0.43229817\n",
      "Iteration 41, loss = 0.43230946\n",
      "Iteration 42, loss = 0.43268339\n",
      "Iteration 43, loss = 0.43232289\n",
      "Iteration 44, loss = 0.43455165\n",
      "Iteration 45, loss = 0.43309210\n",
      "Iteration 46, loss = 0.43226266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55060900\n",
      "Iteration 2, loss = 0.51912141\n",
      "Iteration 3, loss = 0.50986685\n",
      "Iteration 4, loss = 0.49656114\n",
      "Iteration 5, loss = 0.48362996\n",
      "Iteration 6, loss = 0.47009588\n",
      "Iteration 7, loss = 0.45497845\n",
      "Iteration 8, loss = 0.44901309\n",
      "Iteration 9, loss = 0.44643128\n",
      "Iteration 10, loss = 0.44308076\n",
      "Iteration 11, loss = 0.44202392\n",
      "Iteration 12, loss = 0.44110795\n",
      "Iteration 13, loss = 0.44025151\n",
      "Iteration 14, loss = 0.43948338\n",
      "Iteration 15, loss = 0.43842918\n",
      "Iteration 16, loss = 0.43693387\n",
      "Iteration 17, loss = 0.43425520\n",
      "Iteration 18, loss = 0.43190829\n",
      "Iteration 19, loss = 0.43095153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.43202357\n",
      "Iteration 21, loss = 0.42949872\n",
      "Iteration 22, loss = 0.42785727\n",
      "Iteration 23, loss = 0.42836320\n",
      "Iteration 24, loss = 0.42851666\n",
      "Iteration 25, loss = 0.42620147\n",
      "Iteration 26, loss = 0.42665248\n",
      "Iteration 27, loss = 0.42466914\n",
      "Iteration 28, loss = 0.42627620\n",
      "Iteration 29, loss = 0.42668146\n",
      "Iteration 30, loss = 0.42493747\n",
      "Iteration 31, loss = 0.42552571\n",
      "Iteration 32, loss = 0.42527924\n",
      "Iteration 33, loss = 0.42514642\n",
      "Iteration 34, loss = 0.42408764\n",
      "Iteration 35, loss = 0.42469938\n",
      "Iteration 36, loss = 0.42293499\n",
      "Iteration 37, loss = 0.42623256\n",
      "Iteration 38, loss = 0.42415592\n",
      "Iteration 39, loss = 0.42215357\n",
      "Iteration 40, loss = 0.42545575\n",
      "Iteration 41, loss = 0.42374518\n",
      "Iteration 42, loss = 0.42384175\n",
      "Iteration 43, loss = 0.42244481\n",
      "Iteration 44, loss = 0.42285008\n",
      "Iteration 45, loss = 0.42304250\n",
      "Iteration 46, loss = 0.42348273\n",
      "Iteration 47, loss = 0.42480677\n",
      "Iteration 48, loss = 0.42272365\n",
      "Iteration 49, loss = 0.42408819\n",
      "Iteration 50, loss = 0.42553356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.7915276851234155\n",
      "Iteration 1, loss = 0.50979172\n",
      "Iteration 2, loss = 0.46542450\n",
      "Iteration 3, loss = 0.44407542\n",
      "Iteration 4, loss = 0.43512785\n",
      "Iteration 5, loss = 0.43205114\n",
      "Iteration 6, loss = 0.42968234\n",
      "Iteration 7, loss = 0.42840107\n",
      "Iteration 8, loss = 0.42635855\n",
      "Iteration 9, loss = 0.42541495\n",
      "Iteration 10, loss = 0.42601278\n",
      "Iteration 11, loss = 0.42540076\n",
      "Iteration 12, loss = 0.42245772\n",
      "Iteration 13, loss = 0.42139062\n",
      "Iteration 14, loss = 0.42105867\n",
      "Iteration 15, loss = 0.42212769\n",
      "Iteration 16, loss = 0.41905726\n",
      "Iteration 17, loss = 0.42059152\n",
      "Iteration 18, loss = 0.42109037\n",
      "Iteration 19, loss = 0.42182866\n",
      "Iteration 20, loss = 0.41997026\n",
      "Iteration 21, loss = 0.41932033\n",
      "Iteration 22, loss = 0.41839086\n",
      "Iteration 23, loss = 0.41890594\n",
      "Iteration 24, loss = 0.41875817\n",
      "Iteration 25, loss = 0.41749821\n",
      "Iteration 26, loss = 0.41795987\n",
      "Iteration 27, loss = 0.41780541\n",
      "Iteration 28, loss = 0.41631294\n",
      "Iteration 29, loss = 0.41654104\n",
      "Iteration 30, loss = 0.41621650\n",
      "Iteration 31, loss = 0.41616620\n",
      "Iteration 32, loss = 0.41673919\n",
      "Iteration 33, loss = 0.41410083\n",
      "Iteration 34, loss = 0.41519779\n",
      "Iteration 35, loss = 0.41448255\n",
      "Iteration 36, loss = 0.41396499\n",
      "Iteration 37, loss = 0.41529902\n",
      "Iteration 38, loss = 0.41258599\n",
      "Iteration 39, loss = 0.41355045\n",
      "Iteration 40, loss = 0.41235547\n",
      "Iteration 41, loss = 0.41333545\n",
      "Iteration 42, loss = 0.41337624\n",
      "Iteration 43, loss = 0.41390680\n",
      "Iteration 44, loss = 0.41422891\n",
      "Iteration 45, loss = 0.41321546\n",
      "Iteration 46, loss = 0.41418426\n",
      "Iteration 47, loss = 0.41303761\n",
      "Iteration 48, loss = 0.41254980\n",
      "Iteration 49, loss = 0.41306946\n",
      "Iteration 50, loss = 0.41264253\n",
      "Iteration 51, loss = 0.41208711\n",
      "Iteration 52, loss = 0.41373301\n",
      "Iteration 53, loss = 0.41075076\n",
      "Iteration 54, loss = 0.41174439\n",
      "Iteration 55, loss = 0.41252703\n",
      "Iteration 56, loss = 0.41219967\n",
      "Iteration 57, loss = 0.41174970\n",
      "Iteration 58, loss = 0.41217960\n",
      "Iteration 59, loss = 0.41426021\n",
      "Iteration 60, loss = 0.41069459\n",
      "Iteration 61, loss = 0.41171842\n",
      "Iteration 62, loss = 0.41135221\n",
      "Iteration 63, loss = 0.41160330\n",
      "Iteration 64, loss = 0.41118947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51231982\n",
      "Iteration 2, loss = 0.46819068\n",
      "Iteration 3, loss = 0.45048999\n",
      "Iteration 4, loss = 0.43756718\n",
      "Iteration 5, loss = 0.43188937\n",
      "Iteration 6, loss = 0.42952966\n",
      "Iteration 7, loss = 0.42649158\n",
      "Iteration 8, loss = 0.42562626\n",
      "Iteration 9, loss = 0.42320566\n",
      "Iteration 10, loss = 0.42107771\n",
      "Iteration 11, loss = 0.41950654\n",
      "Iteration 12, loss = 0.41931064\n",
      "Iteration 13, loss = 0.41839436\n",
      "Iteration 14, loss = 0.41734291\n",
      "Iteration 15, loss = 0.41848011\n",
      "Iteration 16, loss = 0.41638297\n",
      "Iteration 17, loss = 0.41678820\n",
      "Iteration 18, loss = 0.41538521\n",
      "Iteration 19, loss = 0.41730510\n",
      "Iteration 20, loss = 0.41431579\n",
      "Iteration 21, loss = 0.41613473\n",
      "Iteration 22, loss = 0.41440204\n",
      "Iteration 23, loss = 0.41433684\n",
      "Iteration 24, loss = 0.41464711\n",
      "Iteration 25, loss = 0.41306255\n",
      "Iteration 26, loss = 0.41318833\n",
      "Iteration 27, loss = 0.41427298\n",
      "Iteration 28, loss = 0.41271285\n",
      "Iteration 29, loss = 0.41273006\n",
      "Iteration 30, loss = 0.41261529\n",
      "Iteration 31, loss = 0.41364328\n",
      "Iteration 32, loss = 0.41335352\n",
      "Iteration 33, loss = 0.41245122\n",
      "Iteration 34, loss = 0.41273830\n",
      "Iteration 35, loss = 0.41243788\n",
      "Iteration 36, loss = 0.41159431\n",
      "Iteration 37, loss = 0.41101270\n",
      "Iteration 38, loss = 0.41193394\n",
      "Iteration 39, loss = 0.41211848\n",
      "Iteration 40, loss = 0.41186075\n",
      "Iteration 41, loss = 0.41220386\n",
      "Iteration 42, loss = 0.41183742\n",
      "Iteration 43, loss = 0.41122049\n",
      "Iteration 44, loss = 0.41122192\n",
      "Iteration 45, loss = 0.41156103\n",
      "Iteration 46, loss = 0.41178983\n",
      "Iteration 47, loss = 0.41254670\n",
      "Iteration 48, loss = 0.41133214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51459047\n",
      "Iteration 2, loss = 0.46651420\n",
      "Iteration 3, loss = 0.44371332\n",
      "Iteration 4, loss = 0.43155329\n",
      "Iteration 5, loss = 0.42860805\n",
      "Iteration 6, loss = 0.42575031\n",
      "Iteration 7, loss = 0.42473978\n",
      "Iteration 8, loss = 0.42322256\n",
      "Iteration 9, loss = 0.42218886\n",
      "Iteration 10, loss = 0.42093446\n",
      "Iteration 11, loss = 0.42117692\n",
      "Iteration 12, loss = 0.42085448\n",
      "Iteration 13, loss = 0.42097066\n",
      "Iteration 14, loss = 0.42107808\n",
      "Iteration 15, loss = 0.42086017\n",
      "Iteration 16, loss = 0.42007285\n",
      "Iteration 17, loss = 0.41847653\n",
      "Iteration 18, loss = 0.42111174\n",
      "Iteration 19, loss = 0.41964631\n",
      "Iteration 20, loss = 0.41958015\n",
      "Iteration 21, loss = 0.41973146\n",
      "Iteration 22, loss = 0.41939862\n",
      "Iteration 23, loss = 0.41895087\n",
      "Iteration 24, loss = 0.41821043\n",
      "Iteration 25, loss = 0.41797890\n",
      "Iteration 26, loss = 0.41832688\n",
      "Iteration 27, loss = 0.41838573\n",
      "Iteration 28, loss = 0.41984853\n",
      "Iteration 29, loss = 0.41795310\n",
      "Iteration 30, loss = 0.41768705\n",
      "Iteration 31, loss = 0.41893835\n",
      "Iteration 32, loss = 0.41852075\n",
      "Iteration 33, loss = 0.41751905\n",
      "Iteration 34, loss = 0.41783791\n",
      "Iteration 35, loss = 0.41807424\n",
      "Iteration 36, loss = 0.41713926\n",
      "Iteration 37, loss = 0.41702629\n",
      "Iteration 38, loss = 0.41773647\n",
      "Iteration 39, loss = 0.41683164\n",
      "Iteration 40, loss = 0.41683978\n",
      "Iteration 41, loss = 0.41846122\n",
      "Iteration 42, loss = 0.41803879\n",
      "Iteration 43, loss = 0.41641328\n",
      "Iteration 44, loss = 0.41741492\n",
      "Iteration 45, loss = 0.41793143\n",
      "Iteration 46, loss = 0.41684527\n",
      "Iteration 47, loss = 0.41707140\n",
      "Iteration 48, loss = 0.41767311\n",
      "Iteration 49, loss = 0.41742115\n",
      "Iteration 50, loss = 0.41808462\n",
      "Iteration 51, loss = 0.41700923\n",
      "Iteration 52, loss = 0.41738245\n",
      "Iteration 53, loss = 0.41634606\n",
      "Iteration 54, loss = 0.41690638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.7942795196797864\n",
      "Iteration 1, loss = 0.51599108\n",
      "Iteration 2, loss = 0.46785869\n",
      "Iteration 3, loss = 0.46586799\n",
      "Iteration 4, loss = 0.45217244\n",
      "Iteration 5, loss = 0.42900112\n",
      "Iteration 6, loss = 0.42516274\n",
      "Iteration 7, loss = 0.42290890\n",
      "Iteration 8, loss = 0.42147159\n",
      "Iteration 9, loss = 0.42052262\n",
      "Iteration 10, loss = 0.42075503\n",
      "Iteration 11, loss = 0.41894424\n",
      "Iteration 12, loss = 0.41798293\n",
      "Iteration 13, loss = 0.41763069\n",
      "Iteration 14, loss = 0.41750915\n",
      "Iteration 15, loss = 0.41798679\n",
      "Iteration 16, loss = 0.41689637\n",
      "Iteration 17, loss = 0.41835411\n",
      "Iteration 18, loss = 0.41736282\n",
      "Iteration 19, loss = 0.41788046\n",
      "Iteration 20, loss = 0.41702544\n",
      "Iteration 21, loss = 0.41673345\n",
      "Iteration 22, loss = 0.41721847\n",
      "Iteration 23, loss = 0.41756001\n",
      "Iteration 24, loss = 0.41675943\n",
      "Iteration 25, loss = 0.41742077\n",
      "Iteration 26, loss = 0.41657488\n",
      "Iteration 27, loss = 0.41682251\n",
      "Iteration 28, loss = 0.41600990\n",
      "Iteration 29, loss = 0.41579251\n",
      "Iteration 30, loss = 0.41697779\n",
      "Iteration 31, loss = 0.41640390\n",
      "Iteration 32, loss = 0.41507886\n",
      "Iteration 33, loss = 0.41518204\n",
      "Iteration 34, loss = 0.41547647\n",
      "Iteration 35, loss = 0.41664398\n",
      "Iteration 36, loss = 0.41542191\n",
      "Iteration 37, loss = 0.41590034\n",
      "Iteration 38, loss = 0.41519404\n",
      "Iteration 39, loss = 0.41643129\n",
      "Iteration 40, loss = 0.41540164\n",
      "Iteration 41, loss = 0.41511219\n",
      "Iteration 42, loss = 0.41545325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.41501314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51767102\n",
      "Iteration 2, loss = 0.47167643\n",
      "Iteration 3, loss = 0.46349413\n",
      "Iteration 4, loss = 0.44877489\n",
      "Iteration 5, loss = 0.43625389\n",
      "Iteration 6, loss = 0.42914078\n",
      "Iteration 7, loss = 0.42610778\n",
      "Iteration 8, loss = 0.42463011\n",
      "Iteration 9, loss = 0.42513077\n",
      "Iteration 10, loss = 0.42351146\n",
      "Iteration 11, loss = 0.42275861\n",
      "Iteration 12, loss = 0.42317334\n",
      "Iteration 13, loss = 0.42214421\n",
      "Iteration 14, loss = 0.42195903\n",
      "Iteration 15, loss = 0.42326757\n",
      "Iteration 16, loss = 0.42167062\n",
      "Iteration 17, loss = 0.42205016\n",
      "Iteration 18, loss = 0.42211656\n",
      "Iteration 19, loss = 0.42357137\n",
      "Iteration 20, loss = 0.42146828\n",
      "Iteration 21, loss = 0.42264744\n",
      "Iteration 22, loss = 0.42166501\n",
      "Iteration 23, loss = 0.42157416\n",
      "Iteration 24, loss = 0.42167198\n",
      "Iteration 25, loss = 0.42091924\n",
      "Iteration 26, loss = 0.42101510\n",
      "Iteration 27, loss = 0.42136753\n",
      "Iteration 28, loss = 0.42011274\n",
      "Iteration 29, loss = 0.42122530\n",
      "Iteration 30, loss = 0.42088884\n",
      "Iteration 31, loss = 0.42037907\n",
      "Iteration 32, loss = 0.42098497\n",
      "Iteration 33, loss = 0.42079476\n",
      "Iteration 34, loss = 0.42114389\n",
      "Iteration 35, loss = 0.42100305\n",
      "Iteration 36, loss = 0.42076212\n",
      "Iteration 37, loss = 0.42082569\n",
      "Iteration 38, loss = 0.42109386\n",
      "Iteration 39, loss = 0.42062102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52132886\n",
      "Iteration 2, loss = 0.47249919\n",
      "Iteration 3, loss = 0.46094132\n",
      "Iteration 4, loss = 0.44842654\n",
      "Iteration 5, loss = 0.43252615\n",
      "Iteration 6, loss = 0.42880653\n",
      "Iteration 7, loss = 0.42622486\n",
      "Iteration 8, loss = 0.42614527\n",
      "Iteration 9, loss = 0.42510219\n",
      "Iteration 10, loss = 0.42442067\n",
      "Iteration 11, loss = 0.42405797\n",
      "Iteration 12, loss = 0.42286734\n",
      "Iteration 13, loss = 0.42285185\n",
      "Iteration 14, loss = 0.42257478\n",
      "Iteration 15, loss = 0.42247794\n",
      "Iteration 16, loss = 0.42253811\n",
      "Iteration 17, loss = 0.42168511\n",
      "Iteration 18, loss = 0.42234454\n",
      "Iteration 19, loss = 0.42184726\n",
      "Iteration 20, loss = 0.42205971\n",
      "Iteration 21, loss = 0.42192489\n",
      "Iteration 22, loss = 0.42234361\n",
      "Iteration 23, loss = 0.42159691\n",
      "Iteration 24, loss = 0.42324968\n",
      "Iteration 25, loss = 0.42162863\n",
      "Iteration 26, loss = 0.42206955\n",
      "Iteration 27, loss = 0.42222057\n",
      "Iteration 28, loss = 0.42249533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.7938903713586836\n",
      "Elapsed time: 2 min. and 26.878114700317383 sec.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['workclass',\n",
       "  'education',\n",
       "  'marital-status',\n",
       "  'occupation',\n",
       "  'relationship',\n",
       "  'sex'],\n",
       " 0.8118745830553703]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, we will find the best subset for 6 features with verbose=True to show\n",
    "# process of scoring all subsets\n",
    "best_subset(X, y, mlp, 6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06180a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # next, we will search for the best subset between 1 feature and 8 features.\n",
    "\n",
    "# subsets_df = best_subsets(X, y, mlp, min_subset_len=1, max_subset_len=8, cv=3)\n",
    "# print('Top three feature subsets:')\n",
    "# print(subsets_df.sort_values(by='cv_score', ascending=False).head(3))\n",
    "# best_subset_ftrs = subsets_df[subsets_df.cv_score == max(subsets_df.cv_score)]\n",
    "# print('Best feature subset:')\n",
    "# for f in best_subset_ftrs.features:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "157e7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = test_df.drop('salary', axis=1)\n",
    "y = test_df['salary']\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)  # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f22a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best feature subset\n",
    "\n",
    "# params\n",
    "# X: feature values\n",
    "# y: labels\n",
    "# model: fitted classifier object\n",
    "# subset_len: how many features the desired feature subset will have\n",
    "# cv: number of folds for cross validation scoring\n",
    "def best_subset(X, y, model, subset_len, cv=3, verbose=False):\n",
    "    \n",
    "    # for elapsed time\n",
    "    start = time.time()\n",
    "    \n",
    "    # getting number of features\n",
    "    l = X.shape[1]\n",
    "    \n",
    "    # getting all the different possible combinations of features for the given\n",
    "    # subset length\n",
    "    subsets = list(combinations(range(l), subset_len))\n",
    "    print(f'{len(subsets)} total combinations for subset length {subset_len}.')\n",
    "    \n",
    "    # setting baseline values for best score and subset to update later\n",
    "    best_score = -np.inf\n",
    "    best_subset = None\n",
    "    \n",
    "    # iterating through possible combinations\n",
    "    for subset in subsets:\n",
    "        \n",
    "        # subset is currently an index, need to get feature names to select df\n",
    "        subset_nm = [X.columns[i] for i in subset]\n",
    "        \n",
    "        # getting CV score from model with subsetted features\n",
    "        score = cross_val_score(model, X[subset_nm], y, cv=cv).mean()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Current subset: {subset_nm}\\nScore: {score}')\n",
    "        \n",
    "        # updating best score and best subset if a new best score happens\n",
    "        if score > best_score:\n",
    "            best_score, best_subset = score, subset\n",
    "            \n",
    "    # for elapsed time\n",
    "    end = time.time()\n",
    "    \n",
    "    # printing and returning metrics\n",
    "    print(f'Elapsed time: {int((end - start)/60)} min. and {(end - start)%60} sec.\\n')\n",
    "    return [[X.columns[i] for i in best_subset], best_score]\n",
    "\n",
    "# Function to iterate through multiple subset lengths and return a df of\n",
    "# associated features and CV scores\n",
    "#\n",
    "# params\n",
    "# X: feature values\n",
    "# y: labels\n",
    "# model: fitted classifier object\n",
    "# min_subset_len: lowest number of features for a subset\n",
    "# max_subset_len: greatest number of features for a subset\n",
    "# cv: number of folds for cross validation scoring\n",
    "# verbose: prints each combination\n",
    "def best_subsets(X, y, model, min_subset_len, max_subset_len, cv=3, verbose=False):\n",
    "    \n",
    "    # lists to later create df with\n",
    "    subset_length = list()\n",
    "    subset_features = list()\n",
    "    subset_cv_score = list()\n",
    "    \n",
    "    # iterating through desired subset lengths\n",
    "    for i in range(min_subset_len, max_subset_len+1):\n",
    "        best = best_subset(X, y, mlp, i, cv=cv, verbose=verbose)\n",
    "        \n",
    "        # adding to lists to create df with later\n",
    "        subset_length.append(i)\n",
    "        subset_features.append(best[0])\n",
    "        subset_cv_score.append(best[1])\n",
    "        \n",
    "    # creating and returning dataframe with length, feature list, and score for\n",
    "    # each desired subset length\n",
    "    df = pd.DataFrame(data={'length': subset_length, 'features': subset_features, 'cv_score': subset_cv_score})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd40fffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 total combinations for subset length 6.\n",
      "Iteration 1, loss = 0.57377840\n",
      "Iteration 2, loss = 0.50839639\n",
      "Iteration 3, loss = 0.49721693\n",
      "Iteration 4, loss = 0.49060687\n",
      "Iteration 5, loss = 0.48859896\n",
      "Iteration 6, loss = 0.48093577\n",
      "Iteration 7, loss = 0.47938930\n",
      "Iteration 8, loss = 0.47761786\n",
      "Iteration 9, loss = 0.47567216\n",
      "Iteration 10, loss = 0.46993163\n",
      "Iteration 11, loss = 0.46994371\n",
      "Iteration 12, loss = 0.46681875\n",
      "Iteration 13, loss = 0.46416285\n",
      "Iteration 14, loss = 0.46236532\n",
      "Iteration 15, loss = 0.46157276\n",
      "Iteration 16, loss = 0.46048842\n",
      "Iteration 17, loss = 0.45685223\n",
      "Iteration 18, loss = 0.45850853\n",
      "Iteration 19, loss = 0.45651663\n",
      "Iteration 20, loss = 0.45607661\n",
      "Iteration 21, loss = 0.45521926\n",
      "Iteration 22, loss = 0.45407269\n",
      "Iteration 23, loss = 0.45547599\n",
      "Iteration 24, loss = 0.45109598\n",
      "Iteration 25, loss = 0.45208777\n",
      "Iteration 26, loss = 0.44893924\n",
      "Iteration 27, loss = 0.44365396\n",
      "Iteration 28, loss = 0.44567616\n",
      "Iteration 29, loss = 0.44159605\n",
      "Iteration 30, loss = 0.44184294\n",
      "Iteration 31, loss = 0.43952719\n",
      "Iteration 32, loss = 0.43838323\n",
      "Iteration 33, loss = 0.43886225\n",
      "Iteration 34, loss = 0.43431052\n",
      "Iteration 35, loss = 0.43533820\n",
      "Iteration 36, loss = 0.43586648\n",
      "Iteration 37, loss = 0.43255826\n",
      "Iteration 38, loss = 0.43515279\n",
      "Iteration 39, loss = 0.43366231\n",
      "Iteration 40, loss = 0.43279170\n",
      "Iteration 41, loss = 0.43295328\n",
      "Iteration 42, loss = 0.43255087\n",
      "Iteration 43, loss = 0.43368076\n",
      "Iteration 44, loss = 0.43232997\n",
      "Iteration 45, loss = 0.43088723\n",
      "Iteration 46, loss = 0.43222846\n",
      "Iteration 47, loss = 0.43517685\n",
      "Iteration 48, loss = 0.43144213\n",
      "Iteration 49, loss = 0.43384870\n",
      "Iteration 50, loss = 0.43185958\n",
      "Iteration 51, loss = 0.43068697\n",
      "Iteration 52, loss = 0.43048629\n",
      "Iteration 53, loss = 0.43239991\n",
      "Iteration 54, loss = 0.43281868\n",
      "Iteration 55, loss = 0.43270412\n",
      "Iteration 56, loss = 0.43245670\n",
      "Iteration 57, loss = 0.42962579\n",
      "Iteration 58, loss = 0.42943252\n",
      "Iteration 59, loss = 0.43261199\n",
      "Iteration 60, loss = 0.43088816\n",
      "Iteration 61, loss = 0.42832344\n",
      "Iteration 62, loss = 0.43051312\n",
      "Iteration 63, loss = 0.42886921\n",
      "Iteration 64, loss = 0.43341480\n",
      "Iteration 65, loss = 0.42918462\n",
      "Iteration 66, loss = 0.42957724\n",
      "Iteration 67, loss = 0.42766534\n",
      "Iteration 68, loss = 0.42662921\n",
      "Iteration 69, loss = 0.43270564\n",
      "Iteration 70, loss = 0.43139654\n",
      "Iteration 71, loss = 0.42765833\n",
      "Iteration 72, loss = 0.42591702\n",
      "Iteration 73, loss = 0.42833809\n",
      "Iteration 74, loss = 0.42483463\n",
      "Iteration 75, loss = 0.42839280\n",
      "Iteration 76, loss = 0.42636967\n",
      "Iteration 77, loss = 0.42740921\n",
      "Iteration 78, loss = 0.42666460\n",
      "Iteration 79, loss = 0.42604622\n",
      "Iteration 80, loss = 0.42332958\n",
      "Iteration 81, loss = 0.42760872\n",
      "Iteration 82, loss = 0.42700645\n",
      "Iteration 83, loss = 0.42711053\n",
      "Iteration 84, loss = 0.42941024\n",
      "Iteration 85, loss = 0.42659858\n",
      "Iteration 86, loss = 0.42597748\n",
      "Iteration 87, loss = 0.42599581\n",
      "Iteration 88, loss = 0.42441393\n",
      "Iteration 89, loss = 0.42544436\n",
      "Iteration 90, loss = 0.43061444\n",
      "Iteration 91, loss = 0.42499664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58026526\n",
      "Iteration 2, loss = 0.50415601\n",
      "Iteration 3, loss = 0.49006009\n",
      "Iteration 4, loss = 0.48546783\n",
      "Iteration 5, loss = 0.48069605\n",
      "Iteration 6, loss = 0.47308989\n",
      "Iteration 7, loss = 0.47048617\n",
      "Iteration 8, loss = 0.46744593\n",
      "Iteration 9, loss = 0.46391273\n",
      "Iteration 10, loss = 0.46228481\n",
      "Iteration 11, loss = 0.46032853\n",
      "Iteration 12, loss = 0.45927740\n",
      "Iteration 13, loss = 0.46039524\n",
      "Iteration 14, loss = 0.45665732\n",
      "Iteration 15, loss = 0.45616281\n",
      "Iteration 16, loss = 0.45688330\n",
      "Iteration 17, loss = 0.45332567\n",
      "Iteration 18, loss = 0.45360937\n",
      "Iteration 19, loss = 0.45165416\n",
      "Iteration 20, loss = 0.45131446\n",
      "Iteration 21, loss = 0.45132226\n",
      "Iteration 22, loss = 0.45023662\n",
      "Iteration 23, loss = 0.44815957\n",
      "Iteration 24, loss = 0.44682487\n",
      "Iteration 25, loss = 0.44928277\n",
      "Iteration 26, loss = 0.44497107\n",
      "Iteration 27, loss = 0.44731503\n",
      "Iteration 28, loss = 0.44597513\n",
      "Iteration 29, loss = 0.44254728\n",
      "Iteration 30, loss = 0.44297614\n",
      "Iteration 31, loss = 0.43824096\n",
      "Iteration 32, loss = 0.43906469\n",
      "Iteration 33, loss = 0.43744373\n",
      "Iteration 34, loss = 0.43634144\n",
      "Iteration 35, loss = 0.43642991\n",
      "Iteration 36, loss = 0.43497649\n",
      "Iteration 37, loss = 0.43351697\n",
      "Iteration 38, loss = 0.43313489\n",
      "Iteration 39, loss = 0.43191342\n",
      "Iteration 40, loss = 0.43133789\n",
      "Iteration 41, loss = 0.42895074\n",
      "Iteration 42, loss = 0.42730501\n",
      "Iteration 43, loss = 0.42855417\n",
      "Iteration 44, loss = 0.42716232\n",
      "Iteration 45, loss = 0.42803006\n",
      "Iteration 46, loss = 0.42699035\n",
      "Iteration 47, loss = 0.42807355\n",
      "Iteration 48, loss = 0.42557665\n",
      "Iteration 49, loss = 0.42648841\n",
      "Iteration 50, loss = 0.42779616\n",
      "Iteration 51, loss = 0.42764832\n",
      "Iteration 52, loss = 0.42549277\n",
      "Iteration 53, loss = 0.42623623\n",
      "Iteration 54, loss = 0.42480373\n",
      "Iteration 55, loss = 0.42445554\n",
      "Iteration 56, loss = 0.42514924\n",
      "Iteration 57, loss = 0.42542138\n",
      "Iteration 58, loss = 0.42658357\n",
      "Iteration 59, loss = 0.42282745\n",
      "Iteration 60, loss = 0.42387059\n",
      "Iteration 61, loss = 0.42451713\n",
      "Iteration 62, loss = 0.42420769\n",
      "Iteration 63, loss = 0.42499117\n",
      "Iteration 64, loss = 0.42275430\n",
      "Iteration 65, loss = 0.42215489\n",
      "Iteration 66, loss = 0.42562373\n",
      "Iteration 67, loss = 0.42440181\n",
      "Iteration 68, loss = 0.42242970\n",
      "Iteration 69, loss = 0.42416304\n",
      "Iteration 70, loss = 0.42527530\n",
      "Iteration 71, loss = 0.42618465\n",
      "Iteration 72, loss = 0.42438808\n",
      "Iteration 73, loss = 0.42456266\n",
      "Iteration 74, loss = 0.42343339\n",
      "Iteration 75, loss = 0.42316886\n",
      "Iteration 76, loss = 0.42402462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57513247\n",
      "Iteration 2, loss = 0.50246932\n",
      "Iteration 3, loss = 0.48938988\n",
      "Iteration 4, loss = 0.48293212\n",
      "Iteration 5, loss = 0.47864060\n",
      "Iteration 6, loss = 0.47472594\n",
      "Iteration 7, loss = 0.47347230\n",
      "Iteration 8, loss = 0.47079265\n",
      "Iteration 9, loss = 0.47010023\n",
      "Iteration 10, loss = 0.46665925\n",
      "Iteration 11, loss = 0.46553605\n",
      "Iteration 12, loss = 0.46601070\n",
      "Iteration 13, loss = 0.46606368\n",
      "Iteration 14, loss = 0.46328721\n",
      "Iteration 15, loss = 0.46131402\n",
      "Iteration 16, loss = 0.45983631\n",
      "Iteration 17, loss = 0.45971524\n",
      "Iteration 18, loss = 0.46039658\n",
      "Iteration 19, loss = 0.45889404\n",
      "Iteration 20, loss = 0.45684384\n",
      "Iteration 21, loss = 0.45156989\n",
      "Iteration 22, loss = 0.45127227\n",
      "Iteration 23, loss = 0.44678162\n",
      "Iteration 24, loss = 0.44422495\n",
      "Iteration 25, loss = 0.44450852\n",
      "Iteration 26, loss = 0.44238390\n",
      "Iteration 27, loss = 0.44372822\n",
      "Iteration 28, loss = 0.44091800\n",
      "Iteration 29, loss = 0.43849635\n",
      "Iteration 30, loss = 0.43759638\n",
      "Iteration 31, loss = 0.43970100\n",
      "Iteration 32, loss = 0.43712044\n",
      "Iteration 33, loss = 0.44127672\n",
      "Iteration 34, loss = 0.43843041\n",
      "Iteration 35, loss = 0.44048628\n",
      "Iteration 36, loss = 0.44093722\n",
      "Iteration 37, loss = 0.43837682\n",
      "Iteration 38, loss = 0.43542495\n",
      "Iteration 39, loss = 0.43737744\n",
      "Iteration 40, loss = 0.43605281\n",
      "Iteration 41, loss = 0.43644962\n",
      "Iteration 42, loss = 0.43838431\n",
      "Iteration 43, loss = 0.43855737\n",
      "Iteration 44, loss = 0.43666322\n",
      "Iteration 45, loss = 0.43452952\n",
      "Iteration 46, loss = 0.43675023\n",
      "Iteration 47, loss = 0.43537548\n",
      "Iteration 48, loss = 0.43366304\n",
      "Iteration 49, loss = 0.43575848\n",
      "Iteration 50, loss = 0.43282860\n",
      "Iteration 51, loss = 0.43280464\n",
      "Iteration 52, loss = 0.43093398\n",
      "Iteration 53, loss = 0.43348091\n",
      "Iteration 54, loss = 0.43345105\n",
      "Iteration 55, loss = 0.43088432\n",
      "Iteration 56, loss = 0.43062312\n",
      "Iteration 57, loss = 0.43095066\n",
      "Iteration 58, loss = 0.43180555\n",
      "Iteration 59, loss = 0.43468098\n",
      "Iteration 60, loss = 0.43308495\n",
      "Iteration 61, loss = 0.43091995\n",
      "Iteration 62, loss = 0.43439919\n",
      "Iteration 63, loss = 0.43297383\n",
      "Iteration 64, loss = 0.43139873\n",
      "Iteration 65, loss = 0.43283251\n",
      "Iteration 66, loss = 0.43202120\n",
      "Iteration 67, loss = 0.43285244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
      "Score: 0.7841228639411638\n",
      "Iteration 1, loss = 0.58153212\n",
      "Iteration 2, loss = 0.50509132\n",
      "Iteration 3, loss = 0.49132220\n",
      "Iteration 4, loss = 0.48574040\n",
      "Iteration 5, loss = 0.48403165\n",
      "Iteration 6, loss = 0.47686388\n",
      "Iteration 7, loss = 0.47434099\n",
      "Iteration 8, loss = 0.47321181\n",
      "Iteration 9, loss = 0.47245582\n",
      "Iteration 10, loss = 0.47043902\n",
      "Iteration 11, loss = 0.46860668\n",
      "Iteration 12, loss = 0.46516875\n",
      "Iteration 13, loss = 0.45926675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.45568535\n",
      "Iteration 15, loss = 0.45379502\n",
      "Iteration 16, loss = 0.45232118\n",
      "Iteration 17, loss = 0.44761389\n",
      "Iteration 18, loss = 0.44464679\n",
      "Iteration 19, loss = 0.44396211\n",
      "Iteration 20, loss = 0.44331480\n",
      "Iteration 21, loss = 0.44581736\n",
      "Iteration 22, loss = 0.44238321\n",
      "Iteration 23, loss = 0.43883351\n",
      "Iteration 24, loss = 0.43524349\n",
      "Iteration 25, loss = 0.43775546\n",
      "Iteration 26, loss = 0.43281251\n",
      "Iteration 27, loss = 0.43120793\n",
      "Iteration 28, loss = 0.43378157\n",
      "Iteration 29, loss = 0.42991111\n",
      "Iteration 30, loss = 0.43180181\n",
      "Iteration 31, loss = 0.43206168\n",
      "Iteration 32, loss = 0.43677869\n",
      "Iteration 33, loss = 0.43056504\n",
      "Iteration 34, loss = 0.42868616\n",
      "Iteration 35, loss = 0.42760621\n",
      "Iteration 36, loss = 0.42468261\n",
      "Iteration 37, loss = 0.42583579\n",
      "Iteration 38, loss = 0.42706764\n",
      "Iteration 39, loss = 0.42524004\n",
      "Iteration 40, loss = 0.42530440\n",
      "Iteration 41, loss = 0.42636472\n",
      "Iteration 42, loss = 0.42880072\n",
      "Iteration 43, loss = 0.42421033\n",
      "Iteration 44, loss = 0.42397113\n",
      "Iteration 45, loss = 0.42376743\n",
      "Iteration 46, loss = 0.42429787\n",
      "Iteration 47, loss = 0.42200446\n",
      "Iteration 48, loss = 0.42662295\n",
      "Iteration 49, loss = 0.42516120\n",
      "Iteration 50, loss = 0.42237051\n",
      "Iteration 51, loss = 0.42103315\n",
      "Iteration 52, loss = 0.42078384\n",
      "Iteration 53, loss = 0.42467340\n",
      "Iteration 54, loss = 0.42221236\n",
      "Iteration 55, loss = 0.41993328\n",
      "Iteration 56, loss = 0.42224737\n",
      "Iteration 57, loss = 0.42045227\n",
      "Iteration 58, loss = 0.41777130\n",
      "Iteration 59, loss = 0.42175059\n",
      "Iteration 60, loss = 0.41860797\n",
      "Iteration 61, loss = 0.41952358\n",
      "Iteration 62, loss = 0.41870954\n",
      "Iteration 63, loss = 0.41767124\n",
      "Iteration 64, loss = 0.42096746\n",
      "Iteration 65, loss = 0.41850764\n",
      "Iteration 66, loss = 0.42027375\n",
      "Iteration 67, loss = 0.41810271\n",
      "Iteration 68, loss = 0.41701807\n",
      "Iteration 69, loss = 0.42144287\n",
      "Iteration 70, loss = 0.41653109\n",
      "Iteration 71, loss = 0.41971799\n",
      "Iteration 72, loss = 0.42063526\n",
      "Iteration 73, loss = 0.41815122\n",
      "Iteration 74, loss = 0.41973864\n",
      "Iteration 75, loss = 0.42057627\n",
      "Iteration 76, loss = 0.41625656\n",
      "Iteration 77, loss = 0.41897454\n",
      "Iteration 78, loss = 0.41941413\n",
      "Iteration 79, loss = 0.41724605\n",
      "Iteration 80, loss = 0.41563962\n",
      "Iteration 81, loss = 0.41732773\n",
      "Iteration 82, loss = 0.42047464\n",
      "Iteration 83, loss = 0.41623885\n",
      "Iteration 84, loss = 0.41712349\n",
      "Iteration 85, loss = 0.41585190\n",
      "Iteration 86, loss = 0.41753775\n",
      "Iteration 87, loss = 0.41744064\n",
      "Iteration 88, loss = 0.41569627\n",
      "Iteration 89, loss = 0.41877493\n",
      "Iteration 90, loss = 0.41922107\n",
      "Iteration 91, loss = 0.41496519\n",
      "Iteration 92, loss = 0.41517015\n",
      "Iteration 93, loss = 0.41609383\n",
      "Iteration 94, loss = 0.41591347\n",
      "Iteration 95, loss = 0.41778539\n",
      "Iteration 96, loss = 0.41659866\n",
      "Iteration 97, loss = 0.41936927\n",
      "Iteration 98, loss = 0.41743248\n",
      "Iteration 99, loss = 0.41647824\n",
      "Iteration 100, loss = 0.41384019\n",
      "Iteration 101, loss = 0.41524970\n",
      "Iteration 102, loss = 0.41492007\n",
      "Iteration 103, loss = 0.41814182\n",
      "Iteration 104, loss = 0.41832703\n",
      "Iteration 105, loss = 0.42033535\n",
      "Iteration 106, loss = 0.41731465\n",
      "Iteration 107, loss = 0.41657857\n",
      "Iteration 108, loss = 0.41772449\n",
      "Iteration 109, loss = 0.41953221\n",
      "Iteration 110, loss = 0.41549855\n",
      "Iteration 111, loss = 0.41563932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58671776\n",
      "Iteration 2, loss = 0.50369497\n",
      "Iteration 3, loss = 0.48613660\n",
      "Iteration 4, loss = 0.48066568\n",
      "Iteration 5, loss = 0.47751503\n",
      "Iteration 6, loss = 0.47228251\n",
      "Iteration 7, loss = 0.47220442\n",
      "Iteration 8, loss = 0.47210401\n",
      "Iteration 9, loss = 0.46656012\n",
      "Iteration 10, loss = 0.46184145\n",
      "Iteration 11, loss = 0.45692239\n",
      "Iteration 12, loss = 0.45154728\n",
      "Iteration 13, loss = 0.45030630\n",
      "Iteration 14, loss = 0.44666099\n",
      "Iteration 15, loss = 0.44562870\n",
      "Iteration 16, loss = 0.44602411\n",
      "Iteration 17, loss = 0.44232550\n",
      "Iteration 18, loss = 0.44309159\n",
      "Iteration 19, loss = 0.44188853\n",
      "Iteration 20, loss = 0.44413286\n",
      "Iteration 21, loss = 0.44184812\n",
      "Iteration 22, loss = 0.44423179\n",
      "Iteration 23, loss = 0.44056299\n",
      "Iteration 24, loss = 0.44058381\n",
      "Iteration 25, loss = 0.44079896\n",
      "Iteration 26, loss = 0.43716387\n",
      "Iteration 27, loss = 0.44048880\n",
      "Iteration 28, loss = 0.44117699\n",
      "Iteration 29, loss = 0.44268443\n",
      "Iteration 30, loss = 0.43833191\n",
      "Iteration 31, loss = 0.43350773\n",
      "Iteration 32, loss = 0.43492327\n",
      "Iteration 33, loss = 0.43903426\n",
      "Iteration 34, loss = 0.43537974\n",
      "Iteration 35, loss = 0.43322007\n",
      "Iteration 36, loss = 0.43602603\n",
      "Iteration 37, loss = 0.43417212\n",
      "Iteration 38, loss = 0.43341186\n",
      "Iteration 39, loss = 0.43193297\n",
      "Iteration 40, loss = 0.43128264\n",
      "Iteration 41, loss = 0.43040616\n",
      "Iteration 42, loss = 0.43052851\n",
      "Iteration 43, loss = 0.43067187\n",
      "Iteration 44, loss = 0.42923210\n",
      "Iteration 45, loss = 0.43068000\n",
      "Iteration 46, loss = 0.43064163\n",
      "Iteration 47, loss = 0.42926874\n",
      "Iteration 48, loss = 0.42944642\n",
      "Iteration 49, loss = 0.43065297\n",
      "Iteration 50, loss = 0.43032306\n",
      "Iteration 51, loss = 0.42925259\n",
      "Iteration 52, loss = 0.42681802\n",
      "Iteration 53, loss = 0.42823109\n",
      "Iteration 54, loss = 0.42776975\n",
      "Iteration 55, loss = 0.42839721\n",
      "Iteration 56, loss = 0.42639382\n",
      "Iteration 57, loss = 0.42816602\n",
      "Iteration 58, loss = 0.43038811\n",
      "Iteration 59, loss = 0.42706087\n",
      "Iteration 60, loss = 0.42815538\n",
      "Iteration 61, loss = 0.42689784\n",
      "Iteration 62, loss = 0.42718491\n",
      "Iteration 63, loss = 0.42922448\n",
      "Iteration 64, loss = 0.42617405\n",
      "Iteration 65, loss = 0.42615260\n",
      "Iteration 66, loss = 0.42875069\n",
      "Iteration 67, loss = 0.42631056\n",
      "Iteration 68, loss = 0.42445957\n",
      "Iteration 69, loss = 0.42698223\n",
      "Iteration 70, loss = 0.43123561\n",
      "Iteration 71, loss = 0.42703831\n",
      "Iteration 72, loss = 0.42621296\n",
      "Iteration 73, loss = 0.42618875\n",
      "Iteration 74, loss = 0.42577345\n",
      "Iteration 75, loss = 0.42660965\n",
      "Iteration 76, loss = 0.42851530\n",
      "Iteration 77, loss = 0.42660595\n",
      "Iteration 78, loss = 0.42786751\n",
      "Iteration 79, loss = 0.42565746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57891696\n",
      "Iteration 2, loss = 0.50020692\n",
      "Iteration 3, loss = 0.48447053\n",
      "Iteration 4, loss = 0.47850995\n",
      "Iteration 5, loss = 0.47548191\n",
      "Iteration 6, loss = 0.47213803\n",
      "Iteration 7, loss = 0.47039589\n",
      "Iteration 8, loss = 0.46665918\n",
      "Iteration 9, loss = 0.46324252\n",
      "Iteration 10, loss = 0.45880340\n",
      "Iteration 11, loss = 0.45680189\n",
      "Iteration 12, loss = 0.45447940\n",
      "Iteration 13, loss = 0.45470745\n",
      "Iteration 14, loss = 0.45241316\n",
      "Iteration 15, loss = 0.45357341\n",
      "Iteration 16, loss = 0.44848858\n",
      "Iteration 17, loss = 0.44832115\n",
      "Iteration 18, loss = 0.44732123\n",
      "Iteration 19, loss = 0.44529353\n",
      "Iteration 20, loss = 0.44534587\n",
      "Iteration 21, loss = 0.44206902\n",
      "Iteration 22, loss = 0.44224254\n",
      "Iteration 23, loss = 0.43689844\n",
      "Iteration 24, loss = 0.43654047\n",
      "Iteration 25, loss = 0.43429728\n",
      "Iteration 26, loss = 0.43254580\n",
      "Iteration 27, loss = 0.43047563\n",
      "Iteration 28, loss = 0.43078672\n",
      "Iteration 29, loss = 0.42725644\n",
      "Iteration 30, loss = 0.42783701\n",
      "Iteration 31, loss = 0.42805348\n",
      "Iteration 32, loss = 0.42679004\n",
      "Iteration 33, loss = 0.42908510\n",
      "Iteration 34, loss = 0.43082053\n",
      "Iteration 35, loss = 0.42985814\n",
      "Iteration 36, loss = 0.43033509\n",
      "Iteration 37, loss = 0.42952990\n",
      "Iteration 38, loss = 0.42752470\n",
      "Iteration 39, loss = 0.42399931\n",
      "Iteration 40, loss = 0.42504573\n",
      "Iteration 41, loss = 0.42458637\n",
      "Iteration 42, loss = 0.42841111\n",
      "Iteration 43, loss = 0.42759681\n",
      "Iteration 44, loss = 0.42426284\n",
      "Iteration 45, loss = 0.42514243\n",
      "Iteration 46, loss = 0.42420048\n",
      "Iteration 47, loss = 0.42452286\n",
      "Iteration 48, loss = 0.42556695\n",
      "Iteration 49, loss = 0.42443449\n",
      "Iteration 50, loss = 0.42324679\n",
      "Iteration 51, loss = 0.42275167\n",
      "Iteration 52, loss = 0.42301319\n",
      "Iteration 53, loss = 0.42311151\n",
      "Iteration 54, loss = 0.42218057\n",
      "Iteration 55, loss = 0.42216593\n",
      "Iteration 56, loss = 0.42363000\n",
      "Iteration 57, loss = 0.42599605\n",
      "Iteration 58, loss = 0.42346657\n",
      "Iteration 59, loss = 0.42455662\n",
      "Iteration 60, loss = 0.42199173\n",
      "Iteration 61, loss = 0.42360834\n",
      "Iteration 62, loss = 0.42349851\n",
      "Iteration 63, loss = 0.42271364\n",
      "Iteration 64, loss = 0.42312121\n",
      "Iteration 65, loss = 0.42376112\n",
      "Iteration 66, loss = 0.42402888\n",
      "Iteration 67, loss = 0.42598141\n",
      "Iteration 68, loss = 0.42506560\n",
      "Iteration 69, loss = 0.42195355\n",
      "Iteration 70, loss = 0.42300044\n",
      "Iteration 71, loss = 0.42215290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'sex']\n",
      "Score: 0.7919100151416828\n",
      "Iteration 1, loss = 0.57114388\n",
      "Iteration 2, loss = 0.50712880\n",
      "Iteration 3, loss = 0.49111128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.48225224\n",
      "Iteration 5, loss = 0.48192635\n",
      "Iteration 6, loss = 0.47385249\n",
      "Iteration 7, loss = 0.47348354\n",
      "Iteration 8, loss = 0.47157984\n",
      "Iteration 9, loss = 0.47103623\n",
      "Iteration 10, loss = 0.46639629\n",
      "Iteration 11, loss = 0.46690235\n",
      "Iteration 12, loss = 0.46639040\n",
      "Iteration 13, loss = 0.46413469\n",
      "Iteration 14, loss = 0.46166982\n",
      "Iteration 15, loss = 0.46180431\n",
      "Iteration 16, loss = 0.46038506\n",
      "Iteration 17, loss = 0.45802302\n",
      "Iteration 18, loss = 0.45811468\n",
      "Iteration 19, loss = 0.45576728\n",
      "Iteration 20, loss = 0.45602834\n",
      "Iteration 21, loss = 0.45451533\n",
      "Iteration 22, loss = 0.45246402\n",
      "Iteration 23, loss = 0.45199238\n",
      "Iteration 24, loss = 0.44854902\n",
      "Iteration 25, loss = 0.44681290\n",
      "Iteration 26, loss = 0.44663294\n",
      "Iteration 27, loss = 0.44313465\n",
      "Iteration 28, loss = 0.44230575\n",
      "Iteration 29, loss = 0.43996896\n",
      "Iteration 30, loss = 0.44126758\n",
      "Iteration 31, loss = 0.43906068\n",
      "Iteration 32, loss = 0.43980650\n",
      "Iteration 33, loss = 0.43771488\n",
      "Iteration 34, loss = 0.43694494\n",
      "Iteration 35, loss = 0.43886057\n",
      "Iteration 36, loss = 0.43900394\n",
      "Iteration 37, loss = 0.43469084\n",
      "Iteration 38, loss = 0.43697788\n",
      "Iteration 39, loss = 0.43400971\n",
      "Iteration 40, loss = 0.43433849\n",
      "Iteration 41, loss = 0.43380950\n",
      "Iteration 42, loss = 0.43548429\n",
      "Iteration 43, loss = 0.43446577\n",
      "Iteration 44, loss = 0.43415938\n",
      "Iteration 45, loss = 0.43545538\n",
      "Iteration 46, loss = 0.43484435\n",
      "Iteration 47, loss = 0.43126691\n",
      "Iteration 48, loss = 0.42885031\n",
      "Iteration 49, loss = 0.43100908\n",
      "Iteration 50, loss = 0.42951463\n",
      "Iteration 51, loss = 0.43031164\n",
      "Iteration 52, loss = 0.42873661\n",
      "Iteration 53, loss = 0.43251480\n",
      "Iteration 54, loss = 0.42575133\n",
      "Iteration 55, loss = 0.42603439\n",
      "Iteration 56, loss = 0.42852454\n",
      "Iteration 57, loss = 0.42617774\n",
      "Iteration 58, loss = 0.42348855\n",
      "Iteration 59, loss = 0.42722494\n",
      "Iteration 60, loss = 0.42392721\n",
      "Iteration 61, loss = 0.42350277\n",
      "Iteration 62, loss = 0.42418210\n",
      "Iteration 63, loss = 0.42424369\n",
      "Iteration 64, loss = 0.42477591\n",
      "Iteration 65, loss = 0.42424929\n",
      "Iteration 66, loss = 0.42246869\n",
      "Iteration 67, loss = 0.42236272\n",
      "Iteration 68, loss = 0.42380349\n",
      "Iteration 69, loss = 0.42589556\n",
      "Iteration 70, loss = 0.42686021\n",
      "Iteration 71, loss = 0.42402702\n",
      "Iteration 72, loss = 0.42185938\n",
      "Iteration 73, loss = 0.42389534\n",
      "Iteration 74, loss = 0.42376154\n",
      "Iteration 75, loss = 0.42512347\n",
      "Iteration 76, loss = 0.42430731\n",
      "Iteration 77, loss = 0.42280327\n",
      "Iteration 78, loss = 0.42260136\n",
      "Iteration 79, loss = 0.42319965\n",
      "Iteration 80, loss = 0.42208349\n",
      "Iteration 81, loss = 0.42225807\n",
      "Iteration 82, loss = 0.42375152\n",
      "Iteration 83, loss = 0.42272586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57843336\n",
      "Iteration 2, loss = 0.50499363\n",
      "Iteration 3, loss = 0.48548886\n",
      "Iteration 4, loss = 0.48055551\n",
      "Iteration 5, loss = 0.47634605\n",
      "Iteration 6, loss = 0.47162190\n",
      "Iteration 7, loss = 0.47137131\n",
      "Iteration 8, loss = 0.47283602\n",
      "Iteration 9, loss = 0.46968296\n",
      "Iteration 10, loss = 0.46607386\n",
      "Iteration 11, loss = 0.46606730\n",
      "Iteration 12, loss = 0.46387990\n",
      "Iteration 13, loss = 0.46391176\n",
      "Iteration 14, loss = 0.46042058\n",
      "Iteration 15, loss = 0.45872057\n",
      "Iteration 16, loss = 0.45898438\n",
      "Iteration 17, loss = 0.45559226\n",
      "Iteration 18, loss = 0.45536605\n",
      "Iteration 19, loss = 0.45321881\n",
      "Iteration 20, loss = 0.45301252\n",
      "Iteration 21, loss = 0.45198603\n",
      "Iteration 22, loss = 0.45002792\n",
      "Iteration 23, loss = 0.44497293\n",
      "Iteration 24, loss = 0.44487713\n",
      "Iteration 25, loss = 0.44792518\n",
      "Iteration 26, loss = 0.44128372\n",
      "Iteration 27, loss = 0.44190489\n",
      "Iteration 28, loss = 0.44469300\n",
      "Iteration 29, loss = 0.43870762\n",
      "Iteration 30, loss = 0.43862703\n",
      "Iteration 31, loss = 0.43378537\n",
      "Iteration 32, loss = 0.43868715\n",
      "Iteration 33, loss = 0.44248430\n",
      "Iteration 34, loss = 0.43909011\n",
      "Iteration 35, loss = 0.43469686\n",
      "Iteration 36, loss = 0.43966268\n",
      "Iteration 37, loss = 0.43057878\n",
      "Iteration 38, loss = 0.42717017\n",
      "Iteration 39, loss = 0.42605827\n",
      "Iteration 40, loss = 0.42411044\n",
      "Iteration 41, loss = 0.42381421\n",
      "Iteration 42, loss = 0.42290882\n",
      "Iteration 43, loss = 0.42311326\n",
      "Iteration 44, loss = 0.42179863\n",
      "Iteration 45, loss = 0.42122395\n",
      "Iteration 46, loss = 0.41939336\n",
      "Iteration 47, loss = 0.41908861\n",
      "Iteration 48, loss = 0.41776325\n",
      "Iteration 49, loss = 0.41927470\n",
      "Iteration 50, loss = 0.41735354\n",
      "Iteration 51, loss = 0.41670068\n",
      "Iteration 52, loss = 0.41742506\n",
      "Iteration 53, loss = 0.41762023\n",
      "Iteration 54, loss = 0.41364814\n",
      "Iteration 55, loss = 0.41601546\n",
      "Iteration 56, loss = 0.41459850\n",
      "Iteration 57, loss = 0.41952709\n",
      "Iteration 58, loss = 0.41503616\n",
      "Iteration 59, loss = 0.41344386\n",
      "Iteration 60, loss = 0.41265052\n",
      "Iteration 61, loss = 0.41195027\n",
      "Iteration 62, loss = 0.41399352\n",
      "Iteration 63, loss = 0.41403722\n",
      "Iteration 64, loss = 0.41127667\n",
      "Iteration 65, loss = 0.41223613\n",
      "Iteration 66, loss = 0.41243736\n",
      "Iteration 67, loss = 0.41022160\n",
      "Iteration 68, loss = 0.41047092\n",
      "Iteration 69, loss = 0.41140973\n",
      "Iteration 70, loss = 0.41248367\n",
      "Iteration 71, loss = 0.41054544\n",
      "Iteration 72, loss = 0.41057558\n",
      "Iteration 73, loss = 0.41038403\n",
      "Iteration 74, loss = 0.41107556\n",
      "Iteration 75, loss = 0.41183446\n",
      "Iteration 76, loss = 0.41079900\n",
      "Iteration 77, loss = 0.40999576\n",
      "Iteration 78, loss = 0.40934132\n",
      "Iteration 79, loss = 0.41084756\n",
      "Iteration 80, loss = 0.41081208\n",
      "Iteration 81, loss = 0.41071180\n",
      "Iteration 82, loss = 0.40971224\n",
      "Iteration 83, loss = 0.41356784\n",
      "Iteration 84, loss = 0.40953212\n",
      "Iteration 85, loss = 0.41002782\n",
      "Iteration 86, loss = 0.40873218\n",
      "Iteration 87, loss = 0.40915708\n",
      "Iteration 88, loss = 0.40930471\n",
      "Iteration 89, loss = 0.40986442\n",
      "Iteration 90, loss = 0.40852554\n",
      "Iteration 91, loss = 0.40860372\n",
      "Iteration 92, loss = 0.40737883\n",
      "Iteration 93, loss = 0.40717521\n",
      "Iteration 94, loss = 0.40786388\n",
      "Iteration 95, loss = 0.40775964\n",
      "Iteration 96, loss = 0.40909910\n",
      "Iteration 97, loss = 0.40794649\n",
      "Iteration 98, loss = 0.40789859\n",
      "Iteration 99, loss = 0.40728190\n",
      "Iteration 100, loss = 0.41087124\n",
      "Iteration 101, loss = 0.40935963\n",
      "Iteration 102, loss = 0.40925428\n",
      "Iteration 103, loss = 0.40935402\n",
      "Iteration 104, loss = 0.40759220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57346104\n",
      "Iteration 2, loss = 0.50190479\n",
      "Iteration 3, loss = 0.48538864\n",
      "Iteration 4, loss = 0.47834480\n",
      "Iteration 5, loss = 0.47598262\n",
      "Iteration 6, loss = 0.47257523\n",
      "Iteration 7, loss = 0.47238373\n",
      "Iteration 8, loss = 0.47072107\n",
      "Iteration 9, loss = 0.46875156\n",
      "Iteration 10, loss = 0.46824276\n",
      "Iteration 11, loss = 0.46712537\n",
      "Iteration 12, loss = 0.46663585\n",
      "Iteration 13, loss = 0.46698217\n",
      "Iteration 14, loss = 0.46503449\n",
      "Iteration 15, loss = 0.46542004\n",
      "Iteration 16, loss = 0.46367386\n",
      "Iteration 17, loss = 0.46448065\n",
      "Iteration 18, loss = 0.46400778\n",
      "Iteration 19, loss = 0.46357388\n",
      "Iteration 20, loss = 0.45984016\n",
      "Iteration 21, loss = 0.45901846\n",
      "Iteration 22, loss = 0.45824538\n",
      "Iteration 23, loss = 0.45622903\n",
      "Iteration 24, loss = 0.45650312\n",
      "Iteration 25, loss = 0.45601818\n",
      "Iteration 26, loss = 0.45478832\n",
      "Iteration 27, loss = 0.45422274\n",
      "Iteration 28, loss = 0.45418605\n",
      "Iteration 29, loss = 0.45024078\n",
      "Iteration 30, loss = 0.44986815\n",
      "Iteration 31, loss = 0.44731682\n",
      "Iteration 32, loss = 0.44633396\n",
      "Iteration 33, loss = 0.44560038\n",
      "Iteration 34, loss = 0.44565096\n",
      "Iteration 35, loss = 0.44105445\n",
      "Iteration 36, loss = 0.44236999\n",
      "Iteration 37, loss = 0.43845635\n",
      "Iteration 38, loss = 0.43887134\n",
      "Iteration 39, loss = 0.43599827\n",
      "Iteration 40, loss = 0.43507541\n",
      "Iteration 41, loss = 0.43379365\n",
      "Iteration 42, loss = 0.43849936\n",
      "Iteration 43, loss = 0.43522838\n",
      "Iteration 44, loss = 0.43495089\n",
      "Iteration 45, loss = 0.43083176\n",
      "Iteration 46, loss = 0.43104762\n",
      "Iteration 47, loss = 0.42993847\n",
      "Iteration 48, loss = 0.42973886\n",
      "Iteration 49, loss = 0.43045864\n",
      "Iteration 50, loss = 0.42781523\n",
      "Iteration 51, loss = 0.42754112\n",
      "Iteration 52, loss = 0.42747292\n",
      "Iteration 53, loss = 0.42730212\n",
      "Iteration 54, loss = 0.42987015\n",
      "Iteration 55, loss = 0.42657485\n",
      "Iteration 56, loss = 0.42527175\n",
      "Iteration 57, loss = 0.42456163\n",
      "Iteration 58, loss = 0.42297611\n",
      "Iteration 59, loss = 0.42599201\n",
      "Iteration 60, loss = 0.42375123\n",
      "Iteration 61, loss = 0.42143192\n",
      "Iteration 62, loss = 0.42203541\n",
      "Iteration 63, loss = 0.42088848\n",
      "Iteration 64, loss = 0.42121524\n",
      "Iteration 65, loss = 0.42113027\n",
      "Iteration 66, loss = 0.42453188\n",
      "Iteration 67, loss = 0.42292447\n",
      "Iteration 68, loss = 0.42051652\n",
      "Iteration 69, loss = 0.41927883\n",
      "Iteration 70, loss = 0.41958032\n",
      "Iteration 71, loss = 0.42004648\n",
      "Iteration 72, loss = 0.41985655\n",
      "Iteration 73, loss = 0.41821023\n",
      "Iteration 74, loss = 0.41914954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 75, loss = 0.42200252\n",
      "Iteration 76, loss = 0.41906809\n",
      "Iteration 77, loss = 0.42060675\n",
      "Iteration 78, loss = 0.42034246\n",
      "Iteration 79, loss = 0.42055757\n",
      "Iteration 80, loss = 0.41954270\n",
      "Iteration 81, loss = 0.42029166\n",
      "Iteration 82, loss = 0.41797070\n",
      "Iteration 83, loss = 0.41959204\n",
      "Iteration 84, loss = 0.42021327\n",
      "Iteration 85, loss = 0.42066802\n",
      "Iteration 86, loss = 0.41932488\n",
      "Iteration 87, loss = 0.42026187\n",
      "Iteration 88, loss = 0.41876488\n",
      "Iteration 89, loss = 0.42027042\n",
      "Iteration 90, loss = 0.41812944\n",
      "Iteration 91, loss = 0.42056066\n",
      "Iteration 92, loss = 0.41941529\n",
      "Iteration 93, loss = 0.42053832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'occupation', 'race', 'sex']\n",
      "Score: 0.7945057322085226\n",
      "Iteration 1, loss = 0.56240736\n",
      "Iteration 2, loss = 0.50366215\n",
      "Iteration 3, loss = 0.49434575\n",
      "Iteration 4, loss = 0.49074956\n",
      "Iteration 5, loss = 0.48765931\n",
      "Iteration 6, loss = 0.48453934\n",
      "Iteration 7, loss = 0.48451082\n",
      "Iteration 8, loss = 0.48486536\n",
      "Iteration 9, loss = 0.48035448\n",
      "Iteration 10, loss = 0.47843967\n",
      "Iteration 11, loss = 0.47730562\n",
      "Iteration 12, loss = 0.47576574\n",
      "Iteration 13, loss = 0.47254146\n",
      "Iteration 14, loss = 0.47037914\n",
      "Iteration 15, loss = 0.46759370\n",
      "Iteration 16, loss = 0.46559696\n",
      "Iteration 17, loss = 0.45956916\n",
      "Iteration 18, loss = 0.45669416\n",
      "Iteration 19, loss = 0.45429672\n",
      "Iteration 20, loss = 0.45101981\n",
      "Iteration 21, loss = 0.44817305\n",
      "Iteration 22, loss = 0.44665306\n",
      "Iteration 23, loss = 0.44730671\n",
      "Iteration 24, loss = 0.44561187\n",
      "Iteration 25, loss = 0.44657072\n",
      "Iteration 26, loss = 0.44559732\n",
      "Iteration 27, loss = 0.44335052\n",
      "Iteration 28, loss = 0.44423026\n",
      "Iteration 29, loss = 0.44279030\n",
      "Iteration 30, loss = 0.44418333\n",
      "Iteration 31, loss = 0.44329101\n",
      "Iteration 32, loss = 0.44692830\n",
      "Iteration 33, loss = 0.44450677\n",
      "Iteration 34, loss = 0.44331026\n",
      "Iteration 35, loss = 0.44235232\n",
      "Iteration 36, loss = 0.44387489\n",
      "Iteration 37, loss = 0.44024506\n",
      "Iteration 38, loss = 0.44474535\n",
      "Iteration 39, loss = 0.44173824\n",
      "Iteration 40, loss = 0.44143068\n",
      "Iteration 41, loss = 0.44060306\n",
      "Iteration 42, loss = 0.44069495\n",
      "Iteration 43, loss = 0.44094161\n",
      "Iteration 44, loss = 0.44065754\n",
      "Iteration 45, loss = 0.44132514\n",
      "Iteration 46, loss = 0.44038263\n",
      "Iteration 47, loss = 0.44116325\n",
      "Iteration 48, loss = 0.43973101\n",
      "Iteration 49, loss = 0.44130155\n",
      "Iteration 50, loss = 0.43836910\n",
      "Iteration 51, loss = 0.43839982\n",
      "Iteration 52, loss = 0.43649860\n",
      "Iteration 53, loss = 0.43785154\n",
      "Iteration 54, loss = 0.43645656\n",
      "Iteration 55, loss = 0.43662378\n",
      "Iteration 56, loss = 0.43770296\n",
      "Iteration 57, loss = 0.43690264\n",
      "Iteration 58, loss = 0.43409170\n",
      "Iteration 59, loss = 0.43485967\n",
      "Iteration 60, loss = 0.43592683\n",
      "Iteration 61, loss = 0.43552886\n",
      "Iteration 62, loss = 0.43586674\n",
      "Iteration 63, loss = 0.43437547\n",
      "Iteration 64, loss = 0.43537042\n",
      "Iteration 65, loss = 0.43501705\n",
      "Iteration 66, loss = 0.43505476\n",
      "Iteration 67, loss = 0.43247936\n",
      "Iteration 68, loss = 0.43158761\n",
      "Iteration 69, loss = 0.43577809\n",
      "Iteration 70, loss = 0.43440822\n",
      "Iteration 71, loss = 0.43107428\n",
      "Iteration 72, loss = 0.43096117\n",
      "Iteration 73, loss = 0.43142662\n",
      "Iteration 74, loss = 0.43146557\n",
      "Iteration 75, loss = 0.43188569\n",
      "Iteration 76, loss = 0.43069170\n",
      "Iteration 77, loss = 0.43056481\n",
      "Iteration 78, loss = 0.43105836\n",
      "Iteration 79, loss = 0.43016760\n",
      "Iteration 80, loss = 0.42914369\n",
      "Iteration 81, loss = 0.43045490\n",
      "Iteration 82, loss = 0.43202987\n",
      "Iteration 83, loss = 0.43132034\n",
      "Iteration 84, loss = 0.43033343\n",
      "Iteration 85, loss = 0.42809937\n",
      "Iteration 86, loss = 0.42834564\n",
      "Iteration 87, loss = 0.42932836\n",
      "Iteration 88, loss = 0.42696109\n",
      "Iteration 89, loss = 0.42775639\n",
      "Iteration 90, loss = 0.42815131\n",
      "Iteration 91, loss = 0.42759110\n",
      "Iteration 92, loss = 0.42812722\n",
      "Iteration 93, loss = 0.43050677\n",
      "Iteration 94, loss = 0.42879370\n",
      "Iteration 95, loss = 0.42737229\n",
      "Iteration 96, loss = 0.42881595\n",
      "Iteration 97, loss = 0.42788321\n",
      "Iteration 98, loss = 0.42803199\n",
      "Iteration 99, loss = 0.42731488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56508470\n",
      "Iteration 2, loss = 0.50121369\n",
      "Iteration 3, loss = 0.49061226\n",
      "Iteration 4, loss = 0.48791368\n",
      "Iteration 5, loss = 0.48486789\n",
      "Iteration 6, loss = 0.48010202\n",
      "Iteration 7, loss = 0.47836821\n",
      "Iteration 8, loss = 0.47951099\n",
      "Iteration 9, loss = 0.47373392\n",
      "Iteration 10, loss = 0.47113653\n",
      "Iteration 11, loss = 0.46938723\n",
      "Iteration 12, loss = 0.46645486\n",
      "Iteration 13, loss = 0.46505232\n",
      "Iteration 14, loss = 0.46088003\n",
      "Iteration 15, loss = 0.45947560\n",
      "Iteration 16, loss = 0.45953436\n",
      "Iteration 17, loss = 0.45538255\n",
      "Iteration 18, loss = 0.45458013\n",
      "Iteration 19, loss = 0.45216040\n",
      "Iteration 20, loss = 0.45055607\n",
      "Iteration 21, loss = 0.44548313\n",
      "Iteration 22, loss = 0.44482650\n",
      "Iteration 23, loss = 0.44449306\n",
      "Iteration 24, loss = 0.43961750\n",
      "Iteration 25, loss = 0.44267072\n",
      "Iteration 26, loss = 0.44010272\n",
      "Iteration 27, loss = 0.44041982\n",
      "Iteration 28, loss = 0.44054318\n",
      "Iteration 29, loss = 0.43889588\n",
      "Iteration 30, loss = 0.43878690\n",
      "Iteration 31, loss = 0.43568384\n",
      "Iteration 32, loss = 0.43496217\n",
      "Iteration 33, loss = 0.43738017\n",
      "Iteration 34, loss = 0.43959394\n",
      "Iteration 35, loss = 0.43647633\n",
      "Iteration 36, loss = 0.43673910\n",
      "Iteration 37, loss = 0.43623158\n",
      "Iteration 38, loss = 0.43730960\n",
      "Iteration 39, loss = 0.43603000\n",
      "Iteration 40, loss = 0.43436188\n",
      "Iteration 41, loss = 0.43518234\n",
      "Iteration 42, loss = 0.43448946\n",
      "Iteration 43, loss = 0.43469479\n",
      "Iteration 44, loss = 0.43400142\n",
      "Iteration 45, loss = 0.43387551\n",
      "Iteration 46, loss = 0.43343371\n",
      "Iteration 47, loss = 0.43391421\n",
      "Iteration 48, loss = 0.43427153\n",
      "Iteration 49, loss = 0.43365304\n",
      "Iteration 50, loss = 0.43419367\n",
      "Iteration 51, loss = 0.43273548\n",
      "Iteration 52, loss = 0.43230527\n",
      "Iteration 53, loss = 0.43419217\n",
      "Iteration 54, loss = 0.43181114\n",
      "Iteration 55, loss = 0.43306738\n",
      "Iteration 56, loss = 0.43307194\n",
      "Iteration 57, loss = 0.43441011\n",
      "Iteration 58, loss = 0.43439136\n",
      "Iteration 59, loss = 0.43387560\n",
      "Iteration 60, loss = 0.43320286\n",
      "Iteration 61, loss = 0.43350142\n",
      "Iteration 62, loss = 0.43116714\n",
      "Iteration 63, loss = 0.43231385\n",
      "Iteration 64, loss = 0.43097964\n",
      "Iteration 65, loss = 0.43223701\n",
      "Iteration 66, loss = 0.43236383\n",
      "Iteration 67, loss = 0.43321328\n",
      "Iteration 68, loss = 0.43213303\n",
      "Iteration 69, loss = 0.43291201\n",
      "Iteration 70, loss = 0.43160641\n",
      "Iteration 71, loss = 0.43061810\n",
      "Iteration 72, loss = 0.43054556\n",
      "Iteration 73, loss = 0.43204248\n",
      "Iteration 74, loss = 0.43044860\n",
      "Iteration 75, loss = 0.43016509\n",
      "Iteration 76, loss = 0.43127748\n",
      "Iteration 77, loss = 0.43042768\n",
      "Iteration 78, loss = 0.43101569\n",
      "Iteration 79, loss = 0.43134012\n",
      "Iteration 80, loss = 0.43114127\n",
      "Iteration 81, loss = 0.43171867\n",
      "Iteration 82, loss = 0.43294313\n",
      "Iteration 83, loss = 0.43369966\n",
      "Iteration 84, loss = 0.43069334\n",
      "Iteration 85, loss = 0.43045996\n",
      "Iteration 86, loss = 0.43082173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55654774\n",
      "Iteration 2, loss = 0.50146315\n",
      "Iteration 3, loss = 0.49130940\n",
      "Iteration 4, loss = 0.48760455\n",
      "Iteration 5, loss = 0.48460576\n",
      "Iteration 6, loss = 0.48217353\n",
      "Iteration 7, loss = 0.48183875\n",
      "Iteration 8, loss = 0.47879298\n",
      "Iteration 9, loss = 0.47594973\n",
      "Iteration 10, loss = 0.47336589\n",
      "Iteration 11, loss = 0.47325937\n",
      "Iteration 12, loss = 0.47105281\n",
      "Iteration 13, loss = 0.46945277\n",
      "Iteration 14, loss = 0.46639885\n",
      "Iteration 15, loss = 0.46422106\n",
      "Iteration 16, loss = 0.45947722\n",
      "Iteration 17, loss = 0.45934676\n",
      "Iteration 18, loss = 0.45444797\n",
      "Iteration 19, loss = 0.45367078\n",
      "Iteration 20, loss = 0.44806051\n",
      "Iteration 21, loss = 0.44601961\n",
      "Iteration 22, loss = 0.44425736\n",
      "Iteration 23, loss = 0.44251368\n",
      "Iteration 24, loss = 0.44079165\n",
      "Iteration 25, loss = 0.43951399\n",
      "Iteration 26, loss = 0.44115088\n",
      "Iteration 27, loss = 0.44048475\n",
      "Iteration 28, loss = 0.44164755\n",
      "Iteration 29, loss = 0.43789033\n",
      "Iteration 30, loss = 0.43518974\n",
      "Iteration 31, loss = 0.43325967\n",
      "Iteration 32, loss = 0.43554208\n",
      "Iteration 33, loss = 0.43718384\n",
      "Iteration 34, loss = 0.43490268\n",
      "Iteration 35, loss = 0.43380660\n",
      "Iteration 36, loss = 0.43535254\n",
      "Iteration 37, loss = 0.43490399\n",
      "Iteration 38, loss = 0.43250903\n",
      "Iteration 39, loss = 0.42964340\n",
      "Iteration 40, loss = 0.43037925\n",
      "Iteration 41, loss = 0.43043884\n",
      "Iteration 42, loss = 0.43184609\n",
      "Iteration 43, loss = 0.43120073\n",
      "Iteration 44, loss = 0.43192804\n",
      "Iteration 45, loss = 0.43225621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.42765190\n",
      "Iteration 47, loss = 0.42787710\n",
      "Iteration 48, loss = 0.42801914\n",
      "Iteration 49, loss = 0.42802768\n",
      "Iteration 50, loss = 0.42911780\n",
      "Iteration 51, loss = 0.42924112\n",
      "Iteration 52, loss = 0.42714343\n",
      "Iteration 53, loss = 0.42722331\n",
      "Iteration 54, loss = 0.42894618\n",
      "Iteration 55, loss = 0.42681570\n",
      "Iteration 56, loss = 0.42889000\n",
      "Iteration 57, loss = 0.43042775\n",
      "Iteration 58, loss = 0.43010650\n",
      "Iteration 59, loss = 0.42752771\n",
      "Iteration 60, loss = 0.42702707\n",
      "Iteration 61, loss = 0.42737394\n",
      "Iteration 62, loss = 0.42729230\n",
      "Iteration 63, loss = 0.42693565\n",
      "Iteration 64, loss = 0.42810934\n",
      "Iteration 65, loss = 0.42779466\n",
      "Iteration 66, loss = 0.42740668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'marital-status', 'relationship', 'race', 'sex']\n",
      "Score: 0.7776335712740644\n",
      "Iteration 1, loss = 0.60788281\n",
      "Iteration 2, loss = 0.53781315\n",
      "Iteration 3, loss = 0.52733824\n",
      "Iteration 4, loss = 0.52510092\n",
      "Iteration 5, loss = 0.52014537\n",
      "Iteration 6, loss = 0.51463980\n",
      "Iteration 7, loss = 0.51309703\n",
      "Iteration 8, loss = 0.51208338\n",
      "Iteration 9, loss = 0.50829367\n",
      "Iteration 10, loss = 0.50236464\n",
      "Iteration 11, loss = 0.49829614\n",
      "Iteration 12, loss = 0.49651913\n",
      "Iteration 13, loss = 0.49425993\n",
      "Iteration 14, loss = 0.49219454\n",
      "Iteration 15, loss = 0.49392043\n",
      "Iteration 16, loss = 0.49056521\n",
      "Iteration 17, loss = 0.48598372\n",
      "Iteration 18, loss = 0.48325901\n",
      "Iteration 19, loss = 0.47955627\n",
      "Iteration 20, loss = 0.48053615\n",
      "Iteration 21, loss = 0.47367312\n",
      "Iteration 22, loss = 0.46694388\n",
      "Iteration 23, loss = 0.46700452\n",
      "Iteration 24, loss = 0.46361302\n",
      "Iteration 25, loss = 0.46445108\n",
      "Iteration 26, loss = 0.46167720\n",
      "Iteration 27, loss = 0.45722442\n",
      "Iteration 28, loss = 0.46058746\n",
      "Iteration 29, loss = 0.45880914\n",
      "Iteration 30, loss = 0.45919000\n",
      "Iteration 31, loss = 0.45705114\n",
      "Iteration 32, loss = 0.45729040\n",
      "Iteration 33, loss = 0.45708590\n",
      "Iteration 34, loss = 0.45501849\n",
      "Iteration 35, loss = 0.45433223\n",
      "Iteration 36, loss = 0.45621641\n",
      "Iteration 37, loss = 0.45379267\n",
      "Iteration 38, loss = 0.45508693\n",
      "Iteration 39, loss = 0.45299221\n",
      "Iteration 40, loss = 0.45312890\n",
      "Iteration 41, loss = 0.45636985\n",
      "Iteration 42, loss = 0.45271349\n",
      "Iteration 43, loss = 0.45317735\n",
      "Iteration 44, loss = 0.45242053\n",
      "Iteration 45, loss = 0.45210856\n",
      "Iteration 46, loss = 0.45149431\n",
      "Iteration 47, loss = 0.45019421\n",
      "Iteration 48, loss = 0.45000558\n",
      "Iteration 49, loss = 0.45278682\n",
      "Iteration 50, loss = 0.45513997\n",
      "Iteration 51, loss = 0.45143003\n",
      "Iteration 52, loss = 0.45298003\n",
      "Iteration 53, loss = 0.45472906\n",
      "Iteration 54, loss = 0.45302521\n",
      "Iteration 55, loss = 0.45268456\n",
      "Iteration 56, loss = 0.45035943\n",
      "Iteration 57, loss = 0.45090509\n",
      "Iteration 58, loss = 0.44951176\n",
      "Iteration 59, loss = 0.44956013\n",
      "Iteration 60, loss = 0.44990382\n",
      "Iteration 61, loss = 0.44971328\n",
      "Iteration 62, loss = 0.44843004\n",
      "Iteration 63, loss = 0.44734510\n",
      "Iteration 64, loss = 0.44976174\n",
      "Iteration 65, loss = 0.45022144\n",
      "Iteration 66, loss = 0.44925399\n",
      "Iteration 67, loss = 0.45126228\n",
      "Iteration 68, loss = 0.44963998\n",
      "Iteration 69, loss = 0.45194495\n",
      "Iteration 70, loss = 0.44957679\n",
      "Iteration 71, loss = 0.44928169\n",
      "Iteration 72, loss = 0.45201021\n",
      "Iteration 73, loss = 0.44699107\n",
      "Iteration 74, loss = 0.45093948\n",
      "Iteration 75, loss = 0.45263703\n",
      "Iteration 76, loss = 0.45059171\n",
      "Iteration 77, loss = 0.45049262\n",
      "Iteration 78, loss = 0.44944421\n",
      "Iteration 79, loss = 0.45092187\n",
      "Iteration 80, loss = 0.44966495\n",
      "Iteration 81, loss = 0.44975062\n",
      "Iteration 82, loss = 0.45449520\n",
      "Iteration 83, loss = 0.45241765\n",
      "Iteration 84, loss = 0.45193370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61189807\n",
      "Iteration 2, loss = 0.53843684\n",
      "Iteration 3, loss = 0.52517659\n",
      "Iteration 4, loss = 0.52095751\n",
      "Iteration 5, loss = 0.51528615\n",
      "Iteration 6, loss = 0.51015837\n",
      "Iteration 7, loss = 0.50697834\n",
      "Iteration 8, loss = 0.50464511\n",
      "Iteration 9, loss = 0.50155696\n",
      "Iteration 10, loss = 0.49872891\n",
      "Iteration 11, loss = 0.49592156\n",
      "Iteration 12, loss = 0.49206663\n",
      "Iteration 13, loss = 0.48866590\n",
      "Iteration 14, loss = 0.48338192\n",
      "Iteration 15, loss = 0.47909419\n",
      "Iteration 16, loss = 0.47414905\n",
      "Iteration 17, loss = 0.46290884\n",
      "Iteration 18, loss = 0.45653955\n",
      "Iteration 19, loss = 0.45220253\n",
      "Iteration 20, loss = 0.45115593\n",
      "Iteration 21, loss = 0.44828992\n",
      "Iteration 22, loss = 0.44792294\n",
      "Iteration 23, loss = 0.44484824\n",
      "Iteration 24, loss = 0.44341937\n",
      "Iteration 25, loss = 0.44497581\n",
      "Iteration 26, loss = 0.44319047\n",
      "Iteration 27, loss = 0.44277277\n",
      "Iteration 28, loss = 0.44309670\n",
      "Iteration 29, loss = 0.43799441\n",
      "Iteration 30, loss = 0.43897274\n",
      "Iteration 31, loss = 0.43632709\n",
      "Iteration 32, loss = 0.44079133\n",
      "Iteration 33, loss = 0.43766721\n",
      "Iteration 34, loss = 0.43844701\n",
      "Iteration 35, loss = 0.43696533\n",
      "Iteration 36, loss = 0.43685240\n",
      "Iteration 37, loss = 0.43413068\n",
      "Iteration 38, loss = 0.43793270\n",
      "Iteration 39, loss = 0.43508105\n",
      "Iteration 40, loss = 0.43362307\n",
      "Iteration 41, loss = 0.43485411\n",
      "Iteration 42, loss = 0.43479493\n",
      "Iteration 43, loss = 0.43455712\n",
      "Iteration 44, loss = 0.43411472\n",
      "Iteration 45, loss = 0.43513508\n",
      "Iteration 46, loss = 0.43313425\n",
      "Iteration 47, loss = 0.43434813\n",
      "Iteration 48, loss = 0.43298575\n",
      "Iteration 49, loss = 0.43376533\n",
      "Iteration 50, loss = 0.43426041\n",
      "Iteration 51, loss = 0.43295547\n",
      "Iteration 52, loss = 0.43173201\n",
      "Iteration 53, loss = 0.43523324\n",
      "Iteration 54, loss = 0.43158205\n",
      "Iteration 55, loss = 0.43138434\n",
      "Iteration 56, loss = 0.43177792\n",
      "Iteration 57, loss = 0.43436626\n",
      "Iteration 58, loss = 0.43571985\n",
      "Iteration 59, loss = 0.43505071\n",
      "Iteration 60, loss = 0.43186721\n",
      "Iteration 61, loss = 0.43209440\n",
      "Iteration 62, loss = 0.43101201\n",
      "Iteration 63, loss = 0.43141322\n",
      "Iteration 64, loss = 0.43130596\n",
      "Iteration 65, loss = 0.43188490\n",
      "Iteration 66, loss = 0.43279595\n",
      "Iteration 67, loss = 0.43197444\n",
      "Iteration 68, loss = 0.43122510\n",
      "Iteration 69, loss = 0.43144364\n",
      "Iteration 70, loss = 0.43101933\n",
      "Iteration 71, loss = 0.43167967\n",
      "Iteration 72, loss = 0.43092088\n",
      "Iteration 73, loss = 0.43393313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60326144\n",
      "Iteration 2, loss = 0.53498668\n",
      "Iteration 3, loss = 0.52526211\n",
      "Iteration 4, loss = 0.52131395\n",
      "Iteration 5, loss = 0.51823305\n",
      "Iteration 6, loss = 0.51535788\n",
      "Iteration 7, loss = 0.51310253\n",
      "Iteration 8, loss = 0.50928199\n",
      "Iteration 9, loss = 0.50638821\n",
      "Iteration 10, loss = 0.50361651\n",
      "Iteration 11, loss = 0.50019162\n",
      "Iteration 12, loss = 0.49690031\n",
      "Iteration 13, loss = 0.49482032\n",
      "Iteration 14, loss = 0.49274426\n",
      "Iteration 15, loss = 0.48754756\n",
      "Iteration 16, loss = 0.48251295\n",
      "Iteration 17, loss = 0.47847755\n",
      "Iteration 18, loss = 0.47396119\n",
      "Iteration 19, loss = 0.46937762\n",
      "Iteration 20, loss = 0.46450291\n",
      "Iteration 21, loss = 0.45982617\n",
      "Iteration 22, loss = 0.45825630\n",
      "Iteration 23, loss = 0.45312037\n",
      "Iteration 24, loss = 0.45165361\n",
      "Iteration 25, loss = 0.45057728\n",
      "Iteration 26, loss = 0.44767804\n",
      "Iteration 27, loss = 0.45018469\n",
      "Iteration 28, loss = 0.44952955\n",
      "Iteration 29, loss = 0.44443937\n",
      "Iteration 30, loss = 0.44475918\n",
      "Iteration 31, loss = 0.44291841\n",
      "Iteration 32, loss = 0.44386533\n",
      "Iteration 33, loss = 0.44532713\n",
      "Iteration 34, loss = 0.44278678\n",
      "Iteration 35, loss = 0.44371706\n",
      "Iteration 36, loss = 0.44515918\n",
      "Iteration 37, loss = 0.44409559\n",
      "Iteration 38, loss = 0.43967103\n",
      "Iteration 39, loss = 0.44045680\n",
      "Iteration 40, loss = 0.44152246\n",
      "Iteration 41, loss = 0.44200507\n",
      "Iteration 42, loss = 0.44393837\n",
      "Iteration 43, loss = 0.44700645\n",
      "Iteration 44, loss = 0.44206172\n",
      "Iteration 45, loss = 0.44033238\n",
      "Iteration 46, loss = 0.44014280\n",
      "Iteration 47, loss = 0.43868241\n",
      "Iteration 48, loss = 0.44070027\n",
      "Iteration 49, loss = 0.44006947\n",
      "Iteration 50, loss = 0.43678743\n",
      "Iteration 51, loss = 0.43897533\n",
      "Iteration 52, loss = 0.43547523\n",
      "Iteration 53, loss = 0.43481715\n",
      "Iteration 54, loss = 0.43369967\n",
      "Iteration 55, loss = 0.43419910\n",
      "Iteration 56, loss = 0.43343954\n",
      "Iteration 57, loss = 0.43365664\n",
      "Iteration 58, loss = 0.43622849\n",
      "Iteration 59, loss = 0.43422261\n",
      "Iteration 60, loss = 0.43443995\n",
      "Iteration 61, loss = 0.43140400\n",
      "Iteration 62, loss = 0.43196456\n",
      "Iteration 63, loss = 0.43177556\n",
      "Iteration 64, loss = 0.43406944\n",
      "Iteration 65, loss = 0.43257526\n",
      "Iteration 66, loss = 0.43206082\n",
      "Iteration 67, loss = 0.43323001\n",
      "Iteration 68, loss = 0.43102722\n",
      "Iteration 69, loss = 0.43020127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.42832320\n",
      "Iteration 71, loss = 0.43386264\n",
      "Iteration 72, loss = 0.44040778\n",
      "Iteration 73, loss = 0.43222526\n",
      "Iteration 74, loss = 0.43198237\n",
      "Iteration 75, loss = 0.43504002\n",
      "Iteration 76, loss = 0.43232171\n",
      "Iteration 77, loss = 0.43410218\n",
      "Iteration 78, loss = 0.42892392\n",
      "Iteration 79, loss = 0.42983562\n",
      "Iteration 80, loss = 0.43004756\n",
      "Iteration 81, loss = 0.43131127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'education', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.7763357127406446\n",
      "Iteration 1, loss = 0.59341586\n",
      "Iteration 2, loss = 0.49064608\n",
      "Iteration 3, loss = 0.48389224\n",
      "Iteration 4, loss = 0.48209326\n",
      "Iteration 5, loss = 0.48201024\n",
      "Iteration 6, loss = 0.47610467\n",
      "Iteration 7, loss = 0.47421909\n",
      "Iteration 8, loss = 0.47030449\n",
      "Iteration 9, loss = 0.46498888\n",
      "Iteration 10, loss = 0.45715203\n",
      "Iteration 11, loss = 0.45453585\n",
      "Iteration 12, loss = 0.45247556\n",
      "Iteration 13, loss = 0.44806511\n",
      "Iteration 14, loss = 0.44510870\n",
      "Iteration 15, loss = 0.44388421\n",
      "Iteration 16, loss = 0.44179508\n",
      "Iteration 17, loss = 0.43810350\n",
      "Iteration 18, loss = 0.43999091\n",
      "Iteration 19, loss = 0.43583068\n",
      "Iteration 20, loss = 0.43411575\n",
      "Iteration 21, loss = 0.43266462\n",
      "Iteration 22, loss = 0.43062971\n",
      "Iteration 23, loss = 0.43467544\n",
      "Iteration 24, loss = 0.43408796\n",
      "Iteration 25, loss = 0.43213956\n",
      "Iteration 26, loss = 0.43095445\n",
      "Iteration 27, loss = 0.42968281\n",
      "Iteration 28, loss = 0.43084368\n",
      "Iteration 29, loss = 0.42861189\n",
      "Iteration 30, loss = 0.42981315\n",
      "Iteration 31, loss = 0.42850157\n",
      "Iteration 32, loss = 0.43161917\n",
      "Iteration 33, loss = 0.42912778\n",
      "Iteration 34, loss = 0.42819130\n",
      "Iteration 35, loss = 0.42701703\n",
      "Iteration 36, loss = 0.42806666\n",
      "Iteration 37, loss = 0.42826554\n",
      "Iteration 38, loss = 0.42843148\n",
      "Iteration 39, loss = 0.42915812\n",
      "Iteration 40, loss = 0.42679120\n",
      "Iteration 41, loss = 0.42789497\n",
      "Iteration 42, loss = 0.42709972\n",
      "Iteration 43, loss = 0.42702794\n",
      "Iteration 44, loss = 0.42805286\n",
      "Iteration 45, loss = 0.42611454\n",
      "Iteration 46, loss = 0.42560952\n",
      "Iteration 47, loss = 0.42601418\n",
      "Iteration 48, loss = 0.42568165\n",
      "Iteration 49, loss = 0.42785592\n",
      "Iteration 50, loss = 0.42547133\n",
      "Iteration 51, loss = 0.42627157\n",
      "Iteration 52, loss = 0.42482571\n",
      "Iteration 53, loss = 0.42809918\n",
      "Iteration 54, loss = 0.42687619\n",
      "Iteration 55, loss = 0.42548939\n",
      "Iteration 56, loss = 0.42659013\n",
      "Iteration 57, loss = 0.42590681\n",
      "Iteration 58, loss = 0.42458879\n",
      "Iteration 59, loss = 0.42595977\n",
      "Iteration 60, loss = 0.42530286\n",
      "Iteration 61, loss = 0.42556872\n",
      "Iteration 62, loss = 0.42367088\n",
      "Iteration 63, loss = 0.42573020\n",
      "Iteration 64, loss = 0.42590930\n",
      "Iteration 65, loss = 0.42617109\n",
      "Iteration 66, loss = 0.42682915\n",
      "Iteration 67, loss = 0.42482627\n",
      "Iteration 68, loss = 0.42212877\n",
      "Iteration 69, loss = 0.42714626\n",
      "Iteration 70, loss = 0.42666732\n",
      "Iteration 71, loss = 0.42476013\n",
      "Iteration 72, loss = 0.42338250\n",
      "Iteration 73, loss = 0.42733747\n",
      "Iteration 74, loss = 0.42474924\n",
      "Iteration 75, loss = 0.42670108\n",
      "Iteration 76, loss = 0.42405755\n",
      "Iteration 77, loss = 0.42503291\n",
      "Iteration 78, loss = 0.42484077\n",
      "Iteration 79, loss = 0.42570116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60069002\n",
      "Iteration 2, loss = 0.48547453\n",
      "Iteration 3, loss = 0.47680813\n",
      "Iteration 4, loss = 0.47348223\n",
      "Iteration 5, loss = 0.47186431\n",
      "Iteration 6, loss = 0.46864854\n",
      "Iteration 7, loss = 0.46686164\n",
      "Iteration 8, loss = 0.46346082\n",
      "Iteration 9, loss = 0.45873565\n",
      "Iteration 10, loss = 0.45652726\n",
      "Iteration 11, loss = 0.45425022\n",
      "Iteration 12, loss = 0.45016509\n",
      "Iteration 13, loss = 0.44682814\n",
      "Iteration 14, loss = 0.44315447\n",
      "Iteration 15, loss = 0.43912191\n",
      "Iteration 16, loss = 0.43908316\n",
      "Iteration 17, loss = 0.43443990\n",
      "Iteration 18, loss = 0.43202429\n",
      "Iteration 19, loss = 0.42897847\n",
      "Iteration 20, loss = 0.43265814\n",
      "Iteration 21, loss = 0.42864900\n",
      "Iteration 22, loss = 0.42854116\n",
      "Iteration 23, loss = 0.42632429\n",
      "Iteration 24, loss = 0.42569167\n",
      "Iteration 25, loss = 0.42677155\n",
      "Iteration 26, loss = 0.42490923\n",
      "Iteration 27, loss = 0.42785854\n",
      "Iteration 28, loss = 0.42756855\n",
      "Iteration 29, loss = 0.42402323\n",
      "Iteration 30, loss = 0.42548499\n",
      "Iteration 31, loss = 0.42365283\n",
      "Iteration 32, loss = 0.42438787\n",
      "Iteration 33, loss = 0.42851761\n",
      "Iteration 34, loss = 0.42696935\n",
      "Iteration 35, loss = 0.42513237\n",
      "Iteration 36, loss = 0.42327043\n",
      "Iteration 37, loss = 0.42375384\n",
      "Iteration 38, loss = 0.42343907\n",
      "Iteration 39, loss = 0.42198031\n",
      "Iteration 40, loss = 0.42131828\n",
      "Iteration 41, loss = 0.42192757\n",
      "Iteration 42, loss = 0.42200317\n",
      "Iteration 43, loss = 0.42537893\n",
      "Iteration 44, loss = 0.42558467\n",
      "Iteration 45, loss = 0.42314819\n",
      "Iteration 46, loss = 0.42046417\n",
      "Iteration 47, loss = 0.42189549\n",
      "Iteration 48, loss = 0.42148697\n",
      "Iteration 49, loss = 0.42122638\n",
      "Iteration 50, loss = 0.42165950\n",
      "Iteration 51, loss = 0.42033347\n",
      "Iteration 52, loss = 0.42137605\n",
      "Iteration 53, loss = 0.42147005\n",
      "Iteration 54, loss = 0.41848715\n",
      "Iteration 55, loss = 0.42054944\n",
      "Iteration 56, loss = 0.41887674\n",
      "Iteration 57, loss = 0.42070559\n",
      "Iteration 58, loss = 0.42148712\n",
      "Iteration 59, loss = 0.41960056\n",
      "Iteration 60, loss = 0.41965937\n",
      "Iteration 61, loss = 0.42091180\n",
      "Iteration 62, loss = 0.41930431\n",
      "Iteration 63, loss = 0.41944371\n",
      "Iteration 64, loss = 0.41894038\n",
      "Iteration 65, loss = 0.41896226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59286750\n",
      "Iteration 2, loss = 0.48900234\n",
      "Iteration 3, loss = 0.47849182\n",
      "Iteration 4, loss = 0.47393698\n",
      "Iteration 5, loss = 0.47335762\n",
      "Iteration 6, loss = 0.47174163\n",
      "Iteration 7, loss = 0.47033850\n",
      "Iteration 8, loss = 0.46669638\n",
      "Iteration 9, loss = 0.46685232\n",
      "Iteration 10, loss = 0.46360248\n",
      "Iteration 11, loss = 0.46144861\n",
      "Iteration 12, loss = 0.45416473\n",
      "Iteration 13, loss = 0.45072820\n",
      "Iteration 14, loss = 0.44702757\n",
      "Iteration 15, loss = 0.44318110\n",
      "Iteration 16, loss = 0.44219133\n",
      "Iteration 17, loss = 0.43976794\n",
      "Iteration 18, loss = 0.44143112\n",
      "Iteration 19, loss = 0.43764581\n",
      "Iteration 20, loss = 0.43813736\n",
      "Iteration 21, loss = 0.43946235\n",
      "Iteration 22, loss = 0.43424358\n",
      "Iteration 23, loss = 0.43317433\n",
      "Iteration 24, loss = 0.43739449\n",
      "Iteration 25, loss = 0.43192015\n",
      "Iteration 26, loss = 0.43052272\n",
      "Iteration 27, loss = 0.43475727\n",
      "Iteration 28, loss = 0.43336457\n",
      "Iteration 29, loss = 0.43085280\n",
      "Iteration 30, loss = 0.42926657\n",
      "Iteration 31, loss = 0.42955418\n",
      "Iteration 32, loss = 0.43017492\n",
      "Iteration 33, loss = 0.43512984\n",
      "Iteration 34, loss = 0.42901408\n",
      "Iteration 35, loss = 0.43294557\n",
      "Iteration 36, loss = 0.43236728\n",
      "Iteration 37, loss = 0.43017947\n",
      "Iteration 38, loss = 0.42794367\n",
      "Iteration 39, loss = 0.42718939\n",
      "Iteration 40, loss = 0.42891526\n",
      "Iteration 41, loss = 0.42692309\n",
      "Iteration 42, loss = 0.42933424\n",
      "Iteration 43, loss = 0.42833534\n",
      "Iteration 44, loss = 0.42786967\n",
      "Iteration 45, loss = 0.42877905\n",
      "Iteration 46, loss = 0.42636848\n",
      "Iteration 47, loss = 0.42754021\n",
      "Iteration 48, loss = 0.42725935\n",
      "Iteration 49, loss = 0.42837185\n",
      "Iteration 50, loss = 0.42565564\n",
      "Iteration 51, loss = 0.42471284\n",
      "Iteration 52, loss = 0.42692955\n",
      "Iteration 53, loss = 0.42677949\n",
      "Iteration 54, loss = 0.42752028\n",
      "Iteration 55, loss = 0.42681451\n",
      "Iteration 56, loss = 0.42611591\n",
      "Iteration 57, loss = 0.42548992\n",
      "Iteration 58, loss = 0.42647024\n",
      "Iteration 59, loss = 0.42807905\n",
      "Iteration 60, loss = 0.42709053\n",
      "Iteration 61, loss = 0.42684919\n",
      "Iteration 62, loss = 0.42590066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.8026173480423967\n",
      "Iteration 1, loss = 0.62366322\n",
      "Iteration 2, loss = 0.49041569\n",
      "Iteration 3, loss = 0.48378088\n",
      "Iteration 4, loss = 0.48163160\n",
      "Iteration 5, loss = 0.48093342\n",
      "Iteration 6, loss = 0.47667665\n",
      "Iteration 7, loss = 0.47595364\n",
      "Iteration 8, loss = 0.47500314\n",
      "Iteration 9, loss = 0.47555379\n",
      "Iteration 10, loss = 0.47362743\n",
      "Iteration 11, loss = 0.47322056\n",
      "Iteration 12, loss = 0.47216300\n",
      "Iteration 13, loss = 0.46951399\n",
      "Iteration 14, loss = 0.46965914\n",
      "Iteration 15, loss = 0.46848010\n",
      "Iteration 16, loss = 0.46568646\n",
      "Iteration 17, loss = 0.46430156\n",
      "Iteration 18, loss = 0.46424740\n",
      "Iteration 19, loss = 0.46209368\n",
      "Iteration 20, loss = 0.45842487\n",
      "Iteration 21, loss = 0.45503597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.44926546\n",
      "Iteration 23, loss = 0.45056616\n",
      "Iteration 24, loss = 0.44365307\n",
      "Iteration 25, loss = 0.44379312\n",
      "Iteration 26, loss = 0.44347441\n",
      "Iteration 27, loss = 0.43734791\n",
      "Iteration 28, loss = 0.43423890\n",
      "Iteration 29, loss = 0.43368138\n",
      "Iteration 30, loss = 0.43333889\n",
      "Iteration 31, loss = 0.43367783\n",
      "Iteration 32, loss = 0.43400734\n",
      "Iteration 33, loss = 0.42982651\n",
      "Iteration 34, loss = 0.42539248\n",
      "Iteration 35, loss = 0.42664045\n",
      "Iteration 36, loss = 0.42819270\n",
      "Iteration 37, loss = 0.42522170\n",
      "Iteration 38, loss = 0.42638905\n",
      "Iteration 39, loss = 0.42397374\n",
      "Iteration 40, loss = 0.42283797\n",
      "Iteration 41, loss = 0.42193457\n",
      "Iteration 42, loss = 0.42159351\n",
      "Iteration 43, loss = 0.42149246\n",
      "Iteration 44, loss = 0.42407087\n",
      "Iteration 45, loss = 0.42113104\n",
      "Iteration 46, loss = 0.42030476\n",
      "Iteration 47, loss = 0.42022071\n",
      "Iteration 48, loss = 0.42106322\n",
      "Iteration 49, loss = 0.42058487\n",
      "Iteration 50, loss = 0.41987678\n",
      "Iteration 51, loss = 0.42059575\n",
      "Iteration 52, loss = 0.41971487\n",
      "Iteration 53, loss = 0.42301953\n",
      "Iteration 54, loss = 0.41851380\n",
      "Iteration 55, loss = 0.42066087\n",
      "Iteration 56, loss = 0.42003456\n",
      "Iteration 57, loss = 0.42018178\n",
      "Iteration 58, loss = 0.41766105\n",
      "Iteration 59, loss = 0.42009623\n",
      "Iteration 60, loss = 0.41766599\n",
      "Iteration 61, loss = 0.41711843\n",
      "Iteration 62, loss = 0.41631821\n",
      "Iteration 63, loss = 0.41564916\n",
      "Iteration 64, loss = 0.41617505\n",
      "Iteration 65, loss = 0.41548691\n",
      "Iteration 66, loss = 0.41620447\n",
      "Iteration 67, loss = 0.41604810\n",
      "Iteration 68, loss = 0.41369372\n",
      "Iteration 69, loss = 0.42181128\n",
      "Iteration 70, loss = 0.41870317\n",
      "Iteration 71, loss = 0.41665120\n",
      "Iteration 72, loss = 0.41658116\n",
      "Iteration 73, loss = 0.41742239\n",
      "Iteration 74, loss = 0.41502322\n",
      "Iteration 75, loss = 0.41642192\n",
      "Iteration 76, loss = 0.41464462\n",
      "Iteration 77, loss = 0.41423594\n",
      "Iteration 78, loss = 0.41924188\n",
      "Iteration 79, loss = 0.41793260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63045779\n",
      "Iteration 2, loss = 0.48534147\n",
      "Iteration 3, loss = 0.47628851\n",
      "Iteration 4, loss = 0.47402556\n",
      "Iteration 5, loss = 0.47315210\n",
      "Iteration 6, loss = 0.46842839\n",
      "Iteration 7, loss = 0.46853247\n",
      "Iteration 8, loss = 0.46608184\n",
      "Iteration 9, loss = 0.46170697\n",
      "Iteration 10, loss = 0.45815680\n",
      "Iteration 11, loss = 0.45674917\n",
      "Iteration 12, loss = 0.45477770\n",
      "Iteration 13, loss = 0.45308045\n",
      "Iteration 14, loss = 0.44941887\n",
      "Iteration 15, loss = 0.44771301\n",
      "Iteration 16, loss = 0.44736153\n",
      "Iteration 17, loss = 0.44420308\n",
      "Iteration 18, loss = 0.44404483\n",
      "Iteration 19, loss = 0.44178691\n",
      "Iteration 20, loss = 0.44140936\n",
      "Iteration 21, loss = 0.43855421\n",
      "Iteration 22, loss = 0.43799417\n",
      "Iteration 23, loss = 0.43455826\n",
      "Iteration 24, loss = 0.43407605\n",
      "Iteration 25, loss = 0.43431372\n",
      "Iteration 26, loss = 0.43099668\n",
      "Iteration 27, loss = 0.43220389\n",
      "Iteration 28, loss = 0.42704254\n",
      "Iteration 29, loss = 0.42670791\n",
      "Iteration 30, loss = 0.42538834\n",
      "Iteration 31, loss = 0.42381634\n",
      "Iteration 32, loss = 0.42402333\n",
      "Iteration 33, loss = 0.42526808\n",
      "Iteration 34, loss = 0.42707875\n",
      "Iteration 35, loss = 0.42544555\n",
      "Iteration 36, loss = 0.42263606\n",
      "Iteration 37, loss = 0.42224706\n",
      "Iteration 38, loss = 0.42275902\n",
      "Iteration 39, loss = 0.42056096\n",
      "Iteration 40, loss = 0.41945396\n",
      "Iteration 41, loss = 0.41932360\n",
      "Iteration 42, loss = 0.42064882\n",
      "Iteration 43, loss = 0.42062540\n",
      "Iteration 44, loss = 0.42491465\n",
      "Iteration 45, loss = 0.42636807\n",
      "Iteration 46, loss = 0.42335300\n",
      "Iteration 47, loss = 0.41874181\n",
      "Iteration 48, loss = 0.41943677\n",
      "Iteration 49, loss = 0.41694416\n",
      "Iteration 50, loss = 0.41939305\n",
      "Iteration 51, loss = 0.41700877\n",
      "Iteration 52, loss = 0.41558674\n",
      "Iteration 53, loss = 0.41571816\n",
      "Iteration 54, loss = 0.41417970\n",
      "Iteration 55, loss = 0.41642637\n",
      "Iteration 56, loss = 0.41432177\n",
      "Iteration 57, loss = 0.41561838\n",
      "Iteration 58, loss = 0.41942732\n",
      "Iteration 59, loss = 0.41692123\n",
      "Iteration 60, loss = 0.41382411\n",
      "Iteration 61, loss = 0.41603550\n",
      "Iteration 62, loss = 0.41435424\n",
      "Iteration 63, loss = 0.41426879\n",
      "Iteration 64, loss = 0.41456924\n",
      "Iteration 65, loss = 0.41341825\n",
      "Iteration 66, loss = 0.41426237\n",
      "Iteration 67, loss = 0.41438380\n",
      "Iteration 68, loss = 0.41305868\n",
      "Iteration 69, loss = 0.41463140\n",
      "Iteration 70, loss = 0.41412038\n",
      "Iteration 71, loss = 0.41354907\n",
      "Iteration 72, loss = 0.41442107\n",
      "Iteration 73, loss = 0.41301664\n",
      "Iteration 74, loss = 0.41443258\n",
      "Iteration 75, loss = 0.41594549\n",
      "Iteration 76, loss = 0.41620796\n",
      "Iteration 77, loss = 0.41291109\n",
      "Iteration 78, loss = 0.41399097\n",
      "Iteration 79, loss = 0.41489272\n",
      "Iteration 80, loss = 0.41335817\n",
      "Iteration 81, loss = 0.41328343\n",
      "Iteration 82, loss = 0.41420255\n",
      "Iteration 83, loss = 0.41307315\n",
      "Iteration 84, loss = 0.41558954\n",
      "Iteration 85, loss = 0.41866393\n",
      "Iteration 86, loss = 0.41754084\n",
      "Iteration 87, loss = 0.41489530\n",
      "Iteration 88, loss = 0.41203717\n",
      "Iteration 89, loss = 0.41372040\n",
      "Iteration 90, loss = 0.41454976\n",
      "Iteration 91, loss = 0.41279341\n",
      "Iteration 92, loss = 0.41293748\n",
      "Iteration 93, loss = 0.41174851\n",
      "Iteration 94, loss = 0.41294263\n",
      "Iteration 95, loss = 0.41201925\n",
      "Iteration 96, loss = 0.41145079\n",
      "Iteration 97, loss = 0.41150341\n",
      "Iteration 98, loss = 0.41174385\n",
      "Iteration 99, loss = 0.41184109\n",
      "Iteration 100, loss = 0.41205227\n",
      "Iteration 101, loss = 0.41172559\n",
      "Iteration 102, loss = 0.41300076\n",
      "Iteration 103, loss = 0.41162359\n",
      "Iteration 104, loss = 0.41194372\n",
      "Iteration 105, loss = 0.41149982\n",
      "Iteration 106, loss = 0.41448810\n",
      "Iteration 107, loss = 0.41297572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61974503\n",
      "Iteration 2, loss = 0.48860269\n",
      "Iteration 3, loss = 0.47715259\n",
      "Iteration 4, loss = 0.47402147\n",
      "Iteration 5, loss = 0.47198562\n",
      "Iteration 6, loss = 0.47047856\n",
      "Iteration 7, loss = 0.47016681\n",
      "Iteration 8, loss = 0.46829156\n",
      "Iteration 9, loss = 0.46672189\n",
      "Iteration 10, loss = 0.46469612\n",
      "Iteration 11, loss = 0.46159746\n",
      "Iteration 12, loss = 0.45897902\n",
      "Iteration 13, loss = 0.45604405\n",
      "Iteration 14, loss = 0.45370274\n",
      "Iteration 15, loss = 0.45049507\n",
      "Iteration 16, loss = 0.44581177\n",
      "Iteration 17, loss = 0.44501958\n",
      "Iteration 18, loss = 0.44223639\n",
      "Iteration 19, loss = 0.43921713\n",
      "Iteration 20, loss = 0.43631673\n",
      "Iteration 21, loss = 0.43455261\n",
      "Iteration 22, loss = 0.43273034\n",
      "Iteration 23, loss = 0.43401029\n",
      "Iteration 24, loss = 0.43148357\n",
      "Iteration 25, loss = 0.43038313\n",
      "Iteration 26, loss = 0.43044633\n",
      "Iteration 27, loss = 0.42975290\n",
      "Iteration 28, loss = 0.43523402\n",
      "Iteration 29, loss = 0.42983758\n",
      "Iteration 30, loss = 0.42863250\n",
      "Iteration 31, loss = 0.42930962\n",
      "Iteration 32, loss = 0.42622281\n",
      "Iteration 33, loss = 0.43201065\n",
      "Iteration 34, loss = 0.42840019\n",
      "Iteration 35, loss = 0.42848569\n",
      "Iteration 36, loss = 0.42763102\n",
      "Iteration 37, loss = 0.42761488\n",
      "Iteration 38, loss = 0.42651807\n",
      "Iteration 39, loss = 0.42391172\n",
      "Iteration 40, loss = 0.42508208\n",
      "Iteration 41, loss = 0.42429796\n",
      "Iteration 42, loss = 0.42614586\n",
      "Iteration 43, loss = 0.42494077\n",
      "Iteration 44, loss = 0.42579313\n",
      "Iteration 45, loss = 0.42537551\n",
      "Iteration 46, loss = 0.42382387\n",
      "Iteration 47, loss = 0.42350812\n",
      "Iteration 48, loss = 0.42298103\n",
      "Iteration 49, loss = 0.42719033\n",
      "Iteration 50, loss = 0.42300850\n",
      "Iteration 51, loss = 0.42135763\n",
      "Iteration 52, loss = 0.42185027\n",
      "Iteration 53, loss = 0.42288033\n",
      "Iteration 54, loss = 0.42298117\n",
      "Iteration 55, loss = 0.42267933\n",
      "Iteration 56, loss = 0.42076353\n",
      "Iteration 57, loss = 0.42012618\n",
      "Iteration 58, loss = 0.42213439\n",
      "Iteration 59, loss = 0.42157042\n",
      "Iteration 60, loss = 0.41910720\n",
      "Iteration 61, loss = 0.41897497\n",
      "Iteration 62, loss = 0.42040215\n",
      "Iteration 63, loss = 0.42064389\n",
      "Iteration 64, loss = 0.42101821\n",
      "Iteration 65, loss = 0.42118564\n",
      "Iteration 66, loss = 0.41980492\n",
      "Iteration 67, loss = 0.42200532\n",
      "Iteration 68, loss = 0.41924794\n",
      "Iteration 69, loss = 0.41895671\n",
      "Iteration 70, loss = 0.41912263\n",
      "Iteration 71, loss = 0.41805989\n",
      "Iteration 72, loss = 0.41843286\n",
      "Iteration 73, loss = 0.41863193\n",
      "Iteration 74, loss = 0.41970853\n",
      "Iteration 75, loss = 0.42136347\n",
      "Iteration 76, loss = 0.42010502\n",
      "Iteration 77, loss = 0.42061858\n",
      "Iteration 78, loss = 0.41829364\n",
      "Iteration 79, loss = 0.41924678\n",
      "Iteration 80, loss = 0.41776200\n",
      "Iteration 81, loss = 0.41756410\n",
      "Iteration 82, loss = 0.41762928\n",
      "Iteration 83, loss = 0.41754912\n",
      "Iteration 84, loss = 0.41976938\n",
      "Iteration 85, loss = 0.41944013\n",
      "Iteration 86, loss = 0.41893624\n",
      "Iteration 87, loss = 0.41891970\n",
      "Iteration 88, loss = 0.41689282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 89, loss = 0.42065779\n",
      "Iteration 90, loss = 0.41722387\n",
      "Iteration 91, loss = 0.41801361\n",
      "Iteration 92, loss = 0.41691627\n",
      "Iteration 93, loss = 0.41633203\n",
      "Iteration 94, loss = 0.41701787\n",
      "Iteration 95, loss = 0.41565640\n",
      "Iteration 96, loss = 0.41656196\n",
      "Iteration 97, loss = 0.41663461\n",
      "Iteration 98, loss = 0.41876842\n",
      "Iteration 99, loss = 0.41829626\n",
      "Iteration 100, loss = 0.41451283\n",
      "Iteration 101, loss = 0.41674395\n",
      "Iteration 102, loss = 0.41425360\n",
      "Iteration 103, loss = 0.41564411\n",
      "Iteration 104, loss = 0.41422548\n",
      "Iteration 105, loss = 0.41534802\n",
      "Iteration 106, loss = 0.41562793\n",
      "Iteration 107, loss = 0.41628519\n",
      "Iteration 108, loss = 0.41696742\n",
      "Iteration 109, loss = 0.41288865\n",
      "Iteration 110, loss = 0.41571886\n",
      "Iteration 111, loss = 0.41491081\n",
      "Iteration 112, loss = 0.41452513\n",
      "Iteration 113, loss = 0.41440934\n",
      "Iteration 114, loss = 0.41301819\n",
      "Iteration 115, loss = 0.41419346\n",
      "Iteration 116, loss = 0.41261476\n",
      "Iteration 117, loss = 0.41210266\n",
      "Iteration 118, loss = 0.41201130\n",
      "Iteration 119, loss = 0.41312289\n",
      "Iteration 120, loss = 0.41302707\n",
      "Iteration 121, loss = 0.41237316\n",
      "Iteration 122, loss = 0.41259220\n",
      "Iteration 123, loss = 0.41220496\n",
      "Iteration 124, loss = 0.41246298\n",
      "Iteration 125, loss = 0.41280218\n",
      "Iteration 126, loss = 0.41304331\n",
      "Iteration 127, loss = 0.41244747\n",
      "Iteration 128, loss = 0.41438214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Current subset: ['education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
      "Score: 0.8043478260869565\n",
      "Elapsed time: 1 min. and 4.434809446334839 sec.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['education', 'marital-status', 'occupation', 'relationship', 'race', 'sex'],\n",
       " 0.8043478260869565]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, we will find the best subset for 6 features with verbose=True to show\n",
    "# process of scoring all subsets\n",
    "best_subset(X, y, mlp, 6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0af0e134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 total combinations for subset length 1.\n",
      "Iteration 1, loss = 0.56346776\n",
      "Iteration 2, loss = 0.55350690\n",
      "Iteration 3, loss = 0.55254310\n",
      "Iteration 4, loss = 0.55238880\n",
      "Iteration 5, loss = 0.55132882\n",
      "Iteration 6, loss = 0.55060668\n",
      "Iteration 7, loss = 0.54994861\n",
      "Iteration 8, loss = 0.54956677\n",
      "Iteration 9, loss = 0.54926930\n",
      "Iteration 10, loss = 0.54827445\n",
      "Iteration 11, loss = 0.54864987\n",
      "Iteration 12, loss = 0.54774745\n",
      "Iteration 13, loss = 0.54766285\n",
      "Iteration 14, loss = 0.54705419\n",
      "Iteration 15, loss = 0.54653268\n",
      "Iteration 16, loss = 0.54698462\n",
      "Iteration 17, loss = 0.54655957\n",
      "Iteration 18, loss = 0.54606429\n",
      "Iteration 19, loss = 0.54548374\n",
      "Iteration 20, loss = 0.54539383\n",
      "Iteration 21, loss = 0.54557531\n",
      "Iteration 22, loss = 0.54491636\n",
      "Iteration 23, loss = 0.54517788\n",
      "Iteration 24, loss = 0.54621007\n",
      "Iteration 25, loss = 0.54523984\n",
      "Iteration 26, loss = 0.54585669\n",
      "Iteration 27, loss = 0.54459532\n",
      "Iteration 28, loss = 0.54604265\n",
      "Iteration 29, loss = 0.54483165\n",
      "Iteration 30, loss = 0.54512674\n",
      "Iteration 31, loss = 0.54688851\n",
      "Iteration 32, loss = 0.54578955\n",
      "Iteration 33, loss = 0.54484224\n",
      "Iteration 34, loss = 0.54517180\n",
      "Iteration 35, loss = 0.54520887\n",
      "Iteration 36, loss = 0.54622507\n",
      "Iteration 37, loss = 0.54445647\n",
      "Iteration 38, loss = 0.54466522\n",
      "Iteration 39, loss = 0.54488015\n",
      "Iteration 40, loss = 0.54520484\n",
      "Iteration 41, loss = 0.54457506\n",
      "Iteration 42, loss = 0.54591112\n",
      "Iteration 43, loss = 0.54491862\n",
      "Iteration 44, loss = 0.54451098\n",
      "Iteration 45, loss = 0.54474236\n",
      "Iteration 46, loss = 0.54472576\n",
      "Iteration 47, loss = 0.54460248\n",
      "Iteration 48, loss = 0.54501470\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56386013\n",
      "Iteration 2, loss = 0.55346818\n",
      "Iteration 3, loss = 0.55275216\n",
      "Iteration 4, loss = 0.55256166\n",
      "Iteration 5, loss = 0.55161402\n",
      "Iteration 6, loss = 0.55141147\n",
      "Iteration 7, loss = 0.55059835\n",
      "Iteration 8, loss = 0.55025782\n",
      "Iteration 9, loss = 0.54996058\n",
      "Iteration 10, loss = 0.54958034\n",
      "Iteration 11, loss = 0.54916204\n",
      "Iteration 12, loss = 0.54901316\n",
      "Iteration 13, loss = 0.54829389\n",
      "Iteration 14, loss = 0.54763817\n",
      "Iteration 15, loss = 0.54746439\n",
      "Iteration 16, loss = 0.54628324\n",
      "Iteration 17, loss = 0.54807212\n",
      "Iteration 18, loss = 0.54657920\n",
      "Iteration 19, loss = 0.54698454\n",
      "Iteration 20, loss = 0.54779365\n",
      "Iteration 21, loss = 0.54792963\n",
      "Iteration 22, loss = 0.54605831\n",
      "Iteration 23, loss = 0.54564922\n",
      "Iteration 24, loss = 0.54577142\n",
      "Iteration 25, loss = 0.54553103\n",
      "Iteration 26, loss = 0.54627624\n",
      "Iteration 27, loss = 0.54568665\n",
      "Iteration 28, loss = 0.54518016\n",
      "Iteration 29, loss = 0.54541608\n",
      "Iteration 30, loss = 0.54574711\n",
      "Iteration 31, loss = 0.54611810\n",
      "Iteration 32, loss = 0.54578325\n",
      "Iteration 33, loss = 0.54556393\n",
      "Iteration 34, loss = 0.54552602\n",
      "Iteration 35, loss = 0.54544820\n",
      "Iteration 36, loss = 0.54614804\n",
      "Iteration 37, loss = 0.54587804\n",
      "Iteration 38, loss = 0.54489595\n",
      "Iteration 39, loss = 0.54599478\n",
      "Iteration 40, loss = 0.54584160\n",
      "Iteration 41, loss = 0.54508453\n",
      "Iteration 42, loss = 0.54611082\n",
      "Iteration 43, loss = 0.54554752\n",
      "Iteration 44, loss = 0.54486657\n",
      "Iteration 45, loss = 0.54492932\n",
      "Iteration 46, loss = 0.54574453\n",
      "Iteration 47, loss = 0.54501055\n",
      "Iteration 48, loss = 0.54529646\n",
      "Iteration 49, loss = 0.54531903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56123908\n",
      "Iteration 2, loss = 0.55340748\n",
      "Iteration 3, loss = 0.55231775\n",
      "Iteration 4, loss = 0.55220292\n",
      "Iteration 5, loss = 0.55244353\n",
      "Iteration 6, loss = 0.55209608\n",
      "Iteration 7, loss = 0.55188483\n",
      "Iteration 8, loss = 0.55153993\n",
      "Iteration 9, loss = 0.55113578\n",
      "Iteration 10, loss = 0.55119725\n",
      "Iteration 11, loss = 0.55063710\n",
      "Iteration 12, loss = 0.55122441\n",
      "Iteration 13, loss = 0.55048725\n",
      "Iteration 14, loss = 0.55035536\n",
      "Iteration 15, loss = 0.55029070\n",
      "Iteration 16, loss = 0.54957817\n",
      "Iteration 17, loss = 0.54944636\n",
      "Iteration 18, loss = 0.55013501\n",
      "Iteration 19, loss = 0.54971064\n",
      "Iteration 20, loss = 0.54877687\n",
      "Iteration 21, loss = 0.54914627\n",
      "Iteration 22, loss = 0.54895328\n",
      "Iteration 23, loss = 0.54798713\n",
      "Iteration 24, loss = 0.54868180\n",
      "Iteration 25, loss = 0.54869965\n",
      "Iteration 26, loss = 0.54749340\n",
      "Iteration 27, loss = 0.54804427\n",
      "Iteration 28, loss = 0.54836776\n",
      "Iteration 29, loss = 0.54765356\n",
      "Iteration 30, loss = 0.54783667\n",
      "Iteration 31, loss = 0.54771889\n",
      "Iteration 32, loss = 0.54765458\n",
      "Iteration 33, loss = 0.54804382\n",
      "Iteration 34, loss = 0.54816578\n",
      "Iteration 35, loss = 0.54782214\n",
      "Iteration 36, loss = 0.54749565\n",
      "Iteration 37, loss = 0.54726872\n",
      "Iteration 38, loss = 0.54709527\n",
      "Iteration 39, loss = 0.54738930\n",
      "Iteration 40, loss = 0.54813578\n",
      "Iteration 41, loss = 0.54759079\n",
      "Iteration 42, loss = 0.54783345\n",
      "Iteration 43, loss = 0.54784083\n",
      "Iteration 44, loss = 0.54730983\n",
      "Iteration 45, loss = 0.54719686\n",
      "Iteration 46, loss = 0.54726697\n",
      "Iteration 47, loss = 0.54701222\n",
      "Iteration 48, loss = 0.54712889\n",
      "Iteration 49, loss = 0.54743582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61999249\n",
      "Iteration 2, loss = 0.55221033\n",
      "Iteration 3, loss = 0.54916720\n",
      "Iteration 4, loss = 0.54771659\n",
      "Iteration 5, loss = 0.54696127\n",
      "Iteration 6, loss = 0.54580879\n",
      "Iteration 7, loss = 0.54566469\n",
      "Iteration 8, loss = 0.54559845\n",
      "Iteration 9, loss = 0.54502024\n",
      "Iteration 10, loss = 0.54460262\n",
      "Iteration 11, loss = 0.54391805\n",
      "Iteration 12, loss = 0.54202915\n",
      "Iteration 13, loss = 0.54093119\n",
      "Iteration 14, loss = 0.53966694\n",
      "Iteration 15, loss = 0.53889699\n",
      "Iteration 16, loss = 0.53899517\n",
      "Iteration 17, loss = 0.53882093\n",
      "Iteration 18, loss = 0.53798178\n",
      "Iteration 19, loss = 0.53743775\n",
      "Iteration 20, loss = 0.53726355\n",
      "Iteration 21, loss = 0.53753506\n",
      "Iteration 22, loss = 0.53721462\n",
      "Iteration 23, loss = 0.53748427\n",
      "Iteration 24, loss = 0.53841159\n",
      "Iteration 25, loss = 0.53697531\n",
      "Iteration 26, loss = 0.53715654\n",
      "Iteration 27, loss = 0.53683597\n",
      "Iteration 28, loss = 0.53780453\n",
      "Iteration 29, loss = 0.53749042\n",
      "Iteration 30, loss = 0.53727754\n",
      "Iteration 31, loss = 0.53937413\n",
      "Iteration 32, loss = 0.53783999\n",
      "Iteration 33, loss = 0.53736760\n",
      "Iteration 34, loss = 0.53752373\n",
      "Iteration 35, loss = 0.53709248\n",
      "Iteration 36, loss = 0.53833793\n",
      "Iteration 37, loss = 0.53690794\n",
      "Iteration 38, loss = 0.53697414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61270270\n",
      "Iteration 2, loss = 0.55147783\n",
      "Iteration 3, loss = 0.54880024\n",
      "Iteration 4, loss = 0.54705090\n",
      "Iteration 5, loss = 0.54627831\n",
      "Iteration 6, loss = 0.54524080\n",
      "Iteration 7, loss = 0.54473453\n",
      "Iteration 8, loss = 0.54457823\n",
      "Iteration 9, loss = 0.54475698\n",
      "Iteration 10, loss = 0.54508791\n",
      "Iteration 11, loss = 0.54434384\n",
      "Iteration 12, loss = 0.54340047\n",
      "Iteration 13, loss = 0.54242249\n",
      "Iteration 14, loss = 0.54019011\n",
      "Iteration 15, loss = 0.53898305\n",
      "Iteration 16, loss = 0.53649982\n",
      "Iteration 17, loss = 0.53688912\n",
      "Iteration 18, loss = 0.53519118\n",
      "Iteration 19, loss = 0.53571165\n",
      "Iteration 20, loss = 0.53612525\n",
      "Iteration 21, loss = 0.53651535\n",
      "Iteration 22, loss = 0.53474396\n",
      "Iteration 23, loss = 0.53454705\n",
      "Iteration 24, loss = 0.53476794\n",
      "Iteration 25, loss = 0.53420141\n",
      "Iteration 26, loss = 0.53458104\n",
      "Iteration 27, loss = 0.53455676\n",
      "Iteration 28, loss = 0.53457547\n",
      "Iteration 29, loss = 0.53471114\n",
      "Iteration 30, loss = 0.53414317\n",
      "Iteration 31, loss = 0.53578843\n",
      "Iteration 32, loss = 0.53505314\n",
      "Iteration 33, loss = 0.53436729\n",
      "Iteration 34, loss = 0.53423212\n",
      "Iteration 35, loss = 0.53434733\n",
      "Iteration 36, loss = 0.53523781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62032499\n",
      "Iteration 2, loss = 0.55272244\n",
      "Iteration 3, loss = 0.54983749\n",
      "Iteration 4, loss = 0.54810519\n",
      "Iteration 5, loss = 0.54738605\n",
      "Iteration 6, loss = 0.54664138\n",
      "Iteration 7, loss = 0.54600831\n",
      "Iteration 8, loss = 0.54491586\n",
      "Iteration 9, loss = 0.54293640\n",
      "Iteration 10, loss = 0.54144039\n",
      "Iteration 11, loss = 0.53920739\n",
      "Iteration 12, loss = 0.53872539\n",
      "Iteration 13, loss = 0.53754130\n",
      "Iteration 14, loss = 0.53777459\n",
      "Iteration 15, loss = 0.53757261\n",
      "Iteration 16, loss = 0.53713316\n",
      "Iteration 17, loss = 0.53720267\n",
      "Iteration 18, loss = 0.53820864\n",
      "Iteration 19, loss = 0.53760786\n",
      "Iteration 20, loss = 0.53704155\n",
      "Iteration 21, loss = 0.53721138\n",
      "Iteration 22, loss = 0.53766674\n",
      "Iteration 23, loss = 0.53698386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.53768832\n",
      "Iteration 25, loss = 0.53737029\n",
      "Iteration 26, loss = 0.53679322\n",
      "Iteration 27, loss = 0.53683816\n",
      "Iteration 28, loss = 0.53879956\n",
      "Iteration 29, loss = 0.53769859\n",
      "Iteration 30, loss = 0.53697268\n",
      "Iteration 31, loss = 0.53699174\n",
      "Iteration 32, loss = 0.53724373\n",
      "Iteration 33, loss = 0.53782284\n",
      "Iteration 34, loss = 0.53741896\n",
      "Iteration 35, loss = 0.53715951\n",
      "Iteration 36, loss = 0.53689853\n",
      "Iteration 37, loss = 0.53794864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58741350\n",
      "Iteration 2, loss = 0.53221404\n",
      "Iteration 3, loss = 0.50178241\n",
      "Iteration 4, loss = 0.48212682\n",
      "Iteration 5, loss = 0.47458711\n",
      "Iteration 6, loss = 0.47217856\n",
      "Iteration 7, loss = 0.47058287\n",
      "Iteration 8, loss = 0.46948332\n",
      "Iteration 9, loss = 0.46749837\n",
      "Iteration 10, loss = 0.46586775\n",
      "Iteration 11, loss = 0.46544863\n",
      "Iteration 12, loss = 0.46363250\n",
      "Iteration 13, loss = 0.46240949\n",
      "Iteration 14, loss = 0.46233625\n",
      "Iteration 15, loss = 0.46257464\n",
      "Iteration 16, loss = 0.46107317\n",
      "Iteration 17, loss = 0.46037239\n",
      "Iteration 18, loss = 0.45964500\n",
      "Iteration 19, loss = 0.45870223\n",
      "Iteration 20, loss = 0.45864115\n",
      "Iteration 21, loss = 0.45843312\n",
      "Iteration 22, loss = 0.45809990\n",
      "Iteration 23, loss = 0.45794661\n",
      "Iteration 24, loss = 0.45814837\n",
      "Iteration 25, loss = 0.45742725\n",
      "Iteration 26, loss = 0.46004268\n",
      "Iteration 27, loss = 0.45729310\n",
      "Iteration 28, loss = 0.45795978\n",
      "Iteration 29, loss = 0.45718045\n",
      "Iteration 30, loss = 0.45749866\n",
      "Iteration 31, loss = 0.45901350\n",
      "Iteration 32, loss = 0.45719758\n",
      "Iteration 33, loss = 0.45660989\n",
      "Iteration 34, loss = 0.45716815\n",
      "Iteration 35, loss = 0.45647791\n",
      "Iteration 36, loss = 0.45795408\n",
      "Iteration 37, loss = 0.45723651\n",
      "Iteration 38, loss = 0.45633273\n",
      "Iteration 39, loss = 0.45700214\n",
      "Iteration 40, loss = 0.45721644\n",
      "Iteration 41, loss = 0.45644505\n",
      "Iteration 42, loss = 0.45873674\n",
      "Iteration 43, loss = 0.45672444\n",
      "Iteration 44, loss = 0.45659333\n",
      "Iteration 45, loss = 0.45670429\n",
      "Iteration 46, loss = 0.45752164\n",
      "Iteration 47, loss = 0.45720342\n",
      "Iteration 48, loss = 0.45691905\n",
      "Iteration 49, loss = 0.45670281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58628918\n",
      "Iteration 2, loss = 0.53211393\n",
      "Iteration 3, loss = 0.50136521\n",
      "Iteration 4, loss = 0.48039195\n",
      "Iteration 5, loss = 0.47295171\n",
      "Iteration 6, loss = 0.47164421\n",
      "Iteration 7, loss = 0.46956763\n",
      "Iteration 8, loss = 0.46876143\n",
      "Iteration 9, loss = 0.46698348\n",
      "Iteration 10, loss = 0.46650874\n",
      "Iteration 11, loss = 0.46508487\n",
      "Iteration 12, loss = 0.46392782\n",
      "Iteration 13, loss = 0.46301436\n",
      "Iteration 14, loss = 0.46234200\n",
      "Iteration 15, loss = 0.46125344\n",
      "Iteration 16, loss = 0.46052362\n",
      "Iteration 17, loss = 0.46094747\n",
      "Iteration 18, loss = 0.46031475\n",
      "Iteration 19, loss = 0.46019880\n",
      "Iteration 20, loss = 0.46037022\n",
      "Iteration 21, loss = 0.46094794\n",
      "Iteration 22, loss = 0.45990364\n",
      "Iteration 23, loss = 0.45906601\n",
      "Iteration 24, loss = 0.45871656\n",
      "Iteration 25, loss = 0.45838890\n",
      "Iteration 26, loss = 0.46038342\n",
      "Iteration 27, loss = 0.45889712\n",
      "Iteration 28, loss = 0.45862473\n",
      "Iteration 29, loss = 0.45824902\n",
      "Iteration 30, loss = 0.45863887\n",
      "Iteration 31, loss = 0.45942307\n",
      "Iteration 32, loss = 0.45817906\n",
      "Iteration 33, loss = 0.45867270\n",
      "Iteration 34, loss = 0.45810771\n",
      "Iteration 35, loss = 0.45756722\n",
      "Iteration 36, loss = 0.45744927\n",
      "Iteration 37, loss = 0.45776507\n",
      "Iteration 38, loss = 0.45697308\n",
      "Iteration 39, loss = 0.45829409\n",
      "Iteration 40, loss = 0.45808092\n",
      "Iteration 41, loss = 0.45745923\n",
      "Iteration 42, loss = 0.45806156\n",
      "Iteration 43, loss = 0.45784585\n",
      "Iteration 44, loss = 0.45723276\n",
      "Iteration 45, loss = 0.45752624\n",
      "Iteration 46, loss = 0.45725258\n",
      "Iteration 47, loss = 0.45760774\n",
      "Iteration 48, loss = 0.45729252\n",
      "Iteration 49, loss = 0.45736702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58530627\n",
      "Iteration 2, loss = 0.53104071\n",
      "Iteration 3, loss = 0.50052884\n",
      "Iteration 4, loss = 0.47916327\n",
      "Iteration 5, loss = 0.47456335\n",
      "Iteration 6, loss = 0.47148259\n",
      "Iteration 7, loss = 0.46992633\n",
      "Iteration 8, loss = 0.46889332\n",
      "Iteration 9, loss = 0.46633211\n",
      "Iteration 10, loss = 0.46545273\n",
      "Iteration 11, loss = 0.46443889\n",
      "Iteration 12, loss = 0.46371017\n",
      "Iteration 13, loss = 0.46148939\n",
      "Iteration 14, loss = 0.46044194\n",
      "Iteration 15, loss = 0.46013297\n",
      "Iteration 16, loss = 0.45882702\n",
      "Iteration 17, loss = 0.45781399\n",
      "Iteration 18, loss = 0.45832648\n",
      "Iteration 19, loss = 0.45785145\n",
      "Iteration 20, loss = 0.45676114\n",
      "Iteration 21, loss = 0.45683904\n",
      "Iteration 22, loss = 0.45670316\n",
      "Iteration 23, loss = 0.45635277\n",
      "Iteration 24, loss = 0.45797021\n",
      "Iteration 25, loss = 0.45703073\n",
      "Iteration 26, loss = 0.45623185\n",
      "Iteration 27, loss = 0.45706945\n",
      "Iteration 28, loss = 0.45614156\n",
      "Iteration 29, loss = 0.45547402\n",
      "Iteration 30, loss = 0.45602100\n",
      "Iteration 31, loss = 0.45603667\n",
      "Iteration 32, loss = 0.45553897\n",
      "Iteration 33, loss = 0.45636534\n",
      "Iteration 34, loss = 0.45569274\n",
      "Iteration 35, loss = 0.45578895\n",
      "Iteration 36, loss = 0.45663366\n",
      "Iteration 37, loss = 0.45553404\n",
      "Iteration 38, loss = 0.45542265\n",
      "Iteration 39, loss = 0.45523245\n",
      "Iteration 40, loss = 0.45648355\n",
      "Iteration 41, loss = 0.45604623\n",
      "Iteration 42, loss = 0.45638982\n",
      "Iteration 43, loss = 0.45616104\n",
      "Iteration 44, loss = 0.45521591\n",
      "Iteration 45, loss = 0.45529544\n",
      "Iteration 46, loss = 0.45542168\n",
      "Iteration 47, loss = 0.45551053\n",
      "Iteration 48, loss = 0.45552550\n",
      "Iteration 49, loss = 0.45544196\n",
      "Iteration 50, loss = 0.45533480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64652386\n",
      "Iteration 2, loss = 0.54446014\n",
      "Iteration 3, loss = 0.53825171\n",
      "Iteration 4, loss = 0.53687600\n",
      "Iteration 5, loss = 0.53541811\n",
      "Iteration 6, loss = 0.53420616\n",
      "Iteration 7, loss = 0.53333308\n",
      "Iteration 8, loss = 0.53285194\n",
      "Iteration 9, loss = 0.53226219\n",
      "Iteration 10, loss = 0.53190538\n",
      "Iteration 11, loss = 0.53177819\n",
      "Iteration 12, loss = 0.53098514\n",
      "Iteration 13, loss = 0.53063696\n",
      "Iteration 14, loss = 0.53007564\n",
      "Iteration 15, loss = 0.52903778\n",
      "Iteration 16, loss = 0.52882381\n",
      "Iteration 17, loss = 0.52838216\n",
      "Iteration 18, loss = 0.52836507\n",
      "Iteration 19, loss = 0.52657511\n",
      "Iteration 20, loss = 0.52684603\n",
      "Iteration 21, loss = 0.52609736\n",
      "Iteration 22, loss = 0.52527235\n",
      "Iteration 23, loss = 0.52520842\n",
      "Iteration 24, loss = 0.52529477\n",
      "Iteration 25, loss = 0.52324641\n",
      "Iteration 26, loss = 0.52427481\n",
      "Iteration 27, loss = 0.52377392\n",
      "Iteration 28, loss = 0.52607025\n",
      "Iteration 29, loss = 0.52381071\n",
      "Iteration 30, loss = 0.52263202\n",
      "Iteration 31, loss = 0.52490217\n",
      "Iteration 32, loss = 0.52202698\n",
      "Iteration 33, loss = 0.52292134\n",
      "Iteration 34, loss = 0.52268860\n",
      "Iteration 35, loss = 0.52101067\n",
      "Iteration 36, loss = 0.52177001\n",
      "Iteration 37, loss = 0.52067712\n",
      "Iteration 38, loss = 0.52176447\n",
      "Iteration 39, loss = 0.52209709\n",
      "Iteration 40, loss = 0.52267418\n",
      "Iteration 41, loss = 0.52104093\n",
      "Iteration 42, loss = 0.52259014\n",
      "Iteration 43, loss = 0.52151339\n",
      "Iteration 44, loss = 0.52194463\n",
      "Iteration 45, loss = 0.52035947\n",
      "Iteration 46, loss = 0.52032830\n",
      "Iteration 47, loss = 0.52000720\n",
      "Iteration 48, loss = 0.51983993\n",
      "Iteration 49, loss = 0.52028101\n",
      "Iteration 50, loss = 0.52239861\n",
      "Iteration 51, loss = 0.52031537\n",
      "Iteration 52, loss = 0.52210167\n",
      "Iteration 53, loss = 0.51912654\n",
      "Iteration 54, loss = 0.51992842\n",
      "Iteration 55, loss = 0.52021542\n",
      "Iteration 56, loss = 0.51909731\n",
      "Iteration 57, loss = 0.51894591\n",
      "Iteration 58, loss = 0.51936254\n",
      "Iteration 59, loss = 0.51952130\n",
      "Iteration 60, loss = 0.51946439\n",
      "Iteration 61, loss = 0.51922833\n",
      "Iteration 62, loss = 0.52094147\n",
      "Iteration 63, loss = 0.52117105\n",
      "Iteration 64, loss = 0.52014013\n",
      "Iteration 65, loss = 0.51935762\n",
      "Iteration 66, loss = 0.52055121\n",
      "Iteration 67, loss = 0.52098327\n",
      "Iteration 68, loss = 0.51929415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64068550\n",
      "Iteration 2, loss = 0.53985150\n",
      "Iteration 3, loss = 0.53411560\n",
      "Iteration 4, loss = 0.53215510\n",
      "Iteration 5, loss = 0.53079483\n",
      "Iteration 6, loss = 0.52993506\n",
      "Iteration 7, loss = 0.52843186\n",
      "Iteration 8, loss = 0.52771756\n",
      "Iteration 9, loss = 0.52773229\n",
      "Iteration 10, loss = 0.52735904\n",
      "Iteration 11, loss = 0.52715078\n",
      "Iteration 12, loss = 0.52582993\n",
      "Iteration 13, loss = 0.52600972\n",
      "Iteration 14, loss = 0.52525316\n",
      "Iteration 15, loss = 0.52381828\n",
      "Iteration 16, loss = 0.52301519\n",
      "Iteration 17, loss = 0.52458939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.52276776\n",
      "Iteration 19, loss = 0.52183813\n",
      "Iteration 20, loss = 0.52271784\n",
      "Iteration 21, loss = 0.52223214\n",
      "Iteration 22, loss = 0.52195187\n",
      "Iteration 23, loss = 0.52068430\n",
      "Iteration 24, loss = 0.52008143\n",
      "Iteration 25, loss = 0.51940324\n",
      "Iteration 26, loss = 0.52017902\n",
      "Iteration 27, loss = 0.51951860\n",
      "Iteration 28, loss = 0.51963632\n",
      "Iteration 29, loss = 0.51931753\n",
      "Iteration 30, loss = 0.51890910\n",
      "Iteration 31, loss = 0.51988427\n",
      "Iteration 32, loss = 0.51795752\n",
      "Iteration 33, loss = 0.51797641\n",
      "Iteration 34, loss = 0.51812369\n",
      "Iteration 35, loss = 0.51848172\n",
      "Iteration 36, loss = 0.51825081\n",
      "Iteration 37, loss = 0.51781136\n",
      "Iteration 38, loss = 0.51704414\n",
      "Iteration 39, loss = 0.51765512\n",
      "Iteration 40, loss = 0.51999749\n",
      "Iteration 41, loss = 0.51708123\n",
      "Iteration 42, loss = 0.51772104\n",
      "Iteration 43, loss = 0.52042349\n",
      "Iteration 44, loss = 0.51827798\n",
      "Iteration 45, loss = 0.51699350\n",
      "Iteration 46, loss = 0.51705467\n",
      "Iteration 47, loss = 0.51822493\n",
      "Iteration 48, loss = 0.51642361\n",
      "Iteration 49, loss = 0.51675335\n",
      "Iteration 50, loss = 0.51639760\n",
      "Iteration 51, loss = 0.51636012\n",
      "Iteration 52, loss = 0.51711568\n",
      "Iteration 53, loss = 0.51681013\n",
      "Iteration 54, loss = 0.51684005\n",
      "Iteration 55, loss = 0.51798589\n",
      "Iteration 56, loss = 0.51833915\n",
      "Iteration 57, loss = 0.51704596\n",
      "Iteration 58, loss = 0.51813745\n",
      "Iteration 59, loss = 0.51763096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63826098\n",
      "Iteration 2, loss = 0.54102591\n",
      "Iteration 3, loss = 0.53510926\n",
      "Iteration 4, loss = 0.53381382\n",
      "Iteration 5, loss = 0.53365046\n",
      "Iteration 6, loss = 0.53204586\n",
      "Iteration 7, loss = 0.53136502\n",
      "Iteration 8, loss = 0.53073545\n",
      "Iteration 9, loss = 0.53003615\n",
      "Iteration 10, loss = 0.52955534\n",
      "Iteration 11, loss = 0.52902029\n",
      "Iteration 12, loss = 0.52945073\n",
      "Iteration 13, loss = 0.52865923\n",
      "Iteration 14, loss = 0.52795298\n",
      "Iteration 15, loss = 0.52780409\n",
      "Iteration 16, loss = 0.52663337\n",
      "Iteration 17, loss = 0.52682096\n",
      "Iteration 18, loss = 0.52593610\n",
      "Iteration 19, loss = 0.52542831\n",
      "Iteration 20, loss = 0.52465834\n",
      "Iteration 21, loss = 0.52434476\n",
      "Iteration 22, loss = 0.52370900\n",
      "Iteration 23, loss = 0.52279346\n",
      "Iteration 24, loss = 0.52372155\n",
      "Iteration 25, loss = 0.52315740\n",
      "Iteration 26, loss = 0.52180496\n",
      "Iteration 27, loss = 0.52174697\n",
      "Iteration 28, loss = 0.52257844\n",
      "Iteration 29, loss = 0.52208856\n",
      "Iteration 30, loss = 0.52264810\n",
      "Iteration 31, loss = 0.52093111\n",
      "Iteration 32, loss = 0.52190730\n",
      "Iteration 33, loss = 0.52247140\n",
      "Iteration 34, loss = 0.52159076\n",
      "Iteration 35, loss = 0.52108396\n",
      "Iteration 36, loss = 0.52069499\n",
      "Iteration 37, loss = 0.52119177\n",
      "Iteration 38, loss = 0.52060193\n",
      "Iteration 39, loss = 0.52025666\n",
      "Iteration 40, loss = 0.52073976\n",
      "Iteration 41, loss = 0.52014183\n",
      "Iteration 42, loss = 0.52125245\n",
      "Iteration 43, loss = 0.52110373\n",
      "Iteration 44, loss = 0.51904595\n",
      "Iteration 45, loss = 0.52024329\n",
      "Iteration 46, loss = 0.52001048\n",
      "Iteration 47, loss = 0.52103611\n",
      "Iteration 48, loss = 0.51918482\n",
      "Iteration 49, loss = 0.52019547\n",
      "Iteration 50, loss = 0.52000419\n",
      "Iteration 51, loss = 0.51899084\n",
      "Iteration 52, loss = 0.51934164\n",
      "Iteration 53, loss = 0.51989870\n",
      "Iteration 54, loss = 0.51911652\n",
      "Iteration 55, loss = 0.51954733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58312426\n",
      "Iteration 2, loss = 0.53855064\n",
      "Iteration 3, loss = 0.52093482\n",
      "Iteration 4, loss = 0.50993709\n",
      "Iteration 5, loss = 0.49983641\n",
      "Iteration 6, loss = 0.49315335\n",
      "Iteration 7, loss = 0.48958003\n",
      "Iteration 8, loss = 0.48805267\n",
      "Iteration 9, loss = 0.48597682\n",
      "Iteration 10, loss = 0.48334983\n",
      "Iteration 11, loss = 0.48313444\n",
      "Iteration 12, loss = 0.48065183\n",
      "Iteration 13, loss = 0.47920040\n",
      "Iteration 14, loss = 0.47882834\n",
      "Iteration 15, loss = 0.47862566\n",
      "Iteration 16, loss = 0.47814547\n",
      "Iteration 17, loss = 0.47717992\n",
      "Iteration 18, loss = 0.47707303\n",
      "Iteration 19, loss = 0.47553237\n",
      "Iteration 20, loss = 0.47515096\n",
      "Iteration 21, loss = 0.47557860\n",
      "Iteration 22, loss = 0.47437861\n",
      "Iteration 23, loss = 0.47275805\n",
      "Iteration 24, loss = 0.47333601\n",
      "Iteration 25, loss = 0.47136470\n",
      "Iteration 26, loss = 0.47394140\n",
      "Iteration 27, loss = 0.47106937\n",
      "Iteration 28, loss = 0.47082150\n",
      "Iteration 29, loss = 0.46964512\n",
      "Iteration 30, loss = 0.47075340\n",
      "Iteration 31, loss = 0.47106758\n",
      "Iteration 32, loss = 0.46887852\n",
      "Iteration 33, loss = 0.46834302\n",
      "Iteration 34, loss = 0.46805212\n",
      "Iteration 35, loss = 0.46535441\n",
      "Iteration 36, loss = 0.46652920\n",
      "Iteration 37, loss = 0.46633349\n",
      "Iteration 38, loss = 0.46623415\n",
      "Iteration 39, loss = 0.46476891\n",
      "Iteration 40, loss = 0.46709100\n",
      "Iteration 41, loss = 0.46399874\n",
      "Iteration 42, loss = 0.46615755\n",
      "Iteration 43, loss = 0.46609345\n",
      "Iteration 44, loss = 0.46373147\n",
      "Iteration 45, loss = 0.46372472\n",
      "Iteration 46, loss = 0.46296973\n",
      "Iteration 47, loss = 0.46221952\n",
      "Iteration 48, loss = 0.46142612\n",
      "Iteration 49, loss = 0.46130763\n",
      "Iteration 50, loss = 0.46242996\n",
      "Iteration 51, loss = 0.46127706\n",
      "Iteration 52, loss = 0.46345652\n",
      "Iteration 53, loss = 0.46010067\n",
      "Iteration 54, loss = 0.45940191\n",
      "Iteration 55, loss = 0.45898266\n",
      "Iteration 56, loss = 0.46079186\n",
      "Iteration 57, loss = 0.45859123\n",
      "Iteration 58, loss = 0.45798740\n",
      "Iteration 59, loss = 0.45729644\n",
      "Iteration 60, loss = 0.45686502\n",
      "Iteration 61, loss = 0.45814508\n",
      "Iteration 62, loss = 0.45956114\n",
      "Iteration 63, loss = 0.45703256\n",
      "Iteration 64, loss = 0.45542617\n",
      "Iteration 65, loss = 0.45581977\n",
      "Iteration 66, loss = 0.45508129\n",
      "Iteration 67, loss = 0.45524252\n",
      "Iteration 68, loss = 0.45520270\n",
      "Iteration 69, loss = 0.45493275\n",
      "Iteration 70, loss = 0.45337001\n",
      "Iteration 71, loss = 0.45408376\n",
      "Iteration 72, loss = 0.45230666\n",
      "Iteration 73, loss = 0.45458027\n",
      "Iteration 74, loss = 0.45392816\n",
      "Iteration 75, loss = 0.45199958\n",
      "Iteration 76, loss = 0.45167716\n",
      "Iteration 77, loss = 0.45074881\n",
      "Iteration 78, loss = 0.45053744\n",
      "Iteration 79, loss = 0.45023196\n",
      "Iteration 80, loss = 0.45106267\n",
      "Iteration 81, loss = 0.45139504\n",
      "Iteration 82, loss = 0.45043753\n",
      "Iteration 83, loss = 0.44977481\n",
      "Iteration 84, loss = 0.45164826\n",
      "Iteration 85, loss = 0.45011460\n",
      "Iteration 86, loss = 0.44966812\n",
      "Iteration 87, loss = 0.45049849\n",
      "Iteration 88, loss = 0.44930123\n",
      "Iteration 89, loss = 0.44959572\n",
      "Iteration 90, loss = 0.44965765\n",
      "Iteration 91, loss = 0.44827827\n",
      "Iteration 92, loss = 0.44908158\n",
      "Iteration 93, loss = 0.44833481\n",
      "Iteration 94, loss = 0.44881496\n",
      "Iteration 95, loss = 0.45218507\n",
      "Iteration 96, loss = 0.44916933\n",
      "Iteration 97, loss = 0.44935129\n",
      "Iteration 98, loss = 0.44885654\n",
      "Iteration 99, loss = 0.44892525\n",
      "Iteration 100, loss = 0.44902340\n",
      "Iteration 101, loss = 0.44990540\n",
      "Iteration 102, loss = 0.45004179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58272759\n",
      "Iteration 2, loss = 0.53779432\n",
      "Iteration 3, loss = 0.51930991\n",
      "Iteration 4, loss = 0.50681383\n",
      "Iteration 5, loss = 0.49516122\n",
      "Iteration 6, loss = 0.48922938\n",
      "Iteration 7, loss = 0.48656422\n",
      "Iteration 8, loss = 0.48467531\n",
      "Iteration 9, loss = 0.48258223\n",
      "Iteration 10, loss = 0.48131965\n",
      "Iteration 11, loss = 0.47939083\n",
      "Iteration 12, loss = 0.47828938\n",
      "Iteration 13, loss = 0.47687510\n",
      "Iteration 14, loss = 0.47605443\n",
      "Iteration 15, loss = 0.47555182\n",
      "Iteration 16, loss = 0.47475648\n",
      "Iteration 17, loss = 0.47529182\n",
      "Iteration 18, loss = 0.47379826\n",
      "Iteration 19, loss = 0.47383086\n",
      "Iteration 20, loss = 0.47485397\n",
      "Iteration 21, loss = 0.47442971\n",
      "Iteration 22, loss = 0.47359737\n",
      "Iteration 23, loss = 0.47127175\n",
      "Iteration 24, loss = 0.47128013\n",
      "Iteration 25, loss = 0.47010790\n",
      "Iteration 26, loss = 0.47204438\n",
      "Iteration 27, loss = 0.46981647\n",
      "Iteration 28, loss = 0.46896561\n",
      "Iteration 29, loss = 0.46812159\n",
      "Iteration 30, loss = 0.46805731\n",
      "Iteration 31, loss = 0.46835135\n",
      "Iteration 32, loss = 0.46679414\n",
      "Iteration 33, loss = 0.46734845\n",
      "Iteration 34, loss = 0.46678090\n",
      "Iteration 35, loss = 0.46574376\n",
      "Iteration 36, loss = 0.46626088\n",
      "Iteration 37, loss = 0.46588150\n",
      "Iteration 38, loss = 0.46504059\n",
      "Iteration 39, loss = 0.46505721\n",
      "Iteration 40, loss = 0.46771297\n",
      "Iteration 41, loss = 0.46405332\n",
      "Iteration 42, loss = 0.46417758\n",
      "Iteration 43, loss = 0.46689458\n",
      "Iteration 44, loss = 0.46430350\n",
      "Iteration 45, loss = 0.46361598\n",
      "Iteration 46, loss = 0.46275914\n",
      "Iteration 47, loss = 0.46254124\n",
      "Iteration 48, loss = 0.46159132\n",
      "Iteration 49, loss = 0.46212179\n",
      "Iteration 50, loss = 0.46140590\n",
      "Iteration 51, loss = 0.46167463\n",
      "Iteration 52, loss = 0.46051087\n",
      "Iteration 53, loss = 0.45998660\n",
      "Iteration 54, loss = 0.46015891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.45936492\n",
      "Iteration 56, loss = 0.46138503\n",
      "Iteration 57, loss = 0.45901235\n",
      "Iteration 58, loss = 0.45891503\n",
      "Iteration 59, loss = 0.45911214\n",
      "Iteration 60, loss = 0.45759151\n",
      "Iteration 61, loss = 0.45811599\n",
      "Iteration 62, loss = 0.46028786\n",
      "Iteration 63, loss = 0.46038834\n",
      "Iteration 64, loss = 0.45592254\n",
      "Iteration 65, loss = 0.45876568\n",
      "Iteration 66, loss = 0.45625925\n",
      "Iteration 67, loss = 0.45627119\n",
      "Iteration 68, loss = 0.45691608\n",
      "Iteration 69, loss = 0.45585599\n",
      "Iteration 70, loss = 0.45399674\n",
      "Iteration 71, loss = 0.45400603\n",
      "Iteration 72, loss = 0.45430966\n",
      "Iteration 73, loss = 0.45570063\n",
      "Iteration 74, loss = 0.45356660\n",
      "Iteration 75, loss = 0.45372282\n",
      "Iteration 76, loss = 0.45229062\n",
      "Iteration 77, loss = 0.45298256\n",
      "Iteration 78, loss = 0.45359929\n",
      "Iteration 79, loss = 0.45245616\n",
      "Iteration 80, loss = 0.45280324\n",
      "Iteration 81, loss = 0.45172983\n",
      "Iteration 82, loss = 0.45135542\n",
      "Iteration 83, loss = 0.45032725\n",
      "Iteration 84, loss = 0.45085885\n",
      "Iteration 85, loss = 0.45235344\n",
      "Iteration 86, loss = 0.45095733\n",
      "Iteration 87, loss = 0.45027184\n",
      "Iteration 88, loss = 0.45018167\n",
      "Iteration 89, loss = 0.45106116\n",
      "Iteration 90, loss = 0.45074307\n",
      "Iteration 91, loss = 0.45013900\n",
      "Iteration 92, loss = 0.45158939\n",
      "Iteration 93, loss = 0.44976582\n",
      "Iteration 94, loss = 0.45049606\n",
      "Iteration 95, loss = 0.45073007\n",
      "Iteration 96, loss = 0.45084869\n",
      "Iteration 97, loss = 0.45028118\n",
      "Iteration 98, loss = 0.45145974\n",
      "Iteration 99, loss = 0.45044193\n",
      "Iteration 100, loss = 0.45000723\n",
      "Iteration 101, loss = 0.45241294\n",
      "Iteration 102, loss = 0.45220022\n",
      "Iteration 103, loss = 0.45236118\n",
      "Iteration 104, loss = 0.45191979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58169841\n",
      "Iteration 2, loss = 0.53799648\n",
      "Iteration 3, loss = 0.52103407\n",
      "Iteration 4, loss = 0.50952418\n",
      "Iteration 5, loss = 0.50222141\n",
      "Iteration 6, loss = 0.49177049\n",
      "Iteration 7, loss = 0.48863699\n",
      "Iteration 8, loss = 0.48647730\n",
      "Iteration 9, loss = 0.48367471\n",
      "Iteration 10, loss = 0.48223849\n",
      "Iteration 11, loss = 0.48011710\n",
      "Iteration 12, loss = 0.47909848\n",
      "Iteration 13, loss = 0.47719646\n",
      "Iteration 14, loss = 0.47586431\n",
      "Iteration 15, loss = 0.47566890\n",
      "Iteration 16, loss = 0.47432349\n",
      "Iteration 17, loss = 0.47355584\n",
      "Iteration 18, loss = 0.47385496\n",
      "Iteration 19, loss = 0.47313575\n",
      "Iteration 20, loss = 0.47277223\n",
      "Iteration 21, loss = 0.47194729\n",
      "Iteration 22, loss = 0.47224909\n",
      "Iteration 23, loss = 0.47199123\n",
      "Iteration 24, loss = 0.47197565\n",
      "Iteration 25, loss = 0.47105726\n",
      "Iteration 26, loss = 0.47084782\n",
      "Iteration 27, loss = 0.46988411\n",
      "Iteration 28, loss = 0.46881223\n",
      "Iteration 29, loss = 0.46819089\n",
      "Iteration 30, loss = 0.46901174\n",
      "Iteration 31, loss = 0.46783160\n",
      "Iteration 32, loss = 0.46809811\n",
      "Iteration 33, loss = 0.46915315\n",
      "Iteration 34, loss = 0.46753182\n",
      "Iteration 35, loss = 0.46714074\n",
      "Iteration 36, loss = 0.46727496\n",
      "Iteration 37, loss = 0.46707833\n",
      "Iteration 38, loss = 0.46554997\n",
      "Iteration 39, loss = 0.46540429\n",
      "Iteration 40, loss = 0.46768457\n",
      "Iteration 41, loss = 0.46557851\n",
      "Iteration 42, loss = 0.46528131\n",
      "Iteration 43, loss = 0.46606273\n",
      "Iteration 44, loss = 0.46327509\n",
      "Iteration 45, loss = 0.46380971\n",
      "Iteration 46, loss = 0.46388589\n",
      "Iteration 47, loss = 0.46297592\n",
      "Iteration 48, loss = 0.46287888\n",
      "Iteration 49, loss = 0.46359706\n",
      "Iteration 50, loss = 0.46194446\n",
      "Iteration 51, loss = 0.46262743\n",
      "Iteration 52, loss = 0.46115521\n",
      "Iteration 53, loss = 0.46098836\n",
      "Iteration 54, loss = 0.46102497\n",
      "Iteration 55, loss = 0.46166116\n",
      "Iteration 56, loss = 0.45979958\n",
      "Iteration 57, loss = 0.46000285\n",
      "Iteration 58, loss = 0.46079046\n",
      "Iteration 59, loss = 0.46140973\n",
      "Iteration 60, loss = 0.45870582\n",
      "Iteration 61, loss = 0.45935402\n",
      "Iteration 62, loss = 0.45821103\n",
      "Iteration 63, loss = 0.45936671\n",
      "Iteration 64, loss = 0.45630609\n",
      "Iteration 65, loss = 0.45887205\n",
      "Iteration 66, loss = 0.45985984\n",
      "Iteration 67, loss = 0.45915894\n",
      "Iteration 68, loss = 0.45634495\n",
      "Iteration 69, loss = 0.45636105\n",
      "Iteration 70, loss = 0.45450773\n",
      "Iteration 71, loss = 0.45522650\n",
      "Iteration 72, loss = 0.45652408\n",
      "Iteration 73, loss = 0.45498734\n",
      "Iteration 74, loss = 0.45468611\n",
      "Iteration 75, loss = 0.45389004\n",
      "Iteration 76, loss = 0.45520521\n",
      "Iteration 77, loss = 0.45462125\n",
      "Iteration 78, loss = 0.45368315\n",
      "Iteration 79, loss = 0.45168278\n",
      "Iteration 80, loss = 0.45206679\n",
      "Iteration 81, loss = 0.45352157\n",
      "Iteration 82, loss = 0.45209243\n",
      "Iteration 83, loss = 0.45211746\n",
      "Iteration 84, loss = 0.45164888\n",
      "Iteration 85, loss = 0.45329865\n",
      "Iteration 86, loss = 0.45271788\n",
      "Iteration 87, loss = 0.45083835\n",
      "Iteration 88, loss = 0.45032385\n",
      "Iteration 89, loss = 0.45031581\n",
      "Iteration 90, loss = 0.45240892\n",
      "Iteration 91, loss = 0.44973283\n",
      "Iteration 92, loss = 0.45030725\n",
      "Iteration 93, loss = 0.45153050\n",
      "Iteration 94, loss = 0.45220675\n",
      "Iteration 95, loss = 0.45020974\n",
      "Iteration 96, loss = 0.44969490\n",
      "Iteration 97, loss = 0.44997088\n",
      "Iteration 98, loss = 0.44924490\n",
      "Iteration 99, loss = 0.44944828\n",
      "Iteration 100, loss = 0.44909616\n",
      "Iteration 101, loss = 0.45002368\n",
      "Iteration 102, loss = 0.45191832\n",
      "Iteration 103, loss = 0.45142566\n",
      "Iteration 104, loss = 0.44904017\n",
      "Iteration 105, loss = 0.44998851\n",
      "Iteration 106, loss = 0.45055991\n",
      "Iteration 107, loss = 0.45165777\n",
      "Iteration 108, loss = 0.44966194\n",
      "Iteration 109, loss = 0.45034729\n",
      "Iteration 110, loss = 0.45078206\n",
      "Iteration 111, loss = 0.44934326\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56489063\n",
      "Iteration 2, loss = 0.55499117\n",
      "Iteration 3, loss = 0.55376891\n",
      "Iteration 4, loss = 0.55350893\n",
      "Iteration 5, loss = 0.55260352\n",
      "Iteration 6, loss = 0.55249609\n",
      "Iteration 7, loss = 0.55224085\n",
      "Iteration 8, loss = 0.55257293\n",
      "Iteration 9, loss = 0.55218850\n",
      "Iteration 10, loss = 0.55222507\n",
      "Iteration 11, loss = 0.55278697\n",
      "Iteration 12, loss = 0.55213744\n",
      "Iteration 13, loss = 0.55207848\n",
      "Iteration 14, loss = 0.55221655\n",
      "Iteration 15, loss = 0.55200006\n",
      "Iteration 16, loss = 0.55247811\n",
      "Iteration 17, loss = 0.55220878\n",
      "Iteration 18, loss = 0.55217403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56259227\n",
      "Iteration 2, loss = 0.55393616\n",
      "Iteration 3, loss = 0.55261825\n",
      "Iteration 4, loss = 0.55188862\n",
      "Iteration 5, loss = 0.55113071\n",
      "Iteration 6, loss = 0.55124747\n",
      "Iteration 7, loss = 0.55083866\n",
      "Iteration 8, loss = 0.55091269\n",
      "Iteration 9, loss = 0.55069101\n",
      "Iteration 10, loss = 0.55112008\n",
      "Iteration 11, loss = 0.55092774\n",
      "Iteration 12, loss = 0.55068056\n",
      "Iteration 13, loss = 0.55098072\n",
      "Iteration 14, loss = 0.55072565\n",
      "Iteration 15, loss = 0.55059743\n",
      "Iteration 16, loss = 0.55017980\n",
      "Iteration 17, loss = 0.55156788\n",
      "Iteration 18, loss = 0.55088177\n",
      "Iteration 19, loss = 0.55085561\n",
      "Iteration 20, loss = 0.55181373\n",
      "Iteration 21, loss = 0.55145919\n",
      "Iteration 22, loss = 0.55063080\n",
      "Iteration 23, loss = 0.55070192\n",
      "Iteration 24, loss = 0.55067489\n",
      "Iteration 25, loss = 0.55052558\n",
      "Iteration 26, loss = 0.55096938\n",
      "Iteration 27, loss = 0.55083681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56290004\n",
      "Iteration 2, loss = 0.55501467\n",
      "Iteration 3, loss = 0.55373329\n",
      "Iteration 4, loss = 0.55319929\n",
      "Iteration 5, loss = 0.55342693\n",
      "Iteration 6, loss = 0.55300646\n",
      "Iteration 7, loss = 0.55323569\n",
      "Iteration 8, loss = 0.55288925\n",
      "Iteration 9, loss = 0.55275103\n",
      "Iteration 10, loss = 0.55314956\n",
      "Iteration 11, loss = 0.55291382\n",
      "Iteration 12, loss = 0.55323345\n",
      "Iteration 13, loss = 0.55271868\n",
      "Iteration 14, loss = 0.55282494\n",
      "Iteration 15, loss = 0.55298559\n",
      "Iteration 16, loss = 0.55252026\n",
      "Iteration 17, loss = 0.55268541\n",
      "Iteration 18, loss = 0.55309765\n",
      "Iteration 19, loss = 0.55278863\n",
      "Iteration 20, loss = 0.55254440\n",
      "Iteration 21, loss = 0.55276723\n",
      "Iteration 22, loss = 0.55283873\n",
      "Iteration 23, loss = 0.55252424\n",
      "Iteration 24, loss = 0.55297808\n",
      "Iteration 25, loss = 0.55277405\n",
      "Iteration 26, loss = 0.55277459\n",
      "Iteration 27, loss = 0.55311309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55056874\n",
      "Iteration 2, loss = 0.53552690\n",
      "Iteration 3, loss = 0.53229577\n",
      "Iteration 4, loss = 0.53318628\n",
      "Iteration 5, loss = 0.53262048\n",
      "Iteration 6, loss = 0.53300465\n",
      "Iteration 7, loss = 0.53278111\n",
      "Iteration 8, loss = 0.53345950\n",
      "Iteration 9, loss = 0.53285344\n",
      "Iteration 10, loss = 0.53279294\n",
      "Iteration 11, loss = 0.53386896\n",
      "Iteration 12, loss = 0.53259803\n",
      "Iteration 13, loss = 0.53270418\n",
      "Iteration 14, loss = 0.53277300\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54751508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53126720\n",
      "Iteration 3, loss = 0.52862465\n",
      "Iteration 4, loss = 0.52859518\n",
      "Iteration 5, loss = 0.52914275\n",
      "Iteration 6, loss = 0.52859450\n",
      "Iteration 7, loss = 0.52845498\n",
      "Iteration 8, loss = 0.52896159\n",
      "Iteration 9, loss = 0.52889030\n",
      "Iteration 10, loss = 0.52906378\n",
      "Iteration 11, loss = 0.52902667\n",
      "Iteration 12, loss = 0.52906486\n",
      "Iteration 13, loss = 0.52821708\n",
      "Iteration 14, loss = 0.52850519\n",
      "Iteration 15, loss = 0.52847855\n",
      "Iteration 16, loss = 0.52790803\n",
      "Iteration 17, loss = 0.52977635\n",
      "Iteration 18, loss = 0.52862309\n",
      "Iteration 19, loss = 0.52927632\n",
      "Iteration 20, loss = 0.52987027\n",
      "Iteration 21, loss = 0.52947267\n",
      "Iteration 22, loss = 0.52855197\n",
      "Iteration 23, loss = 0.52838221\n",
      "Iteration 24, loss = 0.52839768\n",
      "Iteration 25, loss = 0.52823003\n",
      "Iteration 26, loss = 0.52915225\n",
      "Iteration 27, loss = 0.52859899\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54773419\n",
      "Iteration 2, loss = 0.53231803\n",
      "Iteration 3, loss = 0.52915601\n",
      "Iteration 4, loss = 0.52938233\n",
      "Iteration 5, loss = 0.53008836\n",
      "Iteration 6, loss = 0.52972731\n",
      "Iteration 7, loss = 0.53033466\n",
      "Iteration 8, loss = 0.53038414\n",
      "Iteration 9, loss = 0.52940218\n",
      "Iteration 10, loss = 0.52989508\n",
      "Iteration 11, loss = 0.52973226\n",
      "Iteration 12, loss = 0.53069910\n",
      "Iteration 13, loss = 0.52943690\n",
      "Iteration 14, loss = 0.52951077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 0 min. and 31.489014148712158 sec.\n",
      "\n",
      "21 total combinations for subset length 2.\n",
      "Iteration 1, loss = 0.57696547\n",
      "Iteration 2, loss = 0.55188789\n",
      "Iteration 3, loss = 0.54953808\n",
      "Iteration 4, loss = 0.54831064\n",
      "Iteration 5, loss = 0.54730176\n",
      "Iteration 6, loss = 0.54650353\n",
      "Iteration 7, loss = 0.54643464\n",
      "Iteration 8, loss = 0.54553413\n",
      "Iteration 9, loss = 0.54498249\n",
      "Iteration 10, loss = 0.54500228\n",
      "Iteration 11, loss = 0.54327293\n",
      "Iteration 12, loss = 0.54297937\n",
      "Iteration 13, loss = 0.54165549\n",
      "Iteration 14, loss = 0.54144012\n",
      "Iteration 15, loss = 0.54192231\n",
      "Iteration 16, loss = 0.54004002\n",
      "Iteration 17, loss = 0.54013312\n",
      "Iteration 18, loss = 0.53927805\n",
      "Iteration 19, loss = 0.53982950\n",
      "Iteration 20, loss = 0.54294608\n",
      "Iteration 21, loss = 0.54018638\n",
      "Iteration 22, loss = 0.53892209\n",
      "Iteration 23, loss = 0.53794319\n",
      "Iteration 24, loss = 0.53650261\n",
      "Iteration 25, loss = 0.53584890\n",
      "Iteration 26, loss = 0.53310755\n",
      "Iteration 27, loss = 0.53402013\n",
      "Iteration 28, loss = 0.53438047\n",
      "Iteration 29, loss = 0.53281047\n",
      "Iteration 30, loss = 0.53390376\n",
      "Iteration 31, loss = 0.53189381\n",
      "Iteration 32, loss = 0.53139372\n",
      "Iteration 33, loss = 0.53136300\n",
      "Iteration 34, loss = 0.53111086\n",
      "Iteration 35, loss = 0.53260737\n",
      "Iteration 36, loss = 0.53015673\n",
      "Iteration 37, loss = 0.53000529\n",
      "Iteration 38, loss = 0.52960382\n",
      "Iteration 39, loss = 0.52730434\n",
      "Iteration 40, loss = 0.52879553\n",
      "Iteration 41, loss = 0.53140742\n",
      "Iteration 42, loss = 0.53051452\n",
      "Iteration 43, loss = 0.53085836\n",
      "Iteration 44, loss = 0.53255017\n",
      "Iteration 45, loss = 0.52957923\n",
      "Iteration 46, loss = 0.52920647\n",
      "Iteration 47, loss = 0.53117037\n",
      "Iteration 48, loss = 0.53095721\n",
      "Iteration 49, loss = 0.52930676\n",
      "Iteration 50, loss = 0.53110600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57780899\n",
      "Iteration 2, loss = 0.55088344\n",
      "Iteration 3, loss = 0.54764271\n",
      "Iteration 4, loss = 0.54873736\n",
      "Iteration 5, loss = 0.54530139\n",
      "Iteration 6, loss = 0.54553139\n",
      "Iteration 7, loss = 0.54397499\n",
      "Iteration 8, loss = 0.54535831\n",
      "Iteration 9, loss = 0.54307125\n",
      "Iteration 10, loss = 0.54191430\n",
      "Iteration 11, loss = 0.54164693\n",
      "Iteration 12, loss = 0.54203754\n",
      "Iteration 13, loss = 0.54135037\n",
      "Iteration 14, loss = 0.54097050\n",
      "Iteration 15, loss = 0.54048287\n",
      "Iteration 16, loss = 0.53774627\n",
      "Iteration 17, loss = 0.53920516\n",
      "Iteration 18, loss = 0.53794815\n",
      "Iteration 19, loss = 0.53796609\n",
      "Iteration 20, loss = 0.53776142\n",
      "Iteration 21, loss = 0.53621410\n",
      "Iteration 22, loss = 0.54020378\n",
      "Iteration 23, loss = 0.53545567\n",
      "Iteration 24, loss = 0.53430321\n",
      "Iteration 25, loss = 0.53555074\n",
      "Iteration 26, loss = 0.53350292\n",
      "Iteration 27, loss = 0.53233795\n",
      "Iteration 28, loss = 0.52941390\n",
      "Iteration 29, loss = 0.53016642\n",
      "Iteration 30, loss = 0.52993338\n",
      "Iteration 31, loss = 0.52828316\n",
      "Iteration 32, loss = 0.52779022\n",
      "Iteration 33, loss = 0.52916297\n",
      "Iteration 34, loss = 0.52700480\n",
      "Iteration 35, loss = 0.52749161\n",
      "Iteration 36, loss = 0.52536537\n",
      "Iteration 37, loss = 0.52519750\n",
      "Iteration 38, loss = 0.52775684\n",
      "Iteration 39, loss = 0.52712846\n",
      "Iteration 40, loss = 0.52725409\n",
      "Iteration 41, loss = 0.52640893\n",
      "Iteration 42, loss = 0.52860059\n",
      "Iteration 43, loss = 0.52858296\n",
      "Iteration 44, loss = 0.52727762\n",
      "Iteration 45, loss = 0.52691485\n",
      "Iteration 46, loss = 0.52578914\n",
      "Iteration 47, loss = 0.52668822\n",
      "Iteration 48, loss = 0.52724048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57531908\n",
      "Iteration 2, loss = 0.55025950\n",
      "Iteration 3, loss = 0.54879777\n",
      "Iteration 4, loss = 0.54719132\n",
      "Iteration 5, loss = 0.54762280\n",
      "Iteration 6, loss = 0.54625162\n",
      "Iteration 7, loss = 0.54344130\n",
      "Iteration 8, loss = 0.54757134\n",
      "Iteration 9, loss = 0.54596680\n",
      "Iteration 10, loss = 0.54472466\n",
      "Iteration 11, loss = 0.54781528\n",
      "Iteration 12, loss = 0.54390464\n",
      "Iteration 13, loss = 0.54456549\n",
      "Iteration 14, loss = 0.54454154\n",
      "Iteration 15, loss = 0.54352995\n",
      "Iteration 16, loss = 0.54528166\n",
      "Iteration 17, loss = 0.54281685\n",
      "Iteration 18, loss = 0.54194358\n",
      "Iteration 19, loss = 0.54208314\n",
      "Iteration 20, loss = 0.54182627\n",
      "Iteration 21, loss = 0.54214895\n",
      "Iteration 22, loss = 0.54165948\n",
      "Iteration 23, loss = 0.54180173\n",
      "Iteration 24, loss = 0.54199445\n",
      "Iteration 25, loss = 0.54233874\n",
      "Iteration 26, loss = 0.54114754\n",
      "Iteration 27, loss = 0.54298307\n",
      "Iteration 28, loss = 0.54094943\n",
      "Iteration 29, loss = 0.54027854\n",
      "Iteration 30, loss = 0.54218445\n",
      "Iteration 31, loss = 0.54130031\n",
      "Iteration 32, loss = 0.54210434\n",
      "Iteration 33, loss = 0.53964096\n",
      "Iteration 34, loss = 0.53969282\n",
      "Iteration 35, loss = 0.53554292\n",
      "Iteration 36, loss = 0.53363323\n",
      "Iteration 37, loss = 0.53327819\n",
      "Iteration 38, loss = 0.53565935\n",
      "Iteration 39, loss = 0.53053723\n",
      "Iteration 40, loss = 0.53057976\n",
      "Iteration 41, loss = 0.52972114\n",
      "Iteration 42, loss = 0.52889669\n",
      "Iteration 43, loss = 0.53192819\n",
      "Iteration 44, loss = 0.52781251\n",
      "Iteration 45, loss = 0.52955060\n",
      "Iteration 46, loss = 0.52746121\n",
      "Iteration 47, loss = 0.52816356\n",
      "Iteration 48, loss = 0.52835402\n",
      "Iteration 49, loss = 0.52725892\n",
      "Iteration 50, loss = 0.52616921\n",
      "Iteration 51, loss = 0.52812153\n",
      "Iteration 52, loss = 0.53575324\n",
      "Iteration 53, loss = 0.52761767\n",
      "Iteration 54, loss = 0.52846780\n",
      "Iteration 55, loss = 0.52669764\n",
      "Iteration 56, loss = 0.52682664\n",
      "Iteration 57, loss = 0.52543476\n",
      "Iteration 58, loss = 0.52461150\n",
      "Iteration 59, loss = 0.52468450\n",
      "Iteration 60, loss = 0.52382290\n",
      "Iteration 61, loss = 0.52575234\n",
      "Iteration 62, loss = 0.52549868\n",
      "Iteration 63, loss = 0.52391849\n",
      "Iteration 64, loss = 0.52472508\n",
      "Iteration 65, loss = 0.52400588\n",
      "Iteration 66, loss = 0.52186501\n",
      "Iteration 67, loss = 0.52582402\n",
      "Iteration 68, loss = 0.52249262\n",
      "Iteration 69, loss = 0.52404343\n",
      "Iteration 70, loss = 0.52549002\n",
      "Iteration 71, loss = 0.52563731\n",
      "Iteration 72, loss = 0.52555097\n",
      "Iteration 73, loss = 0.52245894\n",
      "Iteration 74, loss = 0.52630957\n",
      "Iteration 75, loss = 0.52426107\n",
      "Iteration 76, loss = 0.52523564\n",
      "Iteration 77, loss = 0.52688505\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53120571\n",
      "Iteration 2, loss = 0.50081482\n",
      "Iteration 3, loss = 0.49058104\n",
      "Iteration 4, loss = 0.48446267\n",
      "Iteration 5, loss = 0.47859740\n",
      "Iteration 6, loss = 0.47487025\n",
      "Iteration 7, loss = 0.46906034\n",
      "Iteration 8, loss = 0.46272810\n",
      "Iteration 9, loss = 0.46391524\n",
      "Iteration 10, loss = 0.45646815\n",
      "Iteration 11, loss = 0.45730028\n",
      "Iteration 12, loss = 0.45544799\n",
      "Iteration 13, loss = 0.45428535\n",
      "Iteration 14, loss = 0.45452617\n",
      "Iteration 15, loss = 0.45813799\n",
      "Iteration 16, loss = 0.45575019\n",
      "Iteration 17, loss = 0.45484649\n",
      "Iteration 18, loss = 0.45474636\n",
      "Iteration 19, loss = 0.45527212\n",
      "Iteration 20, loss = 0.45321889\n",
      "Iteration 21, loss = 0.45429935\n",
      "Iteration 22, loss = 0.45345452\n",
      "Iteration 23, loss = 0.45447270\n",
      "Iteration 24, loss = 0.45283403\n",
      "Iteration 25, loss = 0.45490613\n",
      "Iteration 26, loss = 0.45445115\n",
      "Iteration 27, loss = 0.45337127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.45348670\n",
      "Iteration 29, loss = 0.45272211\n",
      "Iteration 30, loss = 0.45348351\n",
      "Iteration 31, loss = 0.45308714\n",
      "Iteration 32, loss = 0.45488549\n",
      "Iteration 33, loss = 0.45422382\n",
      "Iteration 34, loss = 0.45483814\n",
      "Iteration 35, loss = 0.45434631\n",
      "Iteration 36, loss = 0.45248993\n",
      "Iteration 37, loss = 0.45351255\n",
      "Iteration 38, loss = 0.45395917\n",
      "Iteration 39, loss = 0.45243004\n",
      "Iteration 40, loss = 0.45464698\n",
      "Iteration 41, loss = 0.45320359\n",
      "Iteration 42, loss = 0.45258027\n",
      "Iteration 43, loss = 0.45445509\n",
      "Iteration 44, loss = 0.45271453\n",
      "Iteration 45, loss = 0.45267267\n",
      "Iteration 46, loss = 0.45273393\n",
      "Iteration 47, loss = 0.45250458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53245291\n",
      "Iteration 2, loss = 0.50130782\n",
      "Iteration 3, loss = 0.49129335\n",
      "Iteration 4, loss = 0.48351092\n",
      "Iteration 5, loss = 0.47619796\n",
      "Iteration 6, loss = 0.46851242\n",
      "Iteration 7, loss = 0.46202320\n",
      "Iteration 8, loss = 0.46038148\n",
      "Iteration 9, loss = 0.45826773\n",
      "Iteration 10, loss = 0.45832227\n",
      "Iteration 11, loss = 0.45711419\n",
      "Iteration 12, loss = 0.45711672\n",
      "Iteration 13, loss = 0.45577309\n",
      "Iteration 14, loss = 0.45705994\n",
      "Iteration 15, loss = 0.45650680\n",
      "Iteration 16, loss = 0.45714928\n",
      "Iteration 17, loss = 0.45612348\n",
      "Iteration 18, loss = 0.45575023\n",
      "Iteration 19, loss = 0.45487021\n",
      "Iteration 20, loss = 0.45643401\n",
      "Iteration 21, loss = 0.45466983\n",
      "Iteration 22, loss = 0.45453233\n",
      "Iteration 23, loss = 0.45518821\n",
      "Iteration 24, loss = 0.45458420\n",
      "Iteration 25, loss = 0.45518704\n",
      "Iteration 26, loss = 0.45538304\n",
      "Iteration 27, loss = 0.45462431\n",
      "Iteration 28, loss = 0.45384984\n",
      "Iteration 29, loss = 0.45504887\n",
      "Iteration 30, loss = 0.45677608\n",
      "Iteration 31, loss = 0.45621161\n",
      "Iteration 32, loss = 0.45492591\n",
      "Iteration 33, loss = 0.45522545\n",
      "Iteration 34, loss = 0.45734978\n",
      "Iteration 35, loss = 0.45545056\n",
      "Iteration 36, loss = 0.45456664\n",
      "Iteration 37, loss = 0.45300506\n",
      "Iteration 38, loss = 0.45521663\n",
      "Iteration 39, loss = 0.45595782\n",
      "Iteration 40, loss = 0.45453234\n",
      "Iteration 41, loss = 0.45446160\n",
      "Iteration 42, loss = 0.45458444\n",
      "Iteration 43, loss = 0.45527292\n",
      "Iteration 44, loss = 0.45411895\n",
      "Iteration 45, loss = 0.45546394\n",
      "Iteration 46, loss = 0.45660918\n",
      "Iteration 47, loss = 0.45547959\n",
      "Iteration 48, loss = 0.45406160\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52901021\n",
      "Iteration 2, loss = 0.50045898\n",
      "Iteration 3, loss = 0.49253088\n",
      "Iteration 4, loss = 0.48390775\n",
      "Iteration 5, loss = 0.47603453\n",
      "Iteration 6, loss = 0.46987665\n",
      "Iteration 7, loss = 0.46319278\n",
      "Iteration 8, loss = 0.46151132\n",
      "Iteration 9, loss = 0.45821015\n",
      "Iteration 10, loss = 0.45586832\n",
      "Iteration 11, loss = 0.45607645\n",
      "Iteration 12, loss = 0.45663921\n",
      "Iteration 13, loss = 0.45559576\n",
      "Iteration 14, loss = 0.45473298\n",
      "Iteration 15, loss = 0.45381840\n",
      "Iteration 16, loss = 0.45472083\n",
      "Iteration 17, loss = 0.45446818\n",
      "Iteration 18, loss = 0.45263055\n",
      "Iteration 19, loss = 0.45351782\n",
      "Iteration 20, loss = 0.45160561\n",
      "Iteration 21, loss = 0.45038194\n",
      "Iteration 22, loss = 0.45063919\n",
      "Iteration 23, loss = 0.45175515\n",
      "Iteration 24, loss = 0.45185965\n",
      "Iteration 25, loss = 0.44992702\n",
      "Iteration 26, loss = 0.44970609\n",
      "Iteration 27, loss = 0.45145187\n",
      "Iteration 28, loss = 0.45182814\n",
      "Iteration 29, loss = 0.45057949\n",
      "Iteration 30, loss = 0.45047241\n",
      "Iteration 31, loss = 0.44961267\n",
      "Iteration 32, loss = 0.44856118\n",
      "Iteration 33, loss = 0.44989510\n",
      "Iteration 34, loss = 0.45081951\n",
      "Iteration 35, loss = 0.45064161\n",
      "Iteration 36, loss = 0.44896665\n",
      "Iteration 37, loss = 0.44934101\n",
      "Iteration 38, loss = 0.44850823\n",
      "Iteration 39, loss = 0.44856673\n",
      "Iteration 40, loss = 0.44759400\n",
      "Iteration 41, loss = 0.44979409\n",
      "Iteration 42, loss = 0.44700097\n",
      "Iteration 43, loss = 0.44891446\n",
      "Iteration 44, loss = 0.44787164\n",
      "Iteration 45, loss = 0.44745982\n",
      "Iteration 46, loss = 0.45016333\n",
      "Iteration 47, loss = 0.44895110\n",
      "Iteration 48, loss = 0.44814484\n",
      "Iteration 49, loss = 0.44824154\n",
      "Iteration 50, loss = 0.44890574\n",
      "Iteration 51, loss = 0.44783605\n",
      "Iteration 52, loss = 0.44883402\n",
      "Iteration 53, loss = 0.44811194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57473204\n",
      "Iteration 2, loss = 0.54019470\n",
      "Iteration 3, loss = 0.53723299\n",
      "Iteration 4, loss = 0.53329055\n",
      "Iteration 5, loss = 0.53240261\n",
      "Iteration 6, loss = 0.53261808\n",
      "Iteration 7, loss = 0.52800937\n",
      "Iteration 8, loss = 0.52533163\n",
      "Iteration 9, loss = 0.52529996\n",
      "Iteration 10, loss = 0.52653046\n",
      "Iteration 11, loss = 0.52473934\n",
      "Iteration 12, loss = 0.52368446\n",
      "Iteration 13, loss = 0.52385584\n",
      "Iteration 14, loss = 0.52466260\n",
      "Iteration 15, loss = 0.52337067\n",
      "Iteration 16, loss = 0.52348877\n",
      "Iteration 17, loss = 0.52438443\n",
      "Iteration 18, loss = 0.52390598\n",
      "Iteration 19, loss = 0.52444239\n",
      "Iteration 20, loss = 0.52316898\n",
      "Iteration 21, loss = 0.52065507\n",
      "Iteration 22, loss = 0.51883876\n",
      "Iteration 23, loss = 0.51924146\n",
      "Iteration 24, loss = 0.51788129\n",
      "Iteration 25, loss = 0.51745648\n",
      "Iteration 26, loss = 0.51782308\n",
      "Iteration 27, loss = 0.51701826\n",
      "Iteration 28, loss = 0.51776868\n",
      "Iteration 29, loss = 0.51750387\n",
      "Iteration 30, loss = 0.51692623\n",
      "Iteration 31, loss = 0.51744843\n",
      "Iteration 32, loss = 0.51487340\n",
      "Iteration 33, loss = 0.51465272\n",
      "Iteration 34, loss = 0.51309615\n",
      "Iteration 35, loss = 0.51475237\n",
      "Iteration 36, loss = 0.51252233\n",
      "Iteration 37, loss = 0.51338855\n",
      "Iteration 38, loss = 0.51425335\n",
      "Iteration 39, loss = 0.51276043\n",
      "Iteration 40, loss = 0.51515100\n",
      "Iteration 41, loss = 0.51326115\n",
      "Iteration 42, loss = 0.51067757\n",
      "Iteration 43, loss = 0.51124793\n",
      "Iteration 44, loss = 0.51177771\n",
      "Iteration 45, loss = 0.51063483\n",
      "Iteration 46, loss = 0.51024553\n",
      "Iteration 47, loss = 0.51049119\n",
      "Iteration 48, loss = 0.51142058\n",
      "Iteration 49, loss = 0.51178836\n",
      "Iteration 50, loss = 0.51277597\n",
      "Iteration 51, loss = 0.51060402\n",
      "Iteration 52, loss = 0.51413992\n",
      "Iteration 53, loss = 0.50996374\n",
      "Iteration 54, loss = 0.51030743\n",
      "Iteration 55, loss = 0.50859563\n",
      "Iteration 56, loss = 0.50797561\n",
      "Iteration 57, loss = 0.50818764\n",
      "Iteration 58, loss = 0.51031243\n",
      "Iteration 59, loss = 0.50751190\n",
      "Iteration 60, loss = 0.50889170\n",
      "Iteration 61, loss = 0.50984957\n",
      "Iteration 62, loss = 0.50841933\n",
      "Iteration 63, loss = 0.50793054\n",
      "Iteration 64, loss = 0.50912413\n",
      "Iteration 65, loss = 0.50821361\n",
      "Iteration 66, loss = 0.50768723\n",
      "Iteration 67, loss = 0.50743831\n",
      "Iteration 68, loss = 0.50740053\n",
      "Iteration 69, loss = 0.50693965\n",
      "Iteration 70, loss = 0.50997566\n",
      "Iteration 71, loss = 0.51092121\n",
      "Iteration 72, loss = 0.50925218\n",
      "Iteration 73, loss = 0.50633763\n",
      "Iteration 74, loss = 0.50678010\n",
      "Iteration 75, loss = 0.50658319\n",
      "Iteration 76, loss = 0.50573999\n",
      "Iteration 77, loss = 0.50514594\n",
      "Iteration 78, loss = 0.50683087\n",
      "Iteration 79, loss = 0.50516858\n",
      "Iteration 80, loss = 0.50793771\n",
      "Iteration 81, loss = 0.50616271\n",
      "Iteration 82, loss = 0.50503498\n",
      "Iteration 83, loss = 0.50559371\n",
      "Iteration 84, loss = 0.50861686\n",
      "Iteration 85, loss = 0.50623877\n",
      "Iteration 86, loss = 0.50796925\n",
      "Iteration 87, loss = 0.50781353\n",
      "Iteration 88, loss = 0.50739790\n",
      "Iteration 89, loss = 0.50597462\n",
      "Iteration 90, loss = 0.50565092\n",
      "Iteration 91, loss = 0.50719360\n",
      "Iteration 92, loss = 0.50474104\n",
      "Iteration 93, loss = 0.50673841\n",
      "Iteration 94, loss = 0.50669918\n",
      "Iteration 95, loss = 0.50700584\n",
      "Iteration 96, loss = 0.50467917\n",
      "Iteration 97, loss = 0.50576541\n",
      "Iteration 98, loss = 0.50520743\n",
      "Iteration 99, loss = 0.50803534\n",
      "Iteration 100, loss = 0.50717199\n",
      "Iteration 101, loss = 0.50601733\n",
      "Iteration 102, loss = 0.50493448\n",
      "Iteration 103, loss = 0.50521133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57475267\n",
      "Iteration 2, loss = 0.54039126\n",
      "Iteration 3, loss = 0.53387144\n",
      "Iteration 4, loss = 0.53265904\n",
      "Iteration 5, loss = 0.52703539\n",
      "Iteration 6, loss = 0.52681256\n",
      "Iteration 7, loss = 0.52353646\n",
      "Iteration 8, loss = 0.52128844\n",
      "Iteration 9, loss = 0.52090276\n",
      "Iteration 10, loss = 0.52087791\n",
      "Iteration 11, loss = 0.52106323\n",
      "Iteration 12, loss = 0.52049689\n",
      "Iteration 13, loss = 0.52078602\n",
      "Iteration 14, loss = 0.52182204\n",
      "Iteration 15, loss = 0.52040627\n",
      "Iteration 16, loss = 0.51908571\n",
      "Iteration 17, loss = 0.51938666\n",
      "Iteration 18, loss = 0.51931825\n",
      "Iteration 19, loss = 0.52045125\n",
      "Iteration 20, loss = 0.52259758\n",
      "Iteration 21, loss = 0.52050983\n",
      "Iteration 22, loss = 0.52187441\n",
      "Iteration 23, loss = 0.51814099\n",
      "Iteration 24, loss = 0.51720297\n",
      "Iteration 25, loss = 0.51775658\n",
      "Iteration 26, loss = 0.51736822\n",
      "Iteration 27, loss = 0.51795852\n",
      "Iteration 28, loss = 0.51662283\n",
      "Iteration 29, loss = 0.51889157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30, loss = 0.51838752\n",
      "Iteration 31, loss = 0.51951854\n",
      "Iteration 32, loss = 0.51554707\n",
      "Iteration 33, loss = 0.51605388\n",
      "Iteration 34, loss = 0.51456765\n",
      "Iteration 35, loss = 0.51245197\n",
      "Iteration 36, loss = 0.50982585\n",
      "Iteration 37, loss = 0.50709205\n",
      "Iteration 38, loss = 0.50668361\n",
      "Iteration 39, loss = 0.50602523\n",
      "Iteration 40, loss = 0.50751587\n",
      "Iteration 41, loss = 0.50873374\n",
      "Iteration 42, loss = 0.50617572\n",
      "Iteration 43, loss = 0.50560005\n",
      "Iteration 44, loss = 0.50412113\n",
      "Iteration 45, loss = 0.50239822\n",
      "Iteration 46, loss = 0.50338473\n",
      "Iteration 47, loss = 0.50519547\n",
      "Iteration 48, loss = 0.50157555\n",
      "Iteration 49, loss = 0.50221187\n",
      "Iteration 50, loss = 0.50159066\n",
      "Iteration 51, loss = 0.50233150\n",
      "Iteration 52, loss = 0.50496902\n",
      "Iteration 53, loss = 0.50353164\n",
      "Iteration 54, loss = 0.50636072\n",
      "Iteration 55, loss = 0.49942297\n",
      "Iteration 56, loss = 0.50133507\n",
      "Iteration 57, loss = 0.49896358\n",
      "Iteration 58, loss = 0.49911981\n",
      "Iteration 59, loss = 0.49978563\n",
      "Iteration 60, loss = 0.49986233\n",
      "Iteration 61, loss = 0.49714330\n",
      "Iteration 62, loss = 0.49761374\n",
      "Iteration 63, loss = 0.49951813\n",
      "Iteration 64, loss = 0.49923738\n",
      "Iteration 65, loss = 0.49850338\n",
      "Iteration 66, loss = 0.49963306\n",
      "Iteration 67, loss = 0.50399966\n",
      "Iteration 68, loss = 0.49621258\n",
      "Iteration 69, loss = 0.49758786\n",
      "Iteration 70, loss = 0.49902319\n",
      "Iteration 71, loss = 0.49683088\n",
      "Iteration 72, loss = 0.49697983\n",
      "Iteration 73, loss = 0.49694933\n",
      "Iteration 74, loss = 0.49790576\n",
      "Iteration 75, loss = 0.49434800\n",
      "Iteration 76, loss = 0.49985632\n",
      "Iteration 77, loss = 0.49688322\n",
      "Iteration 78, loss = 0.49685346\n",
      "Iteration 79, loss = 0.49631723\n",
      "Iteration 80, loss = 0.49664819\n",
      "Iteration 81, loss = 0.49619980\n",
      "Iteration 82, loss = 0.49758792\n",
      "Iteration 83, loss = 0.49868798\n",
      "Iteration 84, loss = 0.49628324\n",
      "Iteration 85, loss = 0.49639179\n",
      "Iteration 86, loss = 0.49571098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57160621\n",
      "Iteration 2, loss = 0.53710198\n",
      "Iteration 3, loss = 0.53482021\n",
      "Iteration 4, loss = 0.53055445\n",
      "Iteration 5, loss = 0.52717363\n",
      "Iteration 6, loss = 0.52666073\n",
      "Iteration 7, loss = 0.52241098\n",
      "Iteration 8, loss = 0.52341257\n",
      "Iteration 9, loss = 0.52282572\n",
      "Iteration 10, loss = 0.52209979\n",
      "Iteration 11, loss = 0.52322982\n",
      "Iteration 12, loss = 0.52157981\n",
      "Iteration 13, loss = 0.52218497\n",
      "Iteration 14, loss = 0.52325213\n",
      "Iteration 15, loss = 0.52019742\n",
      "Iteration 16, loss = 0.52154480\n",
      "Iteration 17, loss = 0.52232349\n",
      "Iteration 18, loss = 0.52070851\n",
      "Iteration 19, loss = 0.52274385\n",
      "Iteration 20, loss = 0.52086917\n",
      "Iteration 21, loss = 0.51968481\n",
      "Iteration 22, loss = 0.52279643\n",
      "Iteration 23, loss = 0.52007282\n",
      "Iteration 24, loss = 0.52013726\n",
      "Iteration 25, loss = 0.52045148\n",
      "Iteration 26, loss = 0.51877059\n",
      "Iteration 27, loss = 0.52260623\n",
      "Iteration 28, loss = 0.52031291\n",
      "Iteration 29, loss = 0.51931658\n",
      "Iteration 30, loss = 0.51917983\n",
      "Iteration 31, loss = 0.51892614\n",
      "Iteration 32, loss = 0.51798293\n",
      "Iteration 33, loss = 0.51729492\n",
      "Iteration 34, loss = 0.51854873\n",
      "Iteration 35, loss = 0.51622165\n",
      "Iteration 36, loss = 0.51931715\n",
      "Iteration 37, loss = 0.51838180\n",
      "Iteration 38, loss = 0.51632674\n",
      "Iteration 39, loss = 0.51636729\n",
      "Iteration 40, loss = 0.51497113\n",
      "Iteration 41, loss = 0.51827445\n",
      "Iteration 42, loss = 0.51691085\n",
      "Iteration 43, loss = 0.51861890\n",
      "Iteration 44, loss = 0.51563409\n",
      "Iteration 45, loss = 0.51670020\n",
      "Iteration 46, loss = 0.51696714\n",
      "Iteration 47, loss = 0.51558394\n",
      "Iteration 48, loss = 0.51485892\n",
      "Iteration 49, loss = 0.51413108\n",
      "Iteration 50, loss = 0.51633099\n",
      "Iteration 51, loss = 0.51511650\n",
      "Iteration 52, loss = 0.52433715\n",
      "Iteration 53, loss = 0.51462315\n",
      "Iteration 54, loss = 0.51285281\n",
      "Iteration 55, loss = 0.51537994\n",
      "Iteration 56, loss = 0.51283207\n",
      "Iteration 57, loss = 0.51202334\n",
      "Iteration 58, loss = 0.51344838\n",
      "Iteration 59, loss = 0.51330688\n",
      "Iteration 60, loss = 0.51145044\n",
      "Iteration 61, loss = 0.51301922\n",
      "Iteration 62, loss = 0.51097272\n",
      "Iteration 63, loss = 0.51161542\n",
      "Iteration 64, loss = 0.51240517\n",
      "Iteration 65, loss = 0.51161097\n",
      "Iteration 66, loss = 0.50941455\n",
      "Iteration 67, loss = 0.51573341\n",
      "Iteration 68, loss = 0.51113732\n",
      "Iteration 69, loss = 0.51236826\n",
      "Iteration 70, loss = 0.51309944\n",
      "Iteration 71, loss = 0.51220669\n",
      "Iteration 72, loss = 0.51088784\n",
      "Iteration 73, loss = 0.51057789\n",
      "Iteration 74, loss = 0.51332748\n",
      "Iteration 75, loss = 0.51017960\n",
      "Iteration 76, loss = 0.51038675\n",
      "Iteration 77, loss = 0.51023516\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54801028\n",
      "Iteration 2, loss = 0.52569654\n",
      "Iteration 3, loss = 0.51727867\n",
      "Iteration 4, loss = 0.50636899\n",
      "Iteration 5, loss = 0.49261150\n",
      "Iteration 6, loss = 0.47981832\n",
      "Iteration 7, loss = 0.46887723\n",
      "Iteration 8, loss = 0.46663421\n",
      "Iteration 9, loss = 0.46489930\n",
      "Iteration 10, loss = 0.45866299\n",
      "Iteration 11, loss = 0.45811503\n",
      "Iteration 12, loss = 0.45793061\n",
      "Iteration 13, loss = 0.45664823\n",
      "Iteration 14, loss = 0.45666128\n",
      "Iteration 15, loss = 0.45688982\n",
      "Iteration 16, loss = 0.45478747\n",
      "Iteration 17, loss = 0.45409202\n",
      "Iteration 18, loss = 0.45600002\n",
      "Iteration 19, loss = 0.45624979\n",
      "Iteration 20, loss = 0.45435367\n",
      "Iteration 21, loss = 0.45518030\n",
      "Iteration 22, loss = 0.45437537\n",
      "Iteration 23, loss = 0.45428895\n",
      "Iteration 24, loss = 0.45354562\n",
      "Iteration 25, loss = 0.45377127\n",
      "Iteration 26, loss = 0.45314792\n",
      "Iteration 27, loss = 0.45298798\n",
      "Iteration 28, loss = 0.45353051\n",
      "Iteration 29, loss = 0.45544458\n",
      "Iteration 30, loss = 0.45362225\n",
      "Iteration 31, loss = 0.45351812\n",
      "Iteration 32, loss = 0.45267832\n",
      "Iteration 33, loss = 0.45379693\n",
      "Iteration 34, loss = 0.45233995\n",
      "Iteration 35, loss = 0.45379524\n",
      "Iteration 36, loss = 0.45297468\n",
      "Iteration 37, loss = 0.45321749\n",
      "Iteration 38, loss = 0.45478920\n",
      "Iteration 39, loss = 0.45344261\n",
      "Iteration 40, loss = 0.45483380\n",
      "Iteration 41, loss = 0.45459154\n",
      "Iteration 42, loss = 0.45500052\n",
      "Iteration 43, loss = 0.45539048\n",
      "Iteration 44, loss = 0.45524528\n",
      "Iteration 45, loss = 0.45349907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54872568\n",
      "Iteration 2, loss = 0.52498399\n",
      "Iteration 3, loss = 0.51849385\n",
      "Iteration 4, loss = 0.51166782\n",
      "Iteration 5, loss = 0.50456395\n",
      "Iteration 6, loss = 0.49895182\n",
      "Iteration 7, loss = 0.49223886\n",
      "Iteration 8, loss = 0.49128138\n",
      "Iteration 9, loss = 0.48657243\n",
      "Iteration 10, loss = 0.48429721\n",
      "Iteration 11, loss = 0.48160932\n",
      "Iteration 12, loss = 0.48034394\n",
      "Iteration 13, loss = 0.47721635\n",
      "Iteration 14, loss = 0.47792952\n",
      "Iteration 15, loss = 0.47286519\n",
      "Iteration 16, loss = 0.46793620\n",
      "Iteration 17, loss = 0.46676329\n",
      "Iteration 18, loss = 0.46639616\n",
      "Iteration 19, loss = 0.46701568\n",
      "Iteration 20, loss = 0.46449374\n",
      "Iteration 21, loss = 0.46025285\n",
      "Iteration 22, loss = 0.45983391\n",
      "Iteration 23, loss = 0.45908040\n",
      "Iteration 24, loss = 0.45839399\n",
      "Iteration 25, loss = 0.45969631\n",
      "Iteration 26, loss = 0.45797887\n",
      "Iteration 27, loss = 0.45811379\n",
      "Iteration 28, loss = 0.45479349\n",
      "Iteration 29, loss = 0.45744706\n",
      "Iteration 30, loss = 0.45691155\n",
      "Iteration 31, loss = 0.45592449\n",
      "Iteration 32, loss = 0.45576409\n",
      "Iteration 33, loss = 0.46031273\n",
      "Iteration 34, loss = 0.45540689\n",
      "Iteration 35, loss = 0.45512142\n",
      "Iteration 36, loss = 0.45410550\n",
      "Iteration 37, loss = 0.45350626\n",
      "Iteration 38, loss = 0.45517915\n",
      "Iteration 39, loss = 0.45538139\n",
      "Iteration 40, loss = 0.46150305\n",
      "Iteration 41, loss = 0.45469426\n",
      "Iteration 42, loss = 0.45497942\n",
      "Iteration 43, loss = 0.45456619\n",
      "Iteration 44, loss = 0.45557339\n",
      "Iteration 45, loss = 0.45322721\n",
      "Iteration 46, loss = 0.45435517\n",
      "Iteration 47, loss = 0.45642707\n",
      "Iteration 48, loss = 0.45327020\n",
      "Iteration 49, loss = 0.45591704\n",
      "Iteration 50, loss = 0.45574302\n",
      "Iteration 51, loss = 0.45769227\n",
      "Iteration 52, loss = 0.45718547\n",
      "Iteration 53, loss = 0.45618506\n",
      "Iteration 54, loss = 0.45611240\n",
      "Iteration 55, loss = 0.45471455\n",
      "Iteration 56, loss = 0.45660518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54776813\n",
      "Iteration 2, loss = 0.52621430\n",
      "Iteration 3, loss = 0.51956018\n",
      "Iteration 4, loss = 0.51229413\n",
      "Iteration 5, loss = 0.50434586\n",
      "Iteration 6, loss = 0.49889779\n",
      "Iteration 7, loss = 0.49259972\n",
      "Iteration 8, loss = 0.49068035\n",
      "Iteration 9, loss = 0.48618359\n",
      "Iteration 10, loss = 0.48157697\n",
      "Iteration 11, loss = 0.47986136\n",
      "Iteration 12, loss = 0.47895237\n",
      "Iteration 13, loss = 0.47747420\n",
      "Iteration 14, loss = 0.48172251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.47542805\n",
      "Iteration 16, loss = 0.47494549\n",
      "Iteration 17, loss = 0.47428993\n",
      "Iteration 18, loss = 0.47286062\n",
      "Iteration 19, loss = 0.47522303\n",
      "Iteration 20, loss = 0.47062498\n",
      "Iteration 21, loss = 0.47087949\n",
      "Iteration 22, loss = 0.47259596\n",
      "Iteration 23, loss = 0.47038374\n",
      "Iteration 24, loss = 0.46921883\n",
      "Iteration 25, loss = 0.46690536\n",
      "Iteration 26, loss = 0.46745405\n",
      "Iteration 27, loss = 0.46883237\n",
      "Iteration 28, loss = 0.46905889\n",
      "Iteration 29, loss = 0.46933953\n",
      "Iteration 30, loss = 0.46671446\n",
      "Iteration 31, loss = 0.46532258\n",
      "Iteration 32, loss = 0.46501216\n",
      "Iteration 33, loss = 0.46836823\n",
      "Iteration 34, loss = 0.46549105\n",
      "Iteration 35, loss = 0.46338320\n",
      "Iteration 36, loss = 0.46260756\n",
      "Iteration 37, loss = 0.46123950\n",
      "Iteration 38, loss = 0.46360959\n",
      "Iteration 39, loss = 0.46202224\n",
      "Iteration 40, loss = 0.46316687\n",
      "Iteration 41, loss = 0.46151414\n",
      "Iteration 42, loss = 0.45940124\n",
      "Iteration 43, loss = 0.46234271\n",
      "Iteration 44, loss = 0.46004867\n",
      "Iteration 45, loss = 0.46006075\n",
      "Iteration 46, loss = 0.46177426\n",
      "Iteration 47, loss = 0.46105146\n",
      "Iteration 48, loss = 0.45787266\n",
      "Iteration 49, loss = 0.45775422\n",
      "Iteration 50, loss = 0.45796690\n",
      "Iteration 51, loss = 0.45565854\n",
      "Iteration 52, loss = 0.45614869\n",
      "Iteration 53, loss = 0.45449534\n",
      "Iteration 54, loss = 0.45388451\n",
      "Iteration 55, loss = 0.45545714\n",
      "Iteration 56, loss = 0.45500200\n",
      "Iteration 57, loss = 0.45270439\n",
      "Iteration 58, loss = 0.45198603\n",
      "Iteration 59, loss = 0.45635965\n",
      "Iteration 60, loss = 0.45249907\n",
      "Iteration 61, loss = 0.45406745\n",
      "Iteration 62, loss = 0.45557131\n",
      "Iteration 63, loss = 0.45306322\n",
      "Iteration 64, loss = 0.45533225\n",
      "Iteration 65, loss = 0.45209426\n",
      "Iteration 66, loss = 0.45165550\n",
      "Iteration 67, loss = 0.45301041\n",
      "Iteration 68, loss = 0.45586097\n",
      "Iteration 69, loss = 0.45423007\n",
      "Iteration 70, loss = 0.45245677\n",
      "Iteration 71, loss = 0.45497568\n",
      "Iteration 72, loss = 0.45024604\n",
      "Iteration 73, loss = 0.45304710\n",
      "Iteration 74, loss = 0.45325266\n",
      "Iteration 75, loss = 0.45440102\n",
      "Iteration 76, loss = 0.45182377\n",
      "Iteration 77, loss = 0.45124265\n",
      "Iteration 78, loss = 0.45143588\n",
      "Iteration 79, loss = 0.45582528\n",
      "Iteration 80, loss = 0.45173738\n",
      "Iteration 81, loss = 0.45378135\n",
      "Iteration 82, loss = 0.45666955\n",
      "Iteration 83, loss = 0.45281098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57661359\n",
      "Iteration 2, loss = 0.55160071\n",
      "Iteration 3, loss = 0.54697306\n",
      "Iteration 4, loss = 0.54509067\n",
      "Iteration 5, loss = 0.54336521\n",
      "Iteration 6, loss = 0.54396845\n",
      "Iteration 7, loss = 0.54283840\n",
      "Iteration 8, loss = 0.54179377\n",
      "Iteration 9, loss = 0.54189916\n",
      "Iteration 10, loss = 0.54141031\n",
      "Iteration 11, loss = 0.54209641\n",
      "Iteration 12, loss = 0.54149225\n",
      "Iteration 13, loss = 0.54044329\n",
      "Iteration 14, loss = 0.54068142\n",
      "Iteration 15, loss = 0.54152618\n",
      "Iteration 16, loss = 0.54056016\n",
      "Iteration 17, loss = 0.54155781\n",
      "Iteration 18, loss = 0.54061924\n",
      "Iteration 19, loss = 0.54159822\n",
      "Iteration 20, loss = 0.54030087\n",
      "Iteration 21, loss = 0.54076869\n",
      "Iteration 22, loss = 0.53977515\n",
      "Iteration 23, loss = 0.54081955\n",
      "Iteration 24, loss = 0.54049995\n",
      "Iteration 25, loss = 0.53994170\n",
      "Iteration 26, loss = 0.53978957\n",
      "Iteration 27, loss = 0.54032434\n",
      "Iteration 28, loss = 0.54010288\n",
      "Iteration 29, loss = 0.54035080\n",
      "Iteration 30, loss = 0.53952516\n",
      "Iteration 31, loss = 0.54003341\n",
      "Iteration 32, loss = 0.53982112\n",
      "Iteration 33, loss = 0.54039799\n",
      "Iteration 34, loss = 0.54048049\n",
      "Iteration 35, loss = 0.54000193\n",
      "Iteration 36, loss = 0.53939538\n",
      "Iteration 37, loss = 0.53997498\n",
      "Iteration 38, loss = 0.53924026\n",
      "Iteration 39, loss = 0.53980603\n",
      "Iteration 40, loss = 0.54091684\n",
      "Iteration 41, loss = 0.54060393\n",
      "Iteration 42, loss = 0.54034976\n",
      "Iteration 43, loss = 0.54189854\n",
      "Iteration 44, loss = 0.54031874\n",
      "Iteration 45, loss = 0.54029828\n",
      "Iteration 46, loss = 0.54013775\n",
      "Iteration 47, loss = 0.54012542\n",
      "Iteration 48, loss = 0.53942772\n",
      "Iteration 49, loss = 0.53952587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57732074\n",
      "Iteration 2, loss = 0.54998194\n",
      "Iteration 3, loss = 0.54562655\n",
      "Iteration 4, loss = 0.54405217\n",
      "Iteration 5, loss = 0.54300585\n",
      "Iteration 6, loss = 0.54269032\n",
      "Iteration 7, loss = 0.54241128\n",
      "Iteration 8, loss = 0.54189867\n",
      "Iteration 9, loss = 0.54183192\n",
      "Iteration 10, loss = 0.54070730\n",
      "Iteration 11, loss = 0.54053216\n",
      "Iteration 12, loss = 0.53984616\n",
      "Iteration 13, loss = 0.54043758\n",
      "Iteration 14, loss = 0.54136681\n",
      "Iteration 15, loss = 0.54146773\n",
      "Iteration 16, loss = 0.53934071\n",
      "Iteration 17, loss = 0.54043617\n",
      "Iteration 18, loss = 0.54071335\n",
      "Iteration 19, loss = 0.54084224\n",
      "Iteration 20, loss = 0.54207442\n",
      "Iteration 21, loss = 0.53968142\n",
      "Iteration 22, loss = 0.54020025\n",
      "Iteration 23, loss = 0.53954974\n",
      "Iteration 24, loss = 0.53909896\n",
      "Iteration 25, loss = 0.53983752\n",
      "Iteration 26, loss = 0.54003008\n",
      "Iteration 27, loss = 0.53990798\n",
      "Iteration 28, loss = 0.53905682\n",
      "Iteration 29, loss = 0.54170413\n",
      "Iteration 30, loss = 0.53986533\n",
      "Iteration 31, loss = 0.54068656\n",
      "Iteration 32, loss = 0.53905087\n",
      "Iteration 33, loss = 0.54003180\n",
      "Iteration 34, loss = 0.53994498\n",
      "Iteration 35, loss = 0.53925657\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57553705\n",
      "Iteration 2, loss = 0.55099083\n",
      "Iteration 3, loss = 0.54925215\n",
      "Iteration 4, loss = 0.54744692\n",
      "Iteration 5, loss = 0.54641970\n",
      "Iteration 6, loss = 0.54515510\n",
      "Iteration 7, loss = 0.54330296\n",
      "Iteration 8, loss = 0.54530983\n",
      "Iteration 9, loss = 0.54524469\n",
      "Iteration 10, loss = 0.54337267\n",
      "Iteration 11, loss = 0.54359976\n",
      "Iteration 12, loss = 0.54282197\n",
      "Iteration 13, loss = 0.54419194\n",
      "Iteration 14, loss = 0.54315740\n",
      "Iteration 15, loss = 0.54293175\n",
      "Iteration 16, loss = 0.54410042\n",
      "Iteration 17, loss = 0.54377954\n",
      "Iteration 18, loss = 0.54245424\n",
      "Iteration 19, loss = 0.54342006\n",
      "Iteration 20, loss = 0.54240369\n",
      "Iteration 21, loss = 0.54214018\n",
      "Iteration 22, loss = 0.54293840\n",
      "Iteration 23, loss = 0.54387346\n",
      "Iteration 24, loss = 0.54384622\n",
      "Iteration 25, loss = 0.54448460\n",
      "Iteration 26, loss = 0.54138301\n",
      "Iteration 27, loss = 0.54520080\n",
      "Iteration 28, loss = 0.54224109\n",
      "Iteration 29, loss = 0.54340716\n",
      "Iteration 30, loss = 0.54476547\n",
      "Iteration 31, loss = 0.54394869\n",
      "Iteration 32, loss = 0.54237811\n",
      "Iteration 33, loss = 0.54272234\n",
      "Iteration 34, loss = 0.54365616\n",
      "Iteration 35, loss = 0.54298920\n",
      "Iteration 36, loss = 0.54393308\n",
      "Iteration 37, loss = 0.54284176\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58070231\n",
      "Iteration 2, loss = 0.53335994\n",
      "Iteration 3, loss = 0.52435346\n",
      "Iteration 4, loss = 0.52289662\n",
      "Iteration 5, loss = 0.52347730\n",
      "Iteration 6, loss = 0.52556995\n",
      "Iteration 7, loss = 0.52590623\n",
      "Iteration 8, loss = 0.52274973\n",
      "Iteration 9, loss = 0.52295078\n",
      "Iteration 10, loss = 0.52431446\n",
      "Iteration 11, loss = 0.52314918\n",
      "Iteration 12, loss = 0.52287819\n",
      "Iteration 13, loss = 0.52225794\n",
      "Iteration 14, loss = 0.52271741\n",
      "Iteration 15, loss = 0.52396318\n",
      "Iteration 16, loss = 0.52260677\n",
      "Iteration 17, loss = 0.52341545\n",
      "Iteration 18, loss = 0.52288696\n",
      "Iteration 19, loss = 0.52358579\n",
      "Iteration 20, loss = 0.52217567\n",
      "Iteration 21, loss = 0.52213190\n",
      "Iteration 22, loss = 0.52167461\n",
      "Iteration 23, loss = 0.52370066\n",
      "Iteration 24, loss = 0.52204477\n",
      "Iteration 25, loss = 0.52285206\n",
      "Iteration 26, loss = 0.52212048\n",
      "Iteration 27, loss = 0.52297254\n",
      "Iteration 28, loss = 0.52286560\n",
      "Iteration 29, loss = 0.52378839\n",
      "Iteration 30, loss = 0.52248662\n",
      "Iteration 31, loss = 0.52203136\n",
      "Iteration 32, loss = 0.52173452\n",
      "Iteration 33, loss = 0.52269720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58276712\n",
      "Iteration 2, loss = 0.53177629\n",
      "Iteration 3, loss = 0.52104764\n",
      "Iteration 4, loss = 0.52066149\n",
      "Iteration 5, loss = 0.51954691\n",
      "Iteration 6, loss = 0.52146579\n",
      "Iteration 7, loss = 0.52065640\n",
      "Iteration 8, loss = 0.52026358\n",
      "Iteration 9, loss = 0.51951501\n",
      "Iteration 10, loss = 0.51910291\n",
      "Iteration 11, loss = 0.51870693\n",
      "Iteration 12, loss = 0.51888609\n",
      "Iteration 13, loss = 0.51952892\n",
      "Iteration 14, loss = 0.51966471\n",
      "Iteration 15, loss = 0.52004688\n",
      "Iteration 16, loss = 0.51786155\n",
      "Iteration 17, loss = 0.51926250\n",
      "Iteration 18, loss = 0.51967111\n",
      "Iteration 19, loss = 0.51928613\n",
      "Iteration 20, loss = 0.52004955\n",
      "Iteration 21, loss = 0.51881276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.51904182\n",
      "Iteration 23, loss = 0.51826884\n",
      "Iteration 24, loss = 0.51803195\n",
      "Iteration 25, loss = 0.51943513\n",
      "Iteration 26, loss = 0.51895681\n",
      "Iteration 27, loss = 0.51841938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58144178\n",
      "Iteration 2, loss = 0.53357915\n",
      "Iteration 3, loss = 0.52394400\n",
      "Iteration 4, loss = 0.52207016\n",
      "Iteration 5, loss = 0.52132907\n",
      "Iteration 6, loss = 0.52111666\n",
      "Iteration 7, loss = 0.51931404\n",
      "Iteration 8, loss = 0.52204761\n",
      "Iteration 9, loss = 0.52183644\n",
      "Iteration 10, loss = 0.52082000\n",
      "Iteration 11, loss = 0.52185181\n",
      "Iteration 12, loss = 0.52064912\n",
      "Iteration 13, loss = 0.52225423\n",
      "Iteration 14, loss = 0.52116159\n",
      "Iteration 15, loss = 0.52045866\n",
      "Iteration 16, loss = 0.52107352\n",
      "Iteration 17, loss = 0.52082411\n",
      "Iteration 18, loss = 0.51979082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53849121\n",
      "Iteration 2, loss = 0.50156541\n",
      "Iteration 3, loss = 0.49027711\n",
      "Iteration 4, loss = 0.48291163\n",
      "Iteration 5, loss = 0.47626110\n",
      "Iteration 6, loss = 0.47157311\n",
      "Iteration 7, loss = 0.46833407\n",
      "Iteration 8, loss = 0.46386863\n",
      "Iteration 9, loss = 0.46438544\n",
      "Iteration 10, loss = 0.46202289\n",
      "Iteration 11, loss = 0.46065720\n",
      "Iteration 12, loss = 0.45922628\n",
      "Iteration 13, loss = 0.45849243\n",
      "Iteration 14, loss = 0.45964101\n",
      "Iteration 15, loss = 0.46053855\n",
      "Iteration 16, loss = 0.45805398\n",
      "Iteration 17, loss = 0.45867365\n",
      "Iteration 18, loss = 0.45672925\n",
      "Iteration 19, loss = 0.45552776\n",
      "Iteration 20, loss = 0.45735364\n",
      "Iteration 21, loss = 0.45490180\n",
      "Iteration 22, loss = 0.45577397\n",
      "Iteration 23, loss = 0.45401536\n",
      "Iteration 24, loss = 0.45242837\n",
      "Iteration 25, loss = 0.45285073\n",
      "Iteration 26, loss = 0.45234453\n",
      "Iteration 27, loss = 0.45313432\n",
      "Iteration 28, loss = 0.45321389\n",
      "Iteration 29, loss = 0.45151826\n",
      "Iteration 30, loss = 0.45232676\n",
      "Iteration 31, loss = 0.45148123\n",
      "Iteration 32, loss = 0.45250223\n",
      "Iteration 33, loss = 0.45012451\n",
      "Iteration 34, loss = 0.45110796\n",
      "Iteration 35, loss = 0.45181078\n",
      "Iteration 36, loss = 0.44826309\n",
      "Iteration 37, loss = 0.44871109\n",
      "Iteration 38, loss = 0.44811430\n",
      "Iteration 39, loss = 0.44765314\n",
      "Iteration 40, loss = 0.44782521\n",
      "Iteration 41, loss = 0.44860317\n",
      "Iteration 42, loss = 0.44524103\n",
      "Iteration 43, loss = 0.44740605\n",
      "Iteration 44, loss = 0.44846867\n",
      "Iteration 45, loss = 0.44521463\n",
      "Iteration 46, loss = 0.44397588\n",
      "Iteration 47, loss = 0.44517137\n",
      "Iteration 48, loss = 0.44661209\n",
      "Iteration 49, loss = 0.44520283\n",
      "Iteration 50, loss = 0.44716338\n",
      "Iteration 51, loss = 0.44373659\n",
      "Iteration 52, loss = 0.44228114\n",
      "Iteration 53, loss = 0.44208321\n",
      "Iteration 54, loss = 0.44458137\n",
      "Iteration 55, loss = 0.44321236\n",
      "Iteration 56, loss = 0.44189674\n",
      "Iteration 57, loss = 0.44097267\n",
      "Iteration 58, loss = 0.44099549\n",
      "Iteration 59, loss = 0.44163741\n",
      "Iteration 60, loss = 0.44839878\n",
      "Iteration 61, loss = 0.43961257\n",
      "Iteration 62, loss = 0.44140728\n",
      "Iteration 63, loss = 0.44353485\n",
      "Iteration 64, loss = 0.44451412\n",
      "Iteration 65, loss = 0.44090623\n",
      "Iteration 66, loss = 0.44149627\n",
      "Iteration 67, loss = 0.44138434\n",
      "Iteration 68, loss = 0.44051038\n",
      "Iteration 69, loss = 0.44374985\n",
      "Iteration 70, loss = 0.43925395\n",
      "Iteration 71, loss = 0.44122273\n",
      "Iteration 72, loss = 0.43845848\n",
      "Iteration 73, loss = 0.44080380\n",
      "Iteration 74, loss = 0.43839761\n",
      "Iteration 75, loss = 0.43818365\n",
      "Iteration 76, loss = 0.44006162\n",
      "Iteration 77, loss = 0.43849892\n",
      "Iteration 78, loss = 0.43738567\n",
      "Iteration 79, loss = 0.43791774\n",
      "Iteration 80, loss = 0.43774669\n",
      "Iteration 81, loss = 0.43634709\n",
      "Iteration 82, loss = 0.43986471\n",
      "Iteration 83, loss = 0.43646787\n",
      "Iteration 84, loss = 0.43814124\n",
      "Iteration 85, loss = 0.43834330\n",
      "Iteration 86, loss = 0.44031903\n",
      "Iteration 87, loss = 0.44686186\n",
      "Iteration 88, loss = 0.43935205\n",
      "Iteration 89, loss = 0.43693836\n",
      "Iteration 90, loss = 0.43584812\n",
      "Iteration 91, loss = 0.43751295\n",
      "Iteration 92, loss = 0.43608755\n",
      "Iteration 93, loss = 0.43872678\n",
      "Iteration 94, loss = 0.43764217\n",
      "Iteration 95, loss = 0.43678263\n",
      "Iteration 96, loss = 0.43764850\n",
      "Iteration 97, loss = 0.43532786\n",
      "Iteration 98, loss = 0.43848934\n",
      "Iteration 99, loss = 0.43814334\n",
      "Iteration 100, loss = 0.43511773\n",
      "Iteration 101, loss = 0.43756742\n",
      "Iteration 102, loss = 0.43640329\n",
      "Iteration 103, loss = 0.43518750\n",
      "Iteration 104, loss = 0.43653167\n",
      "Iteration 105, loss = 0.43892614\n",
      "Iteration 106, loss = 0.43552813\n",
      "Iteration 107, loss = 0.43528565\n",
      "Iteration 108, loss = 0.43472061\n",
      "Iteration 109, loss = 0.43625209\n",
      "Iteration 110, loss = 0.43785230\n",
      "Iteration 111, loss = 0.43502867\n",
      "Iteration 112, loss = 0.43861614\n",
      "Iteration 113, loss = 0.43605235\n",
      "Iteration 114, loss = 0.43521505\n",
      "Iteration 115, loss = 0.43557044\n",
      "Iteration 116, loss = 0.43480739\n",
      "Iteration 117, loss = 0.43726922\n",
      "Iteration 118, loss = 0.43703657\n",
      "Iteration 119, loss = 0.43679715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53907445\n",
      "Iteration 2, loss = 0.49816240\n",
      "Iteration 3, loss = 0.48543416\n",
      "Iteration 4, loss = 0.48005897\n",
      "Iteration 5, loss = 0.47023964\n",
      "Iteration 6, loss = 0.46440251\n",
      "Iteration 7, loss = 0.45969823\n",
      "Iteration 8, loss = 0.45833471\n",
      "Iteration 9, loss = 0.45730263\n",
      "Iteration 10, loss = 0.45663616\n",
      "Iteration 11, loss = 0.45522353\n",
      "Iteration 12, loss = 0.45389790\n",
      "Iteration 13, loss = 0.45548695\n",
      "Iteration 14, loss = 0.45367666\n",
      "Iteration 15, loss = 0.45258875\n",
      "Iteration 16, loss = 0.45195784\n",
      "Iteration 17, loss = 0.45326447\n",
      "Iteration 18, loss = 0.45128354\n",
      "Iteration 19, loss = 0.45143626\n",
      "Iteration 20, loss = 0.45284431\n",
      "Iteration 21, loss = 0.45056916\n",
      "Iteration 22, loss = 0.45148163\n",
      "Iteration 23, loss = 0.45161762\n",
      "Iteration 24, loss = 0.45011504\n",
      "Iteration 25, loss = 0.45150582\n",
      "Iteration 26, loss = 0.45121096\n",
      "Iteration 27, loss = 0.45030845\n",
      "Iteration 28, loss = 0.44917674\n",
      "Iteration 29, loss = 0.45113396\n",
      "Iteration 30, loss = 0.45033049\n",
      "Iteration 31, loss = 0.44951668\n",
      "Iteration 32, loss = 0.44962936\n",
      "Iteration 33, loss = 0.44974438\n",
      "Iteration 34, loss = 0.44870022\n",
      "Iteration 35, loss = 0.44938632\n",
      "Iteration 36, loss = 0.44936881\n",
      "Iteration 37, loss = 0.44854408\n",
      "Iteration 38, loss = 0.44864415\n",
      "Iteration 39, loss = 0.45189056\n",
      "Iteration 40, loss = 0.45008739\n",
      "Iteration 41, loss = 0.44977766\n",
      "Iteration 42, loss = 0.44969634\n",
      "Iteration 43, loss = 0.44896809\n",
      "Iteration 44, loss = 0.44977100\n",
      "Iteration 45, loss = 0.44950186\n",
      "Iteration 46, loss = 0.44754049\n",
      "Iteration 47, loss = 0.44747308\n",
      "Iteration 48, loss = 0.44904343\n",
      "Iteration 49, loss = 0.44814150\n",
      "Iteration 50, loss = 0.44903504\n",
      "Iteration 51, loss = 0.44874135\n",
      "Iteration 52, loss = 0.44659577\n",
      "Iteration 53, loss = 0.44912736\n",
      "Iteration 54, loss = 0.44986830\n",
      "Iteration 55, loss = 0.44980208\n",
      "Iteration 56, loss = 0.44863211\n",
      "Iteration 57, loss = 0.44774112\n",
      "Iteration 58, loss = 0.44993788\n",
      "Iteration 59, loss = 0.44782895\n",
      "Iteration 60, loss = 0.44752236\n",
      "Iteration 61, loss = 0.44816993\n",
      "Iteration 62, loss = 0.44656124\n",
      "Iteration 63, loss = 0.44945568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53538907\n",
      "Iteration 2, loss = 0.49919784\n",
      "Iteration 3, loss = 0.48889909\n",
      "Iteration 4, loss = 0.47903456\n",
      "Iteration 5, loss = 0.47461635\n",
      "Iteration 6, loss = 0.47234622\n",
      "Iteration 7, loss = 0.46572974\n",
      "Iteration 8, loss = 0.46470734\n",
      "Iteration 9, loss = 0.46323040\n",
      "Iteration 10, loss = 0.46147731\n",
      "Iteration 11, loss = 0.46288579\n",
      "Iteration 12, loss = 0.45872370\n",
      "Iteration 13, loss = 0.46080262\n",
      "Iteration 14, loss = 0.45980821\n",
      "Iteration 15, loss = 0.45896118\n",
      "Iteration 16, loss = 0.46047979\n",
      "Iteration 17, loss = 0.46015672\n",
      "Iteration 18, loss = 0.45756156\n",
      "Iteration 19, loss = 0.45982211\n",
      "Iteration 20, loss = 0.45730163\n",
      "Iteration 21, loss = 0.45743783\n",
      "Iteration 22, loss = 0.45667050\n",
      "Iteration 23, loss = 0.45909866\n",
      "Iteration 24, loss = 0.45767357\n",
      "Iteration 25, loss = 0.45660539\n",
      "Iteration 26, loss = 0.45728800\n",
      "Iteration 27, loss = 0.45667323\n",
      "Iteration 28, loss = 0.45743009\n",
      "Iteration 29, loss = 0.45791594\n",
      "Iteration 30, loss = 0.45665908\n",
      "Iteration 31, loss = 0.45755519\n",
      "Iteration 32, loss = 0.45699218\n",
      "Iteration 33, loss = 0.45741253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57749796\n",
      "Iteration 2, loss = 0.53701801\n",
      "Iteration 3, loss = 0.53388816\n",
      "Iteration 4, loss = 0.52875293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 0.52864678\n",
      "Iteration 6, loss = 0.53071005\n",
      "Iteration 7, loss = 0.52455897\n",
      "Iteration 8, loss = 0.52264707\n",
      "Iteration 9, loss = 0.52224496\n",
      "Iteration 10, loss = 0.52253116\n",
      "Iteration 11, loss = 0.51826669\n",
      "Iteration 12, loss = 0.51610065\n",
      "Iteration 13, loss = 0.51606980\n",
      "Iteration 14, loss = 0.51603079\n",
      "Iteration 15, loss = 0.51519063\n",
      "Iteration 16, loss = 0.51460221\n",
      "Iteration 17, loss = 0.51511861\n",
      "Iteration 18, loss = 0.51447114\n",
      "Iteration 19, loss = 0.51364581\n",
      "Iteration 20, loss = 0.51614757\n",
      "Iteration 21, loss = 0.51475285\n",
      "Iteration 22, loss = 0.51344526\n",
      "Iteration 23, loss = 0.51345846\n",
      "Iteration 24, loss = 0.51344058\n",
      "Iteration 25, loss = 0.51149200\n",
      "Iteration 26, loss = 0.51240350\n",
      "Iteration 27, loss = 0.51322414\n",
      "Iteration 28, loss = 0.51402869\n",
      "Iteration 29, loss = 0.51267911\n",
      "Iteration 30, loss = 0.51211051\n",
      "Iteration 31, loss = 0.51085595\n",
      "Iteration 32, loss = 0.51356845\n",
      "Iteration 33, loss = 0.51213132\n",
      "Iteration 34, loss = 0.50944016\n",
      "Iteration 35, loss = 0.51050906\n",
      "Iteration 36, loss = 0.51052779\n",
      "Iteration 37, loss = 0.50947737\n",
      "Iteration 38, loss = 0.50807116\n",
      "Iteration 39, loss = 0.50772747\n",
      "Iteration 40, loss = 0.51000711\n",
      "Iteration 41, loss = 0.50796458\n",
      "Iteration 42, loss = 0.50677759\n",
      "Iteration 43, loss = 0.50677726\n",
      "Iteration 44, loss = 0.50963793\n",
      "Iteration 45, loss = 0.50758871\n",
      "Iteration 46, loss = 0.50738781\n",
      "Iteration 47, loss = 0.50690595\n",
      "Iteration 48, loss = 0.50655555\n",
      "Iteration 49, loss = 0.50470836\n",
      "Iteration 50, loss = 0.50617132\n",
      "Iteration 51, loss = 0.50489213\n",
      "Iteration 52, loss = 0.50535555\n",
      "Iteration 53, loss = 0.50576455\n",
      "Iteration 54, loss = 0.50558072\n",
      "Iteration 55, loss = 0.50386512\n",
      "Iteration 56, loss = 0.50300275\n",
      "Iteration 57, loss = 0.50494670\n",
      "Iteration 58, loss = 0.50413108\n",
      "Iteration 59, loss = 0.50340285\n",
      "Iteration 60, loss = 0.50887409\n",
      "Iteration 61, loss = 0.50544501\n",
      "Iteration 62, loss = 0.50465981\n",
      "Iteration 63, loss = 0.50391389\n",
      "Iteration 64, loss = 0.51245380\n",
      "Iteration 65, loss = 0.50589546\n",
      "Iteration 66, loss = 0.50668128\n",
      "Iteration 67, loss = 0.50483579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58249947\n",
      "Iteration 2, loss = 0.53605251\n",
      "Iteration 3, loss = 0.53113105\n",
      "Iteration 4, loss = 0.53105864\n",
      "Iteration 5, loss = 0.52841460\n",
      "Iteration 6, loss = 0.52676496\n",
      "Iteration 7, loss = 0.52196114\n",
      "Iteration 8, loss = 0.52129978\n",
      "Iteration 9, loss = 0.52097001\n",
      "Iteration 10, loss = 0.52028880\n",
      "Iteration 11, loss = 0.51896927\n",
      "Iteration 12, loss = 0.51971880\n",
      "Iteration 13, loss = 0.51965409\n",
      "Iteration 14, loss = 0.51734924\n",
      "Iteration 15, loss = 0.51660013\n",
      "Iteration 16, loss = 0.51552445\n",
      "Iteration 17, loss = 0.51581175\n",
      "Iteration 18, loss = 0.51401576\n",
      "Iteration 19, loss = 0.51482450\n",
      "Iteration 20, loss = 0.51625045\n",
      "Iteration 21, loss = 0.51354884\n",
      "Iteration 22, loss = 0.51259229\n",
      "Iteration 23, loss = 0.51180502\n",
      "Iteration 24, loss = 0.51143358\n",
      "Iteration 25, loss = 0.51164175\n",
      "Iteration 26, loss = 0.50999107\n",
      "Iteration 27, loss = 0.51140922\n",
      "Iteration 28, loss = 0.50836357\n",
      "Iteration 29, loss = 0.50852826\n",
      "Iteration 30, loss = 0.50861700\n",
      "Iteration 31, loss = 0.50895090\n",
      "Iteration 32, loss = 0.50772717\n",
      "Iteration 33, loss = 0.50892970\n",
      "Iteration 34, loss = 0.50959827\n",
      "Iteration 35, loss = 0.50735609\n",
      "Iteration 36, loss = 0.50603589\n",
      "Iteration 37, loss = 0.50588439\n",
      "Iteration 38, loss = 0.50605954\n",
      "Iteration 39, loss = 0.50719268\n",
      "Iteration 40, loss = 0.50658708\n",
      "Iteration 41, loss = 0.50513836\n",
      "Iteration 42, loss = 0.50573668\n",
      "Iteration 43, loss = 0.50777422\n",
      "Iteration 44, loss = 0.50869644\n",
      "Iteration 45, loss = 0.50541510\n",
      "Iteration 46, loss = 0.50433199\n",
      "Iteration 47, loss = 0.50568723\n",
      "Iteration 48, loss = 0.50414017\n",
      "Iteration 49, loss = 0.50305753\n",
      "Iteration 50, loss = 0.50305973\n",
      "Iteration 51, loss = 0.50325919\n",
      "Iteration 52, loss = 0.50400760\n",
      "Iteration 53, loss = 0.50453808\n",
      "Iteration 54, loss = 0.50590124\n",
      "Iteration 55, loss = 0.50569537\n",
      "Iteration 56, loss = 0.50418900\n",
      "Iteration 57, loss = 0.50309708\n",
      "Iteration 58, loss = 0.50238589\n",
      "Iteration 59, loss = 0.50143049\n",
      "Iteration 60, loss = 0.50184965\n",
      "Iteration 61, loss = 0.49985813\n",
      "Iteration 62, loss = 0.50192592\n",
      "Iteration 63, loss = 0.50493777\n",
      "Iteration 64, loss = 0.50681169\n",
      "Iteration 65, loss = 0.50333485\n",
      "Iteration 66, loss = 0.50221575\n",
      "Iteration 67, loss = 0.50744901\n",
      "Iteration 68, loss = 0.50165867\n",
      "Iteration 69, loss = 0.50545552\n",
      "Iteration 70, loss = 0.50370755\n",
      "Iteration 71, loss = 0.50169104\n",
      "Iteration 72, loss = 0.50269649\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57882331\n",
      "Iteration 2, loss = 0.53720276\n",
      "Iteration 3, loss = 0.53704279\n",
      "Iteration 4, loss = 0.53383201\n",
      "Iteration 5, loss = 0.53438188\n",
      "Iteration 6, loss = 0.52709246\n",
      "Iteration 7, loss = 0.52407416\n",
      "Iteration 8, loss = 0.52264312\n",
      "Iteration 9, loss = 0.52201006\n",
      "Iteration 10, loss = 0.51852603\n",
      "Iteration 11, loss = 0.51892096\n",
      "Iteration 12, loss = 0.51759376\n",
      "Iteration 13, loss = 0.51676982\n",
      "Iteration 14, loss = 0.51623134\n",
      "Iteration 15, loss = 0.51418544\n",
      "Iteration 16, loss = 0.51653564\n",
      "Iteration 17, loss = 0.51517404\n",
      "Iteration 18, loss = 0.51436106\n",
      "Iteration 19, loss = 0.51636005\n",
      "Iteration 20, loss = 0.51389066\n",
      "Iteration 21, loss = 0.51353478\n",
      "Iteration 22, loss = 0.51363501\n",
      "Iteration 23, loss = 0.51322618\n",
      "Iteration 24, loss = 0.51345276\n",
      "Iteration 25, loss = 0.51331814\n",
      "Iteration 26, loss = 0.51333518\n",
      "Iteration 27, loss = 0.51317076\n",
      "Iteration 28, loss = 0.51178012\n",
      "Iteration 29, loss = 0.51101103\n",
      "Iteration 30, loss = 0.51149831\n",
      "Iteration 31, loss = 0.51232044\n",
      "Iteration 32, loss = 0.51243845\n",
      "Iteration 33, loss = 0.51136832\n",
      "Iteration 34, loss = 0.51313491\n",
      "Iteration 35, loss = 0.51192016\n",
      "Iteration 36, loss = 0.51136414\n",
      "Iteration 37, loss = 0.51118397\n",
      "Iteration 38, loss = 0.51209413\n",
      "Iteration 39, loss = 0.50994386\n",
      "Iteration 40, loss = 0.50884701\n",
      "Iteration 41, loss = 0.50956784\n",
      "Iteration 42, loss = 0.50885151\n",
      "Iteration 43, loss = 0.51231243\n",
      "Iteration 44, loss = 0.51071310\n",
      "Iteration 45, loss = 0.50966210\n",
      "Iteration 46, loss = 0.50946969\n",
      "Iteration 47, loss = 0.50972491\n",
      "Iteration 48, loss = 0.50954177\n",
      "Iteration 49, loss = 0.50797486\n",
      "Iteration 50, loss = 0.50826145\n",
      "Iteration 51, loss = 0.51197474\n",
      "Iteration 52, loss = 0.51373575\n",
      "Iteration 53, loss = 0.50798297\n",
      "Iteration 54, loss = 0.50843093\n",
      "Iteration 55, loss = 0.50996038\n",
      "Iteration 56, loss = 0.50756630\n",
      "Iteration 57, loss = 0.50748579\n",
      "Iteration 58, loss = 0.50619260\n",
      "Iteration 59, loss = 0.50686801\n",
      "Iteration 60, loss = 0.50587212\n",
      "Iteration 61, loss = 0.50839552\n",
      "Iteration 62, loss = 0.50852048\n",
      "Iteration 63, loss = 0.50651513\n",
      "Iteration 64, loss = 0.50651743\n",
      "Iteration 65, loss = 0.50714826\n",
      "Iteration 66, loss = 0.50562217\n",
      "Iteration 67, loss = 0.51656887\n",
      "Iteration 68, loss = 0.50841157\n",
      "Iteration 69, loss = 0.51048475\n",
      "Iteration 70, loss = 0.50717111\n",
      "Iteration 71, loss = 0.50859896\n",
      "Iteration 72, loss = 0.50883591\n",
      "Iteration 73, loss = 0.50723142\n",
      "Iteration 74, loss = 0.51011312\n",
      "Iteration 75, loss = 0.50653200\n",
      "Iteration 76, loss = 0.50634075\n",
      "Iteration 77, loss = 0.50839232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55550209\n",
      "Iteration 2, loss = 0.52798405\n",
      "Iteration 3, loss = 0.52082850\n",
      "Iteration 4, loss = 0.50999436\n",
      "Iteration 5, loss = 0.50035238\n",
      "Iteration 6, loss = 0.49162173\n",
      "Iteration 7, loss = 0.48494020\n",
      "Iteration 8, loss = 0.48206897\n",
      "Iteration 9, loss = 0.48013029\n",
      "Iteration 10, loss = 0.48181937\n",
      "Iteration 11, loss = 0.47907592\n",
      "Iteration 12, loss = 0.47505841\n",
      "Iteration 13, loss = 0.47707218\n",
      "Iteration 14, loss = 0.47494518\n",
      "Iteration 15, loss = 0.47560375\n",
      "Iteration 16, loss = 0.47415709\n",
      "Iteration 17, loss = 0.47205485\n",
      "Iteration 18, loss = 0.47197618\n",
      "Iteration 19, loss = 0.47136283\n",
      "Iteration 20, loss = 0.47175306\n",
      "Iteration 21, loss = 0.46910224\n",
      "Iteration 22, loss = 0.46983359\n",
      "Iteration 23, loss = 0.47108681\n",
      "Iteration 24, loss = 0.46816880\n",
      "Iteration 25, loss = 0.46849505\n",
      "Iteration 26, loss = 0.46834089\n",
      "Iteration 27, loss = 0.46762751\n",
      "Iteration 28, loss = 0.46851547\n",
      "Iteration 29, loss = 0.46925078\n",
      "Iteration 30, loss = 0.46867694\n",
      "Iteration 31, loss = 0.46740234\n",
      "Iteration 32, loss = 0.46818702\n",
      "Iteration 33, loss = 0.46729474\n",
      "Iteration 34, loss = 0.46485168\n",
      "Iteration 35, loss = 0.46508661\n",
      "Iteration 36, loss = 0.46450536\n",
      "Iteration 37, loss = 0.46459436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.46561331\n",
      "Iteration 39, loss = 0.46449333\n",
      "Iteration 40, loss = 0.46368145\n",
      "Iteration 41, loss = 0.46541273\n",
      "Iteration 42, loss = 0.46363298\n",
      "Iteration 43, loss = 0.46428307\n",
      "Iteration 44, loss = 0.46510111\n",
      "Iteration 45, loss = 0.46242214\n",
      "Iteration 46, loss = 0.46159073\n",
      "Iteration 47, loss = 0.46224175\n",
      "Iteration 48, loss = 0.46218393\n",
      "Iteration 49, loss = 0.46100178\n",
      "Iteration 50, loss = 0.46135665\n",
      "Iteration 51, loss = 0.46048933\n",
      "Iteration 52, loss = 0.45937217\n",
      "Iteration 53, loss = 0.45793992\n",
      "Iteration 54, loss = 0.45878143\n",
      "Iteration 55, loss = 0.45880942\n",
      "Iteration 56, loss = 0.45627617\n",
      "Iteration 57, loss = 0.45560519\n",
      "Iteration 58, loss = 0.45537286\n",
      "Iteration 59, loss = 0.45418227\n",
      "Iteration 60, loss = 0.46171762\n",
      "Iteration 61, loss = 0.45439972\n",
      "Iteration 62, loss = 0.45757714\n",
      "Iteration 63, loss = 0.45545382\n",
      "Iteration 64, loss = 0.46118293\n",
      "Iteration 65, loss = 0.45473607\n",
      "Iteration 66, loss = 0.45390421\n",
      "Iteration 67, loss = 0.45281762\n",
      "Iteration 68, loss = 0.45394973\n",
      "Iteration 69, loss = 0.45318174\n",
      "Iteration 70, loss = 0.45213275\n",
      "Iteration 71, loss = 0.45254649\n",
      "Iteration 72, loss = 0.44968349\n",
      "Iteration 73, loss = 0.45183411\n",
      "Iteration 74, loss = 0.45173872\n",
      "Iteration 75, loss = 0.45347296\n",
      "Iteration 76, loss = 0.45090339\n",
      "Iteration 77, loss = 0.44946730\n",
      "Iteration 78, loss = 0.45070009\n",
      "Iteration 79, loss = 0.45249054\n",
      "Iteration 80, loss = 0.45186449\n",
      "Iteration 81, loss = 0.44841828\n",
      "Iteration 82, loss = 0.44808433\n",
      "Iteration 83, loss = 0.44915477\n",
      "Iteration 84, loss = 0.44764822\n",
      "Iteration 85, loss = 0.44832723\n",
      "Iteration 86, loss = 0.45083561\n",
      "Iteration 87, loss = 0.45404624\n",
      "Iteration 88, loss = 0.44675940\n",
      "Iteration 89, loss = 0.44519702\n",
      "Iteration 90, loss = 0.44813997\n",
      "Iteration 91, loss = 0.44930483\n",
      "Iteration 92, loss = 0.44912640\n",
      "Iteration 93, loss = 0.45074773\n",
      "Iteration 94, loss = 0.44947112\n",
      "Iteration 95, loss = 0.45006285\n",
      "Iteration 96, loss = 0.44846293\n",
      "Iteration 97, loss = 0.44614063\n",
      "Iteration 98, loss = 0.44679908\n",
      "Iteration 99, loss = 0.44752672\n",
      "Iteration 100, loss = 0.44631735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55591695\n",
      "Iteration 2, loss = 0.52632136\n",
      "Iteration 3, loss = 0.51770476\n",
      "Iteration 4, loss = 0.50969970\n",
      "Iteration 5, loss = 0.49590927\n",
      "Iteration 6, loss = 0.48727609\n",
      "Iteration 7, loss = 0.48240100\n",
      "Iteration 8, loss = 0.48009681\n",
      "Iteration 9, loss = 0.47748217\n",
      "Iteration 10, loss = 0.47664882\n",
      "Iteration 11, loss = 0.47386393\n",
      "Iteration 12, loss = 0.47334881\n",
      "Iteration 13, loss = 0.47399434\n",
      "Iteration 14, loss = 0.47313568\n",
      "Iteration 15, loss = 0.47043002\n",
      "Iteration 16, loss = 0.46861194\n",
      "Iteration 17, loss = 0.47001841\n",
      "Iteration 18, loss = 0.46820240\n",
      "Iteration 19, loss = 0.46991606\n",
      "Iteration 20, loss = 0.46937986\n",
      "Iteration 21, loss = 0.46691979\n",
      "Iteration 22, loss = 0.46714655\n",
      "Iteration 23, loss = 0.46707418\n",
      "Iteration 24, loss = 0.46697308\n",
      "Iteration 25, loss = 0.46430400\n",
      "Iteration 26, loss = 0.46608163\n",
      "Iteration 27, loss = 0.46200485\n",
      "Iteration 28, loss = 0.46043538\n",
      "Iteration 29, loss = 0.46307458\n",
      "Iteration 30, loss = 0.45987638\n",
      "Iteration 31, loss = 0.46093688\n",
      "Iteration 32, loss = 0.46038964\n",
      "Iteration 33, loss = 0.46103740\n",
      "Iteration 34, loss = 0.45919591\n",
      "Iteration 35, loss = 0.45984551\n",
      "Iteration 36, loss = 0.46003536\n",
      "Iteration 37, loss = 0.45805556\n",
      "Iteration 38, loss = 0.45891947\n",
      "Iteration 39, loss = 0.45924675\n",
      "Iteration 40, loss = 0.45947888\n",
      "Iteration 41, loss = 0.45767172\n",
      "Iteration 42, loss = 0.45809053\n",
      "Iteration 43, loss = 0.45863479\n",
      "Iteration 44, loss = 0.46035979\n",
      "Iteration 45, loss = 0.45734348\n",
      "Iteration 46, loss = 0.45548878\n",
      "Iteration 47, loss = 0.45686018\n",
      "Iteration 48, loss = 0.45714166\n",
      "Iteration 49, loss = 0.45464734\n",
      "Iteration 50, loss = 0.45499614\n",
      "Iteration 51, loss = 0.45320088\n",
      "Iteration 52, loss = 0.45317507\n",
      "Iteration 53, loss = 0.45286738\n",
      "Iteration 54, loss = 0.45279804\n",
      "Iteration 55, loss = 0.45319719\n",
      "Iteration 56, loss = 0.45464965\n",
      "Iteration 57, loss = 0.45193455\n",
      "Iteration 58, loss = 0.44973330\n",
      "Iteration 59, loss = 0.45067993\n",
      "Iteration 60, loss = 0.45105171\n",
      "Iteration 61, loss = 0.44981486\n",
      "Iteration 62, loss = 0.45069389\n",
      "Iteration 63, loss = 0.45151312\n",
      "Iteration 64, loss = 0.45350445\n",
      "Iteration 65, loss = 0.45256286\n",
      "Iteration 66, loss = 0.44845177\n",
      "Iteration 67, loss = 0.45242302\n",
      "Iteration 68, loss = 0.44797851\n",
      "Iteration 69, loss = 0.44879611\n",
      "Iteration 70, loss = 0.45115010\n",
      "Iteration 71, loss = 0.44948091\n",
      "Iteration 72, loss = 0.44856832\n",
      "Iteration 73, loss = 0.44766901\n",
      "Iteration 74, loss = 0.44855161\n",
      "Iteration 75, loss = 0.44842998\n",
      "Iteration 76, loss = 0.44895665\n",
      "Iteration 77, loss = 0.44718493\n",
      "Iteration 78, loss = 0.44675011\n",
      "Iteration 79, loss = 0.44746778\n",
      "Iteration 80, loss = 0.44686937\n",
      "Iteration 81, loss = 0.44696296\n",
      "Iteration 82, loss = 0.44809972\n",
      "Iteration 83, loss = 0.44620510\n",
      "Iteration 84, loss = 0.44747385\n",
      "Iteration 85, loss = 0.44765834\n",
      "Iteration 86, loss = 0.44527322\n",
      "Iteration 87, loss = 0.44786091\n",
      "Iteration 88, loss = 0.44546050\n",
      "Iteration 89, loss = 0.44386939\n",
      "Iteration 90, loss = 0.44704700\n",
      "Iteration 91, loss = 0.44627991\n",
      "Iteration 92, loss = 0.44454582\n",
      "Iteration 93, loss = 0.44673895\n",
      "Iteration 94, loss = 0.44557362\n",
      "Iteration 95, loss = 0.44535865\n",
      "Iteration 96, loss = 0.44422381\n",
      "Iteration 97, loss = 0.44560442\n",
      "Iteration 98, loss = 0.44423276\n",
      "Iteration 99, loss = 0.44535131\n",
      "Iteration 100, loss = 0.44512409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55404412\n",
      "Iteration 2, loss = 0.52797243\n",
      "Iteration 3, loss = 0.52174433\n",
      "Iteration 4, loss = 0.51107568\n",
      "Iteration 5, loss = 0.50133918\n",
      "Iteration 6, loss = 0.49208020\n",
      "Iteration 7, loss = 0.48175630\n",
      "Iteration 8, loss = 0.48166376\n",
      "Iteration 9, loss = 0.47983707\n",
      "Iteration 10, loss = 0.47929314\n",
      "Iteration 11, loss = 0.47956193\n",
      "Iteration 12, loss = 0.47730748\n",
      "Iteration 13, loss = 0.47465978\n",
      "Iteration 14, loss = 0.47770154\n",
      "Iteration 15, loss = 0.47565840\n",
      "Iteration 16, loss = 0.47657541\n",
      "Iteration 17, loss = 0.47736849\n",
      "Iteration 18, loss = 0.47410021\n",
      "Iteration 19, loss = 0.47262414\n",
      "Iteration 20, loss = 0.47113781\n",
      "Iteration 21, loss = 0.47203939\n",
      "Iteration 22, loss = 0.47046860\n",
      "Iteration 23, loss = 0.46913156\n",
      "Iteration 24, loss = 0.47047795\n",
      "Iteration 25, loss = 0.46922009\n",
      "Iteration 26, loss = 0.47163451\n",
      "Iteration 27, loss = 0.47146083\n",
      "Iteration 28, loss = 0.47057013\n",
      "Iteration 29, loss = 0.47303328\n",
      "Iteration 30, loss = 0.46934663\n",
      "Iteration 31, loss = 0.47124167\n",
      "Iteration 32, loss = 0.47177297\n",
      "Iteration 33, loss = 0.46873525\n",
      "Iteration 34, loss = 0.46965970\n",
      "Iteration 35, loss = 0.46999678\n",
      "Iteration 36, loss = 0.47039716\n",
      "Iteration 37, loss = 0.46815100\n",
      "Iteration 38, loss = 0.47173325\n",
      "Iteration 39, loss = 0.46964771\n",
      "Iteration 40, loss = 0.46872071\n",
      "Iteration 41, loss = 0.46815874\n",
      "Iteration 42, loss = 0.46910366\n",
      "Iteration 43, loss = 0.46992793\n",
      "Iteration 44, loss = 0.46823553\n",
      "Iteration 45, loss = 0.46793991\n",
      "Iteration 46, loss = 0.46933651\n",
      "Iteration 47, loss = 0.46815772\n",
      "Iteration 48, loss = 0.47160329\n",
      "Iteration 49, loss = 0.47000808\n",
      "Iteration 50, loss = 0.47100949\n",
      "Iteration 51, loss = 0.46854559\n",
      "Iteration 52, loss = 0.47359223\n",
      "Iteration 53, loss = 0.46874263\n",
      "Iteration 54, loss = 0.46896281\n",
      "Iteration 55, loss = 0.46828164\n",
      "Iteration 56, loss = 0.46737811\n",
      "Iteration 57, loss = 0.46765380\n",
      "Iteration 58, loss = 0.46724134\n",
      "Iteration 59, loss = 0.46683301\n",
      "Iteration 60, loss = 0.46828168\n",
      "Iteration 61, loss = 0.46915790\n",
      "Iteration 62, loss = 0.47084333\n",
      "Iteration 63, loss = 0.46959285\n",
      "Iteration 64, loss = 0.47273811\n",
      "Iteration 65, loss = 0.46696698\n",
      "Iteration 66, loss = 0.46722317\n",
      "Iteration 67, loss = 0.47052503\n",
      "Iteration 68, loss = 0.46711169\n",
      "Iteration 69, loss = 0.46798045\n",
      "Iteration 70, loss = 0.46884578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57593519\n",
      "Iteration 2, loss = 0.54246196\n",
      "Iteration 3, loss = 0.53914210\n",
      "Iteration 4, loss = 0.53798596\n",
      "Iteration 5, loss = 0.53801110\n",
      "Iteration 6, loss = 0.53713813\n",
      "Iteration 7, loss = 0.53608930\n",
      "Iteration 8, loss = 0.53629661\n",
      "Iteration 9, loss = 0.53561229\n",
      "Iteration 10, loss = 0.53674414\n",
      "Iteration 11, loss = 0.53469248\n",
      "Iteration 12, loss = 0.53512093\n",
      "Iteration 13, loss = 0.53420238\n",
      "Iteration 14, loss = 0.53504623\n",
      "Iteration 15, loss = 0.53593280\n",
      "Iteration 16, loss = 0.53482414\n",
      "Iteration 17, loss = 0.53577224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.53515472\n",
      "Iteration 19, loss = 0.53607391\n",
      "Iteration 20, loss = 0.53585545\n",
      "Iteration 21, loss = 0.53524668\n",
      "Iteration 22, loss = 0.53558072\n",
      "Iteration 23, loss = 0.53481489\n",
      "Iteration 24, loss = 0.53364658\n",
      "Iteration 25, loss = 0.53379872\n",
      "Iteration 26, loss = 0.53339183\n",
      "Iteration 27, loss = 0.53427281\n",
      "Iteration 28, loss = 0.53469177\n",
      "Iteration 29, loss = 0.53581952\n",
      "Iteration 30, loss = 0.53362104\n",
      "Iteration 31, loss = 0.53345857\n",
      "Iteration 32, loss = 0.53563794\n",
      "Iteration 33, loss = 0.53373555\n",
      "Iteration 34, loss = 0.53352790\n",
      "Iteration 35, loss = 0.53408824\n",
      "Iteration 36, loss = 0.53317336\n",
      "Iteration 37, loss = 0.53330859\n",
      "Iteration 38, loss = 0.53301943\n",
      "Iteration 39, loss = 0.53272311\n",
      "Iteration 40, loss = 0.53329634\n",
      "Iteration 41, loss = 0.53418718\n",
      "Iteration 42, loss = 0.53247035\n",
      "Iteration 43, loss = 0.53369162\n",
      "Iteration 44, loss = 0.53391102\n",
      "Iteration 45, loss = 0.53409535\n",
      "Iteration 46, loss = 0.53234868\n",
      "Iteration 47, loss = 0.53254340\n",
      "Iteration 48, loss = 0.53341989\n",
      "Iteration 49, loss = 0.53274186\n",
      "Iteration 50, loss = 0.53297515\n",
      "Iteration 51, loss = 0.53241423\n",
      "Iteration 52, loss = 0.53315148\n",
      "Iteration 53, loss = 0.53210989\n",
      "Iteration 54, loss = 0.53248927\n",
      "Iteration 55, loss = 0.53235391\n",
      "Iteration 56, loss = 0.53273331\n",
      "Iteration 57, loss = 0.53340395\n",
      "Iteration 58, loss = 0.53308988\n",
      "Iteration 59, loss = 0.53363270\n",
      "Iteration 60, loss = 0.53406024\n",
      "Iteration 61, loss = 0.53201902\n",
      "Iteration 62, loss = 0.53356267\n",
      "Iteration 63, loss = 0.53272280\n",
      "Iteration 64, loss = 0.53490552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57814600\n",
      "Iteration 2, loss = 0.54166115\n",
      "Iteration 3, loss = 0.53598337\n",
      "Iteration 4, loss = 0.53779630\n",
      "Iteration 5, loss = 0.53449285\n",
      "Iteration 6, loss = 0.53372530\n",
      "Iteration 7, loss = 0.53169359\n",
      "Iteration 8, loss = 0.53318073\n",
      "Iteration 9, loss = 0.53229439\n",
      "Iteration 10, loss = 0.53251282\n",
      "Iteration 11, loss = 0.53143809\n",
      "Iteration 12, loss = 0.53129409\n",
      "Iteration 13, loss = 0.53208876\n",
      "Iteration 14, loss = 0.53177674\n",
      "Iteration 15, loss = 0.53010538\n",
      "Iteration 16, loss = 0.52982926\n",
      "Iteration 17, loss = 0.53144518\n",
      "Iteration 18, loss = 0.53089172\n",
      "Iteration 19, loss = 0.53132743\n",
      "Iteration 20, loss = 0.53119348\n",
      "Iteration 21, loss = 0.53101909\n",
      "Iteration 22, loss = 0.53258290\n",
      "Iteration 23, loss = 0.53339709\n",
      "Iteration 24, loss = 0.53133042\n",
      "Iteration 25, loss = 0.53203405\n",
      "Iteration 26, loss = 0.53105818\n",
      "Iteration 27, loss = 0.53095681\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57432696\n",
      "Iteration 2, loss = 0.54201758\n",
      "Iteration 3, loss = 0.54003748\n",
      "Iteration 4, loss = 0.53809811\n",
      "Iteration 5, loss = 0.53914466\n",
      "Iteration 6, loss = 0.53752139\n",
      "Iteration 7, loss = 0.53470200\n",
      "Iteration 8, loss = 0.53805176\n",
      "Iteration 9, loss = 0.53740087\n",
      "Iteration 10, loss = 0.53606137\n",
      "Iteration 11, loss = 0.53855876\n",
      "Iteration 12, loss = 0.53529522\n",
      "Iteration 13, loss = 0.53650503\n",
      "Iteration 14, loss = 0.53614579\n",
      "Iteration 15, loss = 0.53485083\n",
      "Iteration 16, loss = 0.53700980\n",
      "Iteration 17, loss = 0.53585770\n",
      "Iteration 18, loss = 0.53509897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57569371\n",
      "Iteration 2, loss = 0.52612768\n",
      "Iteration 3, loss = 0.51814292\n",
      "Iteration 4, loss = 0.51532429\n",
      "Iteration 5, loss = 0.51568726\n",
      "Iteration 6, loss = 0.51736046\n",
      "Iteration 7, loss = 0.51461637\n",
      "Iteration 8, loss = 0.51455711\n",
      "Iteration 9, loss = 0.51427047\n",
      "Iteration 10, loss = 0.51464155\n",
      "Iteration 11, loss = 0.51214747\n",
      "Iteration 12, loss = 0.51236156\n",
      "Iteration 13, loss = 0.51266820\n",
      "Iteration 14, loss = 0.51306220\n",
      "Iteration 15, loss = 0.51387772\n",
      "Iteration 16, loss = 0.51378407\n",
      "Iteration 17, loss = 0.51388298\n",
      "Iteration 18, loss = 0.51415487\n",
      "Iteration 19, loss = 0.51306590\n",
      "Iteration 20, loss = 0.51538781\n",
      "Iteration 21, loss = 0.51308417\n",
      "Iteration 22, loss = 0.51348332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57664296\n",
      "Iteration 2, loss = 0.52313090\n",
      "Iteration 3, loss = 0.51179919\n",
      "Iteration 4, loss = 0.51278329\n",
      "Iteration 5, loss = 0.50985284\n",
      "Iteration 6, loss = 0.50677836\n",
      "Iteration 7, loss = 0.50499187\n",
      "Iteration 8, loss = 0.50738856\n",
      "Iteration 9, loss = 0.50537439\n",
      "Iteration 10, loss = 0.50512494\n",
      "Iteration 11, loss = 0.50431856\n",
      "Iteration 12, loss = 0.50394229\n",
      "Iteration 13, loss = 0.50537006\n",
      "Iteration 14, loss = 0.50591868\n",
      "Iteration 15, loss = 0.50348039\n",
      "Iteration 16, loss = 0.50370979\n",
      "Iteration 17, loss = 0.50452032\n",
      "Iteration 18, loss = 0.50463843\n",
      "Iteration 19, loss = 0.50558220\n",
      "Iteration 20, loss = 0.50473754\n",
      "Iteration 21, loss = 0.50392395\n",
      "Iteration 22, loss = 0.50556119\n",
      "Iteration 23, loss = 0.50806484\n",
      "Iteration 24, loss = 0.50483344\n",
      "Iteration 25, loss = 0.50531212\n",
      "Iteration 26, loss = 0.50478079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57207471\n",
      "Iteration 2, loss = 0.52391522\n",
      "Iteration 3, loss = 0.51557948\n",
      "Iteration 4, loss = 0.51166773\n",
      "Iteration 5, loss = 0.51215219\n",
      "Iteration 6, loss = 0.51053901\n",
      "Iteration 7, loss = 0.50720572\n",
      "Iteration 8, loss = 0.51070430\n",
      "Iteration 9, loss = 0.50900826\n",
      "Iteration 10, loss = 0.50792767\n",
      "Iteration 11, loss = 0.51092018\n",
      "Iteration 12, loss = 0.50820524\n",
      "Iteration 13, loss = 0.50894369\n",
      "Iteration 14, loss = 0.50807142\n",
      "Iteration 15, loss = 0.50642627\n",
      "Iteration 16, loss = 0.50996503\n",
      "Iteration 17, loss = 0.50812231\n",
      "Iteration 18, loss = 0.50778256\n",
      "Iteration 19, loss = 0.50799176\n",
      "Iteration 20, loss = 0.50703803\n",
      "Iteration 21, loss = 0.50821419\n",
      "Iteration 22, loss = 0.50735916\n",
      "Iteration 23, loss = 0.50895298\n",
      "Iteration 24, loss = 0.50849542\n",
      "Iteration 25, loss = 0.50900311\n",
      "Iteration 26, loss = 0.50815360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55401070\n",
      "Iteration 2, loss = 0.49802673\n",
      "Iteration 3, loss = 0.49092670\n",
      "Iteration 4, loss = 0.48070940\n",
      "Iteration 5, loss = 0.47453733\n",
      "Iteration 6, loss = 0.46739471\n",
      "Iteration 7, loss = 0.45570676\n",
      "Iteration 8, loss = 0.44608477\n",
      "Iteration 9, loss = 0.44302512\n",
      "Iteration 10, loss = 0.44113483\n",
      "Iteration 11, loss = 0.44238552\n",
      "Iteration 12, loss = 0.43838394\n",
      "Iteration 13, loss = 0.43667732\n",
      "Iteration 14, loss = 0.43702038\n",
      "Iteration 15, loss = 0.43454797\n",
      "Iteration 16, loss = 0.43600524\n",
      "Iteration 17, loss = 0.43389528\n",
      "Iteration 18, loss = 0.43290399\n",
      "Iteration 19, loss = 0.43453708\n",
      "Iteration 20, loss = 0.43264419\n",
      "Iteration 21, loss = 0.43148551\n",
      "Iteration 22, loss = 0.43002150\n",
      "Iteration 23, loss = 0.43042758\n",
      "Iteration 24, loss = 0.42877481\n",
      "Iteration 25, loss = 0.42888021\n",
      "Iteration 26, loss = 0.42811590\n",
      "Iteration 27, loss = 0.42818460\n",
      "Iteration 28, loss = 0.42760795\n",
      "Iteration 29, loss = 0.42819171\n",
      "Iteration 30, loss = 0.43049879\n",
      "Iteration 31, loss = 0.42673235\n",
      "Iteration 32, loss = 0.42736855\n",
      "Iteration 33, loss = 0.42729826\n",
      "Iteration 34, loss = 0.42723112\n",
      "Iteration 35, loss = 0.42796629\n",
      "Iteration 36, loss = 0.42723525\n",
      "Iteration 37, loss = 0.42727255\n",
      "Iteration 38, loss = 0.42757787\n",
      "Iteration 39, loss = 0.42708293\n",
      "Iteration 40, loss = 0.42549359\n",
      "Iteration 41, loss = 0.42776320\n",
      "Iteration 42, loss = 0.42508027\n",
      "Iteration 43, loss = 0.42534991\n",
      "Iteration 44, loss = 0.42632583\n",
      "Iteration 45, loss = 0.42607070\n",
      "Iteration 46, loss = 0.42524050\n",
      "Iteration 47, loss = 0.42484583\n",
      "Iteration 48, loss = 0.42533742\n",
      "Iteration 49, loss = 0.42583369\n",
      "Iteration 50, loss = 0.42579759\n",
      "Iteration 51, loss = 0.42597523\n",
      "Iteration 52, loss = 0.42454428\n",
      "Iteration 53, loss = 0.42407239\n",
      "Iteration 54, loss = 0.42486242\n",
      "Iteration 55, loss = 0.42594781\n",
      "Iteration 56, loss = 0.42406896\n",
      "Iteration 57, loss = 0.42419843\n",
      "Iteration 58, loss = 0.42385181\n",
      "Iteration 59, loss = 0.42643241\n",
      "Iteration 60, loss = 0.42443934\n",
      "Iteration 61, loss = 0.42293001\n",
      "Iteration 62, loss = 0.42230688\n",
      "Iteration 63, loss = 0.42619569\n",
      "Iteration 64, loss = 0.42599877\n",
      "Iteration 65, loss = 0.42447499\n",
      "Iteration 66, loss = 0.42640240\n",
      "Iteration 67, loss = 0.42379917\n",
      "Iteration 68, loss = 0.42302900\n",
      "Iteration 69, loss = 0.42326792\n",
      "Iteration 70, loss = 0.42447073\n",
      "Iteration 71, loss = 0.42253875\n",
      "Iteration 72, loss = 0.42142231\n",
      "Iteration 73, loss = 0.42442809\n",
      "Iteration 74, loss = 0.42556824\n",
      "Iteration 75, loss = 0.42520867\n",
      "Iteration 76, loss = 0.42182290\n",
      "Iteration 77, loss = 0.42183446\n",
      "Iteration 78, loss = 0.42384550\n",
      "Iteration 79, loss = 0.42331299\n",
      "Iteration 80, loss = 0.42322550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 81, loss = 0.42220651\n",
      "Iteration 82, loss = 0.42214102\n",
      "Iteration 83, loss = 0.42265605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54808499\n",
      "Iteration 2, loss = 0.49206326\n",
      "Iteration 3, loss = 0.47959105\n",
      "Iteration 4, loss = 0.47305776\n",
      "Iteration 5, loss = 0.46388843\n",
      "Iteration 6, loss = 0.45829406\n",
      "Iteration 7, loss = 0.44958880\n",
      "Iteration 8, loss = 0.44384979\n",
      "Iteration 9, loss = 0.44147078\n",
      "Iteration 10, loss = 0.43753793\n",
      "Iteration 11, loss = 0.43750129\n",
      "Iteration 12, loss = 0.43492326\n",
      "Iteration 13, loss = 0.43407067\n",
      "Iteration 14, loss = 0.43260640\n",
      "Iteration 15, loss = 0.43150256\n",
      "Iteration 16, loss = 0.43425280\n",
      "Iteration 17, loss = 0.43033935\n",
      "Iteration 18, loss = 0.42950181\n",
      "Iteration 19, loss = 0.42876326\n",
      "Iteration 20, loss = 0.43312598\n",
      "Iteration 21, loss = 0.42941679\n",
      "Iteration 22, loss = 0.42911610\n",
      "Iteration 23, loss = 0.43086384\n",
      "Iteration 24, loss = 0.42658135\n",
      "Iteration 25, loss = 0.42699518\n",
      "Iteration 26, loss = 0.42591386\n",
      "Iteration 27, loss = 0.42804157\n",
      "Iteration 28, loss = 0.42610760\n",
      "Iteration 29, loss = 0.42767710\n",
      "Iteration 30, loss = 0.42649363\n",
      "Iteration 31, loss = 0.42463862\n",
      "Iteration 32, loss = 0.42514310\n",
      "Iteration 33, loss = 0.42533032\n",
      "Iteration 34, loss = 0.42495120\n",
      "Iteration 35, loss = 0.42663419\n",
      "Iteration 36, loss = 0.42632243\n",
      "Iteration 37, loss = 0.42489012\n",
      "Iteration 38, loss = 0.42452946\n",
      "Iteration 39, loss = 0.42710741\n",
      "Iteration 40, loss = 0.42846366\n",
      "Iteration 41, loss = 0.42607859\n",
      "Iteration 42, loss = 0.42320549\n",
      "Iteration 43, loss = 0.42545634\n",
      "Iteration 44, loss = 0.42489301\n",
      "Iteration 45, loss = 0.42283650\n",
      "Iteration 46, loss = 0.42430634\n",
      "Iteration 47, loss = 0.42600377\n",
      "Iteration 48, loss = 0.42376669\n",
      "Iteration 49, loss = 0.42538668\n",
      "Iteration 50, loss = 0.42510359\n",
      "Iteration 51, loss = 0.42452408\n",
      "Iteration 52, loss = 0.42435701\n",
      "Iteration 53, loss = 0.42663522\n",
      "Iteration 54, loss = 0.42449931\n",
      "Iteration 55, loss = 0.42339825\n",
      "Iteration 56, loss = 0.42354535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54965722\n",
      "Iteration 2, loss = 0.49317380\n",
      "Iteration 3, loss = 0.48229152\n",
      "Iteration 4, loss = 0.47159305\n",
      "Iteration 5, loss = 0.46124255\n",
      "Iteration 6, loss = 0.45075265\n",
      "Iteration 7, loss = 0.44418125\n",
      "Iteration 8, loss = 0.43880785\n",
      "Iteration 9, loss = 0.44002832\n",
      "Iteration 10, loss = 0.43566587\n",
      "Iteration 11, loss = 0.43474651\n",
      "Iteration 12, loss = 0.43259928\n",
      "Iteration 13, loss = 0.43232357\n",
      "Iteration 14, loss = 0.43217357\n",
      "Iteration 15, loss = 0.43016096\n",
      "Iteration 16, loss = 0.42996547\n",
      "Iteration 17, loss = 0.43075180\n",
      "Iteration 18, loss = 0.43254967\n",
      "Iteration 19, loss = 0.43056657\n",
      "Iteration 20, loss = 0.42762326\n",
      "Iteration 21, loss = 0.42728210\n",
      "Iteration 22, loss = 0.42807276\n",
      "Iteration 23, loss = 0.42810252\n",
      "Iteration 24, loss = 0.42728196\n",
      "Iteration 25, loss = 0.42599706\n",
      "Iteration 26, loss = 0.42471875\n",
      "Iteration 27, loss = 0.42722591\n",
      "Iteration 28, loss = 0.42674298\n",
      "Iteration 29, loss = 0.42564760\n",
      "Iteration 30, loss = 0.42588561\n",
      "Iteration 31, loss = 0.42686833\n",
      "Iteration 32, loss = 0.42517756\n",
      "Iteration 33, loss = 0.42708838\n",
      "Iteration 34, loss = 0.42440365\n",
      "Iteration 35, loss = 0.42532553\n",
      "Iteration 36, loss = 0.42449631\n",
      "Iteration 37, loss = 0.42429099\n",
      "Iteration 38, loss = 0.42410449\n",
      "Iteration 39, loss = 0.42408782\n",
      "Iteration 40, loss = 0.42300337\n",
      "Iteration 41, loss = 0.42432145\n",
      "Iteration 42, loss = 0.42429882\n",
      "Iteration 43, loss = 0.42606033\n",
      "Iteration 44, loss = 0.42266902\n",
      "Iteration 45, loss = 0.42237868\n",
      "Iteration 46, loss = 0.42593915\n",
      "Iteration 47, loss = 0.42578550\n",
      "Iteration 48, loss = 0.42371752\n",
      "Iteration 49, loss = 0.42248318\n",
      "Iteration 50, loss = 0.42538923\n",
      "Iteration 51, loss = 0.42408056\n",
      "Iteration 52, loss = 0.42966024\n",
      "Iteration 53, loss = 0.42296435\n",
      "Iteration 54, loss = 0.42304246\n",
      "Iteration 55, loss = 0.42311353\n",
      "Iteration 56, loss = 0.42216502\n",
      "Iteration 57, loss = 0.42314002\n",
      "Iteration 58, loss = 0.42168022\n",
      "Iteration 59, loss = 0.42178746\n",
      "Iteration 60, loss = 0.42184956\n",
      "Iteration 61, loss = 0.42447037\n",
      "Iteration 62, loss = 0.42142235\n",
      "Iteration 63, loss = 0.42200976\n",
      "Iteration 64, loss = 0.42243224\n",
      "Iteration 65, loss = 0.42198293\n",
      "Iteration 66, loss = 0.42198315\n",
      "Iteration 67, loss = 0.42556973\n",
      "Iteration 68, loss = 0.42356210\n",
      "Iteration 69, loss = 0.42272313\n",
      "Iteration 70, loss = 0.42472137\n",
      "Iteration 71, loss = 0.42169035\n",
      "Iteration 72, loss = 0.42105101\n",
      "Iteration 73, loss = 0.41993768\n",
      "Iteration 74, loss = 0.42355016\n",
      "Iteration 75, loss = 0.42195826\n",
      "Iteration 76, loss = 0.42230850\n",
      "Iteration 77, loss = 0.42018495\n",
      "Iteration 78, loss = 0.42076267\n",
      "Iteration 79, loss = 0.42460484\n",
      "Iteration 80, loss = 0.42336405\n",
      "Iteration 81, loss = 0.42226058\n",
      "Iteration 82, loss = 0.42128481\n",
      "Iteration 83, loss = 0.42132002\n",
      "Iteration 84, loss = 0.41923708\n",
      "Iteration 85, loss = 0.41985300\n",
      "Iteration 86, loss = 0.42071513\n",
      "Iteration 87, loss = 0.42007757\n",
      "Iteration 88, loss = 0.41871156\n",
      "Iteration 89, loss = 0.41907530\n",
      "Iteration 90, loss = 0.41926816\n",
      "Iteration 91, loss = 0.41993085\n",
      "Iteration 92, loss = 0.41974806\n",
      "Iteration 93, loss = 0.41881697\n",
      "Iteration 94, loss = 0.41825081\n",
      "Iteration 95, loss = 0.41996223\n",
      "Iteration 96, loss = 0.41948866\n",
      "Iteration 97, loss = 0.41810852\n",
      "Iteration 98, loss = 0.41966321\n",
      "Iteration 99, loss = 0.41791900\n",
      "Iteration 100, loss = 0.42047660\n",
      "Iteration 101, loss = 0.41816722\n",
      "Iteration 102, loss = 0.41873385\n",
      "Iteration 103, loss = 0.41947502\n",
      "Iteration 104, loss = 0.41769012\n",
      "Iteration 105, loss = 0.41689343\n",
      "Iteration 106, loss = 0.41891636\n",
      "Iteration 107, loss = 0.41931442\n",
      "Iteration 108, loss = 0.41639817\n",
      "Iteration 109, loss = 0.41638483\n",
      "Iteration 110, loss = 0.41776182\n",
      "Iteration 111, loss = 0.41882477\n",
      "Iteration 112, loss = 0.41965568\n",
      "Iteration 113, loss = 0.42186628\n",
      "Iteration 114, loss = 0.42128959\n",
      "Iteration 115, loss = 0.42200050\n",
      "Iteration 116, loss = 0.42292930\n",
      "Iteration 117, loss = 0.41828592\n",
      "Iteration 118, loss = 0.41573284\n",
      "Iteration 119, loss = 0.41765571\n",
      "Iteration 120, loss = 0.41816114\n",
      "Iteration 121, loss = 0.41677702\n",
      "Iteration 122, loss = 0.41603227\n",
      "Iteration 123, loss = 0.41640533\n",
      "Iteration 124, loss = 0.41517203\n",
      "Iteration 125, loss = 0.41629601\n",
      "Iteration 126, loss = 0.42223446\n",
      "Iteration 127, loss = 0.41944591\n",
      "Iteration 128, loss = 0.42000100\n",
      "Iteration 129, loss = 0.41827737\n",
      "Iteration 130, loss = 0.41743932\n",
      "Iteration 131, loss = 0.41571473\n",
      "Iteration 132, loss = 0.41452417\n",
      "Iteration 133, loss = 0.41542778\n",
      "Iteration 134, loss = 0.41592925\n",
      "Iteration 135, loss = 0.41621500\n",
      "Iteration 136, loss = 0.41469427\n",
      "Iteration 137, loss = 0.41613244\n",
      "Iteration 138, loss = 0.41527324\n",
      "Iteration 139, loss = 0.41605589\n",
      "Iteration 140, loss = 0.41945616\n",
      "Iteration 141, loss = 0.41664470\n",
      "Iteration 142, loss = 0.41832566\n",
      "Iteration 143, loss = 0.41571333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54497790\n",
      "Iteration 2, loss = 0.49792021\n",
      "Iteration 3, loss = 0.48716164\n",
      "Iteration 4, loss = 0.48033954\n",
      "Iteration 5, loss = 0.47700876\n",
      "Iteration 6, loss = 0.47731532\n",
      "Iteration 7, loss = 0.47394000\n",
      "Iteration 8, loss = 0.47115946\n",
      "Iteration 9, loss = 0.46868900\n",
      "Iteration 10, loss = 0.46647006\n",
      "Iteration 11, loss = 0.46405182\n",
      "Iteration 12, loss = 0.46135023\n",
      "Iteration 13, loss = 0.45867274\n",
      "Iteration 14, loss = 0.45933814\n",
      "Iteration 15, loss = 0.46101804\n",
      "Iteration 16, loss = 0.45651426\n",
      "Iteration 17, loss = 0.45599810\n",
      "Iteration 18, loss = 0.45632264\n",
      "Iteration 19, loss = 0.45514605\n",
      "Iteration 20, loss = 0.45602544\n",
      "Iteration 21, loss = 0.45575133\n",
      "Iteration 22, loss = 0.45403259\n",
      "Iteration 23, loss = 0.45606221\n",
      "Iteration 24, loss = 0.45438266\n",
      "Iteration 25, loss = 0.45572055\n",
      "Iteration 26, loss = 0.45578536\n",
      "Iteration 27, loss = 0.45638725\n",
      "Iteration 28, loss = 0.45424217\n",
      "Iteration 29, loss = 0.45588293\n",
      "Iteration 30, loss = 0.45450212\n",
      "Iteration 31, loss = 0.45468563\n",
      "Iteration 32, loss = 0.45415638\n",
      "Iteration 33, loss = 0.45510416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54436387\n",
      "Iteration 2, loss = 0.49574238\n",
      "Iteration 3, loss = 0.48199903\n",
      "Iteration 4, loss = 0.47287164\n",
      "Iteration 5, loss = 0.46844846\n",
      "Iteration 6, loss = 0.46753755\n",
      "Iteration 7, loss = 0.46361836\n",
      "Iteration 8, loss = 0.46130030\n",
      "Iteration 9, loss = 0.46060113\n",
      "Iteration 10, loss = 0.45988519\n",
      "Iteration 11, loss = 0.45801817\n",
      "Iteration 12, loss = 0.45713556\n",
      "Iteration 13, loss = 0.45791624\n",
      "Iteration 14, loss = 0.45686074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.45552026\n",
      "Iteration 16, loss = 0.45563664\n",
      "Iteration 17, loss = 0.45524500\n",
      "Iteration 18, loss = 0.45543976\n",
      "Iteration 19, loss = 0.45517610\n",
      "Iteration 20, loss = 0.45767864\n",
      "Iteration 21, loss = 0.45525842\n",
      "Iteration 22, loss = 0.45545688\n",
      "Iteration 23, loss = 0.45607823\n",
      "Iteration 24, loss = 0.45536762\n",
      "Iteration 25, loss = 0.45523350\n",
      "Iteration 26, loss = 0.45423506\n",
      "Iteration 27, loss = 0.45488968\n",
      "Iteration 28, loss = 0.45415336\n",
      "Iteration 29, loss = 0.45518827\n",
      "Iteration 30, loss = 0.45372478\n",
      "Iteration 31, loss = 0.45348480\n",
      "Iteration 32, loss = 0.45362087\n",
      "Iteration 33, loss = 0.45585426\n",
      "Iteration 34, loss = 0.45508427\n",
      "Iteration 35, loss = 0.45399684\n",
      "Iteration 36, loss = 0.45339735\n",
      "Iteration 37, loss = 0.45348303\n",
      "Iteration 38, loss = 0.45232090\n",
      "Iteration 39, loss = 0.45580791\n",
      "Iteration 40, loss = 0.45596765\n",
      "Iteration 41, loss = 0.45519566\n",
      "Iteration 42, loss = 0.45270722\n",
      "Iteration 43, loss = 0.45352396\n",
      "Iteration 44, loss = 0.45484933\n",
      "Iteration 45, loss = 0.45219393\n",
      "Iteration 46, loss = 0.45492395\n",
      "Iteration 47, loss = 0.45448275\n",
      "Iteration 48, loss = 0.45246973\n",
      "Iteration 49, loss = 0.45246716\n",
      "Iteration 50, loss = 0.45259706\n",
      "Iteration 51, loss = 0.45234763\n",
      "Iteration 52, loss = 0.45448450\n",
      "Iteration 53, loss = 0.45291184\n",
      "Iteration 54, loss = 0.45247106\n",
      "Iteration 55, loss = 0.45272592\n",
      "Iteration 56, loss = 0.45290725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54163934\n",
      "Iteration 2, loss = 0.49648020\n",
      "Iteration 3, loss = 0.48344674\n",
      "Iteration 4, loss = 0.47262861\n",
      "Iteration 5, loss = 0.46726602\n",
      "Iteration 6, loss = 0.46497224\n",
      "Iteration 7, loss = 0.46209841\n",
      "Iteration 8, loss = 0.46137223\n",
      "Iteration 9, loss = 0.45945839\n",
      "Iteration 10, loss = 0.45787506\n",
      "Iteration 11, loss = 0.45697505\n",
      "Iteration 12, loss = 0.45605088\n",
      "Iteration 13, loss = 0.45586326\n",
      "Iteration 14, loss = 0.45667026\n",
      "Iteration 15, loss = 0.45322640\n",
      "Iteration 16, loss = 0.45523921\n",
      "Iteration 17, loss = 0.45876088\n",
      "Iteration 18, loss = 0.45260391\n",
      "Iteration 19, loss = 0.45600475\n",
      "Iteration 20, loss = 0.45238938\n",
      "Iteration 21, loss = 0.45228450\n",
      "Iteration 22, loss = 0.45288436\n",
      "Iteration 23, loss = 0.45292569\n",
      "Iteration 24, loss = 0.45227556\n",
      "Iteration 25, loss = 0.45169482\n",
      "Iteration 26, loss = 0.45255907\n",
      "Iteration 27, loss = 0.45706994\n",
      "Iteration 28, loss = 0.45645006\n",
      "Iteration 29, loss = 0.45436866\n",
      "Iteration 30, loss = 0.45416587\n",
      "Iteration 31, loss = 0.45191331\n",
      "Iteration 32, loss = 0.45109555\n",
      "Iteration 33, loss = 0.45253829\n",
      "Iteration 34, loss = 0.45259024\n",
      "Iteration 35, loss = 0.45171971\n",
      "Iteration 36, loss = 0.45208962\n",
      "Iteration 37, loss = 0.45158732\n",
      "Iteration 38, loss = 0.45315401\n",
      "Iteration 39, loss = 0.45149940\n",
      "Iteration 40, loss = 0.45075848\n",
      "Iteration 41, loss = 0.45168531\n",
      "Iteration 42, loss = 0.45034986\n",
      "Iteration 43, loss = 0.45250690\n",
      "Iteration 44, loss = 0.45094418\n",
      "Iteration 45, loss = 0.44996701\n",
      "Iteration 46, loss = 0.45193643\n",
      "Iteration 47, loss = 0.45265662\n",
      "Iteration 48, loss = 0.45135865\n",
      "Iteration 49, loss = 0.45028201\n",
      "Iteration 50, loss = 0.45244866\n",
      "Iteration 51, loss = 0.45124953\n",
      "Iteration 52, loss = 0.45485670\n",
      "Iteration 53, loss = 0.45069990\n",
      "Iteration 54, loss = 0.45094003\n",
      "Iteration 55, loss = 0.45192389\n",
      "Iteration 56, loss = 0.45104200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52668785\n",
      "Iteration 2, loss = 0.48139509\n",
      "Iteration 3, loss = 0.47482770\n",
      "Iteration 4, loss = 0.47133221\n",
      "Iteration 5, loss = 0.46897859\n",
      "Iteration 6, loss = 0.46765815\n",
      "Iteration 7, loss = 0.46581375\n",
      "Iteration 8, loss = 0.46374237\n",
      "Iteration 9, loss = 0.46284109\n",
      "Iteration 10, loss = 0.46331926\n",
      "Iteration 11, loss = 0.46240053\n",
      "Iteration 12, loss = 0.46073850\n",
      "Iteration 13, loss = 0.46016561\n",
      "Iteration 14, loss = 0.46025575\n",
      "Iteration 15, loss = 0.46095307\n",
      "Iteration 16, loss = 0.45959438\n",
      "Iteration 17, loss = 0.46104456\n",
      "Iteration 18, loss = 0.45947012\n",
      "Iteration 19, loss = 0.45935760\n",
      "Iteration 20, loss = 0.45852066\n",
      "Iteration 21, loss = 0.45889104\n",
      "Iteration 22, loss = 0.45831866\n",
      "Iteration 23, loss = 0.45908336\n",
      "Iteration 24, loss = 0.45758402\n",
      "Iteration 25, loss = 0.45914700\n",
      "Iteration 26, loss = 0.45639688\n",
      "Iteration 27, loss = 0.45629831\n",
      "Iteration 28, loss = 0.45616205\n",
      "Iteration 29, loss = 0.45703251\n",
      "Iteration 30, loss = 0.45733657\n",
      "Iteration 31, loss = 0.45637526\n",
      "Iteration 32, loss = 0.45571519\n",
      "Iteration 33, loss = 0.45599893\n",
      "Iteration 34, loss = 0.45592753\n",
      "Iteration 35, loss = 0.45702748\n",
      "Iteration 36, loss = 0.45512023\n",
      "Iteration 37, loss = 0.45614759\n",
      "Iteration 38, loss = 0.45638319\n",
      "Iteration 39, loss = 0.45604985\n",
      "Iteration 40, loss = 0.45619955\n",
      "Iteration 41, loss = 0.45592883\n",
      "Iteration 42, loss = 0.45538085\n",
      "Iteration 43, loss = 0.45659775\n",
      "Iteration 44, loss = 0.45535978\n",
      "Iteration 45, loss = 0.45514915\n",
      "Iteration 46, loss = 0.45590081\n",
      "Iteration 47, loss = 0.45528319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52719893\n",
      "Iteration 2, loss = 0.47948485\n",
      "Iteration 3, loss = 0.47372260\n",
      "Iteration 4, loss = 0.47179664\n",
      "Iteration 5, loss = 0.46981928\n",
      "Iteration 6, loss = 0.46766092\n",
      "Iteration 7, loss = 0.46532516\n",
      "Iteration 8, loss = 0.46310282\n",
      "Iteration 9, loss = 0.46202743\n",
      "Iteration 10, loss = 0.46056530\n",
      "Iteration 11, loss = 0.46000352\n",
      "Iteration 12, loss = 0.45878918\n",
      "Iteration 13, loss = 0.45816295\n",
      "Iteration 14, loss = 0.45808251\n",
      "Iteration 15, loss = 0.45680961\n",
      "Iteration 16, loss = 0.45602709\n",
      "Iteration 17, loss = 0.45642336\n",
      "Iteration 18, loss = 0.45649483\n",
      "Iteration 19, loss = 0.45527441\n",
      "Iteration 20, loss = 0.45635181\n",
      "Iteration 21, loss = 0.45516415\n",
      "Iteration 22, loss = 0.45586380\n",
      "Iteration 23, loss = 0.45545168\n",
      "Iteration 24, loss = 0.45507403\n",
      "Iteration 25, loss = 0.45699156\n",
      "Iteration 26, loss = 0.45578328\n",
      "Iteration 27, loss = 0.45540610\n",
      "Iteration 28, loss = 0.45477321\n",
      "Iteration 29, loss = 0.45595397\n",
      "Iteration 30, loss = 0.45491938\n",
      "Iteration 31, loss = 0.45482104\n",
      "Iteration 32, loss = 0.45497096\n",
      "Iteration 33, loss = 0.45516022\n",
      "Iteration 34, loss = 0.45517676\n",
      "Iteration 35, loss = 0.45613184\n",
      "Iteration 36, loss = 0.45433733\n",
      "Iteration 37, loss = 0.45368489\n",
      "Iteration 38, loss = 0.45454584\n",
      "Iteration 39, loss = 0.45669811\n",
      "Iteration 40, loss = 0.45611733\n",
      "Iteration 41, loss = 0.45548065\n",
      "Iteration 42, loss = 0.45475559\n",
      "Iteration 43, loss = 0.45578483\n",
      "Iteration 44, loss = 0.45433707\n",
      "Iteration 45, loss = 0.45393823\n",
      "Iteration 46, loss = 0.45733429\n",
      "Iteration 47, loss = 0.45646339\n",
      "Iteration 48, loss = 0.45446709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52427577\n",
      "Iteration 2, loss = 0.48016202\n",
      "Iteration 3, loss = 0.47538394\n",
      "Iteration 4, loss = 0.47155552\n",
      "Iteration 5, loss = 0.46896368\n",
      "Iteration 6, loss = 0.46766138\n",
      "Iteration 7, loss = 0.46486722\n",
      "Iteration 8, loss = 0.46460468\n",
      "Iteration 9, loss = 0.46401158\n",
      "Iteration 10, loss = 0.46206995\n",
      "Iteration 11, loss = 0.46218799\n",
      "Iteration 12, loss = 0.46052165\n",
      "Iteration 13, loss = 0.46095475\n",
      "Iteration 14, loss = 0.46069722\n",
      "Iteration 15, loss = 0.45945273\n",
      "Iteration 16, loss = 0.46074548\n",
      "Iteration 17, loss = 0.46045528\n",
      "Iteration 18, loss = 0.45925415\n",
      "Iteration 19, loss = 0.46023384\n",
      "Iteration 20, loss = 0.45883178\n",
      "Iteration 21, loss = 0.45921938\n",
      "Iteration 22, loss = 0.45893630\n",
      "Iteration 23, loss = 0.45901818\n",
      "Iteration 24, loss = 0.45998974\n",
      "Iteration 25, loss = 0.45781152\n",
      "Iteration 26, loss = 0.45797344\n",
      "Iteration 27, loss = 0.46098339\n",
      "Iteration 28, loss = 0.45814185\n",
      "Iteration 29, loss = 0.45727094\n",
      "Iteration 30, loss = 0.45712264\n",
      "Iteration 31, loss = 0.45572698\n",
      "Iteration 32, loss = 0.45512216\n",
      "Iteration 33, loss = 0.45535581\n",
      "Iteration 34, loss = 0.45576909\n",
      "Iteration 35, loss = 0.45556430\n",
      "Iteration 36, loss = 0.45542868\n",
      "Iteration 37, loss = 0.45452920\n",
      "Iteration 38, loss = 0.45620990\n",
      "Iteration 39, loss = 0.45515077\n",
      "Iteration 40, loss = 0.45452854\n",
      "Iteration 41, loss = 0.45476417\n",
      "Iteration 42, loss = 0.45428434\n",
      "Iteration 43, loss = 0.45512977\n",
      "Iteration 44, loss = 0.45422918\n",
      "Iteration 45, loss = 0.45368503\n",
      "Iteration 46, loss = 0.45591408\n",
      "Iteration 47, loss = 0.45591741\n",
      "Iteration 48, loss = 0.45429964\n",
      "Iteration 49, loss = 0.45394675\n",
      "Iteration 50, loss = 0.45564227\n",
      "Iteration 51, loss = 0.45440135\n",
      "Iteration 52, loss = 0.45681448\n",
      "Iteration 53, loss = 0.45515211\n",
      "Iteration 54, loss = 0.45516726\n",
      "Iteration 55, loss = 0.45422139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.45496750\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52805417\n",
      "Iteration 2, loss = 0.48256489\n",
      "Iteration 3, loss = 0.47622977\n",
      "Iteration 4, loss = 0.47410187\n",
      "Iteration 5, loss = 0.47265306\n",
      "Iteration 6, loss = 0.47272010\n",
      "Iteration 7, loss = 0.47020532\n",
      "Iteration 8, loss = 0.46908697\n",
      "Iteration 9, loss = 0.46788974\n",
      "Iteration 10, loss = 0.46786863\n",
      "Iteration 11, loss = 0.46605339\n",
      "Iteration 12, loss = 0.46450003\n",
      "Iteration 13, loss = 0.46317693\n",
      "Iteration 14, loss = 0.46295019\n",
      "Iteration 15, loss = 0.46257823\n",
      "Iteration 16, loss = 0.46013322\n",
      "Iteration 17, loss = 0.46062474\n",
      "Iteration 18, loss = 0.45905195\n",
      "Iteration 19, loss = 0.45835161\n",
      "Iteration 20, loss = 0.45770768\n",
      "Iteration 21, loss = 0.45707271\n",
      "Iteration 22, loss = 0.45753700\n",
      "Iteration 23, loss = 0.45718356\n",
      "Iteration 24, loss = 0.45626311\n",
      "Iteration 25, loss = 0.45630057\n",
      "Iteration 26, loss = 0.45655822\n",
      "Iteration 27, loss = 0.45613926\n",
      "Iteration 28, loss = 0.45516532\n",
      "Iteration 29, loss = 0.45712531\n",
      "Iteration 30, loss = 0.45632074\n",
      "Iteration 31, loss = 0.45529027\n",
      "Iteration 32, loss = 0.45666192\n",
      "Iteration 33, loss = 0.45730193\n",
      "Iteration 34, loss = 0.45511259\n",
      "Iteration 35, loss = 0.45602715\n",
      "Iteration 36, loss = 0.45489804\n",
      "Iteration 37, loss = 0.45555205\n",
      "Iteration 38, loss = 0.45852667\n",
      "Iteration 39, loss = 0.45673650\n",
      "Iteration 40, loss = 0.45510536\n",
      "Iteration 41, loss = 0.45446258\n",
      "Iteration 42, loss = 0.45461608\n",
      "Iteration 43, loss = 0.45558087\n",
      "Iteration 44, loss = 0.45523783\n",
      "Iteration 45, loss = 0.45473565\n",
      "Iteration 46, loss = 0.45464206\n",
      "Iteration 47, loss = 0.45520845\n",
      "Iteration 48, loss = 0.45439765\n",
      "Iteration 49, loss = 0.45491982\n",
      "Iteration 50, loss = 0.45440151\n",
      "Iteration 51, loss = 0.45548172\n",
      "Iteration 52, loss = 0.45392544\n",
      "Iteration 53, loss = 0.45399377\n",
      "Iteration 54, loss = 0.45508621\n",
      "Iteration 55, loss = 0.45436546\n",
      "Iteration 56, loss = 0.45391297\n",
      "Iteration 57, loss = 0.45412757\n",
      "Iteration 58, loss = 0.45403242\n",
      "Iteration 59, loss = 0.45621669\n",
      "Iteration 60, loss = 0.45470304\n",
      "Iteration 61, loss = 0.45343628\n",
      "Iteration 62, loss = 0.45392547\n",
      "Iteration 63, loss = 0.45626981\n",
      "Iteration 64, loss = 0.45500089\n",
      "Iteration 65, loss = 0.45365420\n",
      "Iteration 66, loss = 0.45452974\n",
      "Iteration 67, loss = 0.45490237\n",
      "Iteration 68, loss = 0.45348883\n",
      "Iteration 69, loss = 0.45440139\n",
      "Iteration 70, loss = 0.45615562\n",
      "Iteration 71, loss = 0.45439733\n",
      "Iteration 72, loss = 0.45285168\n",
      "Iteration 73, loss = 0.45636911\n",
      "Iteration 74, loss = 0.45668390\n",
      "Iteration 75, loss = 0.45614844\n",
      "Iteration 76, loss = 0.45389937\n",
      "Iteration 77, loss = 0.45334586\n",
      "Iteration 78, loss = 0.45428319\n",
      "Iteration 79, loss = 0.45432623\n",
      "Iteration 80, loss = 0.45362146\n",
      "Iteration 81, loss = 0.45272769\n",
      "Iteration 82, loss = 0.45366754\n",
      "Iteration 83, loss = 0.45347577\n",
      "Iteration 84, loss = 0.45333758\n",
      "Iteration 85, loss = 0.45326673\n",
      "Iteration 86, loss = 0.45421488\n",
      "Iteration 87, loss = 0.45337842\n",
      "Iteration 88, loss = 0.45348947\n",
      "Iteration 89, loss = 0.45290875\n",
      "Iteration 90, loss = 0.45285847\n",
      "Iteration 91, loss = 0.45378616\n",
      "Iteration 92, loss = 0.45402284\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52662161\n",
      "Iteration 2, loss = 0.48022181\n",
      "Iteration 3, loss = 0.47396631\n",
      "Iteration 4, loss = 0.47187611\n",
      "Iteration 5, loss = 0.47090956\n",
      "Iteration 6, loss = 0.46976958\n",
      "Iteration 7, loss = 0.46892415\n",
      "Iteration 8, loss = 0.46785056\n",
      "Iteration 9, loss = 0.46687398\n",
      "Iteration 10, loss = 0.46543169\n",
      "Iteration 11, loss = 0.46514943\n",
      "Iteration 12, loss = 0.46373280\n",
      "Iteration 13, loss = 0.46315457\n",
      "Iteration 14, loss = 0.46297198\n",
      "Iteration 15, loss = 0.46172244\n",
      "Iteration 16, loss = 0.46079599\n",
      "Iteration 17, loss = 0.46132945\n",
      "Iteration 18, loss = 0.45999945\n",
      "Iteration 19, loss = 0.45908116\n",
      "Iteration 20, loss = 0.46030722\n",
      "Iteration 21, loss = 0.45848787\n",
      "Iteration 22, loss = 0.45738902\n",
      "Iteration 23, loss = 0.45817735\n",
      "Iteration 24, loss = 0.45716464\n",
      "Iteration 25, loss = 0.45727373\n",
      "Iteration 26, loss = 0.45582363\n",
      "Iteration 27, loss = 0.45527761\n",
      "Iteration 28, loss = 0.45444789\n",
      "Iteration 29, loss = 0.45519846\n",
      "Iteration 30, loss = 0.45473130\n",
      "Iteration 31, loss = 0.45578633\n",
      "Iteration 32, loss = 0.45480790\n",
      "Iteration 33, loss = 0.45493368\n",
      "Iteration 34, loss = 0.45424023\n",
      "Iteration 35, loss = 0.45523391\n",
      "Iteration 36, loss = 0.45458484\n",
      "Iteration 37, loss = 0.45365229\n",
      "Iteration 38, loss = 0.45414102\n",
      "Iteration 39, loss = 0.45564349\n",
      "Iteration 40, loss = 0.45728026\n",
      "Iteration 41, loss = 0.45441110\n",
      "Iteration 42, loss = 0.45414190\n",
      "Iteration 43, loss = 0.45476032\n",
      "Iteration 44, loss = 0.45427271\n",
      "Iteration 45, loss = 0.45349171\n",
      "Iteration 46, loss = 0.45609147\n",
      "Iteration 47, loss = 0.45563233\n",
      "Iteration 48, loss = 0.45388621\n",
      "Iteration 49, loss = 0.45499066\n",
      "Iteration 50, loss = 0.45561532\n",
      "Iteration 51, loss = 0.45354783\n",
      "Iteration 52, loss = 0.45418693\n",
      "Iteration 53, loss = 0.45348025\n",
      "Iteration 54, loss = 0.45461319\n",
      "Iteration 55, loss = 0.45416221\n",
      "Iteration 56, loss = 0.45438646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52385188\n",
      "Iteration 2, loss = 0.48111372\n",
      "Iteration 3, loss = 0.47604297\n",
      "Iteration 4, loss = 0.47221049\n",
      "Iteration 5, loss = 0.47053301\n",
      "Iteration 6, loss = 0.46927177\n",
      "Iteration 7, loss = 0.46706227\n",
      "Iteration 8, loss = 0.46699137\n",
      "Iteration 9, loss = 0.46545836\n",
      "Iteration 10, loss = 0.46390193\n",
      "Iteration 11, loss = 0.46367736\n",
      "Iteration 12, loss = 0.46230430\n",
      "Iteration 13, loss = 0.46171197\n",
      "Iteration 14, loss = 0.46059893\n",
      "Iteration 15, loss = 0.45962227\n",
      "Iteration 16, loss = 0.45984700\n",
      "Iteration 17, loss = 0.45894607\n",
      "Iteration 18, loss = 0.45820796\n",
      "Iteration 19, loss = 0.45721938\n",
      "Iteration 20, loss = 0.45699701\n",
      "Iteration 21, loss = 0.45645582\n",
      "Iteration 22, loss = 0.45555540\n",
      "Iteration 23, loss = 0.45609143\n",
      "Iteration 24, loss = 0.45801023\n",
      "Iteration 25, loss = 0.45356573\n",
      "Iteration 26, loss = 0.45407418\n",
      "Iteration 27, loss = 0.45408551\n",
      "Iteration 28, loss = 0.45686310\n",
      "Iteration 29, loss = 0.45438995\n",
      "Iteration 30, loss = 0.45386716\n",
      "Iteration 31, loss = 0.45420414\n",
      "Iteration 32, loss = 0.45340030\n",
      "Iteration 33, loss = 0.45425157\n",
      "Iteration 34, loss = 0.45483551\n",
      "Iteration 35, loss = 0.45432385\n",
      "Iteration 36, loss = 0.45438853\n",
      "Iteration 37, loss = 0.45413655\n",
      "Iteration 38, loss = 0.45340429\n",
      "Iteration 39, loss = 0.45343039\n",
      "Iteration 40, loss = 0.45288151\n",
      "Iteration 41, loss = 0.45259799\n",
      "Iteration 42, loss = 0.45259564\n",
      "Iteration 43, loss = 0.45337340\n",
      "Iteration 44, loss = 0.45273849\n",
      "Iteration 45, loss = 0.45210972\n",
      "Iteration 46, loss = 0.45398127\n",
      "Iteration 47, loss = 0.45334455\n",
      "Iteration 48, loss = 0.45286537\n",
      "Iteration 49, loss = 0.45298862\n",
      "Iteration 50, loss = 0.45377273\n",
      "Iteration 51, loss = 0.45296430\n",
      "Iteration 52, loss = 0.45357160\n",
      "Iteration 53, loss = 0.45303062\n",
      "Iteration 54, loss = 0.45367714\n",
      "Iteration 55, loss = 0.45184656\n",
      "Iteration 56, loss = 0.45287172\n",
      "Iteration 57, loss = 0.45235846\n",
      "Iteration 58, loss = 0.45231474\n",
      "Iteration 59, loss = 0.45249616\n",
      "Iteration 60, loss = 0.45246818\n",
      "Iteration 61, loss = 0.45348172\n",
      "Iteration 62, loss = 0.45261395\n",
      "Iteration 63, loss = 0.45250575\n",
      "Iteration 64, loss = 0.45258174\n",
      "Iteration 65, loss = 0.45222669\n",
      "Iteration 66, loss = 0.45236102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55738924\n",
      "Iteration 2, loss = 0.52725409\n",
      "Iteration 3, loss = 0.52337128\n",
      "Iteration 4, loss = 0.51983644\n",
      "Iteration 5, loss = 0.51804565\n",
      "Iteration 6, loss = 0.51201033\n",
      "Iteration 7, loss = 0.49867604\n",
      "Iteration 8, loss = 0.49141765\n",
      "Iteration 9, loss = 0.48839627\n",
      "Iteration 10, loss = 0.48612713\n",
      "Iteration 11, loss = 0.48267915\n",
      "Iteration 12, loss = 0.48002504\n",
      "Iteration 13, loss = 0.47883912\n",
      "Iteration 14, loss = 0.47830144\n",
      "Iteration 15, loss = 0.47624071\n",
      "Iteration 16, loss = 0.47860589\n",
      "Iteration 17, loss = 0.47598192\n",
      "Iteration 18, loss = 0.47676945\n",
      "Iteration 19, loss = 0.47636560\n",
      "Iteration 20, loss = 0.47608779\n",
      "Iteration 21, loss = 0.47415343\n",
      "Iteration 22, loss = 0.47124931\n",
      "Iteration 23, loss = 0.47415246\n",
      "Iteration 24, loss = 0.47175835\n",
      "Iteration 25, loss = 0.47023099\n",
      "Iteration 26, loss = 0.47016073\n",
      "Iteration 27, loss = 0.47070304\n",
      "Iteration 28, loss = 0.47032934\n",
      "Iteration 29, loss = 0.47120983\n",
      "Iteration 30, loss = 0.47156761\n",
      "Iteration 31, loss = 0.46893867\n",
      "Iteration 32, loss = 0.46937589\n",
      "Iteration 33, loss = 0.47057038\n",
      "Iteration 34, loss = 0.47042635\n",
      "Iteration 35, loss = 0.47098535\n",
      "Iteration 36, loss = 0.46902427\n",
      "Iteration 37, loss = 0.46965942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.47335746\n",
      "Iteration 39, loss = 0.46923983\n",
      "Iteration 40, loss = 0.46945454\n",
      "Iteration 41, loss = 0.47240014\n",
      "Iteration 42, loss = 0.46814607\n",
      "Iteration 43, loss = 0.46878182\n",
      "Iteration 44, loss = 0.47047713\n",
      "Iteration 45, loss = 0.46602633\n",
      "Iteration 46, loss = 0.46705181\n",
      "Iteration 47, loss = 0.46735805\n",
      "Iteration 48, loss = 0.46571567\n",
      "Iteration 49, loss = 0.46696864\n",
      "Iteration 50, loss = 0.46598148\n",
      "Iteration 51, loss = 0.46578899\n",
      "Iteration 52, loss = 0.46562343\n",
      "Iteration 53, loss = 0.46565713\n",
      "Iteration 54, loss = 0.46685527\n",
      "Iteration 55, loss = 0.46511996\n",
      "Iteration 56, loss = 0.46541600\n",
      "Iteration 57, loss = 0.46565852\n",
      "Iteration 58, loss = 0.46531052\n",
      "Iteration 59, loss = 0.46513110\n",
      "Iteration 60, loss = 0.46924387\n",
      "Iteration 61, loss = 0.46585118\n",
      "Iteration 62, loss = 0.46447827\n",
      "Iteration 63, loss = 0.46690299\n",
      "Iteration 64, loss = 0.46955059\n",
      "Iteration 65, loss = 0.46503244\n",
      "Iteration 66, loss = 0.46613923\n",
      "Iteration 67, loss = 0.46502854\n",
      "Iteration 68, loss = 0.46443616\n",
      "Iteration 69, loss = 0.46575408\n",
      "Iteration 70, loss = 0.46558266\n",
      "Iteration 71, loss = 0.46562493\n",
      "Iteration 72, loss = 0.46377595\n",
      "Iteration 73, loss = 0.46334563\n",
      "Iteration 74, loss = 0.46482442\n",
      "Iteration 75, loss = 0.46601447\n",
      "Iteration 76, loss = 0.46440385\n",
      "Iteration 77, loss = 0.46360786\n",
      "Iteration 78, loss = 0.46317000\n",
      "Iteration 79, loss = 0.46414543\n",
      "Iteration 80, loss = 0.46526185\n",
      "Iteration 81, loss = 0.46387305\n",
      "Iteration 82, loss = 0.46258374\n",
      "Iteration 83, loss = 0.46212751\n",
      "Iteration 84, loss = 0.45990391\n",
      "Iteration 85, loss = 0.45948634\n",
      "Iteration 86, loss = 0.45973932\n",
      "Iteration 87, loss = 0.45964843\n",
      "Iteration 88, loss = 0.45931853\n",
      "Iteration 89, loss = 0.45717003\n",
      "Iteration 90, loss = 0.45819608\n",
      "Iteration 91, loss = 0.45825522\n",
      "Iteration 92, loss = 0.45787531\n",
      "Iteration 93, loss = 0.45696416\n",
      "Iteration 94, loss = 0.45791513\n",
      "Iteration 95, loss = 0.45679191\n",
      "Iteration 96, loss = 0.45561111\n",
      "Iteration 97, loss = 0.45431684\n",
      "Iteration 98, loss = 0.45323198\n",
      "Iteration 99, loss = 0.45447526\n",
      "Iteration 100, loss = 0.45156292\n",
      "Iteration 101, loss = 0.45017921\n",
      "Iteration 102, loss = 0.45117144\n",
      "Iteration 103, loss = 0.44983346\n",
      "Iteration 104, loss = 0.44953067\n",
      "Iteration 105, loss = 0.45121900\n",
      "Iteration 106, loss = 0.44839596\n",
      "Iteration 107, loss = 0.45075176\n",
      "Iteration 108, loss = 0.44638269\n",
      "Iteration 109, loss = 0.44665321\n",
      "Iteration 110, loss = 0.44691765\n",
      "Iteration 111, loss = 0.44392885\n",
      "Iteration 112, loss = 0.44272188\n",
      "Iteration 113, loss = 0.44048445\n",
      "Iteration 114, loss = 0.44148787\n",
      "Iteration 115, loss = 0.44426674\n",
      "Iteration 116, loss = 0.44190131\n",
      "Iteration 117, loss = 0.43901247\n",
      "Iteration 118, loss = 0.44008355\n",
      "Iteration 119, loss = 0.44004706\n",
      "Iteration 120, loss = 0.44165583\n",
      "Iteration 121, loss = 0.43733364\n",
      "Iteration 122, loss = 0.44156624\n",
      "Iteration 123, loss = 0.44217738\n",
      "Iteration 124, loss = 0.43747826\n",
      "Iteration 125, loss = 0.43791898\n",
      "Iteration 126, loss = 0.43639249\n",
      "Iteration 127, loss = 0.43713760\n",
      "Iteration 128, loss = 0.43967638\n",
      "Iteration 129, loss = 0.43579016\n",
      "Iteration 130, loss = 0.44234231\n",
      "Iteration 131, loss = 0.43674684\n",
      "Iteration 132, loss = 0.43867605\n",
      "Iteration 133, loss = 0.44051879\n",
      "Iteration 134, loss = 0.43987193\n",
      "Iteration 135, loss = 0.43776358\n",
      "Iteration 136, loss = 0.43701102\n",
      "Iteration 137, loss = 0.43550317\n",
      "Iteration 138, loss = 0.43905949\n",
      "Iteration 139, loss = 0.43835866\n",
      "Iteration 140, loss = 0.43713051\n",
      "Iteration 141, loss = 0.43708406\n",
      "Iteration 142, loss = 0.43981088\n",
      "Iteration 143, loss = 0.43599741\n",
      "Iteration 144, loss = 0.43473309\n",
      "Iteration 145, loss = 0.43388285\n",
      "Iteration 146, loss = 0.43446618\n",
      "Iteration 147, loss = 0.43427613\n",
      "Iteration 148, loss = 0.43553701\n",
      "Iteration 149, loss = 0.43490531\n",
      "Iteration 150, loss = 0.43381928\n",
      "Iteration 151, loss = 0.43510299\n",
      "Iteration 152, loss = 0.44018999\n",
      "Iteration 153, loss = 0.43606380\n",
      "Iteration 154, loss = 0.43269592\n",
      "Iteration 155, loss = 0.43554417\n",
      "Iteration 156, loss = 0.43293471\n",
      "Iteration 157, loss = 0.43309303\n",
      "Iteration 158, loss = 0.43655828\n",
      "Iteration 159, loss = 0.43412687\n",
      "Iteration 160, loss = 0.43694067\n",
      "Iteration 161, loss = 0.43637566\n",
      "Iteration 162, loss = 0.43459658\n",
      "Iteration 163, loss = 0.43212642\n",
      "Iteration 164, loss = 0.43328015\n",
      "Iteration 165, loss = 0.43253399\n",
      "Iteration 166, loss = 0.43351092\n",
      "Iteration 167, loss = 0.43069363\n",
      "Iteration 168, loss = 0.43213067\n",
      "Iteration 169, loss = 0.43076941\n",
      "Iteration 170, loss = 0.43101929\n",
      "Iteration 171, loss = 0.43099977\n",
      "Iteration 172, loss = 0.43068088\n",
      "Iteration 173, loss = 0.43433987\n",
      "Iteration 174, loss = 0.43251296\n",
      "Iteration 175, loss = 0.43465301\n",
      "Iteration 176, loss = 0.43523228\n",
      "Iteration 177, loss = 0.43419059\n",
      "Iteration 178, loss = 0.43912008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55798716\n",
      "Iteration 2, loss = 0.52485563\n",
      "Iteration 3, loss = 0.51808244\n",
      "Iteration 4, loss = 0.51759148\n",
      "Iteration 5, loss = 0.50988479\n",
      "Iteration 6, loss = 0.50730095\n",
      "Iteration 7, loss = 0.50395019\n",
      "Iteration 8, loss = 0.50252794\n",
      "Iteration 9, loss = 0.50278003\n",
      "Iteration 10, loss = 0.49965884\n",
      "Iteration 11, loss = 0.49819802\n",
      "Iteration 12, loss = 0.49450059\n",
      "Iteration 13, loss = 0.49444457\n",
      "Iteration 14, loss = 0.49180243\n",
      "Iteration 15, loss = 0.48841660\n",
      "Iteration 16, loss = 0.48396633\n",
      "Iteration 17, loss = 0.48750752\n",
      "Iteration 18, loss = 0.48860832\n",
      "Iteration 19, loss = 0.48512639\n",
      "Iteration 20, loss = 0.48272887\n",
      "Iteration 21, loss = 0.48282879\n",
      "Iteration 22, loss = 0.48599138\n",
      "Iteration 23, loss = 0.48240469\n",
      "Iteration 24, loss = 0.48253356\n",
      "Iteration 25, loss = 0.48476804\n",
      "Iteration 26, loss = 0.48268697\n",
      "Iteration 27, loss = 0.48354327\n",
      "Iteration 28, loss = 0.48248569\n",
      "Iteration 29, loss = 0.48204385\n",
      "Iteration 30, loss = 0.48355646\n",
      "Iteration 31, loss = 0.48311863\n",
      "Iteration 32, loss = 0.48403970\n",
      "Iteration 33, loss = 0.48731396\n",
      "Iteration 34, loss = 0.48326271\n",
      "Iteration 35, loss = 0.48328132\n",
      "Iteration 36, loss = 0.48090061\n",
      "Iteration 37, loss = 0.48244263\n",
      "Iteration 38, loss = 0.48088989\n",
      "Iteration 39, loss = 0.48307356\n",
      "Iteration 40, loss = 0.48968680\n",
      "Iteration 41, loss = 0.48140574\n",
      "Iteration 42, loss = 0.48182958\n",
      "Iteration 43, loss = 0.48199813\n",
      "Iteration 44, loss = 0.48230968\n",
      "Iteration 45, loss = 0.47975090\n",
      "Iteration 46, loss = 0.48177763\n",
      "Iteration 47, loss = 0.48048368\n",
      "Iteration 48, loss = 0.47950776\n",
      "Iteration 49, loss = 0.48099928\n",
      "Iteration 50, loss = 0.48091093\n",
      "Iteration 51, loss = 0.48110781\n",
      "Iteration 52, loss = 0.48357602\n",
      "Iteration 53, loss = 0.48108311\n",
      "Iteration 54, loss = 0.48167279\n",
      "Iteration 55, loss = 0.48460812\n",
      "Iteration 56, loss = 0.48347192\n",
      "Iteration 57, loss = 0.48194803\n",
      "Iteration 58, loss = 0.48008269\n",
      "Iteration 59, loss = 0.47904676\n",
      "Iteration 60, loss = 0.48001762\n",
      "Iteration 61, loss = 0.47968033\n",
      "Iteration 62, loss = 0.48221600\n",
      "Iteration 63, loss = 0.48141960\n",
      "Iteration 64, loss = 0.48130124\n",
      "Iteration 65, loss = 0.48022814\n",
      "Iteration 66, loss = 0.48002671\n",
      "Iteration 67, loss = 0.48222215\n",
      "Iteration 68, loss = 0.48384918\n",
      "Iteration 69, loss = 0.48108807\n",
      "Iteration 70, loss = 0.48540819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55826316\n",
      "Iteration 2, loss = 0.52750129\n",
      "Iteration 3, loss = 0.52635390\n",
      "Iteration 4, loss = 0.52270429\n",
      "Iteration 5, loss = 0.51686518\n",
      "Iteration 6, loss = 0.51039513\n",
      "Iteration 7, loss = 0.49483030\n",
      "Iteration 8, loss = 0.49237153\n",
      "Iteration 9, loss = 0.49232751\n",
      "Iteration 10, loss = 0.48988277\n",
      "Iteration 11, loss = 0.49148810\n",
      "Iteration 12, loss = 0.48809064\n",
      "Iteration 13, loss = 0.48896354\n",
      "Iteration 14, loss = 0.49098185\n",
      "Iteration 15, loss = 0.48746011\n",
      "Iteration 16, loss = 0.48667595\n",
      "Iteration 17, loss = 0.48926546\n",
      "Iteration 18, loss = 0.48672947\n",
      "Iteration 19, loss = 0.48836652\n",
      "Iteration 20, loss = 0.48644456\n",
      "Iteration 21, loss = 0.48774912\n",
      "Iteration 22, loss = 0.48877998\n",
      "Iteration 23, loss = 0.48634367\n",
      "Iteration 24, loss = 0.48706819\n",
      "Iteration 25, loss = 0.48514509\n",
      "Iteration 26, loss = 0.48588220\n",
      "Iteration 27, loss = 0.48919869\n",
      "Iteration 28, loss = 0.48806245\n",
      "Iteration 29, loss = 0.48805018\n",
      "Iteration 30, loss = 0.48650064\n",
      "Iteration 31, loss = 0.48569901\n",
      "Iteration 32, loss = 0.48714011\n",
      "Iteration 33, loss = 0.48484334\n",
      "Iteration 34, loss = 0.48609635\n",
      "Iteration 35, loss = 0.48407915\n",
      "Iteration 36, loss = 0.48394962\n",
      "Iteration 37, loss = 0.48419399\n",
      "Iteration 38, loss = 0.48393905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.48292895\n",
      "Iteration 40, loss = 0.48229094\n",
      "Iteration 41, loss = 0.48255244\n",
      "Iteration 42, loss = 0.48213305\n",
      "Iteration 43, loss = 0.48629562\n",
      "Iteration 44, loss = 0.47885496\n",
      "Iteration 45, loss = 0.47420743\n",
      "Iteration 46, loss = 0.47191464\n",
      "Iteration 47, loss = 0.46969642\n",
      "Iteration 48, loss = 0.46648859\n",
      "Iteration 49, loss = 0.46171262\n",
      "Iteration 50, loss = 0.45977934\n",
      "Iteration 51, loss = 0.46057184\n",
      "Iteration 52, loss = 0.46550461\n",
      "Iteration 53, loss = 0.45779080\n",
      "Iteration 54, loss = 0.45350210\n",
      "Iteration 55, loss = 0.45859347\n",
      "Iteration 56, loss = 0.45189715\n",
      "Iteration 57, loss = 0.45073687\n",
      "Iteration 58, loss = 0.44834875\n",
      "Iteration 59, loss = 0.44463569\n",
      "Iteration 60, loss = 0.44631667\n",
      "Iteration 61, loss = 0.44421545\n",
      "Iteration 62, loss = 0.44635034\n",
      "Iteration 63, loss = 0.44367432\n",
      "Iteration 64, loss = 0.44216912\n",
      "Iteration 65, loss = 0.44144147\n",
      "Iteration 66, loss = 0.44066501\n",
      "Iteration 67, loss = 0.44521979\n",
      "Iteration 68, loss = 0.44068754\n",
      "Iteration 69, loss = 0.44074095\n",
      "Iteration 70, loss = 0.44241343\n",
      "Iteration 71, loss = 0.43936843\n",
      "Iteration 72, loss = 0.43792997\n",
      "Iteration 73, loss = 0.43690049\n",
      "Iteration 74, loss = 0.43992786\n",
      "Iteration 75, loss = 0.44254655\n",
      "Iteration 76, loss = 0.43922671\n",
      "Iteration 77, loss = 0.43744351\n",
      "Iteration 78, loss = 0.43797553\n",
      "Iteration 79, loss = 0.43957609\n",
      "Iteration 80, loss = 0.43949212\n",
      "Iteration 81, loss = 0.43802078\n",
      "Iteration 82, loss = 0.44018158\n",
      "Iteration 83, loss = 0.44560736\n",
      "Iteration 84, loss = 0.43770740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58100586\n",
      "Iteration 2, loss = 0.54124477\n",
      "Iteration 3, loss = 0.53742419\n",
      "Iteration 4, loss = 0.53437031\n",
      "Iteration 5, loss = 0.53545660\n",
      "Iteration 6, loss = 0.53881514\n",
      "Iteration 7, loss = 0.53435406\n",
      "Iteration 8, loss = 0.52861813\n",
      "Iteration 9, loss = 0.52736995\n",
      "Iteration 10, loss = 0.52829684\n",
      "Iteration 11, loss = 0.52655659\n",
      "Iteration 12, loss = 0.52440134\n",
      "Iteration 13, loss = 0.52509557\n",
      "Iteration 14, loss = 0.52404004\n",
      "Iteration 15, loss = 0.52278664\n",
      "Iteration 16, loss = 0.52466781\n",
      "Iteration 17, loss = 0.52270263\n",
      "Iteration 18, loss = 0.52314849\n",
      "Iteration 19, loss = 0.52529995\n",
      "Iteration 20, loss = 0.52142366\n",
      "Iteration 21, loss = 0.52263839\n",
      "Iteration 22, loss = 0.52117378\n",
      "Iteration 23, loss = 0.52216077\n",
      "Iteration 24, loss = 0.52169178\n",
      "Iteration 25, loss = 0.52073064\n",
      "Iteration 26, loss = 0.52026717\n",
      "Iteration 27, loss = 0.52158370\n",
      "Iteration 28, loss = 0.52421502\n",
      "Iteration 29, loss = 0.52033041\n",
      "Iteration 30, loss = 0.52132111\n",
      "Iteration 31, loss = 0.52144731\n",
      "Iteration 32, loss = 0.51968921\n",
      "Iteration 33, loss = 0.52013242\n",
      "Iteration 34, loss = 0.51948636\n",
      "Iteration 35, loss = 0.52181996\n",
      "Iteration 36, loss = 0.51945426\n",
      "Iteration 37, loss = 0.52033896\n",
      "Iteration 38, loss = 0.52032567\n",
      "Iteration 39, loss = 0.51924604\n",
      "Iteration 40, loss = 0.52184227\n",
      "Iteration 41, loss = 0.52244816\n",
      "Iteration 42, loss = 0.51898502\n",
      "Iteration 43, loss = 0.51828066\n",
      "Iteration 44, loss = 0.52236780\n",
      "Iteration 45, loss = 0.51767557\n",
      "Iteration 46, loss = 0.51975044\n",
      "Iteration 47, loss = 0.51907907\n",
      "Iteration 48, loss = 0.51722951\n",
      "Iteration 49, loss = 0.52012618\n",
      "Iteration 50, loss = 0.51932575\n",
      "Iteration 51, loss = 0.51930102\n",
      "Iteration 52, loss = 0.51911070\n",
      "Iteration 53, loss = 0.52284339\n",
      "Iteration 54, loss = 0.52120661\n",
      "Iteration 55, loss = 0.51829013\n",
      "Iteration 56, loss = 0.51688135\n",
      "Iteration 57, loss = 0.51877589\n",
      "Iteration 58, loss = 0.52002199\n",
      "Iteration 59, loss = 0.51874041\n",
      "Iteration 60, loss = 0.52153528\n",
      "Iteration 61, loss = 0.51869465\n",
      "Iteration 62, loss = 0.51777112\n",
      "Iteration 63, loss = 0.51797602\n",
      "Iteration 64, loss = 0.52230853\n",
      "Iteration 65, loss = 0.51749219\n",
      "Iteration 66, loss = 0.51919421\n",
      "Iteration 67, loss = 0.52110799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58422403\n",
      "Iteration 2, loss = 0.54060333\n",
      "Iteration 3, loss = 0.53394188\n",
      "Iteration 4, loss = 0.53530130\n",
      "Iteration 5, loss = 0.53019576\n",
      "Iteration 6, loss = 0.53223673\n",
      "Iteration 7, loss = 0.52402491\n",
      "Iteration 8, loss = 0.52233928\n",
      "Iteration 9, loss = 0.52122448\n",
      "Iteration 10, loss = 0.52035232\n",
      "Iteration 11, loss = 0.51896482\n",
      "Iteration 12, loss = 0.51892073\n",
      "Iteration 13, loss = 0.51973494\n",
      "Iteration 14, loss = 0.51947159\n",
      "Iteration 15, loss = 0.51747884\n",
      "Iteration 16, loss = 0.51606524\n",
      "Iteration 17, loss = 0.51708333\n",
      "Iteration 18, loss = 0.51746262\n",
      "Iteration 19, loss = 0.51941878\n",
      "Iteration 20, loss = 0.51820053\n",
      "Iteration 21, loss = 0.51834213\n",
      "Iteration 22, loss = 0.52123014\n",
      "Iteration 23, loss = 0.51644229\n",
      "Iteration 24, loss = 0.51605302\n",
      "Iteration 25, loss = 0.51638743\n",
      "Iteration 26, loss = 0.51625790\n",
      "Iteration 27, loss = 0.51729533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58046825\n",
      "Iteration 2, loss = 0.54187376\n",
      "Iteration 3, loss = 0.53615624\n",
      "Iteration 4, loss = 0.53184661\n",
      "Iteration 5, loss = 0.52758431\n",
      "Iteration 6, loss = 0.52803926\n",
      "Iteration 7, loss = 0.52506742\n",
      "Iteration 8, loss = 0.52532305\n",
      "Iteration 9, loss = 0.52582444\n",
      "Iteration 10, loss = 0.52461853\n",
      "Iteration 11, loss = 0.52421118\n",
      "Iteration 12, loss = 0.52317809\n",
      "Iteration 13, loss = 0.52233357\n",
      "Iteration 14, loss = 0.52245448\n",
      "Iteration 15, loss = 0.52080027\n",
      "Iteration 16, loss = 0.52094272\n",
      "Iteration 17, loss = 0.52065076\n",
      "Iteration 18, loss = 0.51928347\n",
      "Iteration 19, loss = 0.52125239\n",
      "Iteration 20, loss = 0.51964291\n",
      "Iteration 21, loss = 0.51922636\n",
      "Iteration 22, loss = 0.52171149\n",
      "Iteration 23, loss = 0.51982453\n",
      "Iteration 24, loss = 0.51948811\n",
      "Iteration 25, loss = 0.51864014\n",
      "Iteration 26, loss = 0.51816168\n",
      "Iteration 27, loss = 0.52000952\n",
      "Iteration 28, loss = 0.51751167\n",
      "Iteration 29, loss = 0.51765858\n",
      "Iteration 30, loss = 0.51681535\n",
      "Iteration 31, loss = 0.51861054\n",
      "Iteration 32, loss = 0.51768943\n",
      "Iteration 33, loss = 0.51724955\n",
      "Iteration 34, loss = 0.51936602\n",
      "Iteration 35, loss = 0.51741256\n",
      "Iteration 36, loss = 0.51654944\n",
      "Iteration 37, loss = 0.51607938\n",
      "Iteration 38, loss = 0.51798259\n",
      "Iteration 39, loss = 0.51806711\n",
      "Iteration 40, loss = 0.51555186\n",
      "Iteration 41, loss = 0.51492572\n",
      "Iteration 42, loss = 0.51495180\n",
      "Iteration 43, loss = 0.51548814\n",
      "Iteration 44, loss = 0.51500725\n",
      "Iteration 45, loss = 0.51599712\n",
      "Iteration 46, loss = 0.51677090\n",
      "Iteration 47, loss = 0.51665654\n",
      "Iteration 48, loss = 0.51526451\n",
      "Iteration 49, loss = 0.51515723\n",
      "Iteration 50, loss = 0.51663870\n",
      "Iteration 51, loss = 0.51681844\n",
      "Iteration 52, loss = 0.51929790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58157290\n",
      "Iteration 2, loss = 0.52388293\n",
      "Iteration 3, loss = 0.51420706\n",
      "Iteration 4, loss = 0.51172844\n",
      "Iteration 5, loss = 0.51305626\n",
      "Iteration 6, loss = 0.51808927\n",
      "Iteration 7, loss = 0.51288407\n",
      "Iteration 8, loss = 0.50864875\n",
      "Iteration 9, loss = 0.50825071\n",
      "Iteration 10, loss = 0.50863107\n",
      "Iteration 11, loss = 0.50746386\n",
      "Iteration 12, loss = 0.50634664\n",
      "Iteration 13, loss = 0.50535175\n",
      "Iteration 14, loss = 0.50538726\n",
      "Iteration 15, loss = 0.50267746\n",
      "Iteration 16, loss = 0.50271789\n",
      "Iteration 17, loss = 0.50193098\n",
      "Iteration 18, loss = 0.50190269\n",
      "Iteration 19, loss = 0.50229593\n",
      "Iteration 20, loss = 0.50041374\n",
      "Iteration 21, loss = 0.49944476\n",
      "Iteration 22, loss = 0.49905286\n",
      "Iteration 23, loss = 0.49970411\n",
      "Iteration 24, loss = 0.49821946\n",
      "Iteration 25, loss = 0.49798424\n",
      "Iteration 26, loss = 0.49836922\n",
      "Iteration 27, loss = 0.49848179\n",
      "Iteration 28, loss = 0.49881043\n",
      "Iteration 29, loss = 0.49842735\n",
      "Iteration 30, loss = 0.49950479\n",
      "Iteration 31, loss = 0.49858844\n",
      "Iteration 32, loss = 0.49767182\n",
      "Iteration 33, loss = 0.49887639\n",
      "Iteration 34, loss = 0.49721818\n",
      "Iteration 35, loss = 0.49740604\n",
      "Iteration 36, loss = 0.49638884\n",
      "Iteration 37, loss = 0.49697036\n",
      "Iteration 38, loss = 0.49715193\n",
      "Iteration 39, loss = 0.49597024\n",
      "Iteration 40, loss = 0.49688200\n",
      "Iteration 41, loss = 0.49782586\n",
      "Iteration 42, loss = 0.49585611\n",
      "Iteration 43, loss = 0.49586074\n",
      "Iteration 44, loss = 0.49914715\n",
      "Iteration 45, loss = 0.49653555\n",
      "Iteration 46, loss = 0.49713528\n",
      "Iteration 47, loss = 0.49561216\n",
      "Iteration 48, loss = 0.49484358\n",
      "Iteration 49, loss = 0.49637055\n",
      "Iteration 50, loss = 0.49597631\n",
      "Iteration 51, loss = 0.49532808\n",
      "Iteration 52, loss = 0.49572132\n",
      "Iteration 53, loss = 0.49509977\n",
      "Iteration 54, loss = 0.49527552\n",
      "Iteration 55, loss = 0.49440544\n",
      "Iteration 56, loss = 0.49417399\n",
      "Iteration 57, loss = 0.49543835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 0.49484545\n",
      "Iteration 59, loss = 0.49539863\n",
      "Iteration 60, loss = 0.49604728\n",
      "Iteration 61, loss = 0.49527012\n",
      "Iteration 62, loss = 0.49496230\n",
      "Iteration 63, loss = 0.49493970\n",
      "Iteration 64, loss = 0.49843005\n",
      "Iteration 65, loss = 0.49340946\n",
      "Iteration 66, loss = 0.49406981\n",
      "Iteration 67, loss = 0.49199387\n",
      "Iteration 68, loss = 0.49113196\n",
      "Iteration 69, loss = 0.49180407\n",
      "Iteration 70, loss = 0.49220853\n",
      "Iteration 71, loss = 0.49135850\n",
      "Iteration 72, loss = 0.49020998\n",
      "Iteration 73, loss = 0.48968040\n",
      "Iteration 74, loss = 0.49021848\n",
      "Iteration 75, loss = 0.49051273\n",
      "Iteration 76, loss = 0.48987314\n",
      "Iteration 77, loss = 0.48824772\n",
      "Iteration 78, loss = 0.48780770\n",
      "Iteration 79, loss = 0.48785148\n",
      "Iteration 80, loss = 0.48774830\n",
      "Iteration 81, loss = 0.48721177\n",
      "Iteration 82, loss = 0.48737647\n",
      "Iteration 83, loss = 0.48689286\n",
      "Iteration 84, loss = 0.48805693\n",
      "Iteration 85, loss = 0.48794173\n",
      "Iteration 86, loss = 0.48790493\n",
      "Iteration 87, loss = 0.48941996\n",
      "Iteration 88, loss = 0.48604884\n",
      "Iteration 89, loss = 0.48597487\n",
      "Iteration 90, loss = 0.48539330\n",
      "Iteration 91, loss = 0.48720621\n",
      "Iteration 92, loss = 0.48695118\n",
      "Iteration 93, loss = 0.48727144\n",
      "Iteration 94, loss = 0.48730875\n",
      "Iteration 95, loss = 0.48638108\n",
      "Iteration 96, loss = 0.48624360\n",
      "Iteration 97, loss = 0.48571336\n",
      "Iteration 98, loss = 0.48610443\n",
      "Iteration 99, loss = 0.48612754\n",
      "Iteration 100, loss = 0.48594524\n",
      "Iteration 101, loss = 0.48465475\n",
      "Iteration 102, loss = 0.48590091\n",
      "Iteration 103, loss = 0.48496641\n",
      "Iteration 104, loss = 0.48773813\n",
      "Iteration 105, loss = 0.48573128\n",
      "Iteration 106, loss = 0.48662984\n",
      "Iteration 107, loss = 0.48688262\n",
      "Iteration 108, loss = 0.48666940\n",
      "Iteration 109, loss = 0.48788499\n",
      "Iteration 110, loss = 0.48652591\n",
      "Iteration 111, loss = 0.48571588\n",
      "Iteration 112, loss = 0.48657452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58435636\n",
      "Iteration 2, loss = 0.52166673\n",
      "Iteration 3, loss = 0.50728765\n",
      "Iteration 4, loss = 0.50721164\n",
      "Iteration 5, loss = 0.50459515\n",
      "Iteration 6, loss = 0.50540940\n",
      "Iteration 7, loss = 0.50333183\n",
      "Iteration 8, loss = 0.50183708\n",
      "Iteration 9, loss = 0.50215195\n",
      "Iteration 10, loss = 0.50205662\n",
      "Iteration 11, loss = 0.50033596\n",
      "Iteration 12, loss = 0.50045369\n",
      "Iteration 13, loss = 0.50146407\n",
      "Iteration 14, loss = 0.49972647\n",
      "Iteration 15, loss = 0.49682841\n",
      "Iteration 16, loss = 0.49643252\n",
      "Iteration 17, loss = 0.49555863\n",
      "Iteration 18, loss = 0.49490726\n",
      "Iteration 19, loss = 0.49600766\n",
      "Iteration 20, loss = 0.49754475\n",
      "Iteration 21, loss = 0.49470828\n",
      "Iteration 22, loss = 0.49482490\n",
      "Iteration 23, loss = 0.49293522\n",
      "Iteration 24, loss = 0.49253849\n",
      "Iteration 25, loss = 0.49334055\n",
      "Iteration 26, loss = 0.49239189\n",
      "Iteration 27, loss = 0.49326638\n",
      "Iteration 28, loss = 0.49214459\n",
      "Iteration 29, loss = 0.49209269\n",
      "Iteration 30, loss = 0.49222012\n",
      "Iteration 31, loss = 0.49371242\n",
      "Iteration 32, loss = 0.49089257\n",
      "Iteration 33, loss = 0.49233933\n",
      "Iteration 34, loss = 0.49252004\n",
      "Iteration 35, loss = 0.49090270\n",
      "Iteration 36, loss = 0.49124581\n",
      "Iteration 37, loss = 0.49096957\n",
      "Iteration 38, loss = 0.49113834\n",
      "Iteration 39, loss = 0.49248273\n",
      "Iteration 40, loss = 0.49172643\n",
      "Iteration 41, loss = 0.49065559\n",
      "Iteration 42, loss = 0.49060313\n",
      "Iteration 43, loss = 0.49130659\n",
      "Iteration 44, loss = 0.49209495\n",
      "Iteration 45, loss = 0.48970111\n",
      "Iteration 46, loss = 0.49128960\n",
      "Iteration 47, loss = 0.49272881\n",
      "Iteration 48, loss = 0.48994704\n",
      "Iteration 49, loss = 0.49055174\n",
      "Iteration 50, loss = 0.48996872\n",
      "Iteration 51, loss = 0.49089347\n",
      "Iteration 52, loss = 0.49203340\n",
      "Iteration 53, loss = 0.49020514\n",
      "Iteration 54, loss = 0.49054829\n",
      "Iteration 55, loss = 0.48991733\n",
      "Iteration 56, loss = 0.48988258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58056461\n",
      "Iteration 2, loss = 0.52263803\n",
      "Iteration 3, loss = 0.51331725\n",
      "Iteration 4, loss = 0.50901432\n",
      "Iteration 5, loss = 0.50609548\n",
      "Iteration 6, loss = 0.50768076\n",
      "Iteration 7, loss = 0.50396177\n",
      "Iteration 8, loss = 0.50582288\n",
      "Iteration 9, loss = 0.50623404\n",
      "Iteration 10, loss = 0.50488302\n",
      "Iteration 11, loss = 0.50456669\n",
      "Iteration 12, loss = 0.50289235\n",
      "Iteration 13, loss = 0.50352084\n",
      "Iteration 14, loss = 0.50341635\n",
      "Iteration 15, loss = 0.50014408\n",
      "Iteration 16, loss = 0.50007694\n",
      "Iteration 17, loss = 0.49995744\n",
      "Iteration 18, loss = 0.49763838\n",
      "Iteration 19, loss = 0.49874738\n",
      "Iteration 20, loss = 0.49767697\n",
      "Iteration 21, loss = 0.49715456\n",
      "Iteration 22, loss = 0.49783614\n",
      "Iteration 23, loss = 0.49573102\n",
      "Iteration 24, loss = 0.49601793\n",
      "Iteration 25, loss = 0.49534917\n",
      "Iteration 26, loss = 0.49569165\n",
      "Iteration 27, loss = 0.49653794\n",
      "Iteration 28, loss = 0.49586506\n",
      "Iteration 29, loss = 0.49537336\n",
      "Iteration 30, loss = 0.49433537\n",
      "Iteration 31, loss = 0.49487030\n",
      "Iteration 32, loss = 0.49480481\n",
      "Iteration 33, loss = 0.49436826\n",
      "Iteration 34, loss = 0.49572544\n",
      "Iteration 35, loss = 0.49416337\n",
      "Iteration 36, loss = 0.49383843\n",
      "Iteration 37, loss = 0.49393258\n",
      "Iteration 38, loss = 0.49519280\n",
      "Iteration 39, loss = 0.49508837\n",
      "Iteration 40, loss = 0.49284744\n",
      "Iteration 41, loss = 0.49368475\n",
      "Iteration 42, loss = 0.49272648\n",
      "Iteration 43, loss = 0.49541564\n",
      "Iteration 44, loss = 0.49237081\n",
      "Iteration 45, loss = 0.49287964\n",
      "Iteration 46, loss = 0.49543057\n",
      "Iteration 47, loss = 0.49487561\n",
      "Iteration 48, loss = 0.49283600\n",
      "Iteration 49, loss = 0.49246005\n",
      "Iteration 50, loss = 0.49398936\n",
      "Iteration 51, loss = 0.49387886\n",
      "Iteration 52, loss = 0.49772602\n",
      "Iteration 53, loss = 0.49264857\n",
      "Iteration 54, loss = 0.49232055\n",
      "Iteration 55, loss = 0.49370363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54141842\n",
      "Iteration 2, loss = 0.49881340\n",
      "Iteration 3, loss = 0.48880250\n",
      "Iteration 4, loss = 0.48409029\n",
      "Iteration 5, loss = 0.48154372\n",
      "Iteration 6, loss = 0.48072085\n",
      "Iteration 7, loss = 0.47741153\n",
      "Iteration 8, loss = 0.47588101\n",
      "Iteration 9, loss = 0.47431753\n",
      "Iteration 10, loss = 0.47451017\n",
      "Iteration 11, loss = 0.47323164\n",
      "Iteration 12, loss = 0.47096191\n",
      "Iteration 13, loss = 0.47089721\n",
      "Iteration 14, loss = 0.46963612\n",
      "Iteration 15, loss = 0.46971374\n",
      "Iteration 16, loss = 0.46913143\n",
      "Iteration 17, loss = 0.46916906\n",
      "Iteration 18, loss = 0.46715254\n",
      "Iteration 19, loss = 0.46640122\n",
      "Iteration 20, loss = 0.46622320\n",
      "Iteration 21, loss = 0.46639669\n",
      "Iteration 22, loss = 0.46437317\n",
      "Iteration 23, loss = 0.46619263\n",
      "Iteration 24, loss = 0.46510472\n",
      "Iteration 25, loss = 0.46468832\n",
      "Iteration 26, loss = 0.46310121\n",
      "Iteration 27, loss = 0.46348479\n",
      "Iteration 28, loss = 0.46470308\n",
      "Iteration 29, loss = 0.46312558\n",
      "Iteration 30, loss = 0.46465008\n",
      "Iteration 31, loss = 0.46365525\n",
      "Iteration 32, loss = 0.46114291\n",
      "Iteration 33, loss = 0.46308614\n",
      "Iteration 34, loss = 0.46141593\n",
      "Iteration 35, loss = 0.46156846\n",
      "Iteration 36, loss = 0.45990971\n",
      "Iteration 37, loss = 0.46005577\n",
      "Iteration 38, loss = 0.46083375\n",
      "Iteration 39, loss = 0.45975872\n",
      "Iteration 40, loss = 0.46137790\n",
      "Iteration 41, loss = 0.46083689\n",
      "Iteration 42, loss = 0.45869410\n",
      "Iteration 43, loss = 0.45940203\n",
      "Iteration 44, loss = 0.45927830\n",
      "Iteration 45, loss = 0.45789538\n",
      "Iteration 46, loss = 0.45988821\n",
      "Iteration 47, loss = 0.45800540\n",
      "Iteration 48, loss = 0.45795119\n",
      "Iteration 49, loss = 0.46009965\n",
      "Iteration 50, loss = 0.45942246\n",
      "Iteration 51, loss = 0.45805246\n",
      "Iteration 52, loss = 0.45865932\n",
      "Iteration 53, loss = 0.45851080\n",
      "Iteration 54, loss = 0.45875507\n",
      "Iteration 55, loss = 0.45619269\n",
      "Iteration 56, loss = 0.45622110\n",
      "Iteration 57, loss = 0.45711230\n",
      "Iteration 58, loss = 0.45698535\n",
      "Iteration 59, loss = 0.45707230\n",
      "Iteration 60, loss = 0.45882139\n",
      "Iteration 61, loss = 0.45585785\n",
      "Iteration 62, loss = 0.45720782\n",
      "Iteration 63, loss = 0.46027802\n",
      "Iteration 64, loss = 0.46407465\n",
      "Iteration 65, loss = 0.45665430\n",
      "Iteration 66, loss = 0.45509950\n",
      "Iteration 67, loss = 0.45631442\n",
      "Iteration 68, loss = 0.45539807\n",
      "Iteration 69, loss = 0.45472583\n",
      "Iteration 70, loss = 0.45691767\n",
      "Iteration 71, loss = 0.45878147\n",
      "Iteration 72, loss = 0.45571783\n",
      "Iteration 73, loss = 0.45600388\n",
      "Iteration 74, loss = 0.45695253\n",
      "Iteration 75, loss = 0.45591798\n",
      "Iteration 76, loss = 0.45394404\n",
      "Iteration 77, loss = 0.45436153\n",
      "Iteration 78, loss = 0.45494077\n",
      "Iteration 79, loss = 0.45411613\n",
      "Iteration 80, loss = 0.45469133\n",
      "Iteration 81, loss = 0.45359699\n",
      "Iteration 82, loss = 0.45357441\n",
      "Iteration 83, loss = 0.45322919\n",
      "Iteration 84, loss = 0.45603759\n",
      "Iteration 85, loss = 0.45418556\n",
      "Iteration 86, loss = 0.45618429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87, loss = 0.45522174\n",
      "Iteration 88, loss = 0.45254908\n",
      "Iteration 89, loss = 0.45276813\n",
      "Iteration 90, loss = 0.45299623\n",
      "Iteration 91, loss = 0.45500374\n",
      "Iteration 92, loss = 0.45430941\n",
      "Iteration 93, loss = 0.45409854\n",
      "Iteration 94, loss = 0.45376928\n",
      "Iteration 95, loss = 0.45251929\n",
      "Iteration 96, loss = 0.45377120\n",
      "Iteration 97, loss = 0.45234336\n",
      "Iteration 98, loss = 0.45264120\n",
      "Iteration 99, loss = 0.45240944\n",
      "Iteration 100, loss = 0.45170275\n",
      "Iteration 101, loss = 0.45281386\n",
      "Iteration 102, loss = 0.45359817\n",
      "Iteration 103, loss = 0.45244326\n",
      "Iteration 104, loss = 0.45163567\n",
      "Iteration 105, loss = 0.45242794\n",
      "Iteration 106, loss = 0.45318466\n",
      "Iteration 107, loss = 0.45293414\n",
      "Iteration 108, loss = 0.45191024\n",
      "Iteration 109, loss = 0.45217037\n",
      "Iteration 110, loss = 0.45241898\n",
      "Iteration 111, loss = 0.45167174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54043448\n",
      "Iteration 2, loss = 0.49609914\n",
      "Iteration 3, loss = 0.48901181\n",
      "Iteration 4, loss = 0.48695355\n",
      "Iteration 5, loss = 0.48364588\n",
      "Iteration 6, loss = 0.48085776\n",
      "Iteration 7, loss = 0.47906919\n",
      "Iteration 8, loss = 0.47624712\n",
      "Iteration 9, loss = 0.47499144\n",
      "Iteration 10, loss = 0.47387949\n",
      "Iteration 11, loss = 0.47145334\n",
      "Iteration 12, loss = 0.46986815\n",
      "Iteration 13, loss = 0.46951690\n",
      "Iteration 14, loss = 0.47011895\n",
      "Iteration 15, loss = 0.46653950\n",
      "Iteration 16, loss = 0.46667106\n",
      "Iteration 17, loss = 0.46584560\n",
      "Iteration 18, loss = 0.46481028\n",
      "Iteration 19, loss = 0.46421290\n",
      "Iteration 20, loss = 0.46593680\n",
      "Iteration 21, loss = 0.46324628\n",
      "Iteration 22, loss = 0.46344456\n",
      "Iteration 23, loss = 0.46390784\n",
      "Iteration 24, loss = 0.46323501\n",
      "Iteration 25, loss = 0.46276371\n",
      "Iteration 26, loss = 0.46181166\n",
      "Iteration 27, loss = 0.46135959\n",
      "Iteration 28, loss = 0.46081970\n",
      "Iteration 29, loss = 0.46135907\n",
      "Iteration 30, loss = 0.46015331\n",
      "Iteration 31, loss = 0.46029446\n",
      "Iteration 32, loss = 0.45948988\n",
      "Iteration 33, loss = 0.46007074\n",
      "Iteration 34, loss = 0.45909133\n",
      "Iteration 35, loss = 0.45961582\n",
      "Iteration 36, loss = 0.45852158\n",
      "Iteration 37, loss = 0.45749478\n",
      "Iteration 38, loss = 0.45749539\n",
      "Iteration 39, loss = 0.45841677\n",
      "Iteration 40, loss = 0.46065741\n",
      "Iteration 41, loss = 0.45792870\n",
      "Iteration 42, loss = 0.45781757\n",
      "Iteration 43, loss = 0.45831014\n",
      "Iteration 44, loss = 0.45803913\n",
      "Iteration 45, loss = 0.45523262\n",
      "Iteration 46, loss = 0.45327530\n",
      "Iteration 47, loss = 0.45963717\n",
      "Iteration 48, loss = 0.45447086\n",
      "Iteration 49, loss = 0.45235598\n",
      "Iteration 50, loss = 0.45067739\n",
      "Iteration 51, loss = 0.45014630\n",
      "Iteration 52, loss = 0.45072854\n",
      "Iteration 53, loss = 0.45033070\n",
      "Iteration 54, loss = 0.44990035\n",
      "Iteration 55, loss = 0.44863462\n",
      "Iteration 56, loss = 0.44904153\n",
      "Iteration 57, loss = 0.44856555\n",
      "Iteration 58, loss = 0.44860719\n",
      "Iteration 59, loss = 0.44911998\n",
      "Iteration 60, loss = 0.45009333\n",
      "Iteration 61, loss = 0.44888183\n",
      "Iteration 62, loss = 0.44812163\n",
      "Iteration 63, loss = 0.45170361\n",
      "Iteration 64, loss = 0.45502689\n",
      "Iteration 65, loss = 0.44782811\n",
      "Iteration 66, loss = 0.44892089\n",
      "Iteration 67, loss = 0.44950131\n",
      "Iteration 68, loss = 0.44872877\n",
      "Iteration 69, loss = 0.44914308\n",
      "Iteration 70, loss = 0.44987626\n",
      "Iteration 71, loss = 0.44830655\n",
      "Iteration 72, loss = 0.44743440\n",
      "Iteration 73, loss = 0.44701372\n",
      "Iteration 74, loss = 0.44775090\n",
      "Iteration 75, loss = 0.44869234\n",
      "Iteration 76, loss = 0.44972402\n",
      "Iteration 77, loss = 0.44633810\n",
      "Iteration 78, loss = 0.44842390\n",
      "Iteration 79, loss = 0.44758483\n",
      "Iteration 80, loss = 0.44826421\n",
      "Iteration 81, loss = 0.44912416\n",
      "Iteration 82, loss = 0.44826604\n",
      "Iteration 83, loss = 0.44741984\n",
      "Iteration 84, loss = 0.44921874\n",
      "Iteration 85, loss = 0.44669166\n",
      "Iteration 86, loss = 0.44830053\n",
      "Iteration 87, loss = 0.45021720\n",
      "Iteration 88, loss = 0.44832664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54009893\n",
      "Iteration 2, loss = 0.49717780\n",
      "Iteration 3, loss = 0.48987024\n",
      "Iteration 4, loss = 0.48458263\n",
      "Iteration 5, loss = 0.48197702\n",
      "Iteration 6, loss = 0.47984984\n",
      "Iteration 7, loss = 0.47716774\n",
      "Iteration 8, loss = 0.47688725\n",
      "Iteration 9, loss = 0.47490352\n",
      "Iteration 10, loss = 0.47320771\n",
      "Iteration 11, loss = 0.47222093\n",
      "Iteration 12, loss = 0.47116342\n",
      "Iteration 13, loss = 0.47077375\n",
      "Iteration 14, loss = 0.46991858\n",
      "Iteration 15, loss = 0.46794917\n",
      "Iteration 16, loss = 0.46893130\n",
      "Iteration 17, loss = 0.46867432\n",
      "Iteration 18, loss = 0.46625106\n",
      "Iteration 19, loss = 0.46751582\n",
      "Iteration 20, loss = 0.46510875\n",
      "Iteration 21, loss = 0.46473580\n",
      "Iteration 22, loss = 0.46538307\n",
      "Iteration 23, loss = 0.46476016\n",
      "Iteration 24, loss = 0.46572299\n",
      "Iteration 25, loss = 0.46384509\n",
      "Iteration 26, loss = 0.46294692\n",
      "Iteration 27, loss = 0.46466840\n",
      "Iteration 28, loss = 0.46337397\n",
      "Iteration 29, loss = 0.46205313\n",
      "Iteration 30, loss = 0.46110926\n",
      "Iteration 31, loss = 0.46163788\n",
      "Iteration 32, loss = 0.46048498\n",
      "Iteration 33, loss = 0.46017241\n",
      "Iteration 34, loss = 0.46122137\n",
      "Iteration 35, loss = 0.46023288\n",
      "Iteration 36, loss = 0.46005312\n",
      "Iteration 37, loss = 0.45996138\n",
      "Iteration 38, loss = 0.45884843\n",
      "Iteration 39, loss = 0.45910099\n",
      "Iteration 40, loss = 0.45753307\n",
      "Iteration 41, loss = 0.45820885\n",
      "Iteration 42, loss = 0.45698373\n",
      "Iteration 43, loss = 0.45813230\n",
      "Iteration 44, loss = 0.45646553\n",
      "Iteration 45, loss = 0.45670236\n",
      "Iteration 46, loss = 0.45845566\n",
      "Iteration 47, loss = 0.45965807\n",
      "Iteration 48, loss = 0.45635342\n",
      "Iteration 49, loss = 0.45618417\n",
      "Iteration 50, loss = 0.45658214\n",
      "Iteration 51, loss = 0.45612190\n",
      "Iteration 52, loss = 0.45754255\n",
      "Iteration 53, loss = 0.45541347\n",
      "Iteration 54, loss = 0.45523614\n",
      "Iteration 55, loss = 0.45469607\n",
      "Iteration 56, loss = 0.45529684\n",
      "Iteration 57, loss = 0.45460330\n",
      "Iteration 58, loss = 0.45455807\n",
      "Iteration 59, loss = 0.45415526\n",
      "Iteration 60, loss = 0.45420249\n",
      "Iteration 61, loss = 0.45426001\n",
      "Iteration 62, loss = 0.45471543\n",
      "Iteration 63, loss = 0.45385408\n",
      "Iteration 64, loss = 0.45375089\n",
      "Iteration 65, loss = 0.45326154\n",
      "Iteration 66, loss = 0.45328004\n",
      "Iteration 67, loss = 0.45457151\n",
      "Iteration 68, loss = 0.45352616\n",
      "Iteration 69, loss = 0.45344170\n",
      "Iteration 70, loss = 0.45247859\n",
      "Iteration 71, loss = 0.45244393\n",
      "Iteration 72, loss = 0.45190250\n",
      "Iteration 73, loss = 0.45125462\n",
      "Iteration 74, loss = 0.45300404\n",
      "Iteration 75, loss = 0.45228333\n",
      "Iteration 76, loss = 0.45204188\n",
      "Iteration 77, loss = 0.45081827\n",
      "Iteration 78, loss = 0.45177127\n",
      "Iteration 79, loss = 0.45160888\n",
      "Iteration 80, loss = 0.45095973\n",
      "Iteration 81, loss = 0.45153352\n",
      "Iteration 82, loss = 0.45135715\n",
      "Iteration 83, loss = 0.45161640\n",
      "Iteration 84, loss = 0.45173858\n",
      "Iteration 85, loss = 0.45220439\n",
      "Iteration 86, loss = 0.45079105\n",
      "Iteration 87, loss = 0.45081883\n",
      "Iteration 88, loss = 0.45088980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54378249\n",
      "Iteration 2, loss = 0.49796490\n",
      "Iteration 3, loss = 0.48426469\n",
      "Iteration 4, loss = 0.47660784\n",
      "Iteration 5, loss = 0.47241840\n",
      "Iteration 6, loss = 0.47073069\n",
      "Iteration 7, loss = 0.46833408\n",
      "Iteration 8, loss = 0.46528233\n",
      "Iteration 9, loss = 0.46415037\n",
      "Iteration 10, loss = 0.46525642\n",
      "Iteration 11, loss = 0.46371837\n",
      "Iteration 12, loss = 0.46252043\n",
      "Iteration 13, loss = 0.46255309\n",
      "Iteration 14, loss = 0.46147710\n",
      "Iteration 15, loss = 0.46049327\n",
      "Iteration 16, loss = 0.46090072\n",
      "Iteration 17, loss = 0.46060387\n",
      "Iteration 18, loss = 0.46020906\n",
      "Iteration 19, loss = 0.45869135\n",
      "Iteration 20, loss = 0.45822939\n",
      "Iteration 21, loss = 0.45794444\n",
      "Iteration 22, loss = 0.45776453\n",
      "Iteration 23, loss = 0.45848740\n",
      "Iteration 24, loss = 0.45742713\n",
      "Iteration 25, loss = 0.45814439\n",
      "Iteration 26, loss = 0.45666631\n",
      "Iteration 27, loss = 0.45698585\n",
      "Iteration 28, loss = 0.45815608\n",
      "Iteration 29, loss = 0.45738530\n",
      "Iteration 30, loss = 0.45905887\n",
      "Iteration 31, loss = 0.45729632\n",
      "Iteration 32, loss = 0.45598639\n",
      "Iteration 33, loss = 0.45678422\n",
      "Iteration 34, loss = 0.45629432\n",
      "Iteration 35, loss = 0.45571442\n",
      "Iteration 36, loss = 0.45485203\n",
      "Iteration 37, loss = 0.45554193\n",
      "Iteration 38, loss = 0.45626577\n",
      "Iteration 39, loss = 0.45706911\n",
      "Iteration 40, loss = 0.45496740\n",
      "Iteration 41, loss = 0.45473099\n",
      "Iteration 42, loss = 0.45404092\n",
      "Iteration 43, loss = 0.45627088\n",
      "Iteration 44, loss = 0.45427271\n",
      "Iteration 45, loss = 0.45410511\n",
      "Iteration 46, loss = 0.45357582\n",
      "Iteration 47, loss = 0.45328671\n",
      "Iteration 48, loss = 0.45312773\n",
      "Iteration 49, loss = 0.45504052\n",
      "Iteration 50, loss = 0.45328698\n",
      "Iteration 51, loss = 0.45437732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.45273570\n",
      "Iteration 53, loss = 0.45274478\n",
      "Iteration 54, loss = 0.45288137\n",
      "Iteration 55, loss = 0.45269635\n",
      "Iteration 56, loss = 0.45273217\n",
      "Iteration 57, loss = 0.45244782\n",
      "Iteration 58, loss = 0.45286560\n",
      "Iteration 59, loss = 0.45476099\n",
      "Iteration 60, loss = 0.45444033\n",
      "Iteration 61, loss = 0.45163922\n",
      "Iteration 62, loss = 0.45233341\n",
      "Iteration 63, loss = 0.45339102\n",
      "Iteration 64, loss = 0.45320867\n",
      "Iteration 65, loss = 0.45136058\n",
      "Iteration 66, loss = 0.45108755\n",
      "Iteration 67, loss = 0.45174875\n",
      "Iteration 68, loss = 0.45132767\n",
      "Iteration 69, loss = 0.45081673\n",
      "Iteration 70, loss = 0.45379840\n",
      "Iteration 71, loss = 0.45148362\n",
      "Iteration 72, loss = 0.45165907\n",
      "Iteration 73, loss = 0.45069822\n",
      "Iteration 74, loss = 0.45186286\n",
      "Iteration 75, loss = 0.45148299\n",
      "Iteration 76, loss = 0.45036459\n",
      "Iteration 77, loss = 0.44994546\n",
      "Iteration 78, loss = 0.45123206\n",
      "Iteration 79, loss = 0.45217570\n",
      "Iteration 80, loss = 0.45126978\n",
      "Iteration 81, loss = 0.44962892\n",
      "Iteration 82, loss = 0.45026011\n",
      "Iteration 83, loss = 0.45062509\n",
      "Iteration 84, loss = 0.44953928\n",
      "Iteration 85, loss = 0.44980971\n",
      "Iteration 86, loss = 0.45095359\n",
      "Iteration 87, loss = 0.44920196\n",
      "Iteration 88, loss = 0.44899183\n",
      "Iteration 89, loss = 0.44967775\n",
      "Iteration 90, loss = 0.44939621\n",
      "Iteration 91, loss = 0.44953828\n",
      "Iteration 92, loss = 0.45114654\n",
      "Iteration 93, loss = 0.44963709\n",
      "Iteration 94, loss = 0.44984684\n",
      "Iteration 95, loss = 0.44998619\n",
      "Iteration 96, loss = 0.44953853\n",
      "Iteration 97, loss = 0.44970070\n",
      "Iteration 98, loss = 0.44930543\n",
      "Iteration 99, loss = 0.45031216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54185003\n",
      "Iteration 2, loss = 0.49371437\n",
      "Iteration 3, loss = 0.48246428\n",
      "Iteration 4, loss = 0.47638477\n",
      "Iteration 5, loss = 0.47145302\n",
      "Iteration 6, loss = 0.46866434\n",
      "Iteration 7, loss = 0.46668662\n",
      "Iteration 8, loss = 0.46375767\n",
      "Iteration 9, loss = 0.46435758\n",
      "Iteration 10, loss = 0.46433667\n",
      "Iteration 11, loss = 0.46132691\n",
      "Iteration 12, loss = 0.46163580\n",
      "Iteration 13, loss = 0.46084414\n",
      "Iteration 14, loss = 0.46147712\n",
      "Iteration 15, loss = 0.46000997\n",
      "Iteration 16, loss = 0.45894483\n",
      "Iteration 17, loss = 0.45949875\n",
      "Iteration 18, loss = 0.45872070\n",
      "Iteration 19, loss = 0.45898111\n",
      "Iteration 20, loss = 0.45912112\n",
      "Iteration 21, loss = 0.45793513\n",
      "Iteration 22, loss = 0.45732866\n",
      "Iteration 23, loss = 0.45838896\n",
      "Iteration 24, loss = 0.45796373\n",
      "Iteration 25, loss = 0.45763129\n",
      "Iteration 26, loss = 0.45703559\n",
      "Iteration 27, loss = 0.45689956\n",
      "Iteration 28, loss = 0.45692479\n",
      "Iteration 29, loss = 0.45692184\n",
      "Iteration 30, loss = 0.45589383\n",
      "Iteration 31, loss = 0.45539847\n",
      "Iteration 32, loss = 0.45606160\n",
      "Iteration 33, loss = 0.45625981\n",
      "Iteration 34, loss = 0.45567701\n",
      "Iteration 35, loss = 0.45610240\n",
      "Iteration 36, loss = 0.45503085\n",
      "Iteration 37, loss = 0.45494840\n",
      "Iteration 38, loss = 0.45483266\n",
      "Iteration 39, loss = 0.45623946\n",
      "Iteration 40, loss = 0.45671578\n",
      "Iteration 41, loss = 0.45443893\n",
      "Iteration 42, loss = 0.45505173\n",
      "Iteration 43, loss = 0.45504371\n",
      "Iteration 44, loss = 0.45512165\n",
      "Iteration 45, loss = 0.45424471\n",
      "Iteration 46, loss = 0.45551139\n",
      "Iteration 47, loss = 0.45482777\n",
      "Iteration 48, loss = 0.45363029\n",
      "Iteration 49, loss = 0.45394033\n",
      "Iteration 50, loss = 0.45338892\n",
      "Iteration 51, loss = 0.45398564\n",
      "Iteration 52, loss = 0.45313844\n",
      "Iteration 53, loss = 0.45299631\n",
      "Iteration 54, loss = 0.45312856\n",
      "Iteration 55, loss = 0.45291295\n",
      "Iteration 56, loss = 0.45283575\n",
      "Iteration 57, loss = 0.45237945\n",
      "Iteration 58, loss = 0.45225968\n",
      "Iteration 59, loss = 0.45293737\n",
      "Iteration 60, loss = 0.45314105\n",
      "Iteration 61, loss = 0.45256696\n",
      "Iteration 62, loss = 0.45215888\n",
      "Iteration 63, loss = 0.45340102\n",
      "Iteration 64, loss = 0.45305509\n",
      "Iteration 65, loss = 0.45273057\n",
      "Iteration 66, loss = 0.45247742\n",
      "Iteration 67, loss = 0.45323974\n",
      "Iteration 68, loss = 0.45289259\n",
      "Iteration 69, loss = 0.45222150\n",
      "Iteration 70, loss = 0.45328578\n",
      "Iteration 71, loss = 0.45151549\n",
      "Iteration 72, loss = 0.45140996\n",
      "Iteration 73, loss = 0.45169475\n",
      "Iteration 74, loss = 0.45171235\n",
      "Iteration 75, loss = 0.45154170\n",
      "Iteration 76, loss = 0.45245779\n",
      "Iteration 77, loss = 0.45189175\n",
      "Iteration 78, loss = 0.45223510\n",
      "Iteration 79, loss = 0.45074438\n",
      "Iteration 80, loss = 0.45107648\n",
      "Iteration 81, loss = 0.45150995\n",
      "Iteration 82, loss = 0.45186879\n",
      "Iteration 83, loss = 0.45121166\n",
      "Iteration 84, loss = 0.45076882\n",
      "Iteration 85, loss = 0.45083455\n",
      "Iteration 86, loss = 0.45081276\n",
      "Iteration 87, loss = 0.45037859\n",
      "Iteration 88, loss = 0.45042357\n",
      "Iteration 89, loss = 0.45067833\n",
      "Iteration 90, loss = 0.45030246\n",
      "Iteration 91, loss = 0.45129701\n",
      "Iteration 92, loss = 0.45116437\n",
      "Iteration 93, loss = 0.45015311\n",
      "Iteration 94, loss = 0.45057510\n",
      "Iteration 95, loss = 0.45177833\n",
      "Iteration 96, loss = 0.45016901\n",
      "Iteration 97, loss = 0.45001055\n",
      "Iteration 98, loss = 0.44990647\n",
      "Iteration 99, loss = 0.45079632\n",
      "Iteration 100, loss = 0.45049402\n",
      "Iteration 101, loss = 0.45034454\n",
      "Iteration 102, loss = 0.45040289\n",
      "Iteration 103, loss = 0.44978282\n",
      "Iteration 104, loss = 0.45057577\n",
      "Iteration 105, loss = 0.45002425\n",
      "Iteration 106, loss = 0.44993349\n",
      "Iteration 107, loss = 0.45025753\n",
      "Iteration 108, loss = 0.44977013\n",
      "Iteration 109, loss = 0.44959255\n",
      "Iteration 110, loss = 0.44960987\n",
      "Iteration 111, loss = 0.45048198\n",
      "Iteration 112, loss = 0.44919806\n",
      "Iteration 113, loss = 0.45020473\n",
      "Iteration 114, loss = 0.44936625\n",
      "Iteration 115, loss = 0.44937977\n",
      "Iteration 116, loss = 0.44925978\n",
      "Iteration 117, loss = 0.44921534\n",
      "Iteration 118, loss = 0.44962297\n",
      "Iteration 119, loss = 0.44967353\n",
      "Iteration 120, loss = 0.44921503\n",
      "Iteration 121, loss = 0.44896553\n",
      "Iteration 122, loss = 0.44963748\n",
      "Iteration 123, loss = 0.45087062\n",
      "Iteration 124, loss = 0.44971716\n",
      "Iteration 125, loss = 0.44928532\n",
      "Iteration 126, loss = 0.44976379\n",
      "Iteration 127, loss = 0.44924405\n",
      "Iteration 128, loss = 0.44893651\n",
      "Iteration 129, loss = 0.44904193\n",
      "Iteration 130, loss = 0.44879194\n",
      "Iteration 131, loss = 0.44941463\n",
      "Iteration 132, loss = 0.44954470\n",
      "Iteration 133, loss = 0.44903413\n",
      "Iteration 134, loss = 0.44827412\n",
      "Iteration 135, loss = 0.44891519\n",
      "Iteration 136, loss = 0.44866709\n",
      "Iteration 137, loss = 0.44891782\n",
      "Iteration 138, loss = 0.44904007\n",
      "Iteration 139, loss = 0.44902627\n",
      "Iteration 140, loss = 0.44841768\n",
      "Iteration 141, loss = 0.44847383\n",
      "Iteration 142, loss = 0.44867086\n",
      "Iteration 143, loss = 0.44850227\n",
      "Iteration 144, loss = 0.44825396\n",
      "Iteration 145, loss = 0.44939169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54124517\n",
      "Iteration 2, loss = 0.49470101\n",
      "Iteration 3, loss = 0.48369320\n",
      "Iteration 4, loss = 0.47497406\n",
      "Iteration 5, loss = 0.46948503\n",
      "Iteration 6, loss = 0.46655577\n",
      "Iteration 7, loss = 0.46311313\n",
      "Iteration 8, loss = 0.46299908\n",
      "Iteration 9, loss = 0.46214772\n",
      "Iteration 10, loss = 0.46207808\n",
      "Iteration 11, loss = 0.46200340\n",
      "Iteration 12, loss = 0.46048562\n",
      "Iteration 13, loss = 0.45977678\n",
      "Iteration 14, loss = 0.46023705\n",
      "Iteration 15, loss = 0.45883877\n",
      "Iteration 16, loss = 0.45880414\n",
      "Iteration 17, loss = 0.45954679\n",
      "Iteration 18, loss = 0.45790159\n",
      "Iteration 19, loss = 0.45828961\n",
      "Iteration 20, loss = 0.45672424\n",
      "Iteration 21, loss = 0.45738807\n",
      "Iteration 22, loss = 0.45771577\n",
      "Iteration 23, loss = 0.45715091\n",
      "Iteration 24, loss = 0.45848288\n",
      "Iteration 25, loss = 0.45822739\n",
      "Iteration 26, loss = 0.45601455\n",
      "Iteration 27, loss = 0.45662444\n",
      "Iteration 28, loss = 0.45721187\n",
      "Iteration 29, loss = 0.45694122\n",
      "Iteration 30, loss = 0.45543560\n",
      "Iteration 31, loss = 0.45613036\n",
      "Iteration 32, loss = 0.45481938\n",
      "Iteration 33, loss = 0.45490551\n",
      "Iteration 34, loss = 0.45516363\n",
      "Iteration 35, loss = 0.45527046\n",
      "Iteration 36, loss = 0.45466050\n",
      "Iteration 37, loss = 0.45456251\n",
      "Iteration 38, loss = 0.45552775\n",
      "Iteration 39, loss = 0.45488018\n",
      "Iteration 40, loss = 0.45491546\n",
      "Iteration 41, loss = 0.45419943\n",
      "Iteration 42, loss = 0.45290714\n",
      "Iteration 43, loss = 0.45450430\n",
      "Iteration 44, loss = 0.45356860\n",
      "Iteration 45, loss = 0.45309953\n",
      "Iteration 46, loss = 0.45534265\n",
      "Iteration 47, loss = 0.45576498\n",
      "Iteration 48, loss = 0.45276277\n",
      "Iteration 49, loss = 0.45253841\n",
      "Iteration 50, loss = 0.45336405\n",
      "Iteration 51, loss = 0.45311007\n",
      "Iteration 52, loss = 0.45448967\n",
      "Iteration 53, loss = 0.45225826\n",
      "Iteration 54, loss = 0.45240544\n",
      "Iteration 55, loss = 0.45255432\n",
      "Iteration 56, loss = 0.45238668\n",
      "Iteration 57, loss = 0.45180981\n",
      "Iteration 58, loss = 0.45175736\n",
      "Iteration 59, loss = 0.45225943\n",
      "Iteration 60, loss = 0.45233606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 61, loss = 0.45174037\n",
      "Iteration 62, loss = 0.45173866\n",
      "Iteration 63, loss = 0.45197271\n",
      "Iteration 64, loss = 0.45181479\n",
      "Iteration 65, loss = 0.45136962\n",
      "Iteration 66, loss = 0.45153974\n",
      "Iteration 67, loss = 0.45269042\n",
      "Iteration 68, loss = 0.45186581\n",
      "Iteration 69, loss = 0.45172149\n",
      "Iteration 70, loss = 0.45149713\n",
      "Iteration 71, loss = 0.45077534\n",
      "Iteration 72, loss = 0.45063145\n",
      "Iteration 73, loss = 0.45064306\n",
      "Iteration 74, loss = 0.45190406\n",
      "Iteration 75, loss = 0.45019031\n",
      "Iteration 76, loss = 0.45082052\n",
      "Iteration 77, loss = 0.44986102\n",
      "Iteration 78, loss = 0.45061794\n",
      "Iteration 79, loss = 0.45044983\n",
      "Iteration 80, loss = 0.45020575\n",
      "Iteration 81, loss = 0.45046110\n",
      "Iteration 82, loss = 0.45021330\n",
      "Iteration 83, loss = 0.45078769\n",
      "Iteration 84, loss = 0.45047132\n",
      "Iteration 85, loss = 0.45098664\n",
      "Iteration 86, loss = 0.44967909\n",
      "Iteration 87, loss = 0.45050349\n",
      "Iteration 88, loss = 0.45101181\n",
      "Iteration 89, loss = 0.45010558\n",
      "Iteration 90, loss = 0.44935212\n",
      "Iteration 91, loss = 0.45002374\n",
      "Iteration 92, loss = 0.45000141\n",
      "Iteration 93, loss = 0.44948179\n",
      "Iteration 94, loss = 0.44984562\n",
      "Iteration 95, loss = 0.44958693\n",
      "Iteration 96, loss = 0.44976098\n",
      "Iteration 97, loss = 0.44964931\n",
      "Iteration 98, loss = 0.45071974\n",
      "Iteration 99, loss = 0.45033988\n",
      "Iteration 100, loss = 0.45022717\n",
      "Iteration 101, loss = 0.44964087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58280214\n",
      "Iteration 2, loss = 0.53650598\n",
      "Iteration 3, loss = 0.53066147\n",
      "Iteration 4, loss = 0.53025466\n",
      "Iteration 5, loss = 0.53018252\n",
      "Iteration 6, loss = 0.53098271\n",
      "Iteration 7, loss = 0.53039480\n",
      "Iteration 8, loss = 0.52987441\n",
      "Iteration 9, loss = 0.52973270\n",
      "Iteration 10, loss = 0.53052063\n",
      "Iteration 11, loss = 0.52945675\n",
      "Iteration 12, loss = 0.52926123\n",
      "Iteration 13, loss = 0.52929297\n",
      "Iteration 14, loss = 0.52940481\n",
      "Iteration 15, loss = 0.52903958\n",
      "Iteration 16, loss = 0.52907063\n",
      "Iteration 17, loss = 0.53014688\n",
      "Iteration 18, loss = 0.52915367\n",
      "Iteration 19, loss = 0.52950652\n",
      "Iteration 20, loss = 0.52940752\n",
      "Iteration 21, loss = 0.52930969\n",
      "Iteration 22, loss = 0.52895379\n",
      "Iteration 23, loss = 0.52976617\n",
      "Iteration 24, loss = 0.52927400\n",
      "Iteration 25, loss = 0.52898393\n",
      "Iteration 26, loss = 0.52911022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57996885\n",
      "Iteration 2, loss = 0.53140842\n",
      "Iteration 3, loss = 0.52548248\n",
      "Iteration 4, loss = 0.52567784\n",
      "Iteration 5, loss = 0.52487706\n",
      "Iteration 6, loss = 0.52533629\n",
      "Iteration 7, loss = 0.52485749\n",
      "Iteration 8, loss = 0.52459244\n",
      "Iteration 9, loss = 0.52551082\n",
      "Iteration 10, loss = 0.52481038\n",
      "Iteration 11, loss = 0.52498827\n",
      "Iteration 12, loss = 0.52416299\n",
      "Iteration 13, loss = 0.52441672\n",
      "Iteration 14, loss = 0.52404936\n",
      "Iteration 15, loss = 0.52360452\n",
      "Iteration 16, loss = 0.52355411\n",
      "Iteration 17, loss = 0.52440083\n",
      "Iteration 18, loss = 0.52409945\n",
      "Iteration 19, loss = 0.52411349\n",
      "Iteration 20, loss = 0.52469228\n",
      "Iteration 21, loss = 0.52413534\n",
      "Iteration 22, loss = 0.52411855\n",
      "Iteration 23, loss = 0.52412852\n",
      "Iteration 24, loss = 0.52383423\n",
      "Iteration 25, loss = 0.52376706\n",
      "Iteration 26, loss = 0.52416433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57850668\n",
      "Iteration 2, loss = 0.53292835\n",
      "Iteration 3, loss = 0.52866024\n",
      "Iteration 4, loss = 0.52796673\n",
      "Iteration 5, loss = 0.52755565\n",
      "Iteration 6, loss = 0.52724464\n",
      "Iteration 7, loss = 0.52629889\n",
      "Iteration 8, loss = 0.52759128\n",
      "Iteration 9, loss = 0.52764039\n",
      "Iteration 10, loss = 0.52641340\n",
      "Iteration 11, loss = 0.52662505\n",
      "Iteration 12, loss = 0.52623321\n",
      "Iteration 13, loss = 0.52646722\n",
      "Iteration 14, loss = 0.52684546\n",
      "Iteration 15, loss = 0.52605434\n",
      "Iteration 16, loss = 0.52684450\n",
      "Iteration 17, loss = 0.52670960\n",
      "Iteration 18, loss = 0.52625822\n",
      "Iteration 19, loss = 0.52654012\n",
      "Iteration 20, loss = 0.52663240\n",
      "Iteration 21, loss = 0.52689023\n",
      "Iteration 22, loss = 0.52646755\n",
      "Iteration 23, loss = 0.52690765\n",
      "Iteration 24, loss = 0.52712224\n",
      "Iteration 25, loss = 0.52647168\n",
      "Iteration 26, loss = 0.52631185\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 2 min. and 7.448751449584961 sec.\n",
      "\n",
      "35 total combinations for subset length 3.\n",
      "Iteration 1, loss = 1.04285571\n",
      "Iteration 2, loss = 0.76427068\n",
      "Iteration 3, loss = 0.69363258\n",
      "Iteration 4, loss = 0.64450620\n",
      "Iteration 5, loss = 0.61113623\n",
      "Iteration 6, loss = 0.58975430\n",
      "Iteration 7, loss = 0.57622631\n",
      "Iteration 8, loss = 0.56800365\n",
      "Iteration 9, loss = 0.56332539\n",
      "Iteration 10, loss = 0.56049877\n",
      "Iteration 11, loss = 0.55896777\n",
      "Iteration 12, loss = 0.55816054\n",
      "Iteration 13, loss = 0.55772910\n",
      "Iteration 14, loss = 0.55750622\n",
      "Iteration 15, loss = 0.55739271\n",
      "Iteration 16, loss = 0.55733598\n",
      "Iteration 17, loss = 0.55730800\n",
      "Iteration 18, loss = 0.55729700\n",
      "Iteration 19, loss = 0.55730284\n",
      "Iteration 20, loss = 0.55730782\n",
      "Iteration 21, loss = 0.55731615\n",
      "Iteration 22, loss = 0.55731003\n",
      "Iteration 23, loss = 0.55731749\n",
      "Iteration 24, loss = 0.55729943\n",
      "Iteration 25, loss = 0.55730919\n",
      "Iteration 26, loss = 0.55733733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04028513\n",
      "Iteration 2, loss = 0.76407409\n",
      "Iteration 3, loss = 0.69282603\n",
      "Iteration 4, loss = 0.62435316\n",
      "Iteration 5, loss = 0.52679914\n",
      "Iteration 6, loss = 0.51315535\n",
      "Iteration 7, loss = 0.50672113\n",
      "Iteration 8, loss = 0.50053465\n",
      "Iteration 9, loss = 0.49571982\n",
      "Iteration 10, loss = 0.49283613\n",
      "Iteration 11, loss = 0.48921712\n",
      "Iteration 12, loss = 0.48789990\n",
      "Iteration 13, loss = 0.48746854\n",
      "Iteration 14, loss = 0.48754629\n",
      "Iteration 15, loss = 0.48682329\n",
      "Iteration 16, loss = 0.48642799\n",
      "Iteration 17, loss = 0.48586169\n",
      "Iteration 18, loss = 0.48542862\n",
      "Iteration 19, loss = 0.48625730\n",
      "Iteration 20, loss = 0.48562182\n",
      "Iteration 21, loss = 0.48466199\n",
      "Iteration 22, loss = 0.48400080\n",
      "Iteration 23, loss = 0.48490870\n",
      "Iteration 24, loss = 0.48560569\n",
      "Iteration 25, loss = 0.48468484\n",
      "Iteration 26, loss = 0.48410300\n",
      "Iteration 27, loss = 0.48518023\n",
      "Iteration 28, loss = 0.48357274\n",
      "Iteration 29, loss = 0.48247758\n",
      "Iteration 30, loss = 0.48057889\n",
      "Iteration 31, loss = 0.48028884\n",
      "Iteration 32, loss = 0.47937137\n",
      "Iteration 33, loss = 0.47919455\n",
      "Iteration 34, loss = 0.47912260\n",
      "Iteration 35, loss = 0.47924088\n",
      "Iteration 36, loss = 0.47839593\n",
      "Iteration 37, loss = 0.47825853\n",
      "Iteration 38, loss = 0.47897082\n",
      "Iteration 39, loss = 0.47813857\n",
      "Iteration 40, loss = 0.47891162\n",
      "Iteration 41, loss = 0.47738001\n",
      "Iteration 42, loss = 0.47699397\n",
      "Iteration 43, loss = 0.47766956\n",
      "Iteration 44, loss = 0.47730276\n",
      "Iteration 45, loss = 0.47910532\n",
      "Iteration 46, loss = 0.47780439\n",
      "Iteration 47, loss = 0.47613417\n",
      "Iteration 48, loss = 0.47637862\n",
      "Iteration 49, loss = 0.47687038\n",
      "Iteration 50, loss = 0.47741060\n",
      "Iteration 51, loss = 0.47553288\n",
      "Iteration 52, loss = 0.47650972\n",
      "Iteration 53, loss = 0.47724515\n",
      "Iteration 54, loss = 0.47701293\n",
      "Iteration 55, loss = 0.47671492\n",
      "Iteration 56, loss = 0.47734730\n",
      "Iteration 57, loss = 0.47646070\n",
      "Iteration 58, loss = 0.47552982\n",
      "Iteration 59, loss = 0.47570854\n",
      "Iteration 60, loss = 0.47653182\n",
      "Iteration 61, loss = 0.47566702\n",
      "Iteration 62, loss = 0.47543432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03736232\n",
      "Iteration 2, loss = 0.76311274\n",
      "Iteration 3, loss = 0.69236867\n",
      "Iteration 4, loss = 0.64312635\n",
      "Iteration 5, loss = 0.57060713\n",
      "Iteration 6, loss = 0.51938642\n",
      "Iteration 7, loss = 0.50950902\n",
      "Iteration 8, loss = 0.49834931\n",
      "Iteration 9, loss = 0.49065672\n",
      "Iteration 10, loss = 0.48883348\n",
      "Iteration 11, loss = 0.48653325\n",
      "Iteration 12, loss = 0.48454441\n",
      "Iteration 13, loss = 0.48552706\n",
      "Iteration 14, loss = 0.48391434\n",
      "Iteration 15, loss = 0.48380052\n",
      "Iteration 16, loss = 0.48410661\n",
      "Iteration 17, loss = 0.48212277\n",
      "Iteration 18, loss = 0.48186627\n",
      "Iteration 19, loss = 0.47986356\n",
      "Iteration 20, loss = 0.47973646\n",
      "Iteration 21, loss = 0.47972257\n",
      "Iteration 22, loss = 0.48010628\n",
      "Iteration 23, loss = 0.47931217\n",
      "Iteration 24, loss = 0.47896379\n",
      "Iteration 25, loss = 0.47848939\n",
      "Iteration 26, loss = 0.47881845\n",
      "Iteration 27, loss = 0.47739544\n",
      "Iteration 28, loss = 0.47857791\n",
      "Iteration 29, loss = 0.47671810\n",
      "Iteration 30, loss = 0.47531542\n",
      "Iteration 31, loss = 0.47589083\n",
      "Iteration 32, loss = 0.47544682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.47465453\n",
      "Iteration 34, loss = 0.47784766\n",
      "Iteration 35, loss = 0.47374803\n",
      "Iteration 36, loss = 0.47339654\n",
      "Iteration 37, loss = 0.47360831\n",
      "Iteration 38, loss = 0.47383843\n",
      "Iteration 39, loss = 0.47183600\n",
      "Iteration 40, loss = 0.47275182\n",
      "Iteration 41, loss = 0.47081693\n",
      "Iteration 42, loss = 0.47111771\n",
      "Iteration 43, loss = 0.47193829\n",
      "Iteration 44, loss = 0.47287834\n",
      "Iteration 45, loss = 0.47258302\n",
      "Iteration 46, loss = 0.47162152\n",
      "Iteration 47, loss = 0.47085364\n",
      "Iteration 48, loss = 0.47150371\n",
      "Iteration 49, loss = 0.46999677\n",
      "Iteration 50, loss = 0.47022175\n",
      "Iteration 51, loss = 0.46992267\n",
      "Iteration 52, loss = 0.47223762\n",
      "Iteration 53, loss = 0.47020752\n",
      "Iteration 54, loss = 0.46872572\n",
      "Iteration 55, loss = 0.46961234\n",
      "Iteration 56, loss = 0.46921486\n",
      "Iteration 57, loss = 0.46585788\n",
      "Iteration 58, loss = 0.46358196\n",
      "Iteration 59, loss = 0.46200033\n",
      "Iteration 60, loss = 0.46026477\n",
      "Iteration 61, loss = 0.45802674\n",
      "Iteration 62, loss = 0.45903816\n",
      "Iteration 63, loss = 0.46096885\n",
      "Iteration 64, loss = 0.45689820\n",
      "Iteration 65, loss = 0.45735169\n",
      "Iteration 66, loss = 0.45636235\n",
      "Iteration 67, loss = 0.45686703\n",
      "Iteration 68, loss = 0.45844243\n",
      "Iteration 69, loss = 0.45688812\n",
      "Iteration 70, loss = 0.45458254\n",
      "Iteration 71, loss = 0.45479840\n",
      "Iteration 72, loss = 0.45591256\n",
      "Iteration 73, loss = 0.45471959\n",
      "Iteration 74, loss = 0.45378610\n",
      "Iteration 75, loss = 0.45411520\n",
      "Iteration 76, loss = 0.45292072\n",
      "Iteration 77, loss = 0.45279749\n",
      "Iteration 78, loss = 0.45263630\n",
      "Iteration 79, loss = 0.45220596\n",
      "Iteration 80, loss = 0.45254047\n",
      "Iteration 81, loss = 0.45049528\n",
      "Iteration 82, loss = 0.45262536\n",
      "Iteration 83, loss = 0.45152488\n",
      "Iteration 84, loss = 0.45111722\n",
      "Iteration 85, loss = 0.45027466\n",
      "Iteration 86, loss = 0.45152084\n",
      "Iteration 87, loss = 0.45191955\n",
      "Iteration 88, loss = 0.45211639\n",
      "Iteration 89, loss = 0.45093244\n",
      "Iteration 90, loss = 0.45044688\n",
      "Iteration 91, loss = 0.45004523\n",
      "Iteration 92, loss = 0.44827622\n",
      "Iteration 93, loss = 0.44972580\n",
      "Iteration 94, loss = 0.44840124\n",
      "Iteration 95, loss = 0.44967622\n",
      "Iteration 96, loss = 0.44919039\n",
      "Iteration 97, loss = 0.44827333\n",
      "Iteration 98, loss = 0.44718560\n",
      "Iteration 99, loss = 0.44803919\n",
      "Iteration 100, loss = 0.44777193\n",
      "Iteration 101, loss = 0.44886060\n",
      "Iteration 102, loss = 0.44669663\n",
      "Iteration 103, loss = 0.44820565\n",
      "Iteration 104, loss = 0.44684091\n",
      "Iteration 105, loss = 0.44663055\n",
      "Iteration 106, loss = 0.44613348\n",
      "Iteration 107, loss = 0.44526624\n",
      "Iteration 108, loss = 0.44505499\n",
      "Iteration 109, loss = 0.44607655\n",
      "Iteration 110, loss = 0.44605510\n",
      "Iteration 111, loss = 0.44652560\n",
      "Iteration 112, loss = 0.44521819\n",
      "Iteration 113, loss = 0.44625678\n",
      "Iteration 114, loss = 0.44611918\n",
      "Iteration 115, loss = 0.44618258\n",
      "Iteration 116, loss = 0.44829105\n",
      "Iteration 117, loss = 0.44481271\n",
      "Iteration 118, loss = 0.44467237\n",
      "Iteration 119, loss = 0.44541715\n",
      "Iteration 120, loss = 0.44551839\n",
      "Iteration 121, loss = 0.44559114\n",
      "Iteration 122, loss = 0.44530360\n",
      "Iteration 123, loss = 0.44556298\n",
      "Iteration 124, loss = 0.44556833\n",
      "Iteration 125, loss = 0.44381326\n",
      "Iteration 126, loss = 0.44500378\n",
      "Iteration 127, loss = 0.44420576\n",
      "Iteration 128, loss = 0.44532231\n",
      "Iteration 129, loss = 0.44505324\n",
      "Iteration 130, loss = 0.44269955\n",
      "Iteration 131, loss = 0.44259430\n",
      "Iteration 132, loss = 0.44359600\n",
      "Iteration 133, loss = 0.44686524\n",
      "Iteration 134, loss = 0.44512135\n",
      "Iteration 135, loss = 0.44346912\n",
      "Iteration 136, loss = 0.44334995\n",
      "Iteration 137, loss = 0.44317038\n",
      "Iteration 138, loss = 0.44328991\n",
      "Iteration 139, loss = 0.44205878\n",
      "Iteration 140, loss = 0.44217790\n",
      "Iteration 141, loss = 0.44318974\n",
      "Iteration 142, loss = 0.44328356\n",
      "Iteration 143, loss = 0.44407754\n",
      "Iteration 144, loss = 0.44330158\n",
      "Iteration 145, loss = 0.44305205\n",
      "Iteration 146, loss = 0.44207032\n",
      "Iteration 147, loss = 0.44145131\n",
      "Iteration 148, loss = 0.44236275\n",
      "Iteration 149, loss = 0.44240662\n",
      "Iteration 150, loss = 0.44270185\n",
      "Iteration 151, loss = 0.44365159\n",
      "Iteration 152, loss = 0.44519762\n",
      "Iteration 153, loss = 0.44417466\n",
      "Iteration 154, loss = 0.44326871\n",
      "Iteration 155, loss = 0.44313910\n",
      "Iteration 156, loss = 0.44166744\n",
      "Iteration 157, loss = 0.44332213\n",
      "Iteration 158, loss = 0.44208404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08738555\n",
      "Iteration 2, loss = 0.76672667\n",
      "Iteration 3, loss = 0.69523430\n",
      "Iteration 4, loss = 0.64605724\n",
      "Iteration 5, loss = 0.61243391\n",
      "Iteration 6, loss = 0.59079470\n",
      "Iteration 7, loss = 0.57699627\n",
      "Iteration 8, loss = 0.56853646\n",
      "Iteration 9, loss = 0.56367813\n",
      "Iteration 10, loss = 0.56072213\n",
      "Iteration 11, loss = 0.55910443\n",
      "Iteration 12, loss = 0.55824206\n",
      "Iteration 13, loss = 0.55777623\n",
      "Iteration 14, loss = 0.55753306\n",
      "Iteration 15, loss = 0.55740773\n",
      "Iteration 16, loss = 0.55734536\n",
      "Iteration 17, loss = 0.55731406\n",
      "Iteration 18, loss = 0.55730154\n",
      "Iteration 19, loss = 0.55730669\n",
      "Iteration 20, loss = 0.55731105\n",
      "Iteration 21, loss = 0.55731944\n",
      "Iteration 22, loss = 0.55731325\n",
      "Iteration 23, loss = 0.55732076\n",
      "Iteration 24, loss = 0.55730297\n",
      "Iteration 25, loss = 0.55731264\n",
      "Iteration 26, loss = 0.55734060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07801433\n",
      "Iteration 2, loss = 0.76644915\n",
      "Iteration 3, loss = 0.69457530\n",
      "Iteration 4, loss = 0.64568569\n",
      "Iteration 5, loss = 0.61225607\n",
      "Iteration 6, loss = 0.59066160\n",
      "Iteration 7, loss = 0.57675234\n",
      "Iteration 8, loss = 0.56851488\n",
      "Iteration 9, loss = 0.56355788\n",
      "Iteration 10, loss = 0.56069498\n",
      "Iteration 11, loss = 0.55906995\n",
      "Iteration 12, loss = 0.55821110\n",
      "Iteration 13, loss = 0.55777982\n",
      "Iteration 14, loss = 0.55752265\n",
      "Iteration 15, loss = 0.55743816\n",
      "Iteration 16, loss = 0.55735049\n",
      "Iteration 17, loss = 0.55733022\n",
      "Iteration 18, loss = 0.55731365\n",
      "Iteration 19, loss = 0.55731941\n",
      "Iteration 20, loss = 0.55731687\n",
      "Iteration 21, loss = 0.55732385\n",
      "Iteration 22, loss = 0.55733590\n",
      "Iteration 23, loss = 0.55732821\n",
      "Iteration 24, loss = 0.55731353\n",
      "Iteration 25, loss = 0.55731066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07255649\n",
      "Iteration 2, loss = 0.76525571\n",
      "Iteration 3, loss = 0.69387048\n",
      "Iteration 4, loss = 0.64499107\n",
      "Iteration 5, loss = 0.61164448\n",
      "Iteration 6, loss = 0.59000200\n",
      "Iteration 7, loss = 0.57638299\n",
      "Iteration 8, loss = 0.56820901\n",
      "Iteration 9, loss = 0.56322581\n",
      "Iteration 10, loss = 0.56043520\n",
      "Iteration 11, loss = 0.55888953\n",
      "Iteration 12, loss = 0.55800524\n",
      "Iteration 13, loss = 0.55758782\n",
      "Iteration 14, loss = 0.55733743\n",
      "Iteration 15, loss = 0.55725575\n",
      "Iteration 16, loss = 0.55717042\n",
      "Iteration 17, loss = 0.55718007\n",
      "Iteration 18, loss = 0.55713399\n",
      "Iteration 19, loss = 0.55712034\n",
      "Iteration 20, loss = 0.55713143\n",
      "Iteration 21, loss = 0.55711877\n",
      "Iteration 22, loss = 0.55714518\n",
      "Iteration 23, loss = 0.55711552\n",
      "Iteration 24, loss = 0.55712974\n",
      "Iteration 25, loss = 0.55713920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05046610\n",
      "Iteration 2, loss = 0.76468995\n",
      "Iteration 3, loss = 0.69373570\n",
      "Iteration 4, loss = 0.64440214\n",
      "Iteration 5, loss = 0.59107935\n",
      "Iteration 6, loss = 0.54162873\n",
      "Iteration 7, loss = 0.53582613\n",
      "Iteration 8, loss = 0.53218588\n",
      "Iteration 9, loss = 0.52978738\n",
      "Iteration 10, loss = 0.52861377\n",
      "Iteration 11, loss = 0.52540936\n",
      "Iteration 12, loss = 0.52325816\n",
      "Iteration 13, loss = 0.52102466\n",
      "Iteration 14, loss = 0.51949522\n",
      "Iteration 15, loss = 0.51796876\n",
      "Iteration 16, loss = 0.51759966\n",
      "Iteration 17, loss = 0.51563312\n",
      "Iteration 18, loss = 0.51568832\n",
      "Iteration 19, loss = 0.51499023\n",
      "Iteration 20, loss = 0.51489813\n",
      "Iteration 21, loss = 0.51493159\n",
      "Iteration 22, loss = 0.51518731\n",
      "Iteration 23, loss = 0.51518180\n",
      "Iteration 24, loss = 0.51389343\n",
      "Iteration 25, loss = 0.51352150\n",
      "Iteration 26, loss = 0.51362096\n",
      "Iteration 27, loss = 0.50872868\n",
      "Iteration 28, loss = 0.50664047\n",
      "Iteration 29, loss = 0.50472066\n",
      "Iteration 30, loss = 0.50300666\n",
      "Iteration 31, loss = 0.50140183\n",
      "Iteration 32, loss = 0.49847093\n",
      "Iteration 33, loss = 0.49772242\n",
      "Iteration 34, loss = 0.49489631\n",
      "Iteration 35, loss = 0.49311731\n",
      "Iteration 36, loss = 0.49324016\n",
      "Iteration 37, loss = 0.49178765\n",
      "Iteration 38, loss = 0.49091160\n",
      "Iteration 39, loss = 0.48999117\n",
      "Iteration 40, loss = 0.49026377\n",
      "Iteration 41, loss = 0.48963389\n",
      "Iteration 42, loss = 0.48908587\n",
      "Iteration 43, loss = 0.48884225\n",
      "Iteration 44, loss = 0.48829152\n",
      "Iteration 45, loss = 0.48828438\n",
      "Iteration 46, loss = 0.48840819\n",
      "Iteration 47, loss = 0.48952387\n",
      "Iteration 48, loss = 0.48967977\n",
      "Iteration 49, loss = 0.48785864\n",
      "Iteration 50, loss = 0.48705986\n",
      "Iteration 51, loss = 0.48853092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.48773817\n",
      "Iteration 53, loss = 0.48706888\n",
      "Iteration 54, loss = 0.48587210\n",
      "Iteration 55, loss = 0.48781954\n",
      "Iteration 56, loss = 0.48714462\n",
      "Iteration 57, loss = 0.48588200\n",
      "Iteration 58, loss = 0.48573963\n",
      "Iteration 59, loss = 0.48753544\n",
      "Iteration 60, loss = 0.48525417\n",
      "Iteration 61, loss = 0.48625218\n",
      "Iteration 62, loss = 0.48774270\n",
      "Iteration 63, loss = 0.48495922\n",
      "Iteration 64, loss = 0.48679086\n",
      "Iteration 65, loss = 0.48660527\n",
      "Iteration 66, loss = 0.48464177\n",
      "Iteration 67, loss = 0.48581756\n",
      "Iteration 68, loss = 0.48735004\n",
      "Iteration 69, loss = 0.48469617\n",
      "Iteration 70, loss = 0.48562634\n",
      "Iteration 71, loss = 0.48575862\n",
      "Iteration 72, loss = 0.48515788\n",
      "Iteration 73, loss = 0.48388379\n",
      "Iteration 74, loss = 0.48409150\n",
      "Iteration 75, loss = 0.48386652\n",
      "Iteration 76, loss = 0.48386990\n",
      "Iteration 77, loss = 0.48310922\n",
      "Iteration 78, loss = 0.48431292\n",
      "Iteration 79, loss = 0.48431703\n",
      "Iteration 80, loss = 0.48297574\n",
      "Iteration 81, loss = 0.48258206\n",
      "Iteration 82, loss = 0.48310750\n",
      "Iteration 83, loss = 0.48306265\n",
      "Iteration 84, loss = 0.48382086\n",
      "Iteration 85, loss = 0.48341182\n",
      "Iteration 86, loss = 0.48377067\n",
      "Iteration 87, loss = 0.48364646\n",
      "Iteration 88, loss = 0.48267344\n",
      "Iteration 89, loss = 0.48506951\n",
      "Iteration 90, loss = 0.48279248\n",
      "Iteration 91, loss = 0.48186591\n",
      "Iteration 92, loss = 0.48372303\n",
      "Iteration 93, loss = 0.48426688\n",
      "Iteration 94, loss = 0.48188096\n",
      "Iteration 95, loss = 0.48277708\n",
      "Iteration 96, loss = 0.48212585\n",
      "Iteration 97, loss = 0.48294173\n",
      "Iteration 98, loss = 0.48120219\n",
      "Iteration 99, loss = 0.48162877\n",
      "Iteration 100, loss = 0.48151556\n",
      "Iteration 101, loss = 0.48066792\n",
      "Iteration 102, loss = 0.48255946\n",
      "Iteration 103, loss = 0.48465319\n",
      "Iteration 104, loss = 0.48136472\n",
      "Iteration 105, loss = 0.48027589\n",
      "Iteration 106, loss = 0.48190971\n",
      "Iteration 107, loss = 0.48196651\n",
      "Iteration 108, loss = 0.48071981\n",
      "Iteration 109, loss = 0.48273613\n",
      "Iteration 110, loss = 0.48143969\n",
      "Iteration 111, loss = 0.48235786\n",
      "Iteration 112, loss = 0.48241530\n",
      "Iteration 113, loss = 0.48291884\n",
      "Iteration 114, loss = 0.48247086\n",
      "Iteration 115, loss = 0.48067498\n",
      "Iteration 116, loss = 0.48070652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04664253\n",
      "Iteration 2, loss = 0.76455220\n",
      "Iteration 3, loss = 0.69288976\n",
      "Iteration 4, loss = 0.61972842\n",
      "Iteration 5, loss = 0.54738740\n",
      "Iteration 6, loss = 0.54107188\n",
      "Iteration 7, loss = 0.54012622\n",
      "Iteration 8, loss = 0.53834989\n",
      "Iteration 9, loss = 0.53701156\n",
      "Iteration 10, loss = 0.53677817\n",
      "Iteration 11, loss = 0.53633408\n",
      "Iteration 12, loss = 0.53565162\n",
      "Iteration 13, loss = 0.53566887\n",
      "Iteration 14, loss = 0.53568370\n",
      "Iteration 15, loss = 0.53547196\n",
      "Iteration 16, loss = 0.53452500\n",
      "Iteration 17, loss = 0.53432631\n",
      "Iteration 18, loss = 0.53413097\n",
      "Iteration 19, loss = 0.53444798\n",
      "Iteration 20, loss = 0.53386416\n",
      "Iteration 21, loss = 0.53367957\n",
      "Iteration 22, loss = 0.53325623\n",
      "Iteration 23, loss = 0.53404588\n",
      "Iteration 24, loss = 0.53370715\n",
      "Iteration 25, loss = 0.53254312\n",
      "Iteration 26, loss = 0.53237751\n",
      "Iteration 27, loss = 0.53188974\n",
      "Iteration 28, loss = 0.53194282\n",
      "Iteration 29, loss = 0.53171770\n",
      "Iteration 30, loss = 0.53126658\n",
      "Iteration 31, loss = 0.52994622\n",
      "Iteration 32, loss = 0.52982093\n",
      "Iteration 33, loss = 0.52903762\n",
      "Iteration 34, loss = 0.52858156\n",
      "Iteration 35, loss = 0.52736281\n",
      "Iteration 36, loss = 0.52745200\n",
      "Iteration 37, loss = 0.52628306\n",
      "Iteration 38, loss = 0.52470065\n",
      "Iteration 39, loss = 0.52231210\n",
      "Iteration 40, loss = 0.51774884\n",
      "Iteration 41, loss = 0.51247113\n",
      "Iteration 42, loss = 0.50804377\n",
      "Iteration 43, loss = 0.50407858\n",
      "Iteration 44, loss = 0.50250872\n",
      "Iteration 45, loss = 0.50103483\n",
      "Iteration 46, loss = 0.50055741\n",
      "Iteration 47, loss = 0.49986325\n",
      "Iteration 48, loss = 0.50023642\n",
      "Iteration 49, loss = 0.49956557\n",
      "Iteration 50, loss = 0.50068982\n",
      "Iteration 51, loss = 0.49913709\n",
      "Iteration 52, loss = 0.49944201\n",
      "Iteration 53, loss = 0.49834017\n",
      "Iteration 54, loss = 0.49841610\n",
      "Iteration 55, loss = 0.49887166\n",
      "Iteration 56, loss = 0.50053296\n",
      "Iteration 57, loss = 0.49883040\n",
      "Iteration 58, loss = 0.49742718\n",
      "Iteration 59, loss = 0.49830880\n",
      "Iteration 60, loss = 0.49808196\n",
      "Iteration 61, loss = 0.49839520\n",
      "Iteration 62, loss = 0.49770197\n",
      "Iteration 63, loss = 0.49800044\n",
      "Iteration 64, loss = 0.50051181\n",
      "Iteration 65, loss = 0.49854332\n",
      "Iteration 66, loss = 0.49768806\n",
      "Iteration 67, loss = 0.49911040\n",
      "Iteration 68, loss = 0.49795323\n",
      "Iteration 69, loss = 0.49751733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04463023\n",
      "Iteration 2, loss = 0.76364526\n",
      "Iteration 3, loss = 0.69251457\n",
      "Iteration 4, loss = 0.64358615\n",
      "Iteration 5, loss = 0.61002865\n",
      "Iteration 6, loss = 0.56283743\n",
      "Iteration 7, loss = 0.54076067\n",
      "Iteration 8, loss = 0.53627883\n",
      "Iteration 9, loss = 0.53213743\n",
      "Iteration 10, loss = 0.52816715\n",
      "Iteration 11, loss = 0.52537916\n",
      "Iteration 12, loss = 0.52428862\n",
      "Iteration 13, loss = 0.52083056\n",
      "Iteration 14, loss = 0.51856242\n",
      "Iteration 15, loss = 0.51898378\n",
      "Iteration 16, loss = 0.51662088\n",
      "Iteration 17, loss = 0.51674150\n",
      "Iteration 18, loss = 0.51816514\n",
      "Iteration 19, loss = 0.51587318\n",
      "Iteration 20, loss = 0.51548024\n",
      "Iteration 21, loss = 0.51493446\n",
      "Iteration 22, loss = 0.51444879\n",
      "Iteration 23, loss = 0.51645490\n",
      "Iteration 24, loss = 0.51723084\n",
      "Iteration 25, loss = 0.51528703\n",
      "Iteration 26, loss = 0.51895515\n",
      "Iteration 27, loss = 0.51526013\n",
      "Iteration 28, loss = 0.51442363\n",
      "Iteration 29, loss = 0.51490152\n",
      "Iteration 30, loss = 0.51250919\n",
      "Iteration 31, loss = 0.51278174\n",
      "Iteration 32, loss = 0.51317828\n",
      "Iteration 33, loss = 0.51229693\n",
      "Iteration 34, loss = 0.51280555\n",
      "Iteration 35, loss = 0.51276960\n",
      "Iteration 36, loss = 0.51131863\n",
      "Iteration 37, loss = 0.51260632\n",
      "Iteration 38, loss = 0.51203349\n",
      "Iteration 39, loss = 0.51159955\n",
      "Iteration 40, loss = 0.51198462\n",
      "Iteration 41, loss = 0.51188334\n",
      "Iteration 42, loss = 0.51204976\n",
      "Iteration 43, loss = 0.51109560\n",
      "Iteration 44, loss = 0.50974453\n",
      "Iteration 45, loss = 0.51112344\n",
      "Iteration 46, loss = 0.51060157\n",
      "Iteration 47, loss = 0.51078880\n",
      "Iteration 48, loss = 0.51113916\n",
      "Iteration 49, loss = 0.51075292\n",
      "Iteration 50, loss = 0.51056229\n",
      "Iteration 51, loss = 0.50881350\n",
      "Iteration 52, loss = 0.50922259\n",
      "Iteration 53, loss = 0.51011012\n",
      "Iteration 54, loss = 0.50962024\n",
      "Iteration 55, loss = 0.51003313\n",
      "Iteration 56, loss = 0.51085666\n",
      "Iteration 57, loss = 0.50903533\n",
      "Iteration 58, loss = 0.50999695\n",
      "Iteration 59, loss = 0.50930633\n",
      "Iteration 60, loss = 0.50983242\n",
      "Iteration 61, loss = 0.50850672\n",
      "Iteration 62, loss = 0.50967322\n",
      "Iteration 63, loss = 0.50926939\n",
      "Iteration 64, loss = 0.50923073\n",
      "Iteration 65, loss = 0.50829017\n",
      "Iteration 66, loss = 0.50788979\n",
      "Iteration 67, loss = 0.50791621\n",
      "Iteration 68, loss = 0.50863349\n",
      "Iteration 69, loss = 0.50938686\n",
      "Iteration 70, loss = 0.50876441\n",
      "Iteration 71, loss = 0.50744224\n",
      "Iteration 72, loss = 0.50891413\n",
      "Iteration 73, loss = 0.50701565\n",
      "Iteration 74, loss = 0.50783890\n",
      "Iteration 75, loss = 0.50887897\n",
      "Iteration 76, loss = 0.50835648\n",
      "Iteration 77, loss = 0.50893341\n",
      "Iteration 78, loss = 0.50841779\n",
      "Iteration 79, loss = 0.50756731\n",
      "Iteration 80, loss = 0.50753790\n",
      "Iteration 81, loss = 0.50742334\n",
      "Iteration 82, loss = 0.50801614\n",
      "Iteration 83, loss = 0.50778875\n",
      "Iteration 84, loss = 0.50894082\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04463877\n",
      "Iteration 2, loss = 0.76522110\n",
      "Iteration 3, loss = 0.69355526\n",
      "Iteration 4, loss = 0.64427651\n",
      "Iteration 5, loss = 0.61076245\n",
      "Iteration 6, loss = 0.57725282\n",
      "Iteration 7, loss = 0.56092872\n",
      "Iteration 8, loss = 0.55720311\n",
      "Iteration 9, loss = 0.55539592\n",
      "Iteration 10, loss = 0.55366779\n",
      "Iteration 11, loss = 0.55232595\n",
      "Iteration 12, loss = 0.55051539\n",
      "Iteration 13, loss = 0.54904519\n",
      "Iteration 14, loss = 0.54783786\n",
      "Iteration 15, loss = 0.54578056\n",
      "Iteration 16, loss = 0.54507108\n",
      "Iteration 17, loss = 0.54387118\n",
      "Iteration 18, loss = 0.54318365\n",
      "Iteration 19, loss = 0.54245020\n",
      "Iteration 20, loss = 0.54148488\n",
      "Iteration 21, loss = 0.54083774\n",
      "Iteration 22, loss = 0.54205102\n",
      "Iteration 23, loss = 0.54111293\n",
      "Iteration 24, loss = 0.54038907\n",
      "Iteration 25, loss = 0.54070376\n",
      "Iteration 26, loss = 0.53935885\n",
      "Iteration 27, loss = 0.53912017\n",
      "Iteration 28, loss = 0.53881534\n",
      "Iteration 29, loss = 0.53865717\n",
      "Iteration 30, loss = 0.53854854\n",
      "Iteration 31, loss = 0.53856818\n",
      "Iteration 32, loss = 0.53781537\n",
      "Iteration 33, loss = 0.53910735\n",
      "Iteration 34, loss = 0.53856802\n",
      "Iteration 35, loss = 0.53716667\n",
      "Iteration 36, loss = 0.53823247\n",
      "Iteration 37, loss = 0.53752796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.53794396\n",
      "Iteration 39, loss = 0.53755510\n",
      "Iteration 40, loss = 0.53738882\n",
      "Iteration 41, loss = 0.53784159\n",
      "Iteration 42, loss = 0.53681460\n",
      "Iteration 43, loss = 0.53741347\n",
      "Iteration 44, loss = 0.53601940\n",
      "Iteration 45, loss = 0.53593720\n",
      "Iteration 46, loss = 0.53716161\n",
      "Iteration 47, loss = 0.53933122\n",
      "Iteration 48, loss = 0.53678584\n",
      "Iteration 49, loss = 0.53383989\n",
      "Iteration 50, loss = 0.53287558\n",
      "Iteration 51, loss = 0.53184954\n",
      "Iteration 52, loss = 0.53305780\n",
      "Iteration 53, loss = 0.53190639\n",
      "Iteration 54, loss = 0.53363329\n",
      "Iteration 55, loss = 0.53230586\n",
      "Iteration 56, loss = 0.53120806\n",
      "Iteration 57, loss = 0.53139376\n",
      "Iteration 58, loss = 0.53059188\n",
      "Iteration 59, loss = 0.53047728\n",
      "Iteration 60, loss = 0.53033934\n",
      "Iteration 61, loss = 0.52965523\n",
      "Iteration 62, loss = 0.53204734\n",
      "Iteration 63, loss = 0.53088726\n",
      "Iteration 64, loss = 0.52999271\n",
      "Iteration 65, loss = 0.52948814\n",
      "Iteration 66, loss = 0.52997659\n",
      "Iteration 67, loss = 0.52884628\n",
      "Iteration 68, loss = 0.52842383\n",
      "Iteration 69, loss = 0.52821778\n",
      "Iteration 70, loss = 0.52823304\n",
      "Iteration 71, loss = 0.52971294\n",
      "Iteration 72, loss = 0.52826110\n",
      "Iteration 73, loss = 0.52752741\n",
      "Iteration 74, loss = 0.52699817\n",
      "Iteration 75, loss = 0.52897532\n",
      "Iteration 76, loss = 0.52753545\n",
      "Iteration 77, loss = 0.52701677\n",
      "Iteration 78, loss = 0.53119554\n",
      "Iteration 79, loss = 0.52880036\n",
      "Iteration 80, loss = 0.52896023\n",
      "Iteration 81, loss = 0.52699561\n",
      "Iteration 82, loss = 0.52632075\n",
      "Iteration 83, loss = 0.52773388\n",
      "Iteration 84, loss = 0.52704135\n",
      "Iteration 85, loss = 0.52649879\n",
      "Iteration 86, loss = 0.52705767\n",
      "Iteration 87, loss = 0.52767429\n",
      "Iteration 88, loss = 0.52595782\n",
      "Iteration 89, loss = 0.52712311\n",
      "Iteration 90, loss = 0.52731196\n",
      "Iteration 91, loss = 0.52636356\n",
      "Iteration 92, loss = 0.52637151\n",
      "Iteration 93, loss = 0.52704151\n",
      "Iteration 94, loss = 0.52594181\n",
      "Iteration 95, loss = 0.52831518\n",
      "Iteration 96, loss = 0.52722818\n",
      "Iteration 97, loss = 0.52577210\n",
      "Iteration 98, loss = 0.52804916\n",
      "Iteration 99, loss = 0.52694807\n",
      "Iteration 100, loss = 0.52672027\n",
      "Iteration 101, loss = 0.52624659\n",
      "Iteration 102, loss = 0.52571470\n",
      "Iteration 103, loss = 0.52690330\n",
      "Iteration 104, loss = 0.52648670\n",
      "Iteration 105, loss = 0.52655415\n",
      "Iteration 106, loss = 0.52637948\n",
      "Iteration 107, loss = 0.52683506\n",
      "Iteration 108, loss = 0.52669733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04334521\n",
      "Iteration 2, loss = 0.76526649\n",
      "Iteration 3, loss = 0.69297241\n",
      "Iteration 4, loss = 0.64401396\n",
      "Iteration 5, loss = 0.60898126\n",
      "Iteration 6, loss = 0.57010064\n",
      "Iteration 7, loss = 0.56287871\n",
      "Iteration 8, loss = 0.55647760\n",
      "Iteration 9, loss = 0.55426654\n",
      "Iteration 10, loss = 0.55277270\n",
      "Iteration 11, loss = 0.55095913\n",
      "Iteration 12, loss = 0.54905343\n",
      "Iteration 13, loss = 0.54740303\n",
      "Iteration 14, loss = 0.54697550\n",
      "Iteration 15, loss = 0.54554906\n",
      "Iteration 16, loss = 0.54377002\n",
      "Iteration 17, loss = 0.54344402\n",
      "Iteration 18, loss = 0.54219339\n",
      "Iteration 19, loss = 0.54116807\n",
      "Iteration 20, loss = 0.54124695\n",
      "Iteration 21, loss = 0.54095737\n",
      "Iteration 22, loss = 0.53858176\n",
      "Iteration 23, loss = 0.53933326\n",
      "Iteration 24, loss = 0.53933906\n",
      "Iteration 25, loss = 0.53920589\n",
      "Iteration 26, loss = 0.53718962\n",
      "Iteration 27, loss = 0.53726268\n",
      "Iteration 28, loss = 0.53639435\n",
      "Iteration 29, loss = 0.53631601\n",
      "Iteration 30, loss = 0.53646952\n",
      "Iteration 31, loss = 0.53511076\n",
      "Iteration 32, loss = 0.53566657\n",
      "Iteration 33, loss = 0.53552601\n",
      "Iteration 34, loss = 0.53454460\n",
      "Iteration 35, loss = 0.53425100\n",
      "Iteration 36, loss = 0.53451139\n",
      "Iteration 37, loss = 0.53384757\n",
      "Iteration 38, loss = 0.53297707\n",
      "Iteration 39, loss = 0.53380834\n",
      "Iteration 40, loss = 0.53389570\n",
      "Iteration 41, loss = 0.53420921\n",
      "Iteration 42, loss = 0.53215973\n",
      "Iteration 43, loss = 0.53135185\n",
      "Iteration 44, loss = 0.52917109\n",
      "Iteration 45, loss = 0.52765674\n",
      "Iteration 46, loss = 0.52515075\n",
      "Iteration 47, loss = 0.52428435\n",
      "Iteration 48, loss = 0.52414091\n",
      "Iteration 49, loss = 0.52280621\n",
      "Iteration 50, loss = 0.52496993\n",
      "Iteration 51, loss = 0.52249064\n",
      "Iteration 52, loss = 0.52396357\n",
      "Iteration 53, loss = 0.52255960\n",
      "Iteration 54, loss = 0.52218003\n",
      "Iteration 55, loss = 0.52304701\n",
      "Iteration 56, loss = 0.52438567\n",
      "Iteration 57, loss = 0.52365646\n",
      "Iteration 58, loss = 0.52253525\n",
      "Iteration 59, loss = 0.52212447\n",
      "Iteration 60, loss = 0.52218540\n",
      "Iteration 61, loss = 0.52213689\n",
      "Iteration 62, loss = 0.52337499\n",
      "Iteration 63, loss = 0.52182207\n",
      "Iteration 64, loss = 0.52298005\n",
      "Iteration 65, loss = 0.52202321\n",
      "Iteration 66, loss = 0.52222552\n",
      "Iteration 67, loss = 0.52430303\n",
      "Iteration 68, loss = 0.52208762\n",
      "Iteration 69, loss = 0.52190325\n",
      "Iteration 70, loss = 0.52178303\n",
      "Iteration 71, loss = 0.52162902\n",
      "Iteration 72, loss = 0.52182005\n",
      "Iteration 73, loss = 0.52250343\n",
      "Iteration 74, loss = 0.52140720\n",
      "Iteration 75, loss = 0.52170169\n",
      "Iteration 76, loss = 0.52201798\n",
      "Iteration 77, loss = 0.52256579\n",
      "Iteration 78, loss = 0.52277058\n",
      "Iteration 79, loss = 0.52100772\n",
      "Iteration 80, loss = 0.52157636\n",
      "Iteration 81, loss = 0.52062481\n",
      "Iteration 82, loss = 0.52097251\n",
      "Iteration 83, loss = 0.52093616\n",
      "Iteration 84, loss = 0.52113858\n",
      "Iteration 85, loss = 0.52057984\n",
      "Iteration 86, loss = 0.52053032\n",
      "Iteration 87, loss = 0.52210150\n",
      "Iteration 88, loss = 0.52091128\n",
      "Iteration 89, loss = 0.52159797\n",
      "Iteration 90, loss = 0.52063599\n",
      "Iteration 91, loss = 0.52076039\n",
      "Iteration 92, loss = 0.52039066\n",
      "Iteration 93, loss = 0.52089200\n",
      "Iteration 94, loss = 0.52217019\n",
      "Iteration 95, loss = 0.52152759\n",
      "Iteration 96, loss = 0.52157514\n",
      "Iteration 97, loss = 0.51987594\n",
      "Iteration 98, loss = 0.52211631\n",
      "Iteration 99, loss = 0.52067465\n",
      "Iteration 100, loss = 0.52021920\n",
      "Iteration 101, loss = 0.52019844\n",
      "Iteration 102, loss = 0.52073933\n",
      "Iteration 103, loss = 0.52108372\n",
      "Iteration 104, loss = 0.52099152\n",
      "Iteration 105, loss = 0.51977370\n",
      "Iteration 106, loss = 0.51989420\n",
      "Iteration 107, loss = 0.51973840\n",
      "Iteration 108, loss = 0.52008954\n",
      "Iteration 109, loss = 0.52010657\n",
      "Iteration 110, loss = 0.51987724\n",
      "Iteration 111, loss = 0.52047834\n",
      "Iteration 112, loss = 0.52076873\n",
      "Iteration 113, loss = 0.52017889\n",
      "Iteration 114, loss = 0.51978714\n",
      "Iteration 115, loss = 0.51970248\n",
      "Iteration 116, loss = 0.51924192\n",
      "Iteration 117, loss = 0.52008287\n",
      "Iteration 118, loss = 0.51943944\n",
      "Iteration 119, loss = 0.51987447\n",
      "Iteration 120, loss = 0.51947323\n",
      "Iteration 121, loss = 0.51927637\n",
      "Iteration 122, loss = 0.51952185\n",
      "Iteration 123, loss = 0.51944787\n",
      "Iteration 124, loss = 0.52018903\n",
      "Iteration 125, loss = 0.51968053\n",
      "Iteration 126, loss = 0.51968519\n",
      "Iteration 127, loss = 0.52099696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04094632\n",
      "Iteration 2, loss = 0.76431344\n",
      "Iteration 3, loss = 0.69234561\n",
      "Iteration 4, loss = 0.64333426\n",
      "Iteration 5, loss = 0.61029426\n",
      "Iteration 6, loss = 0.58897583\n",
      "Iteration 7, loss = 0.57563089\n",
      "Iteration 8, loss = 0.56768900\n",
      "Iteration 9, loss = 0.56288415\n",
      "Iteration 10, loss = 0.56021954\n",
      "Iteration 11, loss = 0.55875824\n",
      "Iteration 12, loss = 0.55792859\n",
      "Iteration 13, loss = 0.55754284\n",
      "Iteration 14, loss = 0.55731182\n",
      "Iteration 15, loss = 0.55724059\n",
      "Iteration 16, loss = 0.55716119\n",
      "Iteration 17, loss = 0.55717231\n",
      "Iteration 18, loss = 0.55712642\n",
      "Iteration 19, loss = 0.55711268\n",
      "Iteration 20, loss = 0.55712269\n",
      "Iteration 21, loss = 0.55710587\n",
      "Iteration 22, loss = 0.55713825\n",
      "Iteration 23, loss = 0.55710036\n",
      "Iteration 24, loss = 0.55665535\n",
      "Iteration 25, loss = 0.55495783\n",
      "Iteration 26, loss = 0.55451872\n",
      "Iteration 27, loss = 0.55294645\n",
      "Iteration 28, loss = 0.55282047\n",
      "Iteration 29, loss = 0.55187049\n",
      "Iteration 30, loss = 0.55142756\n",
      "Iteration 31, loss = 0.55129921\n",
      "Iteration 32, loss = 0.55101635\n",
      "Iteration 33, loss = 0.55086014\n",
      "Iteration 34, loss = 0.55029743\n",
      "Iteration 35, loss = 0.54871815\n",
      "Iteration 36, loss = 0.54435206\n",
      "Iteration 37, loss = 0.54193456\n",
      "Iteration 38, loss = 0.53936467\n",
      "Iteration 39, loss = 0.53711128\n",
      "Iteration 40, loss = 0.53561839\n",
      "Iteration 41, loss = 0.53453842\n",
      "Iteration 42, loss = 0.53444092\n",
      "Iteration 43, loss = 0.53452228\n",
      "Iteration 44, loss = 0.53329057\n",
      "Iteration 45, loss = 0.53316673\n",
      "Iteration 46, loss = 0.53190492\n",
      "Iteration 47, loss = 0.53121434\n",
      "Iteration 48, loss = 0.53128497\n",
      "Iteration 49, loss = 0.53203901\n",
      "Iteration 50, loss = 0.53166030\n",
      "Iteration 51, loss = 0.53081667\n",
      "Iteration 52, loss = 0.53070619\n",
      "Iteration 53, loss = 0.53099431\n",
      "Iteration 54, loss = 0.53102952\n",
      "Iteration 55, loss = 0.53182786\n",
      "Iteration 56, loss = 0.53053667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 57, loss = 0.53054587\n",
      "Iteration 58, loss = 0.53096435\n",
      "Iteration 59, loss = 0.53090892\n",
      "Iteration 60, loss = 0.53059102\n",
      "Iteration 61, loss = 0.53038942\n",
      "Iteration 62, loss = 0.53064534\n",
      "Iteration 63, loss = 0.53041058\n",
      "Iteration 64, loss = 0.53023505\n",
      "Iteration 65, loss = 0.53028077\n",
      "Iteration 66, loss = 0.53058202\n",
      "Iteration 67, loss = 0.53057108\n",
      "Iteration 68, loss = 0.53188321\n",
      "Iteration 69, loss = 0.53125885\n",
      "Iteration 70, loss = 0.53016744\n",
      "Iteration 71, loss = 0.52985285\n",
      "Iteration 72, loss = 0.52983481\n",
      "Iteration 73, loss = 0.53068123\n",
      "Iteration 74, loss = 0.53076818\n",
      "Iteration 75, loss = 0.53015666\n",
      "Iteration 76, loss = 0.53033649\n",
      "Iteration 77, loss = 0.53000272\n",
      "Iteration 78, loss = 0.52986427\n",
      "Iteration 79, loss = 0.53029366\n",
      "Iteration 80, loss = 0.53032618\n",
      "Iteration 81, loss = 0.52979252\n",
      "Iteration 82, loss = 0.52993832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05325543\n",
      "Iteration 2, loss = 0.76468563\n",
      "Iteration 3, loss = 0.69359174\n",
      "Iteration 4, loss = 0.64457718\n",
      "Iteration 5, loss = 0.61121815\n",
      "Iteration 6, loss = 0.58983792\n",
      "Iteration 7, loss = 0.57628811\n",
      "Iteration 8, loss = 0.56804428\n",
      "Iteration 9, loss = 0.56335193\n",
      "Iteration 10, loss = 0.56051591\n",
      "Iteration 11, loss = 0.55897916\n",
      "Iteration 12, loss = 0.55816844\n",
      "Iteration 13, loss = 0.55773491\n",
      "Iteration 14, loss = 0.55751089\n",
      "Iteration 15, loss = 0.55739682\n",
      "Iteration 16, loss = 0.55733986\n",
      "Iteration 17, loss = 0.55731177\n",
      "Iteration 18, loss = 0.55730080\n",
      "Iteration 19, loss = 0.55730667\n",
      "Iteration 20, loss = 0.55731152\n",
      "Iteration 21, loss = 0.55732029\n",
      "Iteration 22, loss = 0.55731393\n",
      "Iteration 23, loss = 0.55732182\n",
      "Iteration 24, loss = 0.55730373\n",
      "Iteration 25, loss = 0.55731364\n",
      "Iteration 26, loss = 0.55734201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05221398\n",
      "Iteration 2, loss = 0.76456667\n",
      "Iteration 3, loss = 0.69303713\n",
      "Iteration 4, loss = 0.64431501\n",
      "Iteration 5, loss = 0.61112825\n",
      "Iteration 6, loss = 0.58975807\n",
      "Iteration 7, loss = 0.57608647\n",
      "Iteration 8, loss = 0.56805282\n",
      "Iteration 9, loss = 0.56325286\n",
      "Iteration 10, loss = 0.56050226\n",
      "Iteration 11, loss = 0.55895362\n",
      "Iteration 12, loss = 0.55814275\n",
      "Iteration 13, loss = 0.55774111\n",
      "Iteration 14, loss = 0.55750189\n",
      "Iteration 15, loss = 0.55742700\n",
      "Iteration 16, loss = 0.55734493\n",
      "Iteration 17, loss = 0.55732760\n",
      "Iteration 18, loss = 0.55731251\n",
      "Iteration 19, loss = 0.55731899\n",
      "Iteration 20, loss = 0.55731685\n",
      "Iteration 21, loss = 0.55732408\n",
      "Iteration 22, loss = 0.55733617\n",
      "Iteration 23, loss = 0.55732864\n",
      "Iteration 24, loss = 0.55731364\n",
      "Iteration 25, loss = 0.55731076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04900206\n",
      "Iteration 2, loss = 0.76371409\n",
      "Iteration 3, loss = 0.69237648\n",
      "Iteration 4, loss = 0.64362585\n",
      "Iteration 5, loss = 0.61053641\n",
      "Iteration 6, loss = 0.58915871\n",
      "Iteration 7, loss = 0.57576040\n",
      "Iteration 8, loss = 0.56777652\n",
      "Iteration 9, loss = 0.56294076\n",
      "Iteration 10, loss = 0.56025507\n",
      "Iteration 11, loss = 0.55878001\n",
      "Iteration 12, loss = 0.55794191\n",
      "Iteration 13, loss = 0.55755167\n",
      "Iteration 14, loss = 0.55731810\n",
      "Iteration 15, loss = 0.55724531\n",
      "Iteration 16, loss = 0.55716533\n",
      "Iteration 17, loss = 0.55717861\n",
      "Iteration 18, loss = 0.55713294\n",
      "Iteration 19, loss = 0.55712010\n",
      "Iteration 20, loss = 0.55713141\n",
      "Iteration 21, loss = 0.55711876\n",
      "Iteration 22, loss = 0.55714546\n",
      "Iteration 23, loss = 0.55711549\n",
      "Iteration 24, loss = 0.55712991\n",
      "Iteration 25, loss = 0.55713950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06446045\n",
      "Iteration 2, loss = 0.76373111\n",
      "Iteration 3, loss = 0.69576963\n",
      "Iteration 4, loss = 0.64661637\n",
      "Iteration 5, loss = 0.61290065\n",
      "Iteration 6, loss = 0.59112059\n",
      "Iteration 7, loss = 0.57721607\n",
      "Iteration 8, loss = 0.56870106\n",
      "Iteration 9, loss = 0.56381189\n",
      "Iteration 10, loss = 0.56083470\n",
      "Iteration 11, loss = 0.55920121\n",
      "Iteration 12, loss = 0.55832594\n",
      "Iteration 13, loss = 0.55785052\n",
      "Iteration 14, loss = 0.55759999\n",
      "Iteration 15, loss = 0.55746664\n",
      "Iteration 16, loss = 0.55739649\n",
      "Iteration 17, loss = 0.55735846\n",
      "Iteration 18, loss = 0.55733836\n",
      "Iteration 19, loss = 0.55733833\n",
      "Iteration 20, loss = 0.55733968\n",
      "Iteration 21, loss = 0.55734484\n",
      "Iteration 22, loss = 0.55733544\n",
      "Iteration 23, loss = 0.55733949\n",
      "Iteration 24, loss = 0.55731836\n",
      "Iteration 25, loss = 0.55732434\n",
      "Iteration 26, loss = 0.55734867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05816017\n",
      "Iteration 2, loss = 0.76347047\n",
      "Iteration 3, loss = 0.69521467\n",
      "Iteration 4, loss = 0.64628778\n",
      "Iteration 5, loss = 0.61271405\n",
      "Iteration 6, loss = 0.59096451\n",
      "Iteration 7, loss = 0.57697413\n",
      "Iteration 8, loss = 0.56868633\n",
      "Iteration 9, loss = 0.56369576\n",
      "Iteration 10, loss = 0.56081066\n",
      "Iteration 11, loss = 0.55916675\n",
      "Iteration 12, loss = 0.55829626\n",
      "Iteration 13, loss = 0.55785644\n",
      "Iteration 14, loss = 0.55759074\n",
      "Iteration 15, loss = 0.55749841\n",
      "Iteration 16, loss = 0.55740157\n",
      "Iteration 17, loss = 0.55737284\n",
      "Iteration 18, loss = 0.55735031\n",
      "Iteration 19, loss = 0.55735100\n",
      "Iteration 20, loss = 0.55734381\n",
      "Iteration 21, loss = 0.55734591\n",
      "Iteration 22, loss = 0.55735263\n",
      "Iteration 23, loss = 0.55733969\n",
      "Iteration 24, loss = 0.55731940\n",
      "Iteration 25, loss = 0.55731144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05958526\n",
      "Iteration 2, loss = 0.76266504\n",
      "Iteration 3, loss = 0.69458900\n",
      "Iteration 4, loss = 0.64567863\n",
      "Iteration 5, loss = 0.61220767\n",
      "Iteration 6, loss = 0.59042803\n",
      "Iteration 7, loss = 0.57668927\n",
      "Iteration 8, loss = 0.56843713\n",
      "Iteration 9, loss = 0.56339889\n",
      "Iteration 10, loss = 0.56057084\n",
      "Iteration 11, loss = 0.55899687\n",
      "Iteration 12, loss = 0.55809531\n",
      "Iteration 13, loss = 0.55766612\n",
      "Iteration 14, loss = 0.55740572\n",
      "Iteration 15, loss = 0.55731551\n",
      "Iteration 16, loss = 0.55722184\n",
      "Iteration 17, loss = 0.55722318\n",
      "Iteration 18, loss = 0.55717096\n",
      "Iteration 19, loss = 0.55715302\n",
      "Iteration 20, loss = 0.55716010\n",
      "Iteration 21, loss = 0.55714343\n",
      "Iteration 22, loss = 0.55716521\n",
      "Iteration 23, loss = 0.55713118\n",
      "Iteration 24, loss = 0.55714069\n",
      "Iteration 25, loss = 0.55714543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05203040\n",
      "Iteration 2, loss = 0.76439192\n",
      "Iteration 3, loss = 0.69574446\n",
      "Iteration 4, loss = 0.64625292\n",
      "Iteration 5, loss = 0.61251525\n",
      "Iteration 6, loss = 0.59080050\n",
      "Iteration 7, loss = 0.57698075\n",
      "Iteration 8, loss = 0.56853323\n",
      "Iteration 9, loss = 0.56369455\n",
      "Iteration 10, loss = 0.56075658\n",
      "Iteration 11, loss = 0.55914981\n",
      "Iteration 12, loss = 0.55829263\n",
      "Iteration 13, loss = 0.55782622\n",
      "Iteration 14, loss = 0.55757845\n",
      "Iteration 15, loss = 0.55744603\n",
      "Iteration 16, loss = 0.55737406\n",
      "Iteration 17, loss = 0.55733166\n",
      "Iteration 18, loss = 0.55730735\n",
      "Iteration 19, loss = 0.55730669\n",
      "Iteration 20, loss = 0.55731134\n",
      "Iteration 21, loss = 0.55731994\n",
      "Iteration 22, loss = 0.55731367\n",
      "Iteration 23, loss = 0.55732141\n",
      "Iteration 24, loss = 0.55730352\n",
      "Iteration 25, loss = 0.55731332\n",
      "Iteration 26, loss = 0.55734145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05023855\n",
      "Iteration 2, loss = 0.76420352\n",
      "Iteration 3, loss = 0.69531794\n",
      "Iteration 4, loss = 0.64610836\n",
      "Iteration 5, loss = 0.61250665\n",
      "Iteration 6, loss = 0.59077675\n",
      "Iteration 7, loss = 0.57681476\n",
      "Iteration 8, loss = 0.56856579\n",
      "Iteration 9, loss = 0.56360963\n",
      "Iteration 10, loss = 0.56075162\n",
      "Iteration 11, loss = 0.55912801\n",
      "Iteration 12, loss = 0.55826799\n",
      "Iteration 13, loss = 0.55783109\n",
      "Iteration 14, loss = 0.55756600\n",
      "Iteration 15, loss = 0.55747184\n",
      "Iteration 16, loss = 0.55737165\n",
      "Iteration 17, loss = 0.55733847\n",
      "Iteration 18, loss = 0.55731327\n",
      "Iteration 19, loss = 0.55731906\n",
      "Iteration 20, loss = 0.55731671\n",
      "Iteration 21, loss = 0.55732378\n",
      "Iteration 22, loss = 0.55733583\n",
      "Iteration 23, loss = 0.55732823\n",
      "Iteration 24, loss = 0.55731341\n",
      "Iteration 25, loss = 0.55731055\n",
      "Iteration 26, loss = 0.55735155\n",
      "Iteration 27, loss = 0.55732950\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04984037\n",
      "Iteration 2, loss = 0.76346887\n",
      "Iteration 3, loss = 0.69464674\n",
      "Iteration 4, loss = 0.64541136\n",
      "Iteration 5, loss = 0.61190889\n",
      "Iteration 6, loss = 0.59017332\n",
      "Iteration 7, loss = 0.57648777\n",
      "Iteration 8, loss = 0.56828938\n",
      "Iteration 9, loss = 0.56329702\n",
      "Iteration 10, loss = 0.56050384\n",
      "Iteration 11, loss = 0.55895368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.55806291\n",
      "Iteration 13, loss = 0.55763590\n",
      "Iteration 14, loss = 0.55737365\n",
      "Iteration 15, loss = 0.55727823\n",
      "Iteration 16, loss = 0.55717670\n",
      "Iteration 17, loss = 0.55717928\n",
      "Iteration 18, loss = 0.55713346\n",
      "Iteration 19, loss = 0.55712016\n",
      "Iteration 20, loss = 0.55713131\n",
      "Iteration 21, loss = 0.55711868\n",
      "Iteration 22, loss = 0.55714516\n",
      "Iteration 23, loss = 0.55711542\n",
      "Iteration 24, loss = 0.55712969\n",
      "Iteration 25, loss = 0.55713919\n",
      "Iteration 26, loss = 0.55713874\n",
      "Iteration 27, loss = 0.55715736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03864691\n",
      "Iteration 2, loss = 0.76487033\n",
      "Iteration 3, loss = 0.69572205\n",
      "Iteration 4, loss = 0.64586114\n",
      "Iteration 5, loss = 0.61202279\n",
      "Iteration 6, loss = 0.59037828\n",
      "Iteration 7, loss = 0.57665264\n",
      "Iteration 8, loss = 0.56829547\n",
      "Iteration 9, loss = 0.56352580\n",
      "Iteration 10, loss = 0.56063907\n",
      "Iteration 11, loss = 0.55906655\n",
      "Iteration 12, loss = 0.55823013\n",
      "Iteration 13, loss = 0.55777793\n",
      "Iteration 14, loss = 0.55753855\n",
      "Iteration 15, loss = 0.55741287\n",
      "Iteration 16, loss = 0.55734655\n",
      "Iteration 17, loss = 0.55731224\n",
      "Iteration 18, loss = 0.55730091\n",
      "Iteration 19, loss = 0.55730662\n",
      "Iteration 20, loss = 0.55731135\n",
      "Iteration 21, loss = 0.55732003\n",
      "Iteration 22, loss = 0.55731372\n",
      "Iteration 23, loss = 0.55732152\n",
      "Iteration 24, loss = 0.55730354\n",
      "Iteration 25, loss = 0.55731339\n",
      "Iteration 26, loss = 0.55734164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03835651\n",
      "Iteration 2, loss = 0.76486292\n",
      "Iteration 3, loss = 0.69534547\n",
      "Iteration 4, loss = 0.64572376\n",
      "Iteration 5, loss = 0.61201370\n",
      "Iteration 6, loss = 0.59033216\n",
      "Iteration 7, loss = 0.57647873\n",
      "Iteration 8, loss = 0.56831965\n",
      "Iteration 9, loss = 0.56343738\n",
      "Iteration 10, loss = 0.56063001\n",
      "Iteration 11, loss = 0.55904050\n",
      "Iteration 12, loss = 0.55820143\n",
      "Iteration 13, loss = 0.55777878\n",
      "Iteration 14, loss = 0.55752437\n",
      "Iteration 15, loss = 0.55743766\n",
      "Iteration 16, loss = 0.55734675\n",
      "Iteration 17, loss = 0.55732827\n",
      "Iteration 18, loss = 0.55731274\n",
      "Iteration 19, loss = 0.55731899\n",
      "Iteration 20, loss = 0.55731672\n",
      "Iteration 21, loss = 0.55732386\n",
      "Iteration 22, loss = 0.55733593\n",
      "Iteration 23, loss = 0.55732836\n",
      "Iteration 24, loss = 0.55731345\n",
      "Iteration 25, loss = 0.55731058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03901457\n",
      "Iteration 2, loss = 0.76405067\n",
      "Iteration 3, loss = 0.69470527\n",
      "Iteration 4, loss = 0.64503233\n",
      "Iteration 5, loss = 0.61143359\n",
      "Iteration 6, loss = 0.58976526\n",
      "Iteration 7, loss = 0.57617900\n",
      "Iteration 8, loss = 0.56806275\n",
      "Iteration 9, loss = 0.56313805\n",
      "Iteration 10, loss = 0.56039039\n",
      "Iteration 11, loss = 0.55887192\n",
      "Iteration 12, loss = 0.55800264\n",
      "Iteration 13, loss = 0.55759054\n",
      "Iteration 14, loss = 0.55734114\n",
      "Iteration 15, loss = 0.55725606\n",
      "Iteration 16, loss = 0.55716723\n",
      "Iteration 17, loss = 0.55717901\n",
      "Iteration 18, loss = 0.55713322\n",
      "Iteration 19, loss = 0.55712009\n",
      "Iteration 20, loss = 0.55713130\n",
      "Iteration 21, loss = 0.55711866\n",
      "Iteration 22, loss = 0.55714523\n",
      "Iteration 23, loss = 0.55711540\n",
      "Iteration 24, loss = 0.55712973\n",
      "Iteration 25, loss = 0.55713926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03451032\n",
      "Iteration 2, loss = 0.76511561\n",
      "Iteration 3, loss = 0.69545611\n",
      "Iteration 4, loss = 0.64561552\n",
      "Iteration 5, loss = 0.61187003\n",
      "Iteration 6, loss = 0.59027866\n",
      "Iteration 7, loss = 0.57656465\n",
      "Iteration 8, loss = 0.56819470\n",
      "Iteration 9, loss = 0.56343866\n",
      "Iteration 10, loss = 0.56057029\n",
      "Iteration 11, loss = 0.55901196\n",
      "Iteration 12, loss = 0.55818758\n",
      "Iteration 13, loss = 0.55774555\n",
      "Iteration 14, loss = 0.55751652\n",
      "Iteration 15, loss = 0.55739954\n",
      "Iteration 16, loss = 0.55734118\n",
      "Iteration 17, loss = 0.55731228\n",
      "Iteration 18, loss = 0.55730090\n",
      "Iteration 19, loss = 0.55730697\n",
      "Iteration 20, loss = 0.55731178\n",
      "Iteration 21, loss = 0.55732007\n",
      "Iteration 22, loss = 0.55731374\n",
      "Iteration 23, loss = 0.55732152\n",
      "Iteration 24, loss = 0.55730362\n",
      "Iteration 25, loss = 0.55731375\n",
      "Iteration 26, loss = 0.55734174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03344653\n",
      "Iteration 2, loss = 0.76497121\n",
      "Iteration 3, loss = 0.69495125\n",
      "Iteration 4, loss = 0.64541058\n",
      "Iteration 5, loss = 0.61183799\n",
      "Iteration 6, loss = 0.59022049\n",
      "Iteration 7, loss = 0.57636029\n",
      "Iteration 8, loss = 0.56820383\n",
      "Iteration 9, loss = 0.56334969\n",
      "Iteration 10, loss = 0.56056216\n",
      "Iteration 11, loss = 0.55898914\n",
      "Iteration 12, loss = 0.55816330\n",
      "Iteration 13, loss = 0.55775257\n",
      "Iteration 14, loss = 0.55750793\n",
      "Iteration 15, loss = 0.55743018\n",
      "Iteration 16, loss = 0.55734645\n",
      "Iteration 17, loss = 0.55732825\n",
      "Iteration 18, loss = 0.55731273\n",
      "Iteration 19, loss = 0.55731901\n",
      "Iteration 20, loss = 0.55731675\n",
      "Iteration 21, loss = 0.55732390\n",
      "Iteration 22, loss = 0.55733598\n",
      "Iteration 23, loss = 0.55732841\n",
      "Iteration 24, loss = 0.55731349\n",
      "Iteration 25, loss = 0.55731061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03377768\n",
      "Iteration 2, loss = 0.76423466\n",
      "Iteration 3, loss = 0.69435564\n",
      "Iteration 4, loss = 0.64477646\n",
      "Iteration 5, loss = 0.61129750\n",
      "Iteration 6, loss = 0.58966788\n",
      "Iteration 7, loss = 0.57607849\n",
      "Iteration 8, loss = 0.56795141\n",
      "Iteration 9, loss = 0.56304565\n",
      "Iteration 10, loss = 0.56032007\n",
      "Iteration 11, loss = 0.55881891\n",
      "Iteration 12, loss = 0.55796406\n",
      "Iteration 13, loss = 0.55756414\n",
      "Iteration 14, loss = 0.55732465\n",
      "Iteration 15, loss = 0.55724877\n",
      "Iteration 16, loss = 0.55716694\n",
      "Iteration 17, loss = 0.55717898\n",
      "Iteration 18, loss = 0.55713317\n",
      "Iteration 19, loss = 0.55712006\n",
      "Iteration 20, loss = 0.55713129\n",
      "Iteration 21, loss = 0.55711864\n",
      "Iteration 22, loss = 0.55714524\n",
      "Iteration 23, loss = 0.55711537\n",
      "Iteration 24, loss = 0.55712972\n",
      "Iteration 25, loss = 0.55713927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03404688\n",
      "Iteration 2, loss = 0.76389493\n",
      "Iteration 3, loss = 0.69289577\n",
      "Iteration 4, loss = 0.62731650\n",
      "Iteration 5, loss = 0.54381610\n",
      "Iteration 6, loss = 0.53277859\n",
      "Iteration 7, loss = 0.53037461\n",
      "Iteration 8, loss = 0.52864767\n",
      "Iteration 9, loss = 0.52863795\n",
      "Iteration 10, loss = 0.52841595\n",
      "Iteration 11, loss = 0.52775850\n",
      "Iteration 12, loss = 0.52675058\n",
      "Iteration 13, loss = 0.52657202\n",
      "Iteration 14, loss = 0.52732633\n",
      "Iteration 15, loss = 0.52606105\n",
      "Iteration 16, loss = 0.52619095\n",
      "Iteration 17, loss = 0.52605331\n",
      "Iteration 18, loss = 0.52647152\n",
      "Iteration 19, loss = 0.52620778\n",
      "Iteration 20, loss = 0.52620139\n",
      "Iteration 21, loss = 0.52636995\n",
      "Iteration 22, loss = 0.52679118\n",
      "Iteration 23, loss = 0.52631128\n",
      "Iteration 24, loss = 0.52583905\n",
      "Iteration 25, loss = 0.52612989\n",
      "Iteration 26, loss = 0.52618599\n",
      "Iteration 27, loss = 0.52585173\n",
      "Iteration 28, loss = 0.52578334\n",
      "Iteration 29, loss = 0.52610531\n",
      "Iteration 30, loss = 0.52577155\n",
      "Iteration 31, loss = 0.52488001\n",
      "Iteration 32, loss = 0.52429172\n",
      "Iteration 33, loss = 0.52431829\n",
      "Iteration 34, loss = 0.52340500\n",
      "Iteration 35, loss = 0.52313136\n",
      "Iteration 36, loss = 0.52308242\n",
      "Iteration 37, loss = 0.52219926\n",
      "Iteration 38, loss = 0.52194032\n",
      "Iteration 39, loss = 0.52064299\n",
      "Iteration 40, loss = 0.52056341\n",
      "Iteration 41, loss = 0.51636523\n",
      "Iteration 42, loss = 0.50478511\n",
      "Iteration 43, loss = 0.49860806\n",
      "Iteration 44, loss = 0.49651653\n",
      "Iteration 45, loss = 0.49495613\n",
      "Iteration 46, loss = 0.49500165\n",
      "Iteration 47, loss = 0.49601852\n",
      "Iteration 48, loss = 0.49494270\n",
      "Iteration 49, loss = 0.49358741\n",
      "Iteration 50, loss = 0.49365191\n",
      "Iteration 51, loss = 0.49329892\n",
      "Iteration 52, loss = 0.49333803\n",
      "Iteration 53, loss = 0.49372772\n",
      "Iteration 54, loss = 0.49328284\n",
      "Iteration 55, loss = 0.49286481\n",
      "Iteration 56, loss = 0.49255129\n",
      "Iteration 57, loss = 0.49216090\n",
      "Iteration 58, loss = 0.49258899\n",
      "Iteration 59, loss = 0.49266033\n",
      "Iteration 60, loss = 0.49148637\n",
      "Iteration 61, loss = 0.49236437\n",
      "Iteration 62, loss = 0.49442944\n",
      "Iteration 63, loss = 0.49126859\n",
      "Iteration 64, loss = 0.49124307\n",
      "Iteration 65, loss = 0.49063275\n",
      "Iteration 66, loss = 0.49012417\n",
      "Iteration 67, loss = 0.49100202\n",
      "Iteration 68, loss = 0.49007980\n",
      "Iteration 69, loss = 0.48970198\n",
      "Iteration 70, loss = 0.48959345\n",
      "Iteration 71, loss = 0.49135134\n",
      "Iteration 72, loss = 0.48965446\n",
      "Iteration 73, loss = 0.48844123\n",
      "Iteration 74, loss = 0.48800020\n",
      "Iteration 75, loss = 0.48867793\n",
      "Iteration 76, loss = 0.48566489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 77, loss = 0.48494251\n",
      "Iteration 78, loss = 0.48476984\n",
      "Iteration 79, loss = 0.48333092\n",
      "Iteration 80, loss = 0.48291008\n",
      "Iteration 81, loss = 0.47981868\n",
      "Iteration 82, loss = 0.47656544\n",
      "Iteration 83, loss = 0.47663474\n",
      "Iteration 84, loss = 0.47430513\n",
      "Iteration 85, loss = 0.47235994\n",
      "Iteration 86, loss = 0.46878177\n",
      "Iteration 87, loss = 0.46667292\n",
      "Iteration 88, loss = 0.46402467\n",
      "Iteration 89, loss = 0.46367333\n",
      "Iteration 90, loss = 0.46102836\n",
      "Iteration 91, loss = 0.45923926\n",
      "Iteration 92, loss = 0.45583418\n",
      "Iteration 93, loss = 0.45774159\n",
      "Iteration 94, loss = 0.45351222\n",
      "Iteration 95, loss = 0.45603436\n",
      "Iteration 96, loss = 0.45494895\n",
      "Iteration 97, loss = 0.45070200\n",
      "Iteration 98, loss = 0.45211243\n",
      "Iteration 99, loss = 0.44875736\n",
      "Iteration 100, loss = 0.44925125\n",
      "Iteration 101, loss = 0.45013929\n",
      "Iteration 102, loss = 0.44833545\n",
      "Iteration 103, loss = 0.44805544\n",
      "Iteration 104, loss = 0.45115470\n",
      "Iteration 105, loss = 0.45108770\n",
      "Iteration 106, loss = 0.45509133\n",
      "Iteration 107, loss = 0.44692352\n",
      "Iteration 108, loss = 0.44527151\n",
      "Iteration 109, loss = 0.44641709\n",
      "Iteration 110, loss = 0.44485386\n",
      "Iteration 111, loss = 0.44248486\n",
      "Iteration 112, loss = 0.44052408\n",
      "Iteration 113, loss = 0.44881569\n",
      "Iteration 114, loss = 0.44147781\n",
      "Iteration 115, loss = 0.44401462\n",
      "Iteration 116, loss = 0.44097081\n",
      "Iteration 117, loss = 0.44066427\n",
      "Iteration 118, loss = 0.44326406\n",
      "Iteration 119, loss = 0.44199696\n",
      "Iteration 120, loss = 0.43995668\n",
      "Iteration 121, loss = 0.44108510\n",
      "Iteration 122, loss = 0.44051456\n",
      "Iteration 123, loss = 0.44051683\n",
      "Iteration 124, loss = 0.44239638\n",
      "Iteration 125, loss = 0.44090861\n",
      "Iteration 126, loss = 0.43911959\n",
      "Iteration 127, loss = 0.43669305\n",
      "Iteration 128, loss = 0.43732035\n",
      "Iteration 129, loss = 0.43735568\n",
      "Iteration 130, loss = 0.44040796\n",
      "Iteration 131, loss = 0.43815506\n",
      "Iteration 132, loss = 0.43945126\n",
      "Iteration 133, loss = 0.44200292\n",
      "Iteration 134, loss = 0.43966098\n",
      "Iteration 135, loss = 0.43707938\n",
      "Iteration 136, loss = 0.43971058\n",
      "Iteration 137, loss = 0.43644672\n",
      "Iteration 138, loss = 0.43681279\n",
      "Iteration 139, loss = 0.43779737\n",
      "Iteration 140, loss = 0.43799696\n",
      "Iteration 141, loss = 0.43695565\n",
      "Iteration 142, loss = 0.43839490\n",
      "Iteration 143, loss = 0.43604249\n",
      "Iteration 144, loss = 0.43806203\n",
      "Iteration 145, loss = 0.43646556\n",
      "Iteration 146, loss = 0.43623578\n",
      "Iteration 147, loss = 0.43838866\n",
      "Iteration 148, loss = 0.43715879\n",
      "Iteration 149, loss = 0.43780951\n",
      "Iteration 150, loss = 0.43749908\n",
      "Iteration 151, loss = 0.43953659\n",
      "Iteration 152, loss = 0.43670666\n",
      "Iteration 153, loss = 0.43590169\n",
      "Iteration 154, loss = 0.43626088\n",
      "Iteration 155, loss = 0.43445813\n",
      "Iteration 156, loss = 0.43547286\n",
      "Iteration 157, loss = 0.43481602\n",
      "Iteration 158, loss = 0.43500461\n",
      "Iteration 159, loss = 0.43377602\n",
      "Iteration 160, loss = 0.43409276\n",
      "Iteration 161, loss = 0.43443998\n",
      "Iteration 162, loss = 0.43507855\n",
      "Iteration 163, loss = 0.43480154\n",
      "Iteration 164, loss = 0.43324774\n",
      "Iteration 165, loss = 0.43668710\n",
      "Iteration 166, loss = 0.43887752\n",
      "Iteration 167, loss = 0.43795385\n",
      "Iteration 168, loss = 0.43392427\n",
      "Iteration 169, loss = 0.43554933\n",
      "Iteration 170, loss = 0.43453575\n",
      "Iteration 171, loss = 0.43470290\n",
      "Iteration 172, loss = 0.43427057\n",
      "Iteration 173, loss = 0.43677893\n",
      "Iteration 174, loss = 0.43998447\n",
      "Iteration 175, loss = 0.43382567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.03140058\n",
      "Iteration 2, loss = 0.76358969\n",
      "Iteration 3, loss = 0.69250101\n",
      "Iteration 4, loss = 0.64325977\n",
      "Iteration 5, loss = 0.60973933\n",
      "Iteration 6, loss = 0.56921436\n",
      "Iteration 7, loss = 0.52883681\n",
      "Iteration 8, loss = 0.52506307\n",
      "Iteration 9, loss = 0.52413714\n",
      "Iteration 10, loss = 0.52456249\n",
      "Iteration 11, loss = 0.52385136\n",
      "Iteration 12, loss = 0.52249912\n",
      "Iteration 13, loss = 0.52231874\n",
      "Iteration 14, loss = 0.52233659\n",
      "Iteration 15, loss = 0.52193003\n",
      "Iteration 16, loss = 0.52185641\n",
      "Iteration 17, loss = 0.52178496\n",
      "Iteration 18, loss = 0.52205912\n",
      "Iteration 19, loss = 0.52215206\n",
      "Iteration 20, loss = 0.52170496\n",
      "Iteration 21, loss = 0.52175978\n",
      "Iteration 22, loss = 0.52107313\n",
      "Iteration 23, loss = 0.52149738\n",
      "Iteration 24, loss = 0.52100282\n",
      "Iteration 25, loss = 0.52155535\n",
      "Iteration 26, loss = 0.52191064\n",
      "Iteration 27, loss = 0.52128206\n",
      "Iteration 28, loss = 0.52182410\n",
      "Iteration 29, loss = 0.52075934\n",
      "Iteration 30, loss = 0.52094847\n",
      "Iteration 31, loss = 0.52070793\n",
      "Iteration 32, loss = 0.52118461\n",
      "Iteration 33, loss = 0.52101539\n",
      "Iteration 34, loss = 0.52081339\n",
      "Iteration 35, loss = 0.52084850\n",
      "Iteration 36, loss = 0.52092824\n",
      "Iteration 37, loss = 0.52067303\n",
      "Iteration 38, loss = 0.52050807\n",
      "Iteration 39, loss = 0.52167544\n",
      "Iteration 40, loss = 0.52182816\n",
      "Iteration 41, loss = 0.52151910\n",
      "Iteration 42, loss = 0.52082472\n",
      "Iteration 43, loss = 0.52001602\n",
      "Iteration 44, loss = 0.51991183\n",
      "Iteration 45, loss = 0.51952148\n",
      "Iteration 46, loss = 0.51888969\n",
      "Iteration 47, loss = 0.51917096\n",
      "Iteration 48, loss = 0.51911898\n",
      "Iteration 49, loss = 0.51818280\n",
      "Iteration 50, loss = 0.51851586\n",
      "Iteration 51, loss = 0.51758240\n",
      "Iteration 52, loss = 0.51729263\n",
      "Iteration 53, loss = 0.51641626\n",
      "Iteration 54, loss = 0.51663632\n",
      "Iteration 55, loss = 0.51811794\n",
      "Iteration 56, loss = 0.51727978\n",
      "Iteration 57, loss = 0.51640354\n",
      "Iteration 58, loss = 0.51564891\n",
      "Iteration 59, loss = 0.51601587\n",
      "Iteration 60, loss = 0.51522039\n",
      "Iteration 61, loss = 0.51586421\n",
      "Iteration 62, loss = 0.51499568\n",
      "Iteration 63, loss = 0.51428907\n",
      "Iteration 64, loss = 0.51695998\n",
      "Iteration 65, loss = 0.51449069\n",
      "Iteration 66, loss = 0.51395852\n",
      "Iteration 67, loss = 0.51494217\n",
      "Iteration 68, loss = 0.51386080\n",
      "Iteration 69, loss = 0.51374229\n",
      "Iteration 70, loss = 0.51257050\n",
      "Iteration 71, loss = 0.51329463\n",
      "Iteration 72, loss = 0.51241756\n",
      "Iteration 73, loss = 0.51250395\n",
      "Iteration 74, loss = 0.51403467\n",
      "Iteration 75, loss = 0.51233699\n",
      "Iteration 76, loss = 0.51176222\n",
      "Iteration 77, loss = 0.51204737\n",
      "Iteration 78, loss = 0.51258137\n",
      "Iteration 79, loss = 0.51093024\n",
      "Iteration 80, loss = 0.51183699\n",
      "Iteration 81, loss = 0.51170848\n",
      "Iteration 82, loss = 0.51036547\n",
      "Iteration 83, loss = 0.51184406\n",
      "Iteration 84, loss = 0.51218822\n",
      "Iteration 85, loss = 0.50818726\n",
      "Iteration 86, loss = 0.50717501\n",
      "Iteration 87, loss = 0.50517373\n",
      "Iteration 88, loss = 0.50157678\n",
      "Iteration 89, loss = 0.49805003\n",
      "Iteration 90, loss = 0.49379757\n",
      "Iteration 91, loss = 0.49472235\n",
      "Iteration 92, loss = 0.49435697\n",
      "Iteration 93, loss = 0.48891241\n",
      "Iteration 94, loss = 0.48792873\n",
      "Iteration 95, loss = 0.48973637\n",
      "Iteration 96, loss = 0.48721534\n",
      "Iteration 97, loss = 0.48718405\n",
      "Iteration 98, loss = 0.48569554\n",
      "Iteration 99, loss = 0.49088845\n",
      "Iteration 100, loss = 0.48698005\n",
      "Iteration 101, loss = 0.48765748\n",
      "Iteration 102, loss = 0.48675112\n",
      "Iteration 103, loss = 0.48681006\n",
      "Iteration 104, loss = 0.48534429\n",
      "Iteration 105, loss = 0.48678990\n",
      "Iteration 106, loss = 0.48781471\n",
      "Iteration 107, loss = 0.48652803\n",
      "Iteration 108, loss = 0.48474001\n",
      "Iteration 109, loss = 0.48579616\n",
      "Iteration 110, loss = 0.48677262\n",
      "Iteration 111, loss = 0.48535944\n",
      "Iteration 112, loss = 0.48479668\n",
      "Iteration 113, loss = 0.48624808\n",
      "Iteration 114, loss = 0.48688616\n",
      "Iteration 115, loss = 0.48500084\n",
      "Iteration 116, loss = 0.48374133\n",
      "Iteration 117, loss = 0.48636173\n",
      "Iteration 118, loss = 0.48456404\n",
      "Iteration 119, loss = 0.48421798\n",
      "Iteration 120, loss = 0.48407330\n",
      "Iteration 121, loss = 0.48431494\n",
      "Iteration 122, loss = 0.48478083\n",
      "Iteration 123, loss = 0.48459042\n",
      "Iteration 124, loss = 0.48313993\n",
      "Iteration 125, loss = 0.48402295\n",
      "Iteration 126, loss = 0.48379694\n",
      "Iteration 127, loss = 0.48469932\n",
      "Iteration 128, loss = 0.48589932\n",
      "Iteration 129, loss = 0.48636646\n",
      "Iteration 130, loss = 0.48405340\n",
      "Iteration 131, loss = 0.48598275\n",
      "Iteration 132, loss = 0.48405605\n",
      "Iteration 133, loss = 0.48307254\n",
      "Iteration 134, loss = 0.48339658\n",
      "Iteration 135, loss = 0.48435754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.02865496\n",
      "Iteration 2, loss = 0.76246902\n",
      "Iteration 3, loss = 0.69174933\n",
      "Iteration 4, loss = 0.64257243\n",
      "Iteration 5, loss = 0.60948321\n",
      "Iteration 6, loss = 0.58827098\n",
      "Iteration 7, loss = 0.57508021\n",
      "Iteration 8, loss = 0.56730698\n",
      "Iteration 9, loss = 0.56263479\n",
      "Iteration 10, loss = 0.56006364\n",
      "Iteration 11, loss = 0.55866456\n",
      "Iteration 12, loss = 0.55787563\n",
      "Iteration 13, loss = 0.55751392\n",
      "Iteration 14, loss = 0.55729783\n",
      "Iteration 15, loss = 0.55723401\n",
      "Iteration 16, loss = 0.55715974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.55717657\n",
      "Iteration 18, loss = 0.55713108\n",
      "Iteration 19, loss = 0.55711922\n",
      "Iteration 20, loss = 0.55713052\n",
      "Iteration 21, loss = 0.55711786\n",
      "Iteration 22, loss = 0.55714495\n",
      "Iteration 23, loss = 0.55711460\n",
      "Iteration 24, loss = 0.55712955\n",
      "Iteration 25, loss = 0.55713894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01981475\n",
      "Iteration 2, loss = 0.76403029\n",
      "Iteration 3, loss = 0.69260160\n",
      "Iteration 4, loss = 0.64300200\n",
      "Iteration 5, loss = 0.60972128\n",
      "Iteration 6, loss = 0.58863595\n",
      "Iteration 7, loss = 0.57541325\n",
      "Iteration 8, loss = 0.56744489\n",
      "Iteration 9, loss = 0.56295956\n",
      "Iteration 10, loss = 0.56027062\n",
      "Iteration 11, loss = 0.55883086\n",
      "Iteration 12, loss = 0.55807915\n",
      "Iteration 13, loss = 0.55766723\n",
      "Iteration 14, loss = 0.55624811\n",
      "Iteration 15, loss = 0.55060961\n",
      "Iteration 16, loss = 0.54709275\n",
      "Iteration 17, loss = 0.54416215\n",
      "Iteration 18, loss = 0.54166202\n",
      "Iteration 19, loss = 0.54117763\n",
      "Iteration 20, loss = 0.53787201\n",
      "Iteration 21, loss = 0.53728364\n",
      "Iteration 22, loss = 0.53537146\n",
      "Iteration 23, loss = 0.53748772\n",
      "Iteration 24, loss = 0.53334140\n",
      "Iteration 25, loss = 0.53124680\n",
      "Iteration 26, loss = 0.53377735\n",
      "Iteration 27, loss = 0.52898858\n",
      "Iteration 28, loss = 0.52892733\n",
      "Iteration 29, loss = 0.52842416\n",
      "Iteration 30, loss = 0.52689147\n",
      "Iteration 31, loss = 0.52890317\n",
      "Iteration 32, loss = 0.52720156\n",
      "Iteration 33, loss = 0.52612513\n",
      "Iteration 34, loss = 0.52480665\n",
      "Iteration 35, loss = 0.52636215\n",
      "Iteration 36, loss = 0.52600751\n",
      "Iteration 37, loss = 0.52552541\n",
      "Iteration 38, loss = 0.52602569\n",
      "Iteration 39, loss = 0.52431772\n",
      "Iteration 40, loss = 0.52717115\n",
      "Iteration 41, loss = 0.52683170\n",
      "Iteration 42, loss = 0.52406278\n",
      "Iteration 43, loss = 0.52408796\n",
      "Iteration 44, loss = 0.52304406\n",
      "Iteration 45, loss = 0.52366880\n",
      "Iteration 46, loss = 0.52862009\n",
      "Iteration 47, loss = 0.52601487\n",
      "Iteration 48, loss = 0.52655563\n",
      "Iteration 49, loss = 0.52630976\n",
      "Iteration 50, loss = 0.52405050\n",
      "Iteration 51, loss = 0.52372421\n",
      "Iteration 52, loss = 0.52326584\n",
      "Iteration 53, loss = 0.52162531\n",
      "Iteration 54, loss = 0.52069990\n",
      "Iteration 55, loss = 0.52278981\n",
      "Iteration 56, loss = 0.52244570\n",
      "Iteration 57, loss = 0.52502162\n",
      "Iteration 58, loss = 0.52867867\n",
      "Iteration 59, loss = 0.52549534\n",
      "Iteration 60, loss = 0.52408214\n",
      "Iteration 61, loss = 0.52267012\n",
      "Iteration 62, loss = 0.52918439\n",
      "Iteration 63, loss = 0.52215394\n",
      "Iteration 64, loss = 0.52279021\n",
      "Iteration 65, loss = 0.52129558\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01802112\n",
      "Iteration 2, loss = 0.76385877\n",
      "Iteration 3, loss = 0.69197179\n",
      "Iteration 4, loss = 0.64269768\n",
      "Iteration 5, loss = 0.60962029\n",
      "Iteration 6, loss = 0.58856441\n",
      "Iteration 7, loss = 0.57521747\n",
      "Iteration 8, loss = 0.56745653\n",
      "Iteration 9, loss = 0.56286349\n",
      "Iteration 10, loss = 0.56025901\n",
      "Iteration 11, loss = 0.55880855\n",
      "Iteration 12, loss = 0.55805860\n",
      "Iteration 13, loss = 0.55769413\n",
      "Iteration 14, loss = 0.55747709\n",
      "Iteration 15, loss = 0.55741392\n",
      "Iteration 16, loss = 0.55733864\n",
      "Iteration 17, loss = 0.55732487\n",
      "Iteration 18, loss = 0.55731148\n",
      "Iteration 19, loss = 0.55731878\n",
      "Iteration 20, loss = 0.55731708\n",
      "Iteration 21, loss = 0.55732455\n",
      "Iteration 22, loss = 0.55733665\n",
      "Iteration 23, loss = 0.55732922\n",
      "Iteration 24, loss = 0.55731379\n",
      "Iteration 25, loss = 0.55731073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01484208\n",
      "Iteration 2, loss = 0.76257380\n",
      "Iteration 3, loss = 0.69119919\n",
      "Iteration 4, loss = 0.64195044\n",
      "Iteration 5, loss = 0.60896478\n",
      "Iteration 6, loss = 0.58791975\n",
      "Iteration 7, loss = 0.57486174\n",
      "Iteration 8, loss = 0.56716131\n",
      "Iteration 9, loss = 0.56254049\n",
      "Iteration 10, loss = 0.56000472\n",
      "Iteration 11, loss = 0.55862873\n",
      "Iteration 12, loss = 0.55785324\n",
      "Iteration 13, loss = 0.55749164\n",
      "Iteration 14, loss = 0.55724915\n",
      "Iteration 15, loss = 0.55374750\n",
      "Iteration 16, loss = 0.54860369\n",
      "Iteration 17, loss = 0.54587909\n",
      "Iteration 18, loss = 0.54382531\n",
      "Iteration 19, loss = 0.54069829\n",
      "Iteration 20, loss = 0.53776701\n",
      "Iteration 21, loss = 0.53656539\n",
      "Iteration 22, loss = 0.53529048\n",
      "Iteration 23, loss = 0.53502910\n",
      "Iteration 24, loss = 0.53397890\n",
      "Iteration 25, loss = 0.53162098\n",
      "Iteration 26, loss = 0.52968825\n",
      "Iteration 27, loss = 0.52943830\n",
      "Iteration 28, loss = 0.52991406\n",
      "Iteration 29, loss = 0.52831691\n",
      "Iteration 30, loss = 0.52978531\n",
      "Iteration 31, loss = 0.52634185\n",
      "Iteration 32, loss = 0.52982217\n",
      "Iteration 33, loss = 0.52779444\n",
      "Iteration 34, loss = 0.52870164\n",
      "Iteration 35, loss = 0.52684044\n",
      "Iteration 36, loss = 0.52577331\n",
      "Iteration 37, loss = 0.52801691\n",
      "Iteration 38, loss = 0.52911707\n",
      "Iteration 39, loss = 0.52608928\n",
      "Iteration 40, loss = 0.53056111\n",
      "Iteration 41, loss = 0.52638841\n",
      "Iteration 42, loss = 0.52558805\n",
      "Iteration 43, loss = 0.52582799\n",
      "Iteration 44, loss = 0.52489735\n",
      "Iteration 45, loss = 0.52498242\n",
      "Iteration 46, loss = 0.52602714\n",
      "Iteration 47, loss = 0.52540029\n",
      "Iteration 48, loss = 0.52514921\n",
      "Iteration 49, loss = 0.52434730\n",
      "Iteration 50, loss = 0.52548422\n",
      "Iteration 51, loss = 0.52515848\n",
      "Iteration 52, loss = 0.52502853\n",
      "Iteration 53, loss = 0.52518004\n",
      "Iteration 54, loss = 0.52470049\n",
      "Iteration 55, loss = 0.52519629\n",
      "Iteration 56, loss = 0.52370930\n",
      "Iteration 57, loss = 0.52409068\n",
      "Iteration 58, loss = 0.52624874\n",
      "Iteration 59, loss = 0.52675870\n",
      "Iteration 60, loss = 0.52412129\n",
      "Iteration 61, loss = 0.52450222\n",
      "Iteration 62, loss = 0.52423427\n",
      "Iteration 63, loss = 0.52576134\n",
      "Iteration 64, loss = 0.52460063\n",
      "Iteration 65, loss = 0.52493067\n",
      "Iteration 66, loss = 0.52653995\n",
      "Iteration 67, loss = 0.52496400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01957596\n",
      "Iteration 2, loss = 0.76366057\n",
      "Iteration 3, loss = 0.69222414\n",
      "Iteration 4, loss = 0.64287732\n",
      "Iteration 5, loss = 0.60967734\n",
      "Iteration 6, loss = 0.58861848\n",
      "Iteration 7, loss = 0.57539977\n",
      "Iteration 8, loss = 0.56743530\n",
      "Iteration 9, loss = 0.56295325\n",
      "Iteration 10, loss = 0.56026700\n",
      "Iteration 11, loss = 0.55882978\n",
      "Iteration 12, loss = 0.55808165\n",
      "Iteration 13, loss = 0.55768681\n",
      "Iteration 14, loss = 0.55748535\n",
      "Iteration 15, loss = 0.55738440\n",
      "Iteration 16, loss = 0.55733354\n",
      "Iteration 17, loss = 0.55730903\n",
      "Iteration 18, loss = 0.55729975\n",
      "Iteration 19, loss = 0.55730634\n",
      "Iteration 20, loss = 0.55731171\n",
      "Iteration 21, loss = 0.55732087\n",
      "Iteration 22, loss = 0.55731428\n",
      "Iteration 23, loss = 0.55732249\n",
      "Iteration 24, loss = 0.55730398\n",
      "Iteration 25, loss = 0.55731411\n",
      "Iteration 26, loss = 0.55734296\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01678868\n",
      "Iteration 2, loss = 0.76342665\n",
      "Iteration 3, loss = 0.69163300\n",
      "Iteration 4, loss = 0.64257872\n",
      "Iteration 5, loss = 0.60955268\n",
      "Iteration 6, loss = 0.58851020\n",
      "Iteration 7, loss = 0.57517756\n",
      "Iteration 8, loss = 0.56742882\n",
      "Iteration 9, loss = 0.56284532\n",
      "Iteration 10, loss = 0.56024761\n",
      "Iteration 11, loss = 0.55880173\n",
      "Iteration 12, loss = 0.55805458\n",
      "Iteration 13, loss = 0.55769190\n",
      "Iteration 14, loss = 0.55747591\n",
      "Iteration 15, loss = 0.55741330\n",
      "Iteration 16, loss = 0.55733827\n",
      "Iteration 17, loss = 0.55732467\n",
      "Iteration 18, loss = 0.55731138\n",
      "Iteration 19, loss = 0.55731873\n",
      "Iteration 20, loss = 0.55731709\n",
      "Iteration 21, loss = 0.55732463\n",
      "Iteration 22, loss = 0.55733679\n",
      "Iteration 23, loss = 0.55732942\n",
      "Iteration 24, loss = 0.55731405\n",
      "Iteration 25, loss = 0.55731113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01341179\n",
      "Iteration 2, loss = 0.76219344\n",
      "Iteration 3, loss = 0.69087216\n",
      "Iteration 4, loss = 0.64179893\n",
      "Iteration 5, loss = 0.60889381\n",
      "Iteration 6, loss = 0.58786441\n",
      "Iteration 7, loss = 0.57482127\n",
      "Iteration 8, loss = 0.56713363\n",
      "Iteration 9, loss = 0.56252279\n",
      "Iteration 10, loss = 0.55999436\n",
      "Iteration 11, loss = 0.55862350\n",
      "Iteration 12, loss = 0.55785271\n",
      "Iteration 13, loss = 0.55750141\n",
      "Iteration 14, loss = 0.55729170\n",
      "Iteration 15, loss = 0.55723127\n",
      "Iteration 16, loss = 0.55715867\n",
      "Iteration 17, loss = 0.55717701\n",
      "Iteration 18, loss = 0.55713173\n",
      "Iteration 19, loss = 0.55711999\n",
      "Iteration 20, loss = 0.55713156\n",
      "Iteration 21, loss = 0.55711888\n",
      "Iteration 22, loss = 0.55714603\n",
      "Iteration 23, loss = 0.55711557\n",
      "Iteration 24, loss = 0.55713029\n",
      "Iteration 25, loss = 0.55714008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04378668\n",
      "Iteration 2, loss = 0.76605181\n",
      "Iteration 3, loss = 0.69534059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.64555663\n",
      "Iteration 5, loss = 0.61185540\n",
      "Iteration 6, loss = 0.59025983\n",
      "Iteration 7, loss = 0.57654422\n",
      "Iteration 8, loss = 0.56821012\n",
      "Iteration 9, loss = 0.56345974\n",
      "Iteration 10, loss = 0.56058299\n",
      "Iteration 11, loss = 0.55901938\n",
      "Iteration 12, loss = 0.55819182\n",
      "Iteration 13, loss = 0.55774789\n",
      "Iteration 14, loss = 0.55751780\n",
      "Iteration 15, loss = 0.55740018\n",
      "Iteration 16, loss = 0.55734155\n",
      "Iteration 17, loss = 0.55731247\n",
      "Iteration 18, loss = 0.55730103\n",
      "Iteration 19, loss = 0.55730670\n",
      "Iteration 20, loss = 0.55731141\n",
      "Iteration 21, loss = 0.55732007\n",
      "Iteration 22, loss = 0.55731377\n",
      "Iteration 23, loss = 0.55732157\n",
      "Iteration 24, loss = 0.55730358\n",
      "Iteration 25, loss = 0.55731344\n",
      "Iteration 26, loss = 0.55734169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04432350\n",
      "Iteration 2, loss = 0.76589681\n",
      "Iteration 3, loss = 0.69484778\n",
      "Iteration 4, loss = 0.64533174\n",
      "Iteration 5, loss = 0.61181322\n",
      "Iteration 6, loss = 0.59018708\n",
      "Iteration 7, loss = 0.57637515\n",
      "Iteration 8, loss = 0.56824550\n",
      "Iteration 9, loss = 0.56337642\n",
      "Iteration 10, loss = 0.56057855\n",
      "Iteration 11, loss = 0.55899880\n",
      "Iteration 12, loss = 0.55816886\n",
      "Iteration 13, loss = 0.55775567\n",
      "Iteration 14, loss = 0.55750957\n",
      "Iteration 15, loss = 0.55743105\n",
      "Iteration 16, loss = 0.55734688\n",
      "Iteration 17, loss = 0.55732845\n",
      "Iteration 18, loss = 0.55731282\n",
      "Iteration 19, loss = 0.55731904\n",
      "Iteration 20, loss = 0.55731675\n",
      "Iteration 21, loss = 0.55732389\n",
      "Iteration 22, loss = 0.55733596\n",
      "Iteration 23, loss = 0.55732839\n",
      "Iteration 24, loss = 0.55731348\n",
      "Iteration 25, loss = 0.55731061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04277617\n",
      "Iteration 2, loss = 0.76507284\n",
      "Iteration 3, loss = 0.69407736\n",
      "Iteration 4, loss = 0.64460503\n",
      "Iteration 5, loss = 0.61120136\n",
      "Iteration 6, loss = 0.58957831\n",
      "Iteration 7, loss = 0.57604739\n",
      "Iteration 8, loss = 0.56796802\n",
      "Iteration 9, loss = 0.56306318\n",
      "Iteration 10, loss = 0.56033067\n",
      "Iteration 11, loss = 0.55882513\n",
      "Iteration 12, loss = 0.55796756\n",
      "Iteration 13, loss = 0.55756612\n",
      "Iteration 14, loss = 0.55732570\n",
      "Iteration 15, loss = 0.55724936\n",
      "Iteration 16, loss = 0.55716725\n",
      "Iteration 17, loss = 0.55717911\n",
      "Iteration 18, loss = 0.55713327\n",
      "Iteration 19, loss = 0.55712012\n",
      "Iteration 20, loss = 0.55713134\n",
      "Iteration 21, loss = 0.55711869\n",
      "Iteration 22, loss = 0.55714528\n",
      "Iteration 23, loss = 0.55711543\n",
      "Iteration 24, loss = 0.55712977\n",
      "Iteration 25, loss = 0.55713931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04605656\n",
      "Iteration 2, loss = 0.76666550\n",
      "Iteration 3, loss = 0.69499219\n",
      "Iteration 4, loss = 0.64525499\n",
      "Iteration 5, loss = 0.61175328\n",
      "Iteration 6, loss = 0.59023279\n",
      "Iteration 7, loss = 0.57656390\n",
      "Iteration 8, loss = 0.56822832\n",
      "Iteration 9, loss = 0.56347042\n",
      "Iteration 10, loss = 0.56058905\n",
      "Iteration 11, loss = 0.55902276\n",
      "Iteration 12, loss = 0.55819368\n",
      "Iteration 13, loss = 0.55774888\n",
      "Iteration 14, loss = 0.55751831\n",
      "Iteration 15, loss = 0.55740044\n",
      "Iteration 16, loss = 0.55734168\n",
      "Iteration 17, loss = 0.55731254\n",
      "Iteration 18, loss = 0.55730107\n",
      "Iteration 19, loss = 0.55730673\n",
      "Iteration 20, loss = 0.55731143\n",
      "Iteration 21, loss = 0.55732009\n",
      "Iteration 22, loss = 0.55731379\n",
      "Iteration 23, loss = 0.55732158\n",
      "Iteration 24, loss = 0.55730360\n",
      "Iteration 25, loss = 0.55731345\n",
      "Iteration 26, loss = 0.55734171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04683796\n",
      "Iteration 2, loss = 0.76629314\n",
      "Iteration 3, loss = 0.69449238\n",
      "Iteration 4, loss = 0.64513218\n",
      "Iteration 5, loss = 0.61175890\n",
      "Iteration 6, loss = 0.59021617\n",
      "Iteration 7, loss = 0.57640261\n",
      "Iteration 8, loss = 0.56826262\n",
      "Iteration 9, loss = 0.56338679\n",
      "Iteration 10, loss = 0.56058468\n",
      "Iteration 11, loss = 0.55900230\n",
      "Iteration 12, loss = 0.55817084\n",
      "Iteration 13, loss = 0.55775676\n",
      "Iteration 14, loss = 0.55751014\n",
      "Iteration 15, loss = 0.55743136\n",
      "Iteration 16, loss = 0.55734703\n",
      "Iteration 17, loss = 0.55732853\n",
      "Iteration 18, loss = 0.55731285\n",
      "Iteration 19, loss = 0.55731906\n",
      "Iteration 20, loss = 0.55731675\n",
      "Iteration 21, loss = 0.55732389\n",
      "Iteration 22, loss = 0.55733596\n",
      "Iteration 23, loss = 0.55732838\n",
      "Iteration 24, loss = 0.55731348\n",
      "Iteration 25, loss = 0.55731060\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04464700\n",
      "Iteration 2, loss = 0.76543858\n",
      "Iteration 3, loss = 0.69374449\n",
      "Iteration 4, loss = 0.64441038\n",
      "Iteration 5, loss = 0.61114287\n",
      "Iteration 6, loss = 0.58959931\n",
      "Iteration 7, loss = 0.57606464\n",
      "Iteration 8, loss = 0.56797833\n",
      "Iteration 9, loss = 0.56306917\n",
      "Iteration 10, loss = 0.56033407\n",
      "Iteration 11, loss = 0.55882703\n",
      "Iteration 12, loss = 0.55796857\n",
      "Iteration 13, loss = 0.55756665\n",
      "Iteration 14, loss = 0.55732596\n",
      "Iteration 15, loss = 0.55724947\n",
      "Iteration 16, loss = 0.55716728\n",
      "Iteration 17, loss = 0.55717909\n",
      "Iteration 18, loss = 0.55713325\n",
      "Iteration 19, loss = 0.55712009\n",
      "Iteration 20, loss = 0.55713131\n",
      "Iteration 21, loss = 0.55711865\n",
      "Iteration 22, loss = 0.55714524\n",
      "Iteration 23, loss = 0.55711539\n",
      "Iteration 24, loss = 0.55712972\n",
      "Iteration 25, loss = 0.55713926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08749064\n",
      "Iteration 2, loss = 0.76494241\n",
      "Iteration 3, loss = 0.69636521\n",
      "Iteration 4, loss = 0.64702621\n",
      "Iteration 5, loss = 0.61312159\n",
      "Iteration 6, loss = 0.59128677\n",
      "Iteration 7, loss = 0.57732948\n",
      "Iteration 8, loss = 0.56875426\n",
      "Iteration 9, loss = 0.56381659\n",
      "Iteration 10, loss = 0.56080690\n",
      "Iteration 11, loss = 0.55915482\n",
      "Iteration 12, loss = 0.55827129\n",
      "Iteration 13, loss = 0.55779250\n",
      "Iteration 14, loss = 0.55754187\n",
      "Iteration 15, loss = 0.55741220\n",
      "Iteration 16, loss = 0.55734778\n",
      "Iteration 17, loss = 0.55731530\n",
      "Iteration 18, loss = 0.55730222\n",
      "Iteration 19, loss = 0.55730718\n",
      "Iteration 20, loss = 0.55731139\n",
      "Iteration 21, loss = 0.55731971\n",
      "Iteration 22, loss = 0.55731359\n",
      "Iteration 23, loss = 0.55732111\n",
      "Iteration 24, loss = 0.55730346\n",
      "Iteration 25, loss = 0.55731313\n",
      "Iteration 26, loss = 0.55734102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08968292\n",
      "Iteration 2, loss = 0.76486782\n",
      "Iteration 3, loss = 0.69592795\n",
      "Iteration 4, loss = 0.64679985\n",
      "Iteration 5, loss = 0.61308711\n",
      "Iteration 6, loss = 0.59125546\n",
      "Iteration 7, loss = 0.57715766\n",
      "Iteration 8, loss = 0.56878243\n",
      "Iteration 9, loss = 0.56372817\n",
      "Iteration 10, loss = 0.56079990\n",
      "Iteration 11, loss = 0.55913207\n",
      "Iteration 12, loss = 0.55824704\n",
      "Iteration 13, loss = 0.55779983\n",
      "Iteration 14, loss = 0.55753323\n",
      "Iteration 15, loss = 0.55744372\n",
      "Iteration 16, loss = 0.55735314\n",
      "Iteration 17, loss = 0.55733130\n",
      "Iteration 18, loss = 0.55731398\n",
      "Iteration 19, loss = 0.55731939\n",
      "Iteration 20, loss = 0.55731662\n",
      "Iteration 21, loss = 0.55732349\n",
      "Iteration 22, loss = 0.55733550\n",
      "Iteration 23, loss = 0.55732778\n",
      "Iteration 24, loss = 0.55731316\n",
      "Iteration 25, loss = 0.55731033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08738671\n",
      "Iteration 2, loss = 0.76406282\n",
      "Iteration 3, loss = 0.69531983\n",
      "Iteration 4, loss = 0.64620737\n",
      "Iteration 5, loss = 0.61256731\n",
      "Iteration 6, loss = 0.59070752\n",
      "Iteration 7, loss = 0.57686753\n",
      "Iteration 8, loss = 0.56852974\n",
      "Iteration 9, loss = 0.56342998\n",
      "Iteration 10, loss = 0.56056113\n",
      "Iteration 11, loss = 0.55896479\n",
      "Iteration 12, loss = 0.55804816\n",
      "Iteration 13, loss = 0.55761207\n",
      "Iteration 14, loss = 0.55735024\n",
      "Iteration 15, loss = 0.55726257\n",
      "Iteration 16, loss = 0.55717364\n",
      "Iteration 17, loss = 0.55718093\n",
      "Iteration 18, loss = 0.55713451\n",
      "Iteration 19, loss = 0.55712031\n",
      "Iteration 20, loss = 0.55713125\n",
      "Iteration 21, loss = 0.55711857\n",
      "Iteration 22, loss = 0.55714481\n",
      "Iteration 23, loss = 0.55711533\n",
      "Iteration 24, loss = 0.55712943\n",
      "Iteration 25, loss = 0.55713882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13958195\n",
      "Iteration 2, loss = 0.76378282\n",
      "Iteration 3, loss = 0.69587276\n",
      "Iteration 4, loss = 0.64671035\n",
      "Iteration 5, loss = 0.61297478\n",
      "Iteration 6, loss = 0.59117963\n",
      "Iteration 7, loss = 0.57725758\n",
      "Iteration 8, loss = 0.56872004\n",
      "Iteration 9, loss = 0.56381091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.56082147\n",
      "Iteration 11, loss = 0.55918404\n",
      "Iteration 12, loss = 0.55830883\n",
      "Iteration 13, loss = 0.55783309\n",
      "Iteration 14, loss = 0.55758274\n",
      "Iteration 15, loss = 0.55745075\n",
      "Iteration 16, loss = 0.55738277\n",
      "Iteration 17, loss = 0.55734616\n",
      "Iteration 18, loss = 0.55732836\n",
      "Iteration 19, loss = 0.55733076\n",
      "Iteration 20, loss = 0.55733385\n",
      "Iteration 21, loss = 0.55734073\n",
      "Iteration 22, loss = 0.55733312\n",
      "Iteration 23, loss = 0.55733920\n",
      "Iteration 24, loss = 0.55731992\n",
      "Iteration 25, loss = 0.55732800\n",
      "Iteration 26, loss = 0.55735434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13168317\n",
      "Iteration 2, loss = 0.76375225\n",
      "Iteration 3, loss = 0.69542532\n",
      "Iteration 4, loss = 0.64648369\n",
      "Iteration 5, loss = 0.61289878\n",
      "Iteration 6, loss = 0.59110646\n",
      "Iteration 7, loss = 0.57706186\n",
      "Iteration 8, loss = 0.56873610\n",
      "Iteration 9, loss = 0.56371881\n",
      "Iteration 10, loss = 0.56081673\n",
      "Iteration 11, loss = 0.55916684\n",
      "Iteration 12, loss = 0.55829175\n",
      "Iteration 13, loss = 0.55784915\n",
      "Iteration 14, loss = 0.55758313\n",
      "Iteration 15, loss = 0.55749162\n",
      "Iteration 16, loss = 0.55739714\n",
      "Iteration 17, loss = 0.55737065\n",
      "Iteration 18, loss = 0.55734868\n",
      "Iteration 19, loss = 0.55735107\n",
      "Iteration 20, loss = 0.55734577\n",
      "Iteration 21, loss = 0.55734962\n",
      "Iteration 22, loss = 0.55735855\n",
      "Iteration 23, loss = 0.55734765\n",
      "Iteration 24, loss = 0.55732962\n",
      "Iteration 25, loss = 0.55732327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13115771\n",
      "Iteration 2, loss = 0.76299838\n",
      "Iteration 3, loss = 0.69481305\n",
      "Iteration 4, loss = 0.64585519\n",
      "Iteration 5, loss = 0.61233955\n",
      "Iteration 6, loss = 0.59052174\n",
      "Iteration 7, loss = 0.57674746\n",
      "Iteration 8, loss = 0.56846808\n",
      "Iteration 9, loss = 0.56341323\n",
      "Iteration 10, loss = 0.56057514\n",
      "Iteration 11, loss = 0.55899766\n",
      "Iteration 12, loss = 0.55809247\n",
      "Iteration 13, loss = 0.55766227\n",
      "Iteration 14, loss = 0.55740160\n",
      "Iteration 15, loss = 0.55731232\n",
      "Iteration 16, loss = 0.55722063\n",
      "Iteration 17, loss = 0.55722448\n",
      "Iteration 18, loss = 0.55717336\n",
      "Iteration 19, loss = 0.55715399\n",
      "Iteration 20, loss = 0.55716105\n",
      "Iteration 21, loss = 0.55714612\n",
      "Iteration 22, loss = 0.55717035\n",
      "Iteration 23, loss = 0.55713852\n",
      "Iteration 24, loss = 0.55715047\n",
      "Iteration 25, loss = 0.55715754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16322847\n",
      "Iteration 2, loss = 0.76445634\n",
      "Iteration 3, loss = 0.69617636\n",
      "Iteration 4, loss = 0.64670414\n",
      "Iteration 5, loss = 0.61286883\n",
      "Iteration 6, loss = 0.59105553\n",
      "Iteration 7, loss = 0.57715238\n",
      "Iteration 8, loss = 0.56864254\n",
      "Iteration 9, loss = 0.56376083\n",
      "Iteration 10, loss = 0.56079258\n",
      "Iteration 11, loss = 0.55916789\n",
      "Iteration 12, loss = 0.55830005\n",
      "Iteration 13, loss = 0.55782795\n",
      "Iteration 14, loss = 0.55757830\n",
      "Iteration 15, loss = 0.55744539\n",
      "Iteration 16, loss = 0.55737455\n",
      "Iteration 17, loss = 0.55733433\n",
      "Iteration 18, loss = 0.55731175\n",
      "Iteration 19, loss = 0.55730793\n",
      "Iteration 20, loss = 0.55731139\n",
      "Iteration 21, loss = 0.55731991\n",
      "Iteration 22, loss = 0.55731368\n",
      "Iteration 23, loss = 0.55732135\n",
      "Iteration 24, loss = 0.55730354\n",
      "Iteration 25, loss = 0.55731330\n",
      "Iteration 26, loss = 0.55734136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14808104\n",
      "Iteration 2, loss = 0.76427327\n",
      "Iteration 3, loss = 0.69562972\n",
      "Iteration 4, loss = 0.64643055\n",
      "Iteration 5, loss = 0.61275233\n",
      "Iteration 6, loss = 0.59094853\n",
      "Iteration 7, loss = 0.57692930\n",
      "Iteration 8, loss = 0.56863814\n",
      "Iteration 9, loss = 0.56365319\n",
      "Iteration 10, loss = 0.56077480\n",
      "Iteration 11, loss = 0.55913892\n",
      "Iteration 12, loss = 0.55827108\n",
      "Iteration 13, loss = 0.55782954\n",
      "Iteration 14, loss = 0.55756164\n",
      "Iteration 15, loss = 0.55746605\n",
      "Iteration 16, loss = 0.55736443\n",
      "Iteration 17, loss = 0.55733232\n",
      "Iteration 18, loss = 0.55731329\n",
      "Iteration 19, loss = 0.55731923\n",
      "Iteration 20, loss = 0.55731679\n",
      "Iteration 21, loss = 0.55732382\n",
      "Iteration 22, loss = 0.55733586\n",
      "Iteration 23, loss = 0.55732824\n",
      "Iteration 24, loss = 0.55731346\n",
      "Iteration 25, loss = 0.55731059\n",
      "Iteration 26, loss = 0.55735153\n",
      "Iteration 27, loss = 0.55732951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.14604625\n",
      "Iteration 2, loss = 0.76350332\n",
      "Iteration 3, loss = 0.69501757\n",
      "Iteration 4, loss = 0.64574689\n",
      "Iteration 5, loss = 0.61215463\n",
      "Iteration 6, loss = 0.59034620\n",
      "Iteration 7, loss = 0.57660261\n",
      "Iteration 8, loss = 0.56836228\n",
      "Iteration 9, loss = 0.56334187\n",
      "Iteration 10, loss = 0.56053031\n",
      "Iteration 11, loss = 0.55897097\n",
      "Iteration 12, loss = 0.55807586\n",
      "Iteration 13, loss = 0.55764941\n",
      "Iteration 14, loss = 0.55738950\n",
      "Iteration 15, loss = 0.55729883\n",
      "Iteration 16, loss = 0.55720234\n",
      "Iteration 17, loss = 0.55720186\n",
      "Iteration 18, loss = 0.55714372\n",
      "Iteration 19, loss = 0.55712046\n",
      "Iteration 20, loss = 0.55713136\n",
      "Iteration 21, loss = 0.55711873\n",
      "Iteration 22, loss = 0.55714516\n",
      "Iteration 23, loss = 0.55711548\n",
      "Iteration 24, loss = 0.55712972\n",
      "Iteration 25, loss = 0.55713920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17786814\n",
      "Iteration 2, loss = 0.76421420\n",
      "Iteration 3, loss = 0.69598926\n",
      "Iteration 4, loss = 0.64672065\n",
      "Iteration 5, loss = 0.61295145\n",
      "Iteration 6, loss = 0.59113778\n",
      "Iteration 7, loss = 0.57721611\n",
      "Iteration 8, loss = 0.56868086\n",
      "Iteration 9, loss = 0.56377673\n",
      "Iteration 10, loss = 0.56079169\n",
      "Iteration 11, loss = 0.55915645\n",
      "Iteration 12, loss = 0.55828263\n",
      "Iteration 13, loss = 0.55780905\n",
      "Iteration 14, loss = 0.55756016\n",
      "Iteration 15, loss = 0.55743020\n",
      "Iteration 16, loss = 0.55736340\n",
      "Iteration 17, loss = 0.55732802\n",
      "Iteration 18, loss = 0.55731121\n",
      "Iteration 19, loss = 0.55731221\n",
      "Iteration 20, loss = 0.55731216\n",
      "Iteration 21, loss = 0.55731984\n",
      "Iteration 22, loss = 0.55731367\n",
      "Iteration 23, loss = 0.55732127\n",
      "Iteration 24, loss = 0.55730353\n",
      "Iteration 25, loss = 0.55731326\n",
      "Iteration 26, loss = 0.55734124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16045742\n",
      "Iteration 2, loss = 0.76386074\n",
      "Iteration 3, loss = 0.69539809\n",
      "Iteration 4, loss = 0.64633966\n",
      "Iteration 5, loss = 0.61271262\n",
      "Iteration 6, loss = 0.59093223\n",
      "Iteration 7, loss = 0.57691723\n",
      "Iteration 8, loss = 0.56862228\n",
      "Iteration 9, loss = 0.56363243\n",
      "Iteration 10, loss = 0.56075075\n",
      "Iteration 11, loss = 0.55911356\n",
      "Iteration 12, loss = 0.55824652\n",
      "Iteration 13, loss = 0.55780777\n",
      "Iteration 14, loss = 0.55754440\n",
      "Iteration 15, loss = 0.55745360\n",
      "Iteration 16, loss = 0.55735859\n",
      "Iteration 17, loss = 0.55733184\n",
      "Iteration 18, loss = 0.55731341\n",
      "Iteration 19, loss = 0.55731924\n",
      "Iteration 20, loss = 0.55731672\n",
      "Iteration 21, loss = 0.55732372\n",
      "Iteration 22, loss = 0.55733576\n",
      "Iteration 23, loss = 0.55732811\n",
      "Iteration 24, loss = 0.55731337\n",
      "Iteration 25, loss = 0.55731050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15837995\n",
      "Iteration 2, loss = 0.76309586\n",
      "Iteration 3, loss = 0.69480323\n",
      "Iteration 4, loss = 0.64570639\n",
      "Iteration 5, loss = 0.61216854\n",
      "Iteration 6, loss = 0.59035759\n",
      "Iteration 7, loss = 0.57660739\n",
      "Iteration 8, loss = 0.56835547\n",
      "Iteration 9, loss = 0.56332515\n",
      "Iteration 10, loss = 0.56050647\n",
      "Iteration 11, loss = 0.55894342\n",
      "Iteration 12, loss = 0.55804758\n",
      "Iteration 13, loss = 0.55762218\n",
      "Iteration 14, loss = 0.55736533\n",
      "Iteration 15, loss = 0.55727843\n",
      "Iteration 16, loss = 0.55718744\n",
      "Iteration 17, loss = 0.55719208\n",
      "Iteration 18, loss = 0.55714024\n",
      "Iteration 19, loss = 0.55712141\n",
      "Iteration 20, loss = 0.55713132\n",
      "Iteration 21, loss = 0.55711867\n",
      "Iteration 22, loss = 0.55714507\n",
      "Iteration 23, loss = 0.55711543\n",
      "Iteration 24, loss = 0.55712964\n",
      "Iteration 25, loss = 0.55713910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17822651\n",
      "Iteration 2, loss = 0.76435803\n",
      "Iteration 3, loss = 0.69600195\n",
      "Iteration 4, loss = 0.64666802\n",
      "Iteration 5, loss = 0.61286205\n",
      "Iteration 6, loss = 0.59104496\n",
      "Iteration 7, loss = 0.57712110\n",
      "Iteration 8, loss = 0.56858708\n",
      "Iteration 9, loss = 0.56368524\n",
      "Iteration 10, loss = 0.56072154\n",
      "Iteration 11, loss = 0.55910264\n",
      "Iteration 12, loss = 0.55824042\n",
      "Iteration 13, loss = 0.55777506\n",
      "Iteration 14, loss = 0.55753241\n",
      "Iteration 15, loss = 0.55740746\n",
      "Iteration 16, loss = 0.55734534\n",
      "Iteration 17, loss = 0.55731421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.55730179\n",
      "Iteration 19, loss = 0.55730704\n",
      "Iteration 20, loss = 0.55731145\n",
      "Iteration 21, loss = 0.55731991\n",
      "Iteration 22, loss = 0.55731372\n",
      "Iteration 23, loss = 0.55732135\n",
      "Iteration 24, loss = 0.55730357\n",
      "Iteration 25, loss = 0.55731331\n",
      "Iteration 26, loss = 0.55734134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16160005\n",
      "Iteration 2, loss = 0.76415987\n",
      "Iteration 3, loss = 0.69545258\n",
      "Iteration 4, loss = 0.64632321\n",
      "Iteration 5, loss = 0.61268736\n",
      "Iteration 6, loss = 0.59088995\n",
      "Iteration 7, loss = 0.57686011\n",
      "Iteration 8, loss = 0.56854994\n",
      "Iteration 9, loss = 0.56355417\n",
      "Iteration 10, loss = 0.56068936\n",
      "Iteration 11, loss = 0.55906521\n",
      "Iteration 12, loss = 0.55820767\n",
      "Iteration 13, loss = 0.55777754\n",
      "Iteration 14, loss = 0.55752126\n",
      "Iteration 15, loss = 0.55743731\n",
      "Iteration 16, loss = 0.55734998\n",
      "Iteration 17, loss = 0.55732988\n",
      "Iteration 18, loss = 0.55731342\n",
      "Iteration 19, loss = 0.55731925\n",
      "Iteration 20, loss = 0.55731673\n",
      "Iteration 21, loss = 0.55732373\n",
      "Iteration 22, loss = 0.55733578\n",
      "Iteration 23, loss = 0.55732813\n",
      "Iteration 24, loss = 0.55731337\n",
      "Iteration 25, loss = 0.55731051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16131232\n",
      "Iteration 2, loss = 0.76385295\n",
      "Iteration 3, loss = 0.69437763\n",
      "Iteration 4, loss = 0.64475253\n",
      "Iteration 5, loss = 0.61035504\n",
      "Iteration 6, loss = 0.58716050\n",
      "Iteration 7, loss = 0.57153659\n",
      "Iteration 8, loss = 0.56078922\n",
      "Iteration 9, loss = 0.55234879\n",
      "Iteration 10, loss = 0.54468210\n",
      "Iteration 11, loss = 0.53716404\n",
      "Iteration 12, loss = 0.52974635\n",
      "Iteration 13, loss = 0.52271220\n",
      "Iteration 14, loss = 0.51767273\n",
      "Iteration 15, loss = 0.51445055\n",
      "Iteration 16, loss = 0.51221181\n",
      "Iteration 17, loss = 0.51039624\n",
      "Iteration 18, loss = 0.51027451\n",
      "Iteration 19, loss = 0.50911386\n",
      "Iteration 20, loss = 0.50812567\n",
      "Iteration 21, loss = 0.50779066\n",
      "Iteration 22, loss = 0.50660629\n",
      "Iteration 23, loss = 0.50571021\n",
      "Iteration 24, loss = 0.50594899\n",
      "Iteration 25, loss = 0.50634107\n",
      "Iteration 26, loss = 0.50638875\n",
      "Iteration 27, loss = 0.50506088\n",
      "Iteration 28, loss = 0.50623496\n",
      "Iteration 29, loss = 0.50463573\n",
      "Iteration 30, loss = 0.50405971\n",
      "Iteration 31, loss = 0.50400673\n",
      "Iteration 32, loss = 0.50379068\n",
      "Iteration 33, loss = 0.50359942\n",
      "Iteration 34, loss = 0.50338096\n",
      "Iteration 35, loss = 0.50385725\n",
      "Iteration 36, loss = 0.50315084\n",
      "Iteration 37, loss = 0.50360721\n",
      "Iteration 38, loss = 0.50287569\n",
      "Iteration 39, loss = 0.50295906\n",
      "Iteration 40, loss = 0.50319634\n",
      "Iteration 41, loss = 0.50386455\n",
      "Iteration 42, loss = 0.50261484\n",
      "Iteration 43, loss = 0.50232797\n",
      "Iteration 44, loss = 0.50239279\n",
      "Iteration 45, loss = 0.50279611\n",
      "Iteration 46, loss = 0.50267431\n",
      "Iteration 47, loss = 0.50254810\n",
      "Iteration 48, loss = 0.50253597\n",
      "Iteration 49, loss = 0.50261327\n",
      "Iteration 50, loss = 0.50307286\n",
      "Iteration 51, loss = 0.50306921\n",
      "Iteration 52, loss = 0.50254493\n",
      "Iteration 53, loss = 0.50246994\n",
      "Iteration 54, loss = 0.50282349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08546477\n",
      "Iteration 2, loss = 0.76175960\n",
      "Iteration 3, loss = 0.69181665\n",
      "Iteration 4, loss = 0.62971341\n",
      "Iteration 5, loss = 0.54870211\n",
      "Iteration 6, loss = 0.52880615\n",
      "Iteration 7, loss = 0.52470295\n",
      "Iteration 8, loss = 0.52234832\n",
      "Iteration 9, loss = 0.52191474\n",
      "Iteration 10, loss = 0.52238832\n",
      "Iteration 11, loss = 0.52098198\n",
      "Iteration 12, loss = 0.52056683\n",
      "Iteration 13, loss = 0.51984534\n",
      "Iteration 14, loss = 0.51851167\n",
      "Iteration 15, loss = 0.51653612\n",
      "Iteration 16, loss = 0.51600303\n",
      "Iteration 17, loss = 0.51315767\n",
      "Iteration 18, loss = 0.51090072\n",
      "Iteration 19, loss = 0.50843562\n",
      "Iteration 20, loss = 0.50625262\n",
      "Iteration 21, loss = 0.50450717\n",
      "Iteration 22, loss = 0.50435943\n",
      "Iteration 23, loss = 0.50383541\n",
      "Iteration 24, loss = 0.50131102\n",
      "Iteration 25, loss = 0.50093591\n",
      "Iteration 26, loss = 0.49876570\n",
      "Iteration 27, loss = 0.49798815\n",
      "Iteration 28, loss = 0.49843945\n",
      "Iteration 29, loss = 0.49471555\n",
      "Iteration 30, loss = 0.49359152\n",
      "Iteration 31, loss = 0.49053133\n",
      "Iteration 32, loss = 0.48946395\n",
      "Iteration 33, loss = 0.48912272\n",
      "Iteration 34, loss = 0.48525623\n",
      "Iteration 35, loss = 0.48155499\n",
      "Iteration 36, loss = 0.47994955\n",
      "Iteration 37, loss = 0.47556986\n",
      "Iteration 38, loss = 0.47523614\n",
      "Iteration 39, loss = 0.46598629\n",
      "Iteration 40, loss = 0.46483680\n",
      "Iteration 41, loss = 0.46351233\n",
      "Iteration 42, loss = 0.46027875\n",
      "Iteration 43, loss = 0.46095831\n",
      "Iteration 44, loss = 0.46086960\n",
      "Iteration 45, loss = 0.46025971\n",
      "Iteration 46, loss = 0.46351232\n",
      "Iteration 47, loss = 0.46134877\n",
      "Iteration 48, loss = 0.46239990\n",
      "Iteration 49, loss = 0.46125325\n",
      "Iteration 50, loss = 0.45980374\n",
      "Iteration 51, loss = 0.45972827\n",
      "Iteration 52, loss = 0.46036742\n",
      "Iteration 53, loss = 0.45863942\n",
      "Iteration 54, loss = 0.45803686\n",
      "Iteration 55, loss = 0.45995405\n",
      "Iteration 56, loss = 0.45952599\n",
      "Iteration 57, loss = 0.45835687\n",
      "Iteration 58, loss = 0.45813431\n",
      "Iteration 59, loss = 0.46087586\n",
      "Iteration 60, loss = 0.46045143\n",
      "Iteration 61, loss = 0.45899110\n",
      "Iteration 62, loss = 0.46041196\n",
      "Iteration 63, loss = 0.46003609\n",
      "Iteration 64, loss = 0.45940066\n",
      "Iteration 65, loss = 0.45729101\n",
      "Iteration 66, loss = 0.45936617\n",
      "Iteration 67, loss = 0.45764170\n",
      "Iteration 68, loss = 0.45839441\n",
      "Iteration 69, loss = 0.45765472\n",
      "Iteration 70, loss = 0.45838334\n",
      "Iteration 71, loss = 0.45830176\n",
      "Iteration 72, loss = 0.45897647\n",
      "Iteration 73, loss = 0.45754777\n",
      "Iteration 74, loss = 0.45790878\n",
      "Iteration 75, loss = 0.45797411\n",
      "Iteration 76, loss = 0.45835415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08717347\n",
      "Iteration 2, loss = 0.76185655\n",
      "Iteration 3, loss = 0.68992385\n",
      "Iteration 4, loss = 0.58538974\n",
      "Iteration 5, loss = 0.53087304\n",
      "Iteration 6, loss = 0.52127191\n",
      "Iteration 7, loss = 0.51873111\n",
      "Iteration 8, loss = 0.51752686\n",
      "Iteration 9, loss = 0.51643664\n",
      "Iteration 10, loss = 0.51735664\n",
      "Iteration 11, loss = 0.51661259\n",
      "Iteration 12, loss = 0.51566838\n",
      "Iteration 13, loss = 0.51495389\n",
      "Iteration 14, loss = 0.51568341\n",
      "Iteration 15, loss = 0.51563314\n",
      "Iteration 16, loss = 0.51474625\n",
      "Iteration 17, loss = 0.51472305\n",
      "Iteration 18, loss = 0.51469744\n",
      "Iteration 19, loss = 0.51517532\n",
      "Iteration 20, loss = 0.51469789\n",
      "Iteration 21, loss = 0.51431179\n",
      "Iteration 22, loss = 0.51375245\n",
      "Iteration 23, loss = 0.51373492\n",
      "Iteration 24, loss = 0.51282564\n",
      "Iteration 25, loss = 0.51123096\n",
      "Iteration 26, loss = 0.51040480\n",
      "Iteration 27, loss = 0.50992110\n",
      "Iteration 28, loss = 0.50972073\n",
      "Iteration 29, loss = 0.50909013\n",
      "Iteration 30, loss = 0.50929533\n",
      "Iteration 31, loss = 0.50851229\n",
      "Iteration 32, loss = 0.50880265\n",
      "Iteration 33, loss = 0.50858226\n",
      "Iteration 34, loss = 0.50899005\n",
      "Iteration 35, loss = 0.50853445\n",
      "Iteration 36, loss = 0.50845478\n",
      "Iteration 37, loss = 0.50882504\n",
      "Iteration 38, loss = 0.50814054\n",
      "Iteration 39, loss = 0.50872040\n",
      "Iteration 40, loss = 0.50906366\n",
      "Iteration 41, loss = 0.50845699\n",
      "Iteration 42, loss = 0.50748628\n",
      "Iteration 43, loss = 0.50831992\n",
      "Iteration 44, loss = 0.50782126\n",
      "Iteration 45, loss = 0.50784132\n",
      "Iteration 46, loss = 0.50676315\n",
      "Iteration 47, loss = 0.50647729\n",
      "Iteration 48, loss = 0.50640834\n",
      "Iteration 49, loss = 0.50552271\n",
      "Iteration 50, loss = 0.50675778\n",
      "Iteration 51, loss = 0.50409648\n",
      "Iteration 52, loss = 0.50330311\n",
      "Iteration 53, loss = 0.50131300\n",
      "Iteration 54, loss = 0.50047602\n",
      "Iteration 55, loss = 0.49803410\n",
      "Iteration 56, loss = 0.49961728\n",
      "Iteration 57, loss = 0.49851002\n",
      "Iteration 58, loss = 0.49602022\n",
      "Iteration 59, loss = 0.49473200\n",
      "Iteration 60, loss = 0.49468478\n",
      "Iteration 61, loss = 0.49283109\n",
      "Iteration 62, loss = 0.48775738\n",
      "Iteration 63, loss = 0.48426071\n",
      "Iteration 64, loss = 0.48256956\n",
      "Iteration 65, loss = 0.47904117\n",
      "Iteration 66, loss = 0.47726759\n",
      "Iteration 67, loss = 0.47739965\n",
      "Iteration 68, loss = 0.47729918\n",
      "Iteration 69, loss = 0.47572225\n",
      "Iteration 70, loss = 0.47576463\n",
      "Iteration 71, loss = 0.47682760\n",
      "Iteration 72, loss = 0.47639398\n",
      "Iteration 73, loss = 0.47536758\n",
      "Iteration 74, loss = 0.47508402\n",
      "Iteration 75, loss = 0.47522568\n",
      "Iteration 76, loss = 0.47663269\n",
      "Iteration 77, loss = 0.47456512\n",
      "Iteration 78, loss = 0.47569704\n",
      "Iteration 79, loss = 0.47555891\n",
      "Iteration 80, loss = 0.47521681\n",
      "Iteration 81, loss = 0.47522077\n",
      "Iteration 82, loss = 0.47542800\n",
      "Iteration 83, loss = 0.47544267\n",
      "Iteration 84, loss = 0.47607476\n",
      "Iteration 85, loss = 0.47466741\n",
      "Iteration 86, loss = 0.47456788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87, loss = 0.47487117\n",
      "Iteration 88, loss = 0.47385686\n",
      "Iteration 89, loss = 0.47519222\n",
      "Iteration 90, loss = 0.47570097\n",
      "Iteration 91, loss = 0.47459294\n",
      "Iteration 92, loss = 0.47406861\n",
      "Iteration 93, loss = 0.47455305\n",
      "Iteration 94, loss = 0.47512515\n",
      "Iteration 95, loss = 0.47537374\n",
      "Iteration 96, loss = 0.47528949\n",
      "Iteration 97, loss = 0.47406611\n",
      "Iteration 98, loss = 0.47582896\n",
      "Iteration 99, loss = 0.47471764\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.08827729\n",
      "Iteration 2, loss = 0.76091164\n",
      "Iteration 3, loss = 0.67015223\n",
      "Iteration 4, loss = 0.56091786\n",
      "Iteration 5, loss = 0.53046841\n",
      "Iteration 6, loss = 0.52500848\n",
      "Iteration 7, loss = 0.52165773\n",
      "Iteration 8, loss = 0.52067892\n",
      "Iteration 9, loss = 0.51991509\n",
      "Iteration 10, loss = 0.51977631\n",
      "Iteration 11, loss = 0.51929359\n",
      "Iteration 12, loss = 0.51827446\n",
      "Iteration 13, loss = 0.51854892\n",
      "Iteration 14, loss = 0.51870537\n",
      "Iteration 15, loss = 0.51805058\n",
      "Iteration 16, loss = 0.51736181\n",
      "Iteration 17, loss = 0.51758195\n",
      "Iteration 18, loss = 0.51804817\n",
      "Iteration 19, loss = 0.51714931\n",
      "Iteration 20, loss = 0.51771560\n",
      "Iteration 21, loss = 0.51775137\n",
      "Iteration 22, loss = 0.51724078\n",
      "Iteration 23, loss = 0.51714821\n",
      "Iteration 24, loss = 0.51774854\n",
      "Iteration 25, loss = 0.51669574\n",
      "Iteration 26, loss = 0.51784154\n",
      "Iteration 27, loss = 0.51545011\n",
      "Iteration 28, loss = 0.51388230\n",
      "Iteration 29, loss = 0.51402810\n",
      "Iteration 30, loss = 0.51293470\n",
      "Iteration 31, loss = 0.51311557\n",
      "Iteration 32, loss = 0.51318238\n",
      "Iteration 33, loss = 0.51332141\n",
      "Iteration 34, loss = 0.51301290\n",
      "Iteration 35, loss = 0.51326571\n",
      "Iteration 36, loss = 0.51288960\n",
      "Iteration 37, loss = 0.51305357\n",
      "Iteration 38, loss = 0.51311937\n",
      "Iteration 39, loss = 0.51328111\n",
      "Iteration 40, loss = 0.51253099\n",
      "Iteration 41, loss = 0.51248488\n",
      "Iteration 42, loss = 0.51202115\n",
      "Iteration 43, loss = 0.51176096\n",
      "Iteration 44, loss = 0.51208976\n",
      "Iteration 45, loss = 0.51236217\n",
      "Iteration 46, loss = 0.51203188\n",
      "Iteration 47, loss = 0.51134586\n",
      "Iteration 48, loss = 0.51146516\n",
      "Iteration 49, loss = 0.51128292\n",
      "Iteration 50, loss = 0.51228245\n",
      "Iteration 51, loss = 0.51141409\n",
      "Iteration 52, loss = 0.51121962\n",
      "Iteration 53, loss = 0.51107196\n",
      "Iteration 54, loss = 0.51078599\n",
      "Iteration 55, loss = 0.51086925\n",
      "Iteration 56, loss = 0.51052198\n",
      "Iteration 57, loss = 0.51052011\n",
      "Iteration 58, loss = 0.51242735\n",
      "Iteration 59, loss = 0.51074922\n",
      "Iteration 60, loss = 0.51049280\n",
      "Iteration 61, loss = 0.51022331\n",
      "Iteration 62, loss = 0.51071118\n",
      "Iteration 63, loss = 0.50979717\n",
      "Iteration 64, loss = 0.50895798\n",
      "Iteration 65, loss = 0.50866255\n",
      "Iteration 66, loss = 0.50865093\n",
      "Iteration 67, loss = 0.50762996\n",
      "Iteration 68, loss = 0.50615410\n",
      "Iteration 69, loss = 0.50358448\n",
      "Iteration 70, loss = 0.50022820\n",
      "Iteration 71, loss = 0.49128771\n",
      "Iteration 72, loss = 0.48189256\n",
      "Iteration 73, loss = 0.48084016\n",
      "Iteration 74, loss = 0.47940161\n",
      "Iteration 75, loss = 0.47852827\n",
      "Iteration 76, loss = 0.47813183\n",
      "Iteration 77, loss = 0.47719564\n",
      "Iteration 78, loss = 0.47672901\n",
      "Iteration 79, loss = 0.47571588\n",
      "Iteration 80, loss = 0.47617134\n",
      "Iteration 81, loss = 0.47569007\n",
      "Iteration 82, loss = 0.47468388\n",
      "Iteration 83, loss = 0.47364982\n",
      "Iteration 84, loss = 0.47292819\n",
      "Iteration 85, loss = 0.46904805\n",
      "Iteration 86, loss = 0.47094056\n",
      "Iteration 87, loss = 0.47058360\n",
      "Iteration 88, loss = 0.46886290\n",
      "Iteration 89, loss = 0.46727894\n",
      "Iteration 90, loss = 0.46659575\n",
      "Iteration 91, loss = 0.46657278\n",
      "Iteration 92, loss = 0.46591178\n",
      "Iteration 93, loss = 0.46821717\n",
      "Iteration 94, loss = 0.46571581\n",
      "Iteration 95, loss = 0.46503114\n",
      "Iteration 96, loss = 0.46290877\n",
      "Iteration 97, loss = 0.46273545\n",
      "Iteration 98, loss = 0.46400254\n",
      "Iteration 99, loss = 0.46355137\n",
      "Iteration 100, loss = 0.46675298\n",
      "Iteration 101, loss = 0.46084542\n",
      "Iteration 102, loss = 0.46287167\n",
      "Iteration 103, loss = 0.46298482\n",
      "Iteration 104, loss = 0.46311460\n",
      "Iteration 105, loss = 0.46311028\n",
      "Iteration 106, loss = 0.46164086\n",
      "Iteration 107, loss = 0.46150281\n",
      "Iteration 108, loss = 0.46098287\n",
      "Iteration 109, loss = 0.46144102\n",
      "Iteration 110, loss = 0.46168033\n",
      "Iteration 111, loss = 0.46120018\n",
      "Iteration 112, loss = 0.46147824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09631678\n",
      "Iteration 2, loss = 0.76172399\n",
      "Iteration 3, loss = 0.69187282\n",
      "Iteration 4, loss = 0.64255800\n",
      "Iteration 5, loss = 0.60940937\n",
      "Iteration 6, loss = 0.58840606\n",
      "Iteration 7, loss = 0.57523598\n",
      "Iteration 8, loss = 0.56732371\n",
      "Iteration 9, loss = 0.56288313\n",
      "Iteration 10, loss = 0.56022527\n",
      "Iteration 11, loss = 0.55880521\n",
      "Iteration 12, loss = 0.55806745\n",
      "Iteration 13, loss = 0.55767884\n",
      "Iteration 14, loss = 0.55748081\n",
      "Iteration 15, loss = 0.55738154\n",
      "Iteration 16, loss = 0.55733186\n",
      "Iteration 17, loss = 0.55730732\n",
      "Iteration 18, loss = 0.55729903\n",
      "Iteration 19, loss = 0.55730619\n",
      "Iteration 20, loss = 0.55731024\n",
      "Iteration 21, loss = 0.55731929\n",
      "Iteration 22, loss = 0.55731266\n",
      "Iteration 23, loss = 0.55732117\n",
      "Iteration 24, loss = 0.55730232\n",
      "Iteration 25, loss = 0.55731119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09773606\n",
      "Iteration 2, loss = 0.76185289\n",
      "Iteration 3, loss = 0.69160195\n",
      "Iteration 4, loss = 0.64252457\n",
      "Iteration 5, loss = 0.60947936\n",
      "Iteration 6, loss = 0.58842820\n",
      "Iteration 7, loss = 0.57511811\n",
      "Iteration 8, loss = 0.56739208\n",
      "Iteration 9, loss = 0.56282336\n",
      "Iteration 10, loss = 0.56023486\n",
      "Iteration 11, loss = 0.55879465\n",
      "Iteration 12, loss = 0.55805071\n",
      "Iteration 13, loss = 0.55768985\n",
      "Iteration 14, loss = 0.55747493\n",
      "Iteration 15, loss = 0.55741280\n",
      "Iteration 16, loss = 0.55733805\n",
      "Iteration 17, loss = 0.55732454\n",
      "Iteration 18, loss = 0.55731107\n",
      "Iteration 19, loss = 0.55731827\n",
      "Iteration 20, loss = 0.55731608\n",
      "Iteration 21, loss = 0.55732267\n",
      "Iteration 22, loss = 0.55720616\n",
      "Iteration 23, loss = 0.55329704\n",
      "Iteration 24, loss = 0.54737017\n",
      "Iteration 25, loss = 0.54297581\n",
      "Iteration 26, loss = 0.54053053\n",
      "Iteration 27, loss = 0.54032251\n",
      "Iteration 28, loss = 0.53780988\n",
      "Iteration 29, loss = 0.53546400\n",
      "Iteration 30, loss = 0.53470878\n",
      "Iteration 31, loss = 0.53335481\n",
      "Iteration 32, loss = 0.53082645\n",
      "Iteration 33, loss = 0.52779590\n",
      "Iteration 34, loss = 0.52470688\n",
      "Iteration 35, loss = 0.52219365\n",
      "Iteration 36, loss = 0.52011518\n",
      "Iteration 37, loss = 0.51637698\n",
      "Iteration 38, loss = 0.51616916\n",
      "Iteration 39, loss = 0.51568029\n",
      "Iteration 40, loss = 0.51332488\n",
      "Iteration 41, loss = 0.51163357\n",
      "Iteration 42, loss = 0.51051250\n",
      "Iteration 43, loss = 0.51109839\n",
      "Iteration 44, loss = 0.50890466\n",
      "Iteration 45, loss = 0.50728430\n",
      "Iteration 46, loss = 0.50678511\n",
      "Iteration 47, loss = 0.50549939\n",
      "Iteration 48, loss = 0.50706111\n",
      "Iteration 49, loss = 0.50500564\n",
      "Iteration 50, loss = 0.50586109\n",
      "Iteration 51, loss = 0.50490024\n",
      "Iteration 52, loss = 0.50488043\n",
      "Iteration 53, loss = 0.50280793\n",
      "Iteration 54, loss = 0.50283842\n",
      "Iteration 55, loss = 0.50274440\n",
      "Iteration 56, loss = 0.50240884\n",
      "Iteration 57, loss = 0.50491912\n",
      "Iteration 58, loss = 0.50507250\n",
      "Iteration 59, loss = 0.50255407\n",
      "Iteration 60, loss = 0.50181383\n",
      "Iteration 61, loss = 0.50073296\n",
      "Iteration 62, loss = 0.50276027\n",
      "Iteration 63, loss = 0.50055810\n",
      "Iteration 64, loss = 0.50005441\n",
      "Iteration 65, loss = 0.50038779\n",
      "Iteration 66, loss = 0.50104698\n",
      "Iteration 67, loss = 0.50084720\n",
      "Iteration 68, loss = 0.50138174\n",
      "Iteration 69, loss = 0.50046010\n",
      "Iteration 70, loss = 0.50132593\n",
      "Iteration 71, loss = 0.50117202\n",
      "Iteration 72, loss = 0.50006717\n",
      "Iteration 73, loss = 0.49999013\n",
      "Iteration 74, loss = 0.50091313\n",
      "Iteration 75, loss = 0.49957343\n",
      "Iteration 76, loss = 0.50320136\n",
      "Iteration 77, loss = 0.50091705\n",
      "Iteration 78, loss = 0.50166070\n",
      "Iteration 79, loss = 0.50056731\n",
      "Iteration 80, loss = 0.50091845\n",
      "Iteration 81, loss = 0.50097072\n",
      "Iteration 82, loss = 0.49885108\n",
      "Iteration 83, loss = 0.50091240\n",
      "Iteration 84, loss = 0.50008370\n",
      "Iteration 85, loss = 0.49791676\n",
      "Iteration 86, loss = 0.49699428\n",
      "Iteration 87, loss = 0.49676466\n",
      "Iteration 88, loss = 0.49653476\n",
      "Iteration 89, loss = 0.49675641\n",
      "Iteration 90, loss = 0.49763641\n",
      "Iteration 91, loss = 0.49747234\n",
      "Iteration 92, loss = 0.49602849\n",
      "Iteration 93, loss = 0.49945484\n",
      "Iteration 94, loss = 0.49745261\n",
      "Iteration 95, loss = 0.49704212\n",
      "Iteration 96, loss = 0.49627287\n",
      "Iteration 97, loss = 0.49556666\n",
      "Iteration 98, loss = 0.49558973\n",
      "Iteration 99, loss = 0.49413038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss = 0.49494520\n",
      "Iteration 101, loss = 0.49410373\n",
      "Iteration 102, loss = 0.49585175\n",
      "Iteration 103, loss = 0.49603054\n",
      "Iteration 104, loss = 0.49520560\n",
      "Iteration 105, loss = 0.49345462\n",
      "Iteration 106, loss = 0.49346668\n",
      "Iteration 107, loss = 0.49351514\n",
      "Iteration 108, loss = 0.49347020\n",
      "Iteration 109, loss = 0.49390197\n",
      "Iteration 110, loss = 0.49276220\n",
      "Iteration 111, loss = 0.49345304\n",
      "Iteration 112, loss = 0.49498870\n",
      "Iteration 113, loss = 0.49416044\n",
      "Iteration 114, loss = 0.49407893\n",
      "Iteration 115, loss = 0.49567285\n",
      "Iteration 116, loss = 0.49333050\n",
      "Iteration 117, loss = 0.49276878\n",
      "Iteration 118, loss = 0.49250421\n",
      "Iteration 119, loss = 0.49481579\n",
      "Iteration 120, loss = 0.49322768\n",
      "Iteration 121, loss = 0.49364310\n",
      "Iteration 122, loss = 0.49536984\n",
      "Iteration 123, loss = 0.49397179\n",
      "Iteration 124, loss = 0.49284874\n",
      "Iteration 125, loss = 0.49481647\n",
      "Iteration 126, loss = 0.49429530\n",
      "Iteration 127, loss = 0.49434640\n",
      "Iteration 128, loss = 0.49400554\n",
      "Iteration 129, loss = 0.49292751\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09701811\n",
      "Iteration 2, loss = 0.76089802\n",
      "Iteration 3, loss = 0.69101380\n",
      "Iteration 4, loss = 0.64189192\n",
      "Iteration 5, loss = 0.60893665\n",
      "Iteration 6, loss = 0.58786635\n",
      "Iteration 7, loss = 0.57480861\n",
      "Iteration 8, loss = 0.56712614\n",
      "Iteration 9, loss = 0.56251822\n",
      "Iteration 10, loss = 0.55999053\n",
      "Iteration 11, loss = 0.55862039\n",
      "Iteration 12, loss = 0.55784878\n",
      "Iteration 13, loss = 0.55749638\n",
      "Iteration 14, loss = 0.55728740\n",
      "Iteration 15, loss = 0.55722627\n",
      "Iteration 16, loss = 0.55714977\n",
      "Iteration 17, loss = 0.55716520\n",
      "Iteration 18, loss = 0.55711945\n",
      "Iteration 19, loss = 0.55710483\n",
      "Iteration 20, loss = 0.55711420\n",
      "Iteration 21, loss = 0.55689744\n",
      "Iteration 22, loss = 0.54972676\n",
      "Iteration 23, loss = 0.54684597\n",
      "Iteration 24, loss = 0.54479307\n",
      "Iteration 25, loss = 0.54350297\n",
      "Iteration 26, loss = 0.54113613\n",
      "Iteration 27, loss = 0.53931036\n",
      "Iteration 28, loss = 0.54150619\n",
      "Iteration 29, loss = 0.53674043\n",
      "Iteration 30, loss = 0.53359043\n",
      "Iteration 31, loss = 0.53085658\n",
      "Iteration 32, loss = 0.53088783\n",
      "Iteration 33, loss = 0.52886112\n",
      "Iteration 34, loss = 0.52727552\n",
      "Iteration 35, loss = 0.52452167\n",
      "Iteration 36, loss = 0.52250837\n",
      "Iteration 37, loss = 0.52195680\n",
      "Iteration 38, loss = 0.51940437\n",
      "Iteration 39, loss = 0.51656136\n",
      "Iteration 40, loss = 0.51889218\n",
      "Iteration 41, loss = 0.51619520\n",
      "Iteration 42, loss = 0.51451382\n",
      "Iteration 43, loss = 0.51474686\n",
      "Iteration 44, loss = 0.51321773\n",
      "Iteration 45, loss = 0.51316892\n",
      "Iteration 46, loss = 0.51149033\n",
      "Iteration 47, loss = 0.50982924\n",
      "Iteration 48, loss = 0.51245020\n",
      "Iteration 49, loss = 0.50898323\n",
      "Iteration 50, loss = 0.50820639\n",
      "Iteration 51, loss = 0.51129310\n",
      "Iteration 52, loss = 0.50746948\n",
      "Iteration 53, loss = 0.50761180\n",
      "Iteration 54, loss = 0.50899826\n",
      "Iteration 55, loss = 0.50639722\n",
      "Iteration 56, loss = 0.50763734\n",
      "Iteration 57, loss = 0.50708929\n",
      "Iteration 58, loss = 0.50738472\n",
      "Iteration 59, loss = 0.50745989\n",
      "Iteration 60, loss = 0.50744951\n",
      "Iteration 61, loss = 0.50613503\n",
      "Iteration 62, loss = 0.50525608\n",
      "Iteration 63, loss = 0.50649203\n",
      "Iteration 64, loss = 0.50501559\n",
      "Iteration 65, loss = 0.50464816\n",
      "Iteration 66, loss = 0.50596069\n",
      "Iteration 67, loss = 0.50453545\n",
      "Iteration 68, loss = 0.50565461\n",
      "Iteration 69, loss = 0.50568629\n",
      "Iteration 70, loss = 0.50751998\n",
      "Iteration 71, loss = 0.50537273\n",
      "Iteration 72, loss = 0.50614244\n",
      "Iteration 73, loss = 0.50470929\n",
      "Iteration 74, loss = 0.50416907\n",
      "Iteration 75, loss = 0.50474576\n",
      "Iteration 76, loss = 0.50729480\n",
      "Iteration 77, loss = 0.50432758\n",
      "Iteration 78, loss = 0.50535791\n",
      "Iteration 79, loss = 0.50493524\n",
      "Iteration 80, loss = 0.50371300\n",
      "Iteration 81, loss = 0.50366111\n",
      "Iteration 82, loss = 0.50532346\n",
      "Iteration 83, loss = 0.50390620\n",
      "Iteration 84, loss = 0.50353280\n",
      "Iteration 85, loss = 0.50382784\n",
      "Iteration 86, loss = 0.50643537\n",
      "Iteration 87, loss = 0.50489098\n",
      "Iteration 88, loss = 0.50373009\n",
      "Iteration 89, loss = 0.50358778\n",
      "Iteration 90, loss = 0.50372771\n",
      "Iteration 91, loss = 0.50421546\n",
      "Iteration 92, loss = 0.50289671\n",
      "Iteration 93, loss = 0.50386646\n",
      "Iteration 94, loss = 0.50607736\n",
      "Iteration 95, loss = 0.50821884\n",
      "Iteration 96, loss = 0.50368721\n",
      "Iteration 97, loss = 0.50430430\n",
      "Iteration 98, loss = 0.50407031\n",
      "Iteration 99, loss = 0.50383514\n",
      "Iteration 100, loss = 0.50329254\n",
      "Iteration 101, loss = 0.50298387\n",
      "Iteration 102, loss = 0.50322741\n",
      "Iteration 103, loss = 0.50639636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10219300\n",
      "Iteration 2, loss = 0.76216342\n",
      "Iteration 3, loss = 0.69181514\n",
      "Iteration 4, loss = 0.64245570\n",
      "Iteration 5, loss = 0.60936439\n",
      "Iteration 6, loss = 0.58840569\n",
      "Iteration 7, loss = 0.57523898\n",
      "Iteration 8, loss = 0.56737632\n",
      "Iteration 9, loss = 0.56290906\n",
      "Iteration 10, loss = 0.56022922\n",
      "Iteration 11, loss = 0.55877516\n",
      "Iteration 12, loss = 0.55797441\n",
      "Iteration 13, loss = 0.55768807\n",
      "Iteration 14, loss = 0.55748711\n",
      "Iteration 15, loss = 0.55738501\n",
      "Iteration 16, loss = 0.55733375\n",
      "Iteration 17, loss = 0.55730908\n",
      "Iteration 18, loss = 0.55729977\n",
      "Iteration 19, loss = 0.55730638\n",
      "Iteration 20, loss = 0.55731177\n",
      "Iteration 21, loss = 0.55732095\n",
      "Iteration 22, loss = 0.55731434\n",
      "Iteration 23, loss = 0.55732259\n",
      "Iteration 24, loss = 0.55730403\n",
      "Iteration 25, loss = 0.55731419\n",
      "Iteration 26, loss = 0.55734309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10170921\n",
      "Iteration 2, loss = 0.76216010\n",
      "Iteration 3, loss = 0.69153711\n",
      "Iteration 4, loss = 0.64237371\n",
      "Iteration 5, loss = 0.60940544\n",
      "Iteration 6, loss = 0.58840697\n",
      "Iteration 7, loss = 0.57510734\n",
      "Iteration 8, loss = 0.56737817\n",
      "Iteration 9, loss = 0.56280992\n",
      "Iteration 10, loss = 0.56021586\n",
      "Iteration 11, loss = 0.55876785\n",
      "Iteration 12, loss = 0.55803365\n",
      "Iteration 13, loss = 0.55768098\n",
      "Iteration 14, loss = 0.55747575\n",
      "Iteration 15, loss = 0.55741326\n",
      "Iteration 16, loss = 0.55733825\n",
      "Iteration 17, loss = 0.55732470\n",
      "Iteration 18, loss = 0.55731142\n",
      "Iteration 19, loss = 0.55731880\n",
      "Iteration 20, loss = 0.55731717\n",
      "Iteration 21, loss = 0.55732473\n",
      "Iteration 22, loss = 0.55733689\n",
      "Iteration 23, loss = 0.55732953\n",
      "Iteration 24, loss = 0.55731413\n",
      "Iteration 25, loss = 0.55731122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.10341432\n",
      "Iteration 2, loss = 0.76120630\n",
      "Iteration 3, loss = 0.69096415\n",
      "Iteration 4, loss = 0.64179061\n",
      "Iteration 5, loss = 0.60890556\n",
      "Iteration 6, loss = 0.58788133\n",
      "Iteration 7, loss = 0.57483680\n",
      "Iteration 8, loss = 0.56714557\n",
      "Iteration 9, loss = 0.56253077\n",
      "Iteration 10, loss = 0.55999761\n",
      "Iteration 11, loss = 0.55862344\n",
      "Iteration 12, loss = 0.55785145\n",
      "Iteration 13, loss = 0.55749884\n",
      "Iteration 14, loss = 0.55728927\n",
      "Iteration 15, loss = 0.55722573\n",
      "Iteration 16, loss = 0.55715202\n",
      "Iteration 17, loss = 0.55716831\n",
      "Iteration 18, loss = 0.55712402\n",
      "Iteration 19, loss = 0.55710725\n",
      "Iteration 20, loss = 0.55713205\n",
      "Iteration 21, loss = 0.55711899\n",
      "Iteration 22, loss = 0.55714615\n",
      "Iteration 23, loss = 0.55711568\n",
      "Iteration 24, loss = 0.55713039\n",
      "Iteration 25, loss = 0.55714017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19004552\n",
      "Iteration 2, loss = 0.76453900\n",
      "Iteration 3, loss = 0.69596850\n",
      "Iteration 4, loss = 0.64673020\n",
      "Iteration 5, loss = 0.61294996\n",
      "Iteration 6, loss = 0.59112445\n",
      "Iteration 7, loss = 0.57718844\n",
      "Iteration 8, loss = 0.56863646\n",
      "Iteration 9, loss = 0.56372545\n",
      "Iteration 10, loss = 0.56074823\n",
      "Iteration 11, loss = 0.55911890\n",
      "Iteration 12, loss = 0.55825002\n",
      "Iteration 13, loss = 0.55778049\n",
      "Iteration 14, loss = 0.55753536\n",
      "Iteration 15, loss = 0.55740895\n",
      "Iteration 16, loss = 0.55734613\n",
      "Iteration 17, loss = 0.55731458\n",
      "Iteration 18, loss = 0.55730196\n",
      "Iteration 19, loss = 0.55730712\n",
      "Iteration 20, loss = 0.55731148\n",
      "Iteration 21, loss = 0.55731989\n",
      "Iteration 22, loss = 0.55731373\n",
      "Iteration 23, loss = 0.55732132\n",
      "Iteration 24, loss = 0.55730358\n",
      "Iteration 25, loss = 0.55731330\n",
      "Iteration 26, loss = 0.55734129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17622880\n",
      "Iteration 2, loss = 0.76451240\n",
      "Iteration 3, loss = 0.69547613\n",
      "Iteration 4, loss = 0.64648911\n",
      "Iteration 5, loss = 0.61286560\n",
      "Iteration 6, loss = 0.59104135\n",
      "Iteration 7, loss = 0.57697329\n",
      "Iteration 8, loss = 0.56863077\n",
      "Iteration 9, loss = 0.56362516\n",
      "Iteration 10, loss = 0.56073451\n",
      "Iteration 11, loss = 0.55909247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.55822370\n",
      "Iteration 13, loss = 0.55778663\n",
      "Iteration 14, loss = 0.55752616\n",
      "Iteration 15, loss = 0.55743996\n",
      "Iteration 16, loss = 0.55735131\n",
      "Iteration 17, loss = 0.55733051\n",
      "Iteration 18, loss = 0.55731370\n",
      "Iteration 19, loss = 0.55731936\n",
      "Iteration 20, loss = 0.55731674\n",
      "Iteration 21, loss = 0.55732369\n",
      "Iteration 22, loss = 0.55733572\n",
      "Iteration 23, loss = 0.55732805\n",
      "Iteration 24, loss = 0.55731334\n",
      "Iteration 25, loss = 0.55731048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16527807\n",
      "Iteration 2, loss = 0.76361870\n",
      "Iteration 3, loss = 0.69483411\n",
      "Iteration 4, loss = 0.64576444\n",
      "Iteration 5, loss = 0.61222619\n",
      "Iteration 6, loss = 0.59039730\n",
      "Iteration 7, loss = 0.57661494\n",
      "Iteration 8, loss = 0.56833134\n",
      "Iteration 9, loss = 0.56329203\n",
      "Iteration 10, loss = 0.56047358\n",
      "Iteration 11, loss = 0.55891133\n",
      "Iteration 12, loss = 0.55801710\n",
      "Iteration 13, loss = 0.55759429\n",
      "Iteration 14, loss = 0.55734073\n",
      "Iteration 15, loss = 0.55725747\n",
      "Iteration 16, loss = 0.55717120\n",
      "Iteration 17, loss = 0.55718029\n",
      "Iteration 18, loss = 0.55713412\n",
      "Iteration 19, loss = 0.55712033\n",
      "Iteration 20, loss = 0.55713139\n",
      "Iteration 21, loss = 0.55711872\n",
      "Iteration 22, loss = 0.55714510\n",
      "Iteration 23, loss = 0.55711548\n",
      "Iteration 24, loss = 0.55712968\n",
      "Iteration 25, loss = 0.55713913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19783870\n",
      "Iteration 2, loss = 0.76543321\n",
      "Iteration 3, loss = 0.69617136\n",
      "Iteration 4, loss = 0.64655931\n",
      "Iteration 5, loss = 0.61283513\n",
      "Iteration 6, loss = 0.59105379\n",
      "Iteration 7, loss = 0.57714876\n",
      "Iteration 8, loss = 0.56862463\n",
      "Iteration 9, loss = 0.56372872\n",
      "Iteration 10, loss = 0.56075032\n",
      "Iteration 11, loss = 0.55911992\n",
      "Iteration 12, loss = 0.55825052\n",
      "Iteration 13, loss = 0.55778073\n",
      "Iteration 14, loss = 0.55753549\n",
      "Iteration 15, loss = 0.55740903\n",
      "Iteration 16, loss = 0.55734619\n",
      "Iteration 17, loss = 0.55731463\n",
      "Iteration 18, loss = 0.55730201\n",
      "Iteration 19, loss = 0.55730717\n",
      "Iteration 20, loss = 0.55731153\n",
      "Iteration 21, loss = 0.55731994\n",
      "Iteration 22, loss = 0.55731377\n",
      "Iteration 23, loss = 0.55732137\n",
      "Iteration 24, loss = 0.55730363\n",
      "Iteration 25, loss = 0.55731335\n",
      "Iteration 26, loss = 0.55734134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.18337415\n",
      "Iteration 2, loss = 0.76517486\n",
      "Iteration 3, loss = 0.69572327\n",
      "Iteration 4, loss = 0.64629401\n",
      "Iteration 5, loss = 0.61270927\n",
      "Iteration 6, loss = 0.59093939\n",
      "Iteration 7, loss = 0.57691833\n",
      "Iteration 8, loss = 0.56861309\n",
      "Iteration 9, loss = 0.56361481\n",
      "Iteration 10, loss = 0.56072725\n",
      "Iteration 11, loss = 0.55908774\n",
      "Iteration 12, loss = 0.55822077\n",
      "Iteration 13, loss = 0.55778492\n",
      "Iteration 14, loss = 0.55752522\n",
      "Iteration 15, loss = 0.55743946\n",
      "Iteration 16, loss = 0.55735107\n",
      "Iteration 17, loss = 0.55733041\n",
      "Iteration 18, loss = 0.55731368\n",
      "Iteration 19, loss = 0.55731938\n",
      "Iteration 20, loss = 0.55731678\n",
      "Iteration 21, loss = 0.55732374\n",
      "Iteration 22, loss = 0.55733578\n",
      "Iteration 23, loss = 0.55732811\n",
      "Iteration 24, loss = 0.55731339\n",
      "Iteration 25, loss = 0.55731053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17394349\n",
      "Iteration 2, loss = 0.76445609\n",
      "Iteration 3, loss = 0.69508186\n",
      "Iteration 4, loss = 0.64561752\n",
      "Iteration 5, loss = 0.61211619\n",
      "Iteration 6, loss = 0.59033860\n",
      "Iteration 7, loss = 0.57659114\n",
      "Iteration 8, loss = 0.56833561\n",
      "Iteration 9, loss = 0.56330078\n",
      "Iteration 10, loss = 0.56047878\n",
      "Iteration 11, loss = 0.55891433\n",
      "Iteration 12, loss = 0.55801876\n",
      "Iteration 13, loss = 0.55759520\n",
      "Iteration 14, loss = 0.55734119\n",
      "Iteration 15, loss = 0.55725771\n",
      "Iteration 16, loss = 0.55717130\n",
      "Iteration 17, loss = 0.55718030\n",
      "Iteration 18, loss = 0.55713412\n",
      "Iteration 19, loss = 0.55712031\n",
      "Iteration 20, loss = 0.55713137\n",
      "Iteration 21, loss = 0.55711870\n",
      "Iteration 22, loss = 0.55714507\n",
      "Iteration 23, loss = 0.55711545\n",
      "Iteration 24, loss = 0.55712965\n",
      "Iteration 25, loss = 0.55713909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27622218\n",
      "Iteration 2, loss = 0.76534057\n",
      "Iteration 3, loss = 0.69792577\n",
      "Iteration 4, loss = 0.64864265\n",
      "Iteration 5, loss = 0.61451958\n",
      "Iteration 6, loss = 0.59228906\n",
      "Iteration 7, loss = 0.57803572\n",
      "Iteration 8, loss = 0.56923447\n",
      "Iteration 9, loss = 0.56413114\n",
      "Iteration 10, loss = 0.56100453\n",
      "Iteration 11, loss = 0.55927476\n",
      "Iteration 12, loss = 0.55834201\n",
      "Iteration 13, loss = 0.55783247\n",
      "Iteration 14, loss = 0.55756362\n",
      "Iteration 15, loss = 0.55742320\n",
      "Iteration 16, loss = 0.55735354\n",
      "Iteration 17, loss = 0.55731797\n",
      "Iteration 18, loss = 0.55730337\n",
      "Iteration 19, loss = 0.55730767\n",
      "Iteration 20, loss = 0.55731143\n",
      "Iteration 21, loss = 0.55731944\n",
      "Iteration 22, loss = 0.55731347\n",
      "Iteration 23, loss = 0.55732074\n",
      "Iteration 24, loss = 0.55730336\n",
      "Iteration 25, loss = 0.55731288\n",
      "Iteration 26, loss = 0.55734048\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25334463\n",
      "Iteration 2, loss = 0.76516568\n",
      "Iteration 3, loss = 0.69719281\n",
      "Iteration 4, loss = 0.64811982\n",
      "Iteration 5, loss = 0.61415779\n",
      "Iteration 6, loss = 0.59199262\n",
      "Iteration 7, loss = 0.57767880\n",
      "Iteration 8, loss = 0.56913604\n",
      "Iteration 9, loss = 0.56395864\n",
      "Iteration 10, loss = 0.56094436\n",
      "Iteration 11, loss = 0.55921910\n",
      "Iteration 12, loss = 0.55829822\n",
      "Iteration 13, loss = 0.55782895\n",
      "Iteration 14, loss = 0.55754898\n",
      "Iteration 15, loss = 0.55745226\n",
      "Iteration 16, loss = 0.55735745\n",
      "Iteration 17, loss = 0.55733336\n",
      "Iteration 18, loss = 0.55731490\n",
      "Iteration 19, loss = 0.55731977\n",
      "Iteration 20, loss = 0.55731669\n",
      "Iteration 21, loss = 0.55732336\n",
      "Iteration 22, loss = 0.55733535\n",
      "Iteration 23, loss = 0.55732753\n",
      "Iteration 24, loss = 0.55731307\n",
      "Iteration 25, loss = 0.55731024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.24708374\n",
      "Iteration 2, loss = 0.76440136\n",
      "Iteration 3, loss = 0.69662520\n",
      "Iteration 4, loss = 0.64757105\n",
      "Iteration 5, loss = 0.61372680\n",
      "Iteration 6, loss = 0.59151608\n",
      "Iteration 7, loss = 0.57741734\n",
      "Iteration 8, loss = 0.56890352\n",
      "Iteration 9, loss = 0.56367331\n",
      "Iteration 10, loss = 0.56071403\n",
      "Iteration 11, loss = 0.55905768\n",
      "Iteration 12, loss = 0.55810204\n",
      "Iteration 13, loss = 0.55764299\n",
      "Iteration 14, loss = 0.55736692\n",
      "Iteration 15, loss = 0.55727167\n",
      "Iteration 16, loss = 0.55717815\n",
      "Iteration 17, loss = 0.55718241\n",
      "Iteration 18, loss = 0.55713552\n",
      "Iteration 19, loss = 0.55712059\n",
      "Iteration 20, loss = 0.55713134\n",
      "Iteration 21, loss = 0.55711862\n",
      "Iteration 22, loss = 0.55714466\n",
      "Iteration 23, loss = 0.55711538\n",
      "Iteration 24, loss = 0.55712934\n",
      "Iteration 25, loss = 0.55713863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96099960\n",
      "Iteration 2, loss = 0.76030529\n",
      "Iteration 3, loss = 0.68917216\n",
      "Iteration 4, loss = 0.63988510\n",
      "Iteration 5, loss = 0.60720343\n",
      "Iteration 6, loss = 0.58676822\n",
      "Iteration 7, loss = 0.57410770\n",
      "Iteration 8, loss = 0.56639665\n",
      "Iteration 9, loss = 0.53885256\n",
      "Iteration 10, loss = 0.50907252\n",
      "Iteration 11, loss = 0.49639686\n",
      "Iteration 12, loss = 0.49162270\n",
      "Iteration 13, loss = 0.48954327\n",
      "Iteration 14, loss = 0.48594361\n",
      "Iteration 15, loss = 0.48328845\n",
      "Iteration 16, loss = 0.48192197\n",
      "Iteration 17, loss = 0.48115931\n",
      "Iteration 18, loss = 0.48291489\n",
      "Iteration 19, loss = 0.48082090\n",
      "Iteration 20, loss = 0.47910716\n",
      "Iteration 21, loss = 0.47893985\n",
      "Iteration 22, loss = 0.47848679\n",
      "Iteration 23, loss = 0.47967947\n",
      "Iteration 24, loss = 0.47713853\n",
      "Iteration 25, loss = 0.47635323\n",
      "Iteration 26, loss = 0.47596813\n",
      "Iteration 27, loss = 0.47631184\n",
      "Iteration 28, loss = 0.47531842\n",
      "Iteration 29, loss = 0.47449080\n",
      "Iteration 30, loss = 0.47399454\n",
      "Iteration 31, loss = 0.47383048\n",
      "Iteration 32, loss = 0.47140261\n",
      "Iteration 33, loss = 0.47142883\n",
      "Iteration 34, loss = 0.46979266\n",
      "Iteration 35, loss = 0.46865224\n",
      "Iteration 36, loss = 0.46910179\n",
      "Iteration 37, loss = 0.46768253\n",
      "Iteration 38, loss = 0.46700464\n",
      "Iteration 39, loss = 0.46633828\n",
      "Iteration 40, loss = 0.46569040\n",
      "Iteration 41, loss = 0.46486814\n",
      "Iteration 42, loss = 0.46468436\n",
      "Iteration 43, loss = 0.46456535\n",
      "Iteration 44, loss = 0.46604783\n",
      "Iteration 45, loss = 0.46323684\n",
      "Iteration 46, loss = 0.46347217\n",
      "Iteration 47, loss = 0.46778462\n",
      "Iteration 48, loss = 0.46217071\n",
      "Iteration 49, loss = 0.45916656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, loss = 0.45116190\n",
      "Iteration 51, loss = 0.46409856\n",
      "Iteration 52, loss = 0.45501907\n",
      "Iteration 53, loss = 0.44695437\n",
      "Iteration 54, loss = 0.44702134\n",
      "Iteration 55, loss = 0.44461968\n",
      "Iteration 56, loss = 0.44519544\n",
      "Iteration 57, loss = 0.44557762\n",
      "Iteration 58, loss = 0.45142975\n",
      "Iteration 59, loss = 0.44953412\n",
      "Iteration 60, loss = 0.44450424\n",
      "Iteration 61, loss = 0.44457967\n",
      "Iteration 62, loss = 0.44373097\n",
      "Iteration 63, loss = 0.44420667\n",
      "Iteration 64, loss = 0.44310451\n",
      "Iteration 65, loss = 0.44040571\n",
      "Iteration 66, loss = 0.44180102\n",
      "Iteration 67, loss = 0.44150173\n",
      "Iteration 68, loss = 0.44720599\n",
      "Iteration 69, loss = 0.44147802\n",
      "Iteration 70, loss = 0.43970558\n",
      "Iteration 71, loss = 0.44465286\n",
      "Iteration 72, loss = 0.44288052\n",
      "Iteration 73, loss = 0.43910038\n",
      "Iteration 74, loss = 0.44182949\n",
      "Iteration 75, loss = 0.43973943\n",
      "Iteration 76, loss = 0.44115603\n",
      "Iteration 77, loss = 0.44608124\n",
      "Iteration 78, loss = 0.44415743\n",
      "Iteration 79, loss = 0.44304231\n",
      "Iteration 80, loss = 0.44318569\n",
      "Iteration 81, loss = 0.44027743\n",
      "Iteration 82, loss = 0.43573504\n",
      "Iteration 83, loss = 0.43371471\n",
      "Iteration 84, loss = 0.43522346\n",
      "Iteration 85, loss = 0.43398352\n",
      "Iteration 86, loss = 0.43338726\n",
      "Iteration 87, loss = 0.43073588\n",
      "Iteration 88, loss = 0.43089121\n",
      "Iteration 89, loss = 0.43080194\n",
      "Iteration 90, loss = 0.43186206\n",
      "Iteration 91, loss = 0.43021181\n",
      "Iteration 92, loss = 0.42981231\n",
      "Iteration 93, loss = 0.43084866\n",
      "Iteration 94, loss = 0.43124709\n",
      "Iteration 95, loss = 0.43144015\n",
      "Iteration 96, loss = 0.43017670\n",
      "Iteration 97, loss = 0.43128151\n",
      "Iteration 98, loss = 0.43401030\n",
      "Iteration 99, loss = 0.43117542\n",
      "Iteration 100, loss = 0.43022414\n",
      "Iteration 101, loss = 0.43117202\n",
      "Iteration 102, loss = 0.42862199\n",
      "Iteration 103, loss = 0.43099804\n",
      "Iteration 104, loss = 0.43053685\n",
      "Iteration 105, loss = 0.43411839\n",
      "Iteration 106, loss = 0.43169915\n",
      "Iteration 107, loss = 0.43310871\n",
      "Iteration 108, loss = 0.43479548\n",
      "Iteration 109, loss = 0.43153323\n",
      "Iteration 110, loss = 0.43111987\n",
      "Iteration 111, loss = 0.42951880\n",
      "Iteration 112, loss = 0.43287610\n",
      "Iteration 113, loss = 0.43166787\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96482824\n",
      "Iteration 2, loss = 0.76033374\n",
      "Iteration 3, loss = 0.68895771\n",
      "Iteration 4, loss = 0.63990710\n",
      "Iteration 5, loss = 0.60733517\n",
      "Iteration 6, loss = 0.58685291\n",
      "Iteration 7, loss = 0.57402901\n",
      "Iteration 8, loss = 0.56667989\n",
      "Iteration 9, loss = 0.56238507\n",
      "Iteration 10, loss = 0.55998438\n",
      "Iteration 11, loss = 0.55866436\n",
      "Iteration 12, loss = 0.55799183\n",
      "Iteration 13, loss = 0.55767262\n",
      "Iteration 14, loss = 0.55748055\n",
      "Iteration 15, loss = 0.55742858\n",
      "Iteration 16, loss = 0.55735685\n",
      "Iteration 17, loss = 0.55734246\n",
      "Iteration 18, loss = 0.55731102\n",
      "Iteration 19, loss = 0.54884774\n",
      "Iteration 20, loss = 0.52031222\n",
      "Iteration 21, loss = 0.49978013\n",
      "Iteration 22, loss = 0.48926556\n",
      "Iteration 23, loss = 0.48307108\n",
      "Iteration 24, loss = 0.47906970\n",
      "Iteration 25, loss = 0.47624700\n",
      "Iteration 26, loss = 0.47396759\n",
      "Iteration 27, loss = 0.47314588\n",
      "Iteration 28, loss = 0.47490918\n",
      "Iteration 29, loss = 0.47116937\n",
      "Iteration 30, loss = 0.47313752\n",
      "Iteration 31, loss = 0.47073836\n",
      "Iteration 32, loss = 0.47159476\n",
      "Iteration 33, loss = 0.47028438\n",
      "Iteration 34, loss = 0.47074513\n",
      "Iteration 35, loss = 0.46839185\n",
      "Iteration 36, loss = 0.46819817\n",
      "Iteration 37, loss = 0.47188741\n",
      "Iteration 38, loss = 0.47129952\n",
      "Iteration 39, loss = 0.46725006\n",
      "Iteration 40, loss = 0.46894502\n",
      "Iteration 41, loss = 0.46543341\n",
      "Iteration 42, loss = 0.46587247\n",
      "Iteration 43, loss = 0.46513277\n",
      "Iteration 44, loss = 0.46434823\n",
      "Iteration 45, loss = 0.46521161\n",
      "Iteration 46, loss = 0.46515054\n",
      "Iteration 47, loss = 0.46441625\n",
      "Iteration 48, loss = 0.46531137\n",
      "Iteration 49, loss = 0.46373509\n",
      "Iteration 50, loss = 0.46309863\n",
      "Iteration 51, loss = 0.46296251\n",
      "Iteration 52, loss = 0.46401190\n",
      "Iteration 53, loss = 0.46277485\n",
      "Iteration 54, loss = 0.46336703\n",
      "Iteration 55, loss = 0.46343400\n",
      "Iteration 56, loss = 0.46189744\n",
      "Iteration 57, loss = 0.46121302\n",
      "Iteration 58, loss = 0.46069016\n",
      "Iteration 59, loss = 0.46377214\n",
      "Iteration 60, loss = 0.46169577\n",
      "Iteration 61, loss = 0.46106346\n",
      "Iteration 62, loss = 0.45907949\n",
      "Iteration 63, loss = 0.45783216\n",
      "Iteration 64, loss = 0.45978063\n",
      "Iteration 65, loss = 0.45860218\n",
      "Iteration 66, loss = 0.45580175\n",
      "Iteration 67, loss = 0.45734469\n",
      "Iteration 68, loss = 0.45450715\n",
      "Iteration 69, loss = 0.45640571\n",
      "Iteration 70, loss = 0.45289762\n",
      "Iteration 71, loss = 0.45193177\n",
      "Iteration 72, loss = 0.45276589\n",
      "Iteration 73, loss = 0.45171600\n",
      "Iteration 74, loss = 0.45190260\n",
      "Iteration 75, loss = 0.45289367\n",
      "Iteration 76, loss = 0.45189947\n",
      "Iteration 77, loss = 0.45356775\n",
      "Iteration 78, loss = 0.45349229\n",
      "Iteration 79, loss = 0.45274596\n",
      "Iteration 80, loss = 0.45274811\n",
      "Iteration 81, loss = 0.45343785\n",
      "Iteration 82, loss = 0.45323098\n",
      "Iteration 83, loss = 0.45415281\n",
      "Iteration 84, loss = 0.45415363\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.95915865\n",
      "Iteration 2, loss = 0.75934226\n",
      "Iteration 3, loss = 0.68824658\n",
      "Iteration 4, loss = 0.63919294\n",
      "Iteration 5, loss = 0.60673429\n",
      "Iteration 6, loss = 0.58625288\n",
      "Iteration 7, loss = 0.57370442\n",
      "Iteration 8, loss = 0.56640394\n",
      "Iteration 9, loss = 0.56207235\n",
      "Iteration 10, loss = 0.55973151\n",
      "Iteration 11, loss = 0.55320862\n",
      "Iteration 12, loss = 0.52928112\n",
      "Iteration 13, loss = 0.50787386\n",
      "Iteration 14, loss = 0.49641442\n",
      "Iteration 15, loss = 0.49017936\n",
      "Iteration 16, loss = 0.48482926\n",
      "Iteration 17, loss = 0.48161752\n",
      "Iteration 18, loss = 0.47834540\n",
      "Iteration 19, loss = 0.47726353\n",
      "Iteration 20, loss = 0.47551184\n",
      "Iteration 21, loss = 0.47419569\n",
      "Iteration 22, loss = 0.47466312\n",
      "Iteration 23, loss = 0.47366279\n",
      "Iteration 24, loss = 0.47395165\n",
      "Iteration 25, loss = 0.47286112\n",
      "Iteration 26, loss = 0.47400998\n",
      "Iteration 27, loss = 0.47232646\n",
      "Iteration 28, loss = 0.47291012\n",
      "Iteration 29, loss = 0.47273886\n",
      "Iteration 30, loss = 0.47217551\n",
      "Iteration 31, loss = 0.47174788\n",
      "Iteration 32, loss = 0.47111086\n",
      "Iteration 33, loss = 0.46983120\n",
      "Iteration 34, loss = 0.47124473\n",
      "Iteration 35, loss = 0.46882731\n",
      "Iteration 36, loss = 0.46861926\n",
      "Iteration 37, loss = 0.46733783\n",
      "Iteration 38, loss = 0.46948897\n",
      "Iteration 39, loss = 0.46347164\n",
      "Iteration 40, loss = 0.46133768\n",
      "Iteration 41, loss = 0.46216872\n",
      "Iteration 42, loss = 0.46001015\n",
      "Iteration 43, loss = 0.45693066\n",
      "Iteration 44, loss = 0.45566497\n",
      "Iteration 45, loss = 0.45636365\n",
      "Iteration 46, loss = 0.45717266\n",
      "Iteration 47, loss = 0.45679065\n",
      "Iteration 48, loss = 0.45513988\n",
      "Iteration 49, loss = 0.45675934\n",
      "Iteration 50, loss = 0.45562922\n",
      "Iteration 51, loss = 0.45368454\n",
      "Iteration 52, loss = 0.45548042\n",
      "Iteration 53, loss = 0.45372749\n",
      "Iteration 54, loss = 0.45422538\n",
      "Iteration 55, loss = 0.45309560\n",
      "Iteration 56, loss = 0.45264104\n",
      "Iteration 57, loss = 0.45286139\n",
      "Iteration 58, loss = 0.45401489\n",
      "Iteration 59, loss = 0.45390344\n",
      "Iteration 60, loss = 0.45291987\n",
      "Iteration 61, loss = 0.45130474\n",
      "Iteration 62, loss = 0.45142768\n",
      "Iteration 63, loss = 0.45098648\n",
      "Iteration 64, loss = 0.44957211\n",
      "Iteration 65, loss = 0.44870091\n",
      "Iteration 66, loss = 0.44537494\n",
      "Iteration 67, loss = 0.44516127\n",
      "Iteration 68, loss = 0.44383438\n",
      "Iteration 69, loss = 0.44299404\n",
      "Iteration 70, loss = 0.44271502\n",
      "Iteration 71, loss = 0.43846676\n",
      "Iteration 72, loss = 0.43913617\n",
      "Iteration 73, loss = 0.43969522\n",
      "Iteration 74, loss = 0.43828847\n",
      "Iteration 75, loss = 0.43833677\n",
      "Iteration 76, loss = 0.43644536\n",
      "Iteration 77, loss = 0.43778673\n",
      "Iteration 78, loss = 0.43676732\n",
      "Iteration 79, loss = 0.43830015\n",
      "Iteration 80, loss = 0.43688536\n",
      "Iteration 81, loss = 0.43583110\n",
      "Iteration 82, loss = 0.43546040\n",
      "Iteration 83, loss = 0.43410785\n",
      "Iteration 84, loss = 0.43667436\n",
      "Iteration 85, loss = 0.43398858\n",
      "Iteration 86, loss = 0.43563662\n",
      "Iteration 87, loss = 0.43478008\n",
      "Iteration 88, loss = 0.43493063\n",
      "Iteration 89, loss = 0.43204039\n",
      "Iteration 90, loss = 0.43175790\n",
      "Iteration 91, loss = 0.43603270\n",
      "Iteration 92, loss = 0.43546686\n",
      "Iteration 93, loss = 0.43613880\n",
      "Iteration 94, loss = 0.43215050\n",
      "Iteration 95, loss = 0.43387537\n",
      "Iteration 96, loss = 0.43177333\n",
      "Iteration 97, loss = 0.43346060\n",
      "Iteration 98, loss = 0.43283752\n",
      "Iteration 99, loss = 0.43326006\n",
      "Iteration 100, loss = 0.43301220\n",
      "Iteration 101, loss = 0.43432612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.97195720\n",
      "Iteration 2, loss = 0.76028917\n",
      "Iteration 3, loss = 0.68942760\n",
      "Iteration 4, loss = 0.64010090\n",
      "Iteration 5, loss = 0.60736462\n",
      "Iteration 6, loss = 0.58687771\n",
      "Iteration 7, loss = 0.57417493\n",
      "Iteration 8, loss = 0.56269979\n",
      "Iteration 9, loss = 0.52789708\n",
      "Iteration 10, loss = 0.50352186\n",
      "Iteration 11, loss = 0.49481749\n",
      "Iteration 12, loss = 0.49146152\n",
      "Iteration 13, loss = 0.48921836\n",
      "Iteration 14, loss = 0.48573755\n",
      "Iteration 15, loss = 0.48547148\n",
      "Iteration 16, loss = 0.48482103\n",
      "Iteration 17, loss = 0.48237182\n",
      "Iteration 18, loss = 0.48358185\n",
      "Iteration 19, loss = 0.48306648\n",
      "Iteration 20, loss = 0.48429143\n",
      "Iteration 21, loss = 0.48149209\n",
      "Iteration 22, loss = 0.48095219\n",
      "Iteration 23, loss = 0.48238601\n",
      "Iteration 24, loss = 0.48185824\n",
      "Iteration 25, loss = 0.48054758\n",
      "Iteration 26, loss = 0.48127839\n",
      "Iteration 27, loss = 0.47946704\n",
      "Iteration 28, loss = 0.48153986\n",
      "Iteration 29, loss = 0.48025723\n",
      "Iteration 30, loss = 0.47987894\n",
      "Iteration 31, loss = 0.48122634\n",
      "Iteration 32, loss = 0.47950881\n",
      "Iteration 33, loss = 0.47931345\n",
      "Iteration 34, loss = 0.47990789\n",
      "Iteration 35, loss = 0.47912263\n",
      "Iteration 36, loss = 0.47906262\n",
      "Iteration 37, loss = 0.47851275\n",
      "Iteration 38, loss = 0.47881822\n",
      "Iteration 39, loss = 0.47974147\n",
      "Iteration 40, loss = 0.47969506\n",
      "Iteration 41, loss = 0.47800616\n",
      "Iteration 42, loss = 0.47848325\n",
      "Iteration 43, loss = 0.48047782\n",
      "Iteration 44, loss = 0.47992001\n",
      "Iteration 45, loss = 0.47792127\n",
      "Iteration 46, loss = 0.47921788\n",
      "Iteration 47, loss = 0.47793331\n",
      "Iteration 48, loss = 0.47575459\n",
      "Iteration 49, loss = 0.47392119\n",
      "Iteration 50, loss = 0.46953060\n",
      "Iteration 51, loss = 0.46769636\n",
      "Iteration 52, loss = 0.46533433\n",
      "Iteration 53, loss = 0.46468293\n",
      "Iteration 54, loss = 0.46241088\n",
      "Iteration 55, loss = 0.46173324\n",
      "Iteration 56, loss = 0.46144218\n",
      "Iteration 57, loss = 0.45972477\n",
      "Iteration 58, loss = 0.46381872\n",
      "Iteration 59, loss = 0.46047934\n",
      "Iteration 60, loss = 0.45872386\n",
      "Iteration 61, loss = 0.45742255\n",
      "Iteration 62, loss = 0.45674825\n",
      "Iteration 63, loss = 0.45669360\n",
      "Iteration 64, loss = 0.45624238\n",
      "Iteration 65, loss = 0.45648350\n",
      "Iteration 66, loss = 0.45632841\n",
      "Iteration 67, loss = 0.45604875\n",
      "Iteration 68, loss = 0.45679739\n",
      "Iteration 69, loss = 0.45545575\n",
      "Iteration 70, loss = 0.45830555\n",
      "Iteration 71, loss = 0.45821143\n",
      "Iteration 72, loss = 0.45545585\n",
      "Iteration 73, loss = 0.45559623\n",
      "Iteration 74, loss = 0.45469911\n",
      "Iteration 75, loss = 0.45471779\n",
      "Iteration 76, loss = 0.45462544\n",
      "Iteration 77, loss = 0.45456307\n",
      "Iteration 78, loss = 0.45442239\n",
      "Iteration 79, loss = 0.45693233\n",
      "Iteration 80, loss = 0.45459213\n",
      "Iteration 81, loss = 0.45485086\n",
      "Iteration 82, loss = 0.45509880\n",
      "Iteration 83, loss = 0.45466830\n",
      "Iteration 84, loss = 0.45505836\n",
      "Iteration 85, loss = 0.45388650\n",
      "Iteration 86, loss = 0.45313698\n",
      "Iteration 87, loss = 0.44973316\n",
      "Iteration 88, loss = 0.44767547\n",
      "Iteration 89, loss = 0.44598651\n",
      "Iteration 90, loss = 0.44403614\n",
      "Iteration 91, loss = 0.44260562\n",
      "Iteration 92, loss = 0.44215409\n",
      "Iteration 93, loss = 0.44128129\n",
      "Iteration 94, loss = 0.44051472\n",
      "Iteration 95, loss = 0.44396482\n",
      "Iteration 96, loss = 0.44397969\n",
      "Iteration 97, loss = 0.43967920\n",
      "Iteration 98, loss = 0.44295565\n",
      "Iteration 99, loss = 0.44248895\n",
      "Iteration 100, loss = 0.43816904\n",
      "Iteration 101, loss = 0.44243918\n",
      "Iteration 102, loss = 0.44172981\n",
      "Iteration 103, loss = 0.43798748\n",
      "Iteration 104, loss = 0.43902062\n",
      "Iteration 105, loss = 0.44024264\n",
      "Iteration 106, loss = 0.44080931\n",
      "Iteration 107, loss = 0.43823767\n",
      "Iteration 108, loss = 0.43921646\n",
      "Iteration 109, loss = 0.44002760\n",
      "Iteration 110, loss = 0.43946039\n",
      "Iteration 111, loss = 0.43892470\n",
      "Iteration 112, loss = 0.44071445\n",
      "Iteration 113, loss = 0.44107253\n",
      "Iteration 114, loss = 0.43923849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97935480\n",
      "Iteration 2, loss = 0.76034480\n",
      "Iteration 3, loss = 0.68928462\n",
      "Iteration 4, loss = 0.62090989\n",
      "Iteration 5, loss = 0.52627220\n",
      "Iteration 6, loss = 0.49322198\n",
      "Iteration 7, loss = 0.48374653\n",
      "Iteration 8, loss = 0.47835254\n",
      "Iteration 9, loss = 0.47661636\n",
      "Iteration 10, loss = 0.47605810\n",
      "Iteration 11, loss = 0.47559050\n",
      "Iteration 12, loss = 0.47657832\n",
      "Iteration 13, loss = 0.47476781\n",
      "Iteration 14, loss = 0.47492319\n",
      "Iteration 15, loss = 0.47454623\n",
      "Iteration 16, loss = 0.47438673\n",
      "Iteration 17, loss = 0.47391030\n",
      "Iteration 18, loss = 0.47454004\n",
      "Iteration 19, loss = 0.47388622\n",
      "Iteration 20, loss = 0.47494222\n",
      "Iteration 21, loss = 0.47258443\n",
      "Iteration 22, loss = 0.47350896\n",
      "Iteration 23, loss = 0.47234029\n",
      "Iteration 24, loss = 0.47064117\n",
      "Iteration 25, loss = 0.47066270\n",
      "Iteration 26, loss = 0.46982983\n",
      "Iteration 27, loss = 0.46945151\n",
      "Iteration 28, loss = 0.46833651\n",
      "Iteration 29, loss = 0.46453251\n",
      "Iteration 30, loss = 0.46244247\n",
      "Iteration 31, loss = 0.46152887\n",
      "Iteration 32, loss = 0.46092479\n",
      "Iteration 33, loss = 0.46041380\n",
      "Iteration 34, loss = 0.45906663\n",
      "Iteration 35, loss = 0.45890311\n",
      "Iteration 36, loss = 0.45888823\n",
      "Iteration 37, loss = 0.45740617\n",
      "Iteration 38, loss = 0.45653189\n",
      "Iteration 39, loss = 0.45595671\n",
      "Iteration 40, loss = 0.45645731\n",
      "Iteration 41, loss = 0.45669700\n",
      "Iteration 42, loss = 0.45480312\n",
      "Iteration 43, loss = 0.45510696\n",
      "Iteration 44, loss = 0.45408012\n",
      "Iteration 45, loss = 0.45410437\n",
      "Iteration 46, loss = 0.45423364\n",
      "Iteration 47, loss = 0.45336413\n",
      "Iteration 48, loss = 0.45250756\n",
      "Iteration 49, loss = 0.45278263\n",
      "Iteration 50, loss = 0.45168749\n",
      "Iteration 51, loss = 0.45301541\n",
      "Iteration 52, loss = 0.45260074\n",
      "Iteration 53, loss = 0.45145946\n",
      "Iteration 54, loss = 0.45162694\n",
      "Iteration 55, loss = 0.45171231\n",
      "Iteration 56, loss = 0.45104765\n",
      "Iteration 57, loss = 0.45102151\n",
      "Iteration 58, loss = 0.45070809\n",
      "Iteration 59, loss = 0.45205119\n",
      "Iteration 60, loss = 0.44955815\n",
      "Iteration 61, loss = 0.45109845\n",
      "Iteration 62, loss = 0.45013561\n",
      "Iteration 63, loss = 0.45148734\n",
      "Iteration 64, loss = 0.45371065\n",
      "Iteration 65, loss = 0.45017510\n",
      "Iteration 66, loss = 0.44951323\n",
      "Iteration 67, loss = 0.45124406\n",
      "Iteration 68, loss = 0.45003041\n",
      "Iteration 69, loss = 0.45128323\n",
      "Iteration 70, loss = 0.44971766\n",
      "Iteration 71, loss = 0.44927856\n",
      "Iteration 72, loss = 0.45108804\n",
      "Iteration 73, loss = 0.44967970\n",
      "Iteration 74, loss = 0.44898508\n",
      "Iteration 75, loss = 0.44932732\n",
      "Iteration 76, loss = 0.44848149\n",
      "Iteration 77, loss = 0.44906890\n",
      "Iteration 78, loss = 0.45118272\n",
      "Iteration 79, loss = 0.44952608\n",
      "Iteration 80, loss = 0.44915768\n",
      "Iteration 81, loss = 0.44929784\n",
      "Iteration 82, loss = 0.44876823\n",
      "Iteration 83, loss = 0.44836977\n",
      "Iteration 84, loss = 0.44896234\n",
      "Iteration 85, loss = 0.44970157\n",
      "Iteration 86, loss = 0.44899847\n",
      "Iteration 87, loss = 0.44913800\n",
      "Iteration 88, loss = 0.44911924\n",
      "Iteration 89, loss = 0.44897532\n",
      "Iteration 90, loss = 0.45073123\n",
      "Iteration 91, loss = 0.44954905\n",
      "Iteration 92, loss = 0.44903350\n",
      "Iteration 93, loss = 0.44801723\n",
      "Iteration 94, loss = 0.44923317\n",
      "Iteration 95, loss = 0.45348836\n",
      "Iteration 96, loss = 0.44941062\n",
      "Iteration 97, loss = 0.44868784\n",
      "Iteration 98, loss = 0.44883501\n",
      "Iteration 99, loss = 0.45209427\n",
      "Iteration 100, loss = 0.44929025\n",
      "Iteration 101, loss = 0.44909353\n",
      "Iteration 102, loss = 0.44811468\n",
      "Iteration 103, loss = 0.44827723\n",
      "Iteration 104, loss = 0.45012710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.96965393\n",
      "Iteration 2, loss = 0.75931564\n",
      "Iteration 3, loss = 0.68838431\n",
      "Iteration 4, loss = 0.63930137\n",
      "Iteration 5, loss = 0.60680689\n",
      "Iteration 6, loss = 0.58628862\n",
      "Iteration 7, loss = 0.57363301\n",
      "Iteration 8, loss = 0.54695612\n",
      "Iteration 9, loss = 0.51165235\n",
      "Iteration 10, loss = 0.49557711\n",
      "Iteration 11, loss = 0.48914115\n",
      "Iteration 12, loss = 0.48761841\n",
      "Iteration 13, loss = 0.48556376\n",
      "Iteration 14, loss = 0.48512571\n",
      "Iteration 15, loss = 0.48222711\n",
      "Iteration 16, loss = 0.48060534\n",
      "Iteration 17, loss = 0.47960489\n",
      "Iteration 18, loss = 0.47941066\n",
      "Iteration 19, loss = 0.47779225\n",
      "Iteration 20, loss = 0.47807109\n",
      "Iteration 21, loss = 0.47799047\n",
      "Iteration 22, loss = 0.47789268\n",
      "Iteration 23, loss = 0.47823635\n",
      "Iteration 24, loss = 0.47827973\n",
      "Iteration 25, loss = 0.47721447\n",
      "Iteration 26, loss = 0.47785597\n",
      "Iteration 27, loss = 0.47682459\n",
      "Iteration 28, loss = 0.47845270\n",
      "Iteration 29, loss = 0.47766146\n",
      "Iteration 30, loss = 0.47689887\n",
      "Iteration 31, loss = 0.47700593\n",
      "Iteration 32, loss = 0.47704956\n",
      "Iteration 33, loss = 0.47574277\n",
      "Iteration 34, loss = 0.47679103\n",
      "Iteration 35, loss = 0.47533307\n",
      "Iteration 36, loss = 0.47557772\n",
      "Iteration 37, loss = 0.47639912\n",
      "Iteration 38, loss = 0.47838448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.47636196\n",
      "Iteration 40, loss = 0.47616104\n",
      "Iteration 41, loss = 0.47642323\n",
      "Iteration 42, loss = 0.47551125\n",
      "Iteration 43, loss = 0.47522290\n",
      "Iteration 44, loss = 0.47550459\n",
      "Iteration 45, loss = 0.47460411\n",
      "Iteration 46, loss = 0.47457440\n",
      "Iteration 47, loss = 0.47485132\n",
      "Iteration 48, loss = 0.47446094\n",
      "Iteration 49, loss = 0.47427058\n",
      "Iteration 50, loss = 0.47451900\n",
      "Iteration 51, loss = 0.47362969\n",
      "Iteration 52, loss = 0.47039595\n",
      "Iteration 53, loss = 0.46623619\n",
      "Iteration 54, loss = 0.46476705\n",
      "Iteration 55, loss = 0.46308083\n",
      "Iteration 56, loss = 0.46143155\n",
      "Iteration 57, loss = 0.46044232\n",
      "Iteration 58, loss = 0.45982140\n",
      "Iteration 59, loss = 0.46047580\n",
      "Iteration 60, loss = 0.45951202\n",
      "Iteration 61, loss = 0.45985063\n",
      "Iteration 62, loss = 0.45802212\n",
      "Iteration 63, loss = 0.45859738\n",
      "Iteration 64, loss = 0.45729520\n",
      "Iteration 65, loss = 0.45690615\n",
      "Iteration 66, loss = 0.45681896\n",
      "Iteration 67, loss = 0.45821297\n",
      "Iteration 68, loss = 0.45741767\n",
      "Iteration 69, loss = 0.45625814\n",
      "Iteration 70, loss = 0.45557906\n",
      "Iteration 71, loss = 0.45512924\n",
      "Iteration 72, loss = 0.45571397\n",
      "Iteration 73, loss = 0.45475376\n",
      "Iteration 74, loss = 0.45360541\n",
      "Iteration 75, loss = 0.45457484\n",
      "Iteration 76, loss = 0.45342552\n",
      "Iteration 77, loss = 0.45271864\n",
      "Iteration 78, loss = 0.45278149\n",
      "Iteration 79, loss = 0.45197213\n",
      "Iteration 80, loss = 0.45158791\n",
      "Iteration 81, loss = 0.45192767\n",
      "Iteration 82, loss = 0.45169088\n",
      "Iteration 83, loss = 0.45032642\n",
      "Iteration 84, loss = 0.45404986\n",
      "Iteration 85, loss = 0.44998509\n",
      "Iteration 86, loss = 0.44977741\n",
      "Iteration 87, loss = 0.44926257\n",
      "Iteration 88, loss = 0.45073859\n",
      "Iteration 89, loss = 0.45040422\n",
      "Iteration 90, loss = 0.45012378\n",
      "Iteration 91, loss = 0.44925504\n",
      "Iteration 92, loss = 0.44972084\n",
      "Iteration 93, loss = 0.44992121\n",
      "Iteration 94, loss = 0.44870738\n",
      "Iteration 95, loss = 0.45038243\n",
      "Iteration 96, loss = 0.44896457\n",
      "Iteration 97, loss = 0.44844736\n",
      "Iteration 98, loss = 0.44829538\n",
      "Iteration 99, loss = 0.45153923\n",
      "Iteration 100, loss = 0.44921873\n",
      "Iteration 101, loss = 0.44926669\n",
      "Iteration 102, loss = 0.44911288\n",
      "Iteration 103, loss = 0.44869568\n",
      "Iteration 104, loss = 0.44888695\n",
      "Iteration 105, loss = 0.44977466\n",
      "Iteration 106, loss = 0.45026671\n",
      "Iteration 107, loss = 0.44911114\n",
      "Iteration 108, loss = 0.44925385\n",
      "Iteration 109, loss = 0.44881287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97994154\n",
      "Iteration 2, loss = 0.76083752\n",
      "Iteration 3, loss = 0.68962280\n",
      "Iteration 4, loss = 0.64028431\n",
      "Iteration 5, loss = 0.60750456\n",
      "Iteration 6, loss = 0.58697211\n",
      "Iteration 7, loss = 0.57422664\n",
      "Iteration 8, loss = 0.56664498\n",
      "Iteration 9, loss = 0.56244847\n",
      "Iteration 10, loss = 0.55995909\n",
      "Iteration 11, loss = 0.55864894\n",
      "Iteration 12, loss = 0.55797865\n",
      "Iteration 13, loss = 0.55763088\n",
      "Iteration 14, loss = 0.55745628\n",
      "Iteration 15, loss = 0.55737069\n",
      "Iteration 16, loss = 0.55732670\n",
      "Iteration 17, loss = 0.55730619\n",
      "Iteration 18, loss = 0.55729874\n",
      "Iteration 19, loss = 0.55730609\n",
      "Iteration 20, loss = 0.55731203\n",
      "Iteration 21, loss = 0.55732164\n",
      "Iteration 22, loss = 0.55731475\n",
      "Iteration 23, loss = 0.55732336\n",
      "Iteration 24, loss = 0.55730431\n",
      "Iteration 25, loss = 0.55731472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98576027\n",
      "Iteration 2, loss = 0.76071401\n",
      "Iteration 3, loss = 0.68941196\n",
      "Iteration 4, loss = 0.64030848\n",
      "Iteration 5, loss = 0.60763455\n",
      "Iteration 6, loss = 0.58704618\n",
      "Iteration 7, loss = 0.57412805\n",
      "Iteration 8, loss = 0.56671875\n",
      "Iteration 9, loss = 0.56239281\n",
      "Iteration 10, loss = 0.55997103\n",
      "Iteration 11, loss = 0.55864027\n",
      "Iteration 12, loss = 0.55796267\n",
      "Iteration 13, loss = 0.55764157\n",
      "Iteration 14, loss = 0.55744992\n",
      "Iteration 15, loss = 0.55739984\n",
      "Iteration 16, loss = 0.55733192\n",
      "Iteration 17, loss = 0.55732201\n",
      "Iteration 18, loss = 0.55731045\n",
      "Iteration 19, loss = 0.55731864\n",
      "Iteration 20, loss = 0.55731748\n",
      "Iteration 21, loss = 0.55732533\n",
      "Iteration 22, loss = 0.55733757\n",
      "Iteration 23, loss = 0.55733036\n",
      "Iteration 24, loss = 0.55731455\n",
      "Iteration 25, loss = 0.55731160\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.97720651\n",
      "Iteration 2, loss = 0.75969789\n",
      "Iteration 3, loss = 0.68860033\n",
      "Iteration 4, loss = 0.63949369\n",
      "Iteration 5, loss = 0.60695786\n",
      "Iteration 6, loss = 0.58639393\n",
      "Iteration 7, loss = 0.57377287\n",
      "Iteration 8, loss = 0.56642124\n",
      "Iteration 9, loss = 0.56206989\n",
      "Iteration 10, loss = 0.55971803\n",
      "Iteration 11, loss = 0.55846096\n",
      "Iteration 12, loss = 0.55776206\n",
      "Iteration 13, loss = 0.55745125\n",
      "Iteration 14, loss = 0.55726594\n",
      "Iteration 15, loss = 0.55721783\n",
      "Iteration 16, loss = 0.55715249\n",
      "Iteration 17, loss = 0.55717588\n",
      "Iteration 18, loss = 0.55713076\n",
      "Iteration 19, loss = 0.55712003\n",
      "Iteration 20, loss = 0.55713185\n",
      "Iteration 21, loss = 0.55711908\n",
      "Iteration 22, loss = 0.55714673\n",
      "Iteration 23, loss = 0.55711571\n",
      "Iteration 24, loss = 0.55713077\n",
      "Iteration 25, loss = 0.55714077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99379553\n",
      "Iteration 2, loss = 0.76123959\n",
      "Iteration 3, loss = 0.69148140\n",
      "Iteration 4, loss = 0.64214131\n",
      "Iteration 5, loss = 0.60906549\n",
      "Iteration 6, loss = 0.58816970\n",
      "Iteration 7, loss = 0.57510012\n",
      "Iteration 8, loss = 0.56725522\n",
      "Iteration 9, loss = 0.56285760\n",
      "Iteration 10, loss = 0.56022854\n",
      "Iteration 11, loss = 0.55882620\n",
      "Iteration 12, loss = 0.55809677\n",
      "Iteration 13, loss = 0.55771092\n",
      "Iteration 14, loss = 0.55751099\n",
      "Iteration 15, loss = 0.55740814\n",
      "Iteration 16, loss = 0.55735177\n",
      "Iteration 17, loss = 0.55732056\n",
      "Iteration 18, loss = 0.55730382\n",
      "Iteration 19, loss = 0.55730613\n",
      "Iteration 20, loss = 0.55731174\n",
      "Iteration 21, loss = 0.55732107\n",
      "Iteration 22, loss = 0.55731436\n",
      "Iteration 23, loss = 0.55732272\n",
      "Iteration 24, loss = 0.55730403\n",
      "Iteration 25, loss = 0.55731425\n",
      "Iteration 26, loss = 0.55734329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99129733\n",
      "Iteration 2, loss = 0.76111209\n",
      "Iteration 3, loss = 0.69103721\n",
      "Iteration 4, loss = 0.64195171\n",
      "Iteration 5, loss = 0.60902484\n",
      "Iteration 6, loss = 0.58812322\n",
      "Iteration 7, loss = 0.57492216\n",
      "Iteration 8, loss = 0.56727714\n",
      "Iteration 9, loss = 0.56276695\n",
      "Iteration 10, loss = 0.56021804\n",
      "Iteration 11, loss = 0.55879945\n",
      "Iteration 12, loss = 0.55806507\n",
      "Iteration 13, loss = 0.55770505\n",
      "Iteration 14, loss = 0.55748574\n",
      "Iteration 15, loss = 0.55741540\n",
      "Iteration 16, loss = 0.55733558\n",
      "Iteration 17, loss = 0.55732352\n",
      "Iteration 18, loss = 0.55731095\n",
      "Iteration 19, loss = 0.55731862\n",
      "Iteration 20, loss = 0.55731717\n",
      "Iteration 21, loss = 0.55732483\n",
      "Iteration 22, loss = 0.55733700\n",
      "Iteration 23, loss = 0.55732970\n",
      "Iteration 24, loss = 0.55731418\n",
      "Iteration 25, loss = 0.55731125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.98760481\n",
      "Iteration 2, loss = 0.76023533\n",
      "Iteration 3, loss = 0.69024662\n",
      "Iteration 4, loss = 0.64116660\n",
      "Iteration 5, loss = 0.60836976\n",
      "Iteration 6, loss = 0.58748112\n",
      "Iteration 7, loss = 0.57457065\n",
      "Iteration 8, loss = 0.56698638\n",
      "Iteration 9, loss = 0.56245017\n",
      "Iteration 10, loss = 0.55997148\n",
      "Iteration 11, loss = 0.55862978\n",
      "Iteration 12, loss = 0.55787512\n",
      "Iteration 13, loss = 0.55752994\n",
      "Iteration 14, loss = 0.55732048\n",
      "Iteration 15, loss = 0.55725666\n",
      "Iteration 16, loss = 0.55717793\n",
      "Iteration 17, loss = 0.55718982\n",
      "Iteration 18, loss = 0.55713530\n",
      "Iteration 19, loss = 0.55711995\n",
      "Iteration 20, loss = 0.55713161\n",
      "Iteration 21, loss = 0.55711893\n",
      "Iteration 22, loss = 0.55714622\n",
      "Iteration 23, loss = 0.55711560\n",
      "Iteration 24, loss = 0.55713042\n",
      "Iteration 25, loss = 0.55714028\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.00707553\n",
      "Iteration 2, loss = 0.76165481\n",
      "Iteration 3, loss = 0.69193913\n",
      "Iteration 4, loss = 0.64252638\n",
      "Iteration 5, loss = 0.60934628\n",
      "Iteration 6, loss = 0.58835028\n",
      "Iteration 7, loss = 0.57519122\n",
      "Iteration 8, loss = 0.56727225\n",
      "Iteration 9, loss = 0.56282758\n",
      "Iteration 10, loss = 0.56019062\n",
      "Iteration 11, loss = 0.55878549\n",
      "Iteration 12, loss = 0.55805664\n",
      "Iteration 13, loss = 0.55767329\n",
      "Iteration 14, loss = 0.55747834\n",
      "Iteration 15, loss = 0.55738109\n",
      "Iteration 16, loss = 0.55733189\n",
      "Iteration 17, loss = 0.55730834\n",
      "Iteration 18, loss = 0.55729950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.55730627\n",
      "Iteration 20, loss = 0.55731177\n",
      "Iteration 21, loss = 0.55732103\n",
      "Iteration 22, loss = 0.55731437\n",
      "Iteration 23, loss = 0.55732268\n",
      "Iteration 24, loss = 0.55730405\n",
      "Iteration 25, loss = 0.55731425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.00388688\n",
      "Iteration 2, loss = 0.76143811\n",
      "Iteration 3, loss = 0.69144526\n",
      "Iteration 4, loss = 0.64228273\n",
      "Iteration 5, loss = 0.60925908\n",
      "Iteration 6, loss = 0.58826066\n",
      "Iteration 7, loss = 0.57497457\n",
      "Iteration 8, loss = 0.56725962\n",
      "Iteration 9, loss = 0.56272470\n",
      "Iteration 10, loss = 0.56017509\n",
      "Iteration 11, loss = 0.55875991\n",
      "Iteration 12, loss = 0.55803098\n",
      "Iteration 13, loss = 0.55767902\n",
      "Iteration 14, loss = 0.55746928\n",
      "Iteration 15, loss = 0.55740986\n",
      "Iteration 16, loss = 0.55733665\n",
      "Iteration 17, loss = 0.55732398\n",
      "Iteration 18, loss = 0.55731113\n",
      "Iteration 19, loss = 0.55731868\n",
      "Iteration 20, loss = 0.55731716\n",
      "Iteration 21, loss = 0.55732478\n",
      "Iteration 22, loss = 0.55733695\n",
      "Iteration 23, loss = 0.55732962\n",
      "Iteration 24, loss = 0.55731415\n",
      "Iteration 25, loss = 0.55731122\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99857065\n",
      "Iteration 2, loss = 0.76064259\n",
      "Iteration 3, loss = 0.69073238\n",
      "Iteration 4, loss = 0.64154715\n",
      "Iteration 5, loss = 0.60863023\n",
      "Iteration 6, loss = 0.58763342\n",
      "Iteration 7, loss = 0.57462789\n",
      "Iteration 8, loss = 0.56696618\n",
      "Iteration 9, loss = 0.56240660\n",
      "Iteration 10, loss = 0.55992470\n",
      "Iteration 11, loss = 0.55858303\n",
      "Iteration 12, loss = 0.55783035\n",
      "Iteration 13, loss = 0.55748911\n",
      "Iteration 14, loss = 0.55728540\n",
      "Iteration 15, loss = 0.55722799\n",
      "Iteration 16, loss = 0.55715717\n",
      "Iteration 17, loss = 0.55717670\n",
      "Iteration 18, loss = 0.55713149\n",
      "Iteration 19, loss = 0.55712000\n",
      "Iteration 20, loss = 0.55713163\n",
      "Iteration 21, loss = 0.55711893\n",
      "Iteration 22, loss = 0.55714619\n",
      "Iteration 23, loss = 0.55711561\n",
      "Iteration 24, loss = 0.55713040\n",
      "Iteration 25, loss = 0.55714024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04946335\n",
      "Iteration 2, loss = 0.76339570\n",
      "Iteration 3, loss = 0.69444621\n",
      "Iteration 4, loss = 0.64494753\n",
      "Iteration 5, loss = 0.61134812\n",
      "Iteration 6, loss = 0.58986628\n",
      "Iteration 7, loss = 0.57627935\n",
      "Iteration 8, loss = 0.56802905\n",
      "Iteration 9, loss = 0.56334075\n",
      "Iteration 10, loss = 0.56051029\n",
      "Iteration 11, loss = 0.55897784\n",
      "Iteration 12, loss = 0.55816776\n",
      "Iteration 13, loss = 0.55773273\n",
      "Iteration 14, loss = 0.55750655\n",
      "Iteration 15, loss = 0.55739462\n",
      "Iteration 16, loss = 0.55733875\n",
      "Iteration 17, loss = 0.55731130\n",
      "Iteration 18, loss = 0.55730064\n",
      "Iteration 19, loss = 0.55730664\n",
      "Iteration 20, loss = 0.55731158\n",
      "Iteration 21, loss = 0.55732042\n",
      "Iteration 22, loss = 0.55731401\n",
      "Iteration 23, loss = 0.55732195\n",
      "Iteration 24, loss = 0.55730379\n",
      "Iteration 25, loss = 0.55731374\n",
      "Iteration 26, loss = 0.55734219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04843182\n",
      "Iteration 2, loss = 0.76312094\n",
      "Iteration 3, loss = 0.69394028\n",
      "Iteration 4, loss = 0.64472018\n",
      "Iteration 5, loss = 0.61128068\n",
      "Iteration 6, loss = 0.58979757\n",
      "Iteration 7, loss = 0.57608196\n",
      "Iteration 8, loss = 0.56803647\n",
      "Iteration 9, loss = 0.56323558\n",
      "Iteration 10, loss = 0.56048699\n",
      "Iteration 11, loss = 0.55894013\n",
      "Iteration 12, loss = 0.55813089\n",
      "Iteration 13, loss = 0.55773389\n",
      "Iteration 14, loss = 0.55749802\n",
      "Iteration 15, loss = 0.55742497\n",
      "Iteration 16, loss = 0.55734396\n",
      "Iteration 17, loss = 0.55732721\n",
      "Iteration 18, loss = 0.55731240\n",
      "Iteration 19, loss = 0.55731901\n",
      "Iteration 20, loss = 0.55731695\n",
      "Iteration 21, loss = 0.55732423\n",
      "Iteration 22, loss = 0.55733633\n",
      "Iteration 23, loss = 0.55732883\n",
      "Iteration 24, loss = 0.55731376\n",
      "Iteration 25, loss = 0.55731087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04280583\n",
      "Iteration 2, loss = 0.76216896\n",
      "Iteration 3, loss = 0.69313758\n",
      "Iteration 4, loss = 0.64391993\n",
      "Iteration 5, loss = 0.61061174\n",
      "Iteration 6, loss = 0.58914507\n",
      "Iteration 7, loss = 0.57572430\n",
      "Iteration 8, loss = 0.56774260\n",
      "Iteration 9, loss = 0.56291505\n",
      "Iteration 10, loss = 0.56023751\n",
      "Iteration 11, loss = 0.55876654\n",
      "Iteration 12, loss = 0.55792971\n",
      "Iteration 13, loss = 0.55754264\n",
      "Iteration 14, loss = 0.55731330\n",
      "Iteration 15, loss = 0.55724275\n",
      "Iteration 16, loss = 0.55716412\n",
      "Iteration 17, loss = 0.55717834\n",
      "Iteration 18, loss = 0.55713274\n",
      "Iteration 19, loss = 0.55712011\n",
      "Iteration 20, loss = 0.55713147\n",
      "Iteration 21, loss = 0.55711881\n",
      "Iteration 22, loss = 0.55714559\n",
      "Iteration 23, loss = 0.55711553\n",
      "Iteration 24, loss = 0.55713000\n",
      "Iteration 25, loss = 0.55713963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28181051\n",
      "Iteration 2, loss = 0.76574841\n",
      "Iteration 3, loss = 0.69829798\n",
      "Iteration 4, loss = 0.64886272\n",
      "Iteration 5, loss = 0.61467399\n",
      "Iteration 6, loss = 0.59242203\n",
      "Iteration 7, loss = 0.57811569\n",
      "Iteration 8, loss = 0.56927912\n",
      "Iteration 9, loss = 0.56415737\n",
      "Iteration 10, loss = 0.56101780\n",
      "Iteration 11, loss = 0.55927861\n",
      "Iteration 12, loss = 0.55833888\n",
      "Iteration 13, loss = 0.55783023\n",
      "Iteration 14, loss = 0.55756237\n",
      "Iteration 15, loss = 0.55742259\n",
      "Iteration 16, loss = 0.55735327\n",
      "Iteration 17, loss = 0.55731790\n",
      "Iteration 18, loss = 0.55730341\n",
      "Iteration 19, loss = 0.55730776\n",
      "Iteration 20, loss = 0.55731157\n",
      "Iteration 21, loss = 0.55731959\n",
      "Iteration 22, loss = 0.55731362\n",
      "Iteration 23, loss = 0.55732091\n",
      "Iteration 24, loss = 0.55730351\n",
      "Iteration 25, loss = 0.55731305\n",
      "Iteration 26, loss = 0.55734067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27372659\n",
      "Iteration 2, loss = 0.76561623\n",
      "Iteration 3, loss = 0.69786451\n",
      "Iteration 4, loss = 0.64867201\n",
      "Iteration 5, loss = 0.61462233\n",
      "Iteration 6, loss = 0.59236005\n",
      "Iteration 7, loss = 0.57791712\n",
      "Iteration 8, loss = 0.56928409\n",
      "Iteration 9, loss = 0.56404163\n",
      "Iteration 10, loss = 0.56098724\n",
      "Iteration 11, loss = 0.55924414\n",
      "Iteration 12, loss = 0.55831269\n",
      "Iteration 13, loss = 0.55783709\n",
      "Iteration 14, loss = 0.55755338\n",
      "Iteration 15, loss = 0.55745467\n",
      "Iteration 16, loss = 0.55735871\n",
      "Iteration 17, loss = 0.55733401\n",
      "Iteration 18, loss = 0.55731525\n",
      "Iteration 19, loss = 0.55731998\n",
      "Iteration 20, loss = 0.55731682\n",
      "Iteration 21, loss = 0.55732345\n",
      "Iteration 22, loss = 0.55733543\n",
      "Iteration 23, loss = 0.55732759\n",
      "Iteration 24, loss = 0.55731317\n",
      "Iteration 25, loss = 0.55731034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.26501492\n",
      "Iteration 2, loss = 0.76465214\n",
      "Iteration 3, loss = 0.69700451\n",
      "Iteration 4, loss = 0.64778803\n",
      "Iteration 5, loss = 0.61386670\n",
      "Iteration 6, loss = 0.59163481\n",
      "Iteration 7, loss = 0.57750235\n",
      "Iteration 8, loss = 0.56894933\n",
      "Iteration 9, loss = 0.56369307\n",
      "Iteration 10, loss = 0.56071679\n",
      "Iteration 11, loss = 0.55905385\n",
      "Iteration 12, loss = 0.55809944\n",
      "Iteration 13, loss = 0.55764138\n",
      "Iteration 14, loss = 0.55736602\n",
      "Iteration 15, loss = 0.55727122\n",
      "Iteration 16, loss = 0.55717797\n",
      "Iteration 17, loss = 0.55718244\n",
      "Iteration 18, loss = 0.55713558\n",
      "Iteration 19, loss = 0.55712070\n",
      "Iteration 20, loss = 0.55713147\n",
      "Iteration 21, loss = 0.55711876\n",
      "Iteration 22, loss = 0.55714481\n",
      "Iteration 23, loss = 0.55711552\n",
      "Iteration 24, loss = 0.55712950\n",
      "Iteration 25, loss = 0.55713880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29769941\n",
      "Iteration 2, loss = 0.76632005\n",
      "Iteration 3, loss = 0.69855684\n",
      "Iteration 4, loss = 0.64891963\n",
      "Iteration 5, loss = 0.61473557\n",
      "Iteration 6, loss = 0.59248295\n",
      "Iteration 7, loss = 0.57816319\n",
      "Iteration 8, loss = 0.56931182\n",
      "Iteration 9, loss = 0.56417751\n",
      "Iteration 10, loss = 0.56103154\n",
      "Iteration 11, loss = 0.55929021\n",
      "Iteration 12, loss = 0.55835072\n",
      "Iteration 13, loss = 0.55783726\n",
      "Iteration 14, loss = 0.55756619\n",
      "Iteration 15, loss = 0.55742453\n",
      "Iteration 16, loss = 0.55735429\n",
      "Iteration 17, loss = 0.55731839\n",
      "Iteration 18, loss = 0.55730364\n",
      "Iteration 19, loss = 0.55730787\n",
      "Iteration 20, loss = 0.55731160\n",
      "Iteration 21, loss = 0.55731957\n",
      "Iteration 22, loss = 0.55731362\n",
      "Iteration 23, loss = 0.55732088\n",
      "Iteration 24, loss = 0.55730352\n",
      "Iteration 25, loss = 0.55731304\n",
      "Iteration 26, loss = 0.55734061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29120703\n",
      "Iteration 2, loss = 0.76626789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.69817960\n",
      "Iteration 4, loss = 0.64877528\n",
      "Iteration 5, loss = 0.61473068\n",
      "Iteration 6, loss = 0.59246356\n",
      "Iteration 7, loss = 0.57800113\n",
      "Iteration 8, loss = 0.56934875\n",
      "Iteration 9, loss = 0.56409441\n",
      "Iteration 10, loss = 0.56102830\n",
      "Iteration 11, loss = 0.55926915\n",
      "Iteration 12, loss = 0.55832746\n",
      "Iteration 13, loss = 0.55784552\n",
      "Iteration 14, loss = 0.55755795\n",
      "Iteration 15, loss = 0.55745715\n",
      "Iteration 16, loss = 0.55735995\n",
      "Iteration 17, loss = 0.55733460\n",
      "Iteration 18, loss = 0.55731550\n",
      "Iteration 19, loss = 0.55732007\n",
      "Iteration 20, loss = 0.55731681\n",
      "Iteration 21, loss = 0.55732339\n",
      "Iteration 22, loss = 0.55733536\n",
      "Iteration 23, loss = 0.55732749\n",
      "Iteration 24, loss = 0.55731311\n",
      "Iteration 25, loss = 0.55731028\n",
      "Iteration 26, loss = 0.55735037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28194128\n",
      "Iteration 2, loss = 0.76522261\n",
      "Iteration 3, loss = 0.69733092\n",
      "Iteration 4, loss = 0.64793299\n",
      "Iteration 5, loss = 0.61397663\n",
      "Iteration 6, loss = 0.59173621\n",
      "Iteration 7, loss = 0.57758156\n",
      "Iteration 8, loss = 0.56900688\n",
      "Iteration 9, loss = 0.56373650\n",
      "Iteration 10, loss = 0.56075187\n",
      "Iteration 11, loss = 0.55907985\n",
      "Iteration 12, loss = 0.55811453\n",
      "Iteration 13, loss = 0.55765003\n",
      "Iteration 14, loss = 0.55737069\n",
      "Iteration 15, loss = 0.55727377\n",
      "Iteration 16, loss = 0.55717923\n",
      "Iteration 17, loss = 0.55718286\n",
      "Iteration 18, loss = 0.55713585\n",
      "Iteration 19, loss = 0.55712078\n",
      "Iteration 20, loss = 0.55713149\n",
      "Iteration 21, loss = 0.55711876\n",
      "Iteration 22, loss = 0.55714476\n",
      "Iteration 23, loss = 0.55711552\n",
      "Iteration 24, loss = 0.55712946\n",
      "Iteration 25, loss = 0.55713873\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41023174\n",
      "Iteration 2, loss = 0.76720940\n",
      "Iteration 3, loss = 0.70087702\n",
      "Iteration 4, loss = 0.65153242\n",
      "Iteration 5, loss = 0.61696501\n",
      "Iteration 6, loss = 0.59418800\n",
      "Iteration 7, loss = 0.57938187\n",
      "Iteration 8, loss = 0.57015675\n",
      "Iteration 9, loss = 0.56474080\n",
      "Iteration 10, loss = 0.56139141\n",
      "Iteration 11, loss = 0.55951206\n",
      "Iteration 12, loss = 0.55848344\n",
      "Iteration 13, loss = 0.55791338\n",
      "Iteration 14, loss = 0.55760822\n",
      "Iteration 15, loss = 0.55744618\n",
      "Iteration 16, loss = 0.55736573\n",
      "Iteration 17, loss = 0.55732375\n",
      "Iteration 18, loss = 0.55730596\n",
      "Iteration 19, loss = 0.55730882\n",
      "Iteration 20, loss = 0.55731166\n",
      "Iteration 21, loss = 0.55731902\n",
      "Iteration 22, loss = 0.55731333\n",
      "Iteration 23, loss = 0.55732013\n",
      "Iteration 24, loss = 0.55730325\n",
      "Iteration 25, loss = 0.55731250\n",
      "Iteration 26, loss = 0.55733957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.40445813\n",
      "Iteration 2, loss = 0.76696692\n",
      "Iteration 3, loss = 0.70026044\n",
      "Iteration 4, loss = 0.65115747\n",
      "Iteration 5, loss = 0.61675153\n",
      "Iteration 6, loss = 0.59399397\n",
      "Iteration 7, loss = 0.57910525\n",
      "Iteration 8, loss = 0.57011634\n",
      "Iteration 9, loss = 0.56460490\n",
      "Iteration 10, loss = 0.56135458\n",
      "Iteration 11, loss = 0.55946925\n",
      "Iteration 12, loss = 0.55844704\n",
      "Iteration 13, loss = 0.55791450\n",
      "Iteration 14, loss = 0.55759581\n",
      "Iteration 15, loss = 0.55747786\n",
      "Iteration 16, loss = 0.55737050\n",
      "Iteration 17, loss = 0.55733963\n",
      "Iteration 18, loss = 0.55731771\n",
      "Iteration 19, loss = 0.55732092\n",
      "Iteration 20, loss = 0.55731684\n",
      "Iteration 21, loss = 0.55732297\n",
      "Iteration 22, loss = 0.55733486\n",
      "Iteration 23, loss = 0.55732675\n",
      "Iteration 24, loss = 0.55731273\n",
      "Iteration 25, loss = 0.55730993\n",
      "Iteration 26, loss = 0.55734928\n",
      "Iteration 27, loss = 0.55732796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.39228632\n",
      "Iteration 2, loss = 0.76607879\n",
      "Iteration 3, loss = 0.69948245\n",
      "Iteration 4, loss = 0.65035050\n",
      "Iteration 5, loss = 0.61606232\n",
      "Iteration 6, loss = 0.59332513\n",
      "Iteration 7, loss = 0.57871045\n",
      "Iteration 8, loss = 0.56978501\n",
      "Iteration 9, loss = 0.56425190\n",
      "Iteration 10, loss = 0.56108074\n",
      "Iteration 11, loss = 0.55928248\n",
      "Iteration 12, loss = 0.55823379\n",
      "Iteration 13, loss = 0.55771927\n",
      "Iteration 14, loss = 0.55740854\n",
      "Iteration 15, loss = 0.55729456\n",
      "Iteration 16, loss = 0.55718962\n",
      "Iteration 17, loss = 0.55718639\n",
      "Iteration 18, loss = 0.55713811\n",
      "Iteration 19, loss = 0.55712133\n",
      "Iteration 20, loss = 0.55713156\n",
      "Iteration 21, loss = 0.55711868\n",
      "Iteration 22, loss = 0.55714425\n",
      "Iteration 23, loss = 0.55711543\n",
      "Iteration 24, loss = 0.55712907\n",
      "Iteration 25, loss = 0.55713814\n",
      "Iteration 26, loss = 0.55713772\n",
      "Iteration 27, loss = 0.55715547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06064010\n",
      "Iteration 2, loss = 0.76398109\n",
      "Iteration 3, loss = 0.69503954\n",
      "Iteration 4, loss = 0.64540672\n",
      "Iteration 5, loss = 0.61163249\n",
      "Iteration 6, loss = 0.59005989\n",
      "Iteration 7, loss = 0.57642148\n",
      "Iteration 8, loss = 0.56812303\n",
      "Iteration 9, loss = 0.56339796\n",
      "Iteration 10, loss = 0.56054200\n",
      "Iteration 11, loss = 0.55899367\n",
      "Iteration 12, loss = 0.55817639\n",
      "Iteration 13, loss = 0.55773914\n",
      "Iteration 14, loss = 0.55751308\n",
      "Iteration 15, loss = 0.55739789\n",
      "Iteration 16, loss = 0.55734042\n",
      "Iteration 17, loss = 0.55731204\n",
      "Iteration 18, loss = 0.55730094\n",
      "Iteration 19, loss = 0.55730676\n",
      "Iteration 20, loss = 0.55731157\n",
      "Iteration 21, loss = 0.55732032\n",
      "Iteration 22, loss = 0.55731397\n",
      "Iteration 23, loss = 0.55732183\n",
      "Iteration 24, loss = 0.55730376\n",
      "Iteration 25, loss = 0.55731366\n",
      "Iteration 26, loss = 0.55734200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06409131\n",
      "Iteration 2, loss = 0.76386277\n",
      "Iteration 3, loss = 0.69462975\n",
      "Iteration 4, loss = 0.64521442\n",
      "Iteration 5, loss = 0.61159457\n",
      "Iteration 6, loss = 0.59004923\n",
      "Iteration 7, loss = 0.57626571\n",
      "Iteration 8, loss = 0.56816168\n",
      "Iteration 9, loss = 0.56331748\n",
      "Iteration 10, loss = 0.56053972\n",
      "Iteration 11, loss = 0.55897467\n",
      "Iteration 12, loss = 0.55815444\n",
      "Iteration 13, loss = 0.55774746\n",
      "Iteration 14, loss = 0.55750518\n",
      "Iteration 15, loss = 0.55742877\n",
      "Iteration 16, loss = 0.55734581\n",
      "Iteration 17, loss = 0.55732804\n",
      "Iteration 18, loss = 0.55731273\n",
      "Iteration 19, loss = 0.55731912\n",
      "Iteration 20, loss = 0.55731692\n",
      "Iteration 21, loss = 0.55732412\n",
      "Iteration 22, loss = 0.55733620\n",
      "Iteration 23, loss = 0.55732865\n",
      "Iteration 24, loss = 0.55731368\n",
      "Iteration 25, loss = 0.55731080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05914501\n",
      "Iteration 2, loss = 0.76307471\n",
      "Iteration 3, loss = 0.69388793\n",
      "Iteration 4, loss = 0.64442923\n",
      "Iteration 5, loss = 0.61091343\n",
      "Iteration 6, loss = 0.58938042\n",
      "Iteration 7, loss = 0.57589101\n",
      "Iteration 8, loss = 0.56785260\n",
      "Iteration 9, loss = 0.56298399\n",
      "Iteration 10, loss = 0.56027930\n",
      "Iteration 11, loss = 0.55879336\n",
      "Iteration 12, loss = 0.55794898\n",
      "Iteration 13, loss = 0.55755547\n",
      "Iteration 14, loss = 0.55732004\n",
      "Iteration 15, loss = 0.55724636\n",
      "Iteration 16, loss = 0.55716585\n",
      "Iteration 17, loss = 0.55717882\n",
      "Iteration 18, loss = 0.55713310\n",
      "Iteration 19, loss = 0.55712019\n",
      "Iteration 20, loss = 0.55713148\n",
      "Iteration 21, loss = 0.55711882\n",
      "Iteration 22, loss = 0.55714551\n",
      "Iteration 23, loss = 0.55711555\n",
      "Iteration 24, loss = 0.55712996\n",
      "Iteration 25, loss = 0.55713954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 2 min. and 25.28913450241089 sec.\n",
      "\n",
      "35 total combinations for subset length 4.\n",
      "Iteration 1, loss = 0.53959711\n",
      "Iteration 2, loss = 0.49639589\n",
      "Iteration 3, loss = 0.48211677\n",
      "Iteration 4, loss = 0.47721943\n",
      "Iteration 5, loss = 0.47120687\n",
      "Iteration 6, loss = 0.46758823\n",
      "Iteration 7, loss = 0.46396874\n",
      "Iteration 8, loss = 0.46155441\n",
      "Iteration 9, loss = 0.46054737\n",
      "Iteration 10, loss = 0.45861868\n",
      "Iteration 11, loss = 0.45844166\n",
      "Iteration 12, loss = 0.45410945\n",
      "Iteration 13, loss = 0.44894838\n",
      "Iteration 14, loss = 0.44524580\n",
      "Iteration 15, loss = 0.44362237\n",
      "Iteration 16, loss = 0.44444909\n",
      "Iteration 17, loss = 0.43943849\n",
      "Iteration 18, loss = 0.43558282\n",
      "Iteration 19, loss = 0.43355495\n",
      "Iteration 20, loss = 0.43139240\n",
      "Iteration 21, loss = 0.43018653\n",
      "Iteration 22, loss = 0.42920845\n",
      "Iteration 23, loss = 0.42603704\n",
      "Iteration 24, loss = 0.42761255\n",
      "Iteration 25, loss = 0.42599966\n",
      "Iteration 26, loss = 0.42588462\n",
      "Iteration 27, loss = 0.42384418\n",
      "Iteration 28, loss = 0.42602375\n",
      "Iteration 29, loss = 0.42359202\n",
      "Iteration 30, loss = 0.42478754\n",
      "Iteration 31, loss = 0.42547480\n",
      "Iteration 32, loss = 0.42163156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.42148308\n",
      "Iteration 34, loss = 0.42360299\n",
      "Iteration 35, loss = 0.42383206\n",
      "Iteration 36, loss = 0.42323772\n",
      "Iteration 37, loss = 0.42368809\n",
      "Iteration 38, loss = 0.42530735\n",
      "Iteration 39, loss = 0.42271663\n",
      "Iteration 40, loss = 0.42020095\n",
      "Iteration 41, loss = 0.42080166\n",
      "Iteration 42, loss = 0.42356543\n",
      "Iteration 43, loss = 0.42020085\n",
      "Iteration 44, loss = 0.41954879\n",
      "Iteration 45, loss = 0.42018387\n",
      "Iteration 46, loss = 0.42154345\n",
      "Iteration 47, loss = 0.41924440\n",
      "Iteration 48, loss = 0.41937459\n",
      "Iteration 49, loss = 0.41867100\n",
      "Iteration 50, loss = 0.41934555\n",
      "Iteration 51, loss = 0.42083398\n",
      "Iteration 52, loss = 0.42082678\n",
      "Iteration 53, loss = 0.41983490\n",
      "Iteration 54, loss = 0.42104785\n",
      "Iteration 55, loss = 0.42299160\n",
      "Iteration 56, loss = 0.42050350\n",
      "Iteration 57, loss = 0.42165743\n",
      "Iteration 58, loss = 0.41899813\n",
      "Iteration 59, loss = 0.41797179\n",
      "Iteration 60, loss = 0.42042609\n",
      "Iteration 61, loss = 0.42014273\n",
      "Iteration 62, loss = 0.41933089\n",
      "Iteration 63, loss = 0.41924374\n",
      "Iteration 64, loss = 0.41925697\n",
      "Iteration 65, loss = 0.42052127\n",
      "Iteration 66, loss = 0.41730642\n",
      "Iteration 67, loss = 0.41884161\n",
      "Iteration 68, loss = 0.41733510\n",
      "Iteration 69, loss = 0.41869475\n",
      "Iteration 70, loss = 0.42248803\n",
      "Iteration 71, loss = 0.41885822\n",
      "Iteration 72, loss = 0.41919687\n",
      "Iteration 73, loss = 0.42017196\n",
      "Iteration 74, loss = 0.42010037\n",
      "Iteration 75, loss = 0.41697130\n",
      "Iteration 76, loss = 0.41715245\n",
      "Iteration 77, loss = 0.41680821\n",
      "Iteration 78, loss = 0.41711319\n",
      "Iteration 79, loss = 0.42148131\n",
      "Iteration 80, loss = 0.41614912\n",
      "Iteration 81, loss = 0.41766251\n",
      "Iteration 82, loss = 0.41753902\n",
      "Iteration 83, loss = 0.41774380\n",
      "Iteration 84, loss = 0.42090908\n",
      "Iteration 85, loss = 0.41584822\n",
      "Iteration 86, loss = 0.41870710\n",
      "Iteration 87, loss = 0.41769961\n",
      "Iteration 88, loss = 0.42125305\n",
      "Iteration 89, loss = 0.41607489\n",
      "Iteration 90, loss = 0.42125167\n",
      "Iteration 91, loss = 0.41779802\n",
      "Iteration 92, loss = 0.41782547\n",
      "Iteration 93, loss = 0.41632066\n",
      "Iteration 94, loss = 0.41868753\n",
      "Iteration 95, loss = 0.41593822\n",
      "Iteration 96, loss = 0.41497842\n",
      "Iteration 97, loss = 0.41542017\n",
      "Iteration 98, loss = 0.41747786\n",
      "Iteration 99, loss = 0.41535387\n",
      "Iteration 100, loss = 0.41395537\n",
      "Iteration 101, loss = 0.41617290\n",
      "Iteration 102, loss = 0.41709439\n",
      "Iteration 103, loss = 0.41592623\n",
      "Iteration 104, loss = 0.41458487\n",
      "Iteration 105, loss = 0.41555395\n",
      "Iteration 106, loss = 0.41667528\n",
      "Iteration 107, loss = 0.41865616\n",
      "Iteration 108, loss = 0.41764416\n",
      "Iteration 109, loss = 0.41652355\n",
      "Iteration 110, loss = 0.41533068\n",
      "Iteration 111, loss = 0.41645205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54000284\n",
      "Iteration 2, loss = 0.50167804\n",
      "Iteration 3, loss = 0.48030482\n",
      "Iteration 4, loss = 0.46874360\n",
      "Iteration 5, loss = 0.46332829\n",
      "Iteration 6, loss = 0.45998504\n",
      "Iteration 7, loss = 0.45921219\n",
      "Iteration 8, loss = 0.45667239\n",
      "Iteration 9, loss = 0.45506354\n",
      "Iteration 10, loss = 0.45517756\n",
      "Iteration 11, loss = 0.45435815\n",
      "Iteration 12, loss = 0.45187198\n",
      "Iteration 13, loss = 0.45082115\n",
      "Iteration 14, loss = 0.44278261\n",
      "Iteration 15, loss = 0.44457423\n",
      "Iteration 16, loss = 0.43904381\n",
      "Iteration 17, loss = 0.43955122\n",
      "Iteration 18, loss = 0.43941160\n",
      "Iteration 19, loss = 0.43821535\n",
      "Iteration 20, loss = 0.43753058\n",
      "Iteration 21, loss = 0.43791875\n",
      "Iteration 22, loss = 0.43649328\n",
      "Iteration 23, loss = 0.43776439\n",
      "Iteration 24, loss = 0.43748682\n",
      "Iteration 25, loss = 0.43510392\n",
      "Iteration 26, loss = 0.43719122\n",
      "Iteration 27, loss = 0.43541462\n",
      "Iteration 28, loss = 0.43519234\n",
      "Iteration 29, loss = 0.43589815\n",
      "Iteration 30, loss = 0.43352551\n",
      "Iteration 31, loss = 0.43205390\n",
      "Iteration 32, loss = 0.43086558\n",
      "Iteration 33, loss = 0.42985282\n",
      "Iteration 34, loss = 0.42987374\n",
      "Iteration 35, loss = 0.43040604\n",
      "Iteration 36, loss = 0.43160234\n",
      "Iteration 37, loss = 0.42962926\n",
      "Iteration 38, loss = 0.42772281\n",
      "Iteration 39, loss = 0.42930732\n",
      "Iteration 40, loss = 0.42841867\n",
      "Iteration 41, loss = 0.42734340\n",
      "Iteration 42, loss = 0.42733019\n",
      "Iteration 43, loss = 0.43034219\n",
      "Iteration 44, loss = 0.42703029\n",
      "Iteration 45, loss = 0.42755925\n",
      "Iteration 46, loss = 0.42612357\n",
      "Iteration 47, loss = 0.42636557\n",
      "Iteration 48, loss = 0.42729652\n",
      "Iteration 49, loss = 0.42731154\n",
      "Iteration 50, loss = 0.42650803\n",
      "Iteration 51, loss = 0.42746135\n",
      "Iteration 52, loss = 0.42580570\n",
      "Iteration 53, loss = 0.42620642\n",
      "Iteration 54, loss = 0.42554129\n",
      "Iteration 55, loss = 0.42566316\n",
      "Iteration 56, loss = 0.42392672\n",
      "Iteration 57, loss = 0.42479484\n",
      "Iteration 58, loss = 0.42327189\n",
      "Iteration 59, loss = 0.42359576\n",
      "Iteration 60, loss = 0.42361719\n",
      "Iteration 61, loss = 0.42177666\n",
      "Iteration 62, loss = 0.42209011\n",
      "Iteration 63, loss = 0.42184509\n",
      "Iteration 64, loss = 0.42225153\n",
      "Iteration 65, loss = 0.42334131\n",
      "Iteration 66, loss = 0.42131510\n",
      "Iteration 67, loss = 0.41982479\n",
      "Iteration 68, loss = 0.42103055\n",
      "Iteration 69, loss = 0.42008893\n",
      "Iteration 70, loss = 0.42004627\n",
      "Iteration 71, loss = 0.42080608\n",
      "Iteration 72, loss = 0.42059089\n",
      "Iteration 73, loss = 0.41977427\n",
      "Iteration 74, loss = 0.41959350\n",
      "Iteration 75, loss = 0.42209276\n",
      "Iteration 76, loss = 0.41840842\n",
      "Iteration 77, loss = 0.41783659\n",
      "Iteration 78, loss = 0.41689107\n",
      "Iteration 79, loss = 0.41664777\n",
      "Iteration 80, loss = 0.41689631\n",
      "Iteration 81, loss = 0.41689933\n",
      "Iteration 82, loss = 0.41567865\n",
      "Iteration 83, loss = 0.41697640\n",
      "Iteration 84, loss = 0.41832459\n",
      "Iteration 85, loss = 0.41794289\n",
      "Iteration 86, loss = 0.41521228\n",
      "Iteration 87, loss = 0.41639034\n",
      "Iteration 88, loss = 0.41621841\n",
      "Iteration 89, loss = 0.41809996\n",
      "Iteration 90, loss = 0.41744024\n",
      "Iteration 91, loss = 0.41467266\n",
      "Iteration 92, loss = 0.41614047\n",
      "Iteration 93, loss = 0.41473090\n",
      "Iteration 94, loss = 0.41479422\n",
      "Iteration 95, loss = 0.41623819\n",
      "Iteration 96, loss = 0.41431585\n",
      "Iteration 97, loss = 0.41405307\n",
      "Iteration 98, loss = 0.41447150\n",
      "Iteration 99, loss = 0.41424873\n",
      "Iteration 100, loss = 0.41639770\n",
      "Iteration 101, loss = 0.41815817\n",
      "Iteration 102, loss = 0.41493882\n",
      "Iteration 103, loss = 0.41525444\n",
      "Iteration 104, loss = 0.41809669\n",
      "Iteration 105, loss = 0.41575063\n",
      "Iteration 106, loss = 0.41808039\n",
      "Iteration 107, loss = 0.41595483\n",
      "Iteration 108, loss = 0.41589128\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53736568\n",
      "Iteration 2, loss = 0.49640793\n",
      "Iteration 3, loss = 0.48294804\n",
      "Iteration 4, loss = 0.47239648\n",
      "Iteration 5, loss = 0.46751958\n",
      "Iteration 6, loss = 0.46404009\n",
      "Iteration 7, loss = 0.46256893\n",
      "Iteration 8, loss = 0.45994691\n",
      "Iteration 9, loss = 0.45882992\n",
      "Iteration 10, loss = 0.45628583\n",
      "Iteration 11, loss = 0.45626925\n",
      "Iteration 12, loss = 0.45316930\n",
      "Iteration 13, loss = 0.45279408\n",
      "Iteration 14, loss = 0.45077660\n",
      "Iteration 15, loss = 0.45091045\n",
      "Iteration 16, loss = 0.44883331\n",
      "Iteration 17, loss = 0.44853304\n",
      "Iteration 18, loss = 0.44450105\n",
      "Iteration 19, loss = 0.44120132\n",
      "Iteration 20, loss = 0.43715731\n",
      "Iteration 21, loss = 0.43814346\n",
      "Iteration 22, loss = 0.43238201\n",
      "Iteration 23, loss = 0.43165013\n",
      "Iteration 24, loss = 0.43015348\n",
      "Iteration 25, loss = 0.43277506\n",
      "Iteration 26, loss = 0.43379470\n",
      "Iteration 27, loss = 0.42836182\n",
      "Iteration 28, loss = 0.42700516\n",
      "Iteration 29, loss = 0.42353528\n",
      "Iteration 30, loss = 0.42197993\n",
      "Iteration 31, loss = 0.42127978\n",
      "Iteration 32, loss = 0.42212061\n",
      "Iteration 33, loss = 0.42183090\n",
      "Iteration 34, loss = 0.42083157\n",
      "Iteration 35, loss = 0.42175435\n",
      "Iteration 36, loss = 0.42020572\n",
      "Iteration 37, loss = 0.42010925\n",
      "Iteration 38, loss = 0.42109390\n",
      "Iteration 39, loss = 0.41965036\n",
      "Iteration 40, loss = 0.42039190\n",
      "Iteration 41, loss = 0.42138712\n",
      "Iteration 42, loss = 0.41917842\n",
      "Iteration 43, loss = 0.42290105\n",
      "Iteration 44, loss = 0.42195331\n",
      "Iteration 45, loss = 0.42109852\n",
      "Iteration 46, loss = 0.42200973\n",
      "Iteration 47, loss = 0.41996671\n",
      "Iteration 48, loss = 0.42031647\n",
      "Iteration 49, loss = 0.41745349\n",
      "Iteration 50, loss = 0.41783072\n",
      "Iteration 51, loss = 0.41915535\n",
      "Iteration 52, loss = 0.42108518\n",
      "Iteration 53, loss = 0.42308421\n",
      "Iteration 54, loss = 0.41845318\n",
      "Iteration 55, loss = 0.41968711\n",
      "Iteration 56, loss = 0.41814243\n",
      "Iteration 57, loss = 0.41860541\n",
      "Iteration 58, loss = 0.41964598\n",
      "Iteration 59, loss = 0.42212623\n",
      "Iteration 60, loss = 0.41942494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52548043\n",
      "Iteration 2, loss = 0.49197238\n",
      "Iteration 3, loss = 0.47996474\n",
      "Iteration 4, loss = 0.47308906\n",
      "Iteration 5, loss = 0.46954613\n",
      "Iteration 6, loss = 0.46759095\n",
      "Iteration 7, loss = 0.46584532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.46560390\n",
      "Iteration 9, loss = 0.46322441\n",
      "Iteration 10, loss = 0.46125858\n",
      "Iteration 11, loss = 0.46224415\n",
      "Iteration 12, loss = 0.45940753\n",
      "Iteration 13, loss = 0.45856744\n",
      "Iteration 14, loss = 0.45868541\n",
      "Iteration 15, loss = 0.45659599\n",
      "Iteration 16, loss = 0.45570989\n",
      "Iteration 17, loss = 0.45380232\n",
      "Iteration 18, loss = 0.45115537\n",
      "Iteration 19, loss = 0.45091591\n",
      "Iteration 20, loss = 0.45034736\n",
      "Iteration 21, loss = 0.45150892\n",
      "Iteration 22, loss = 0.44703908\n",
      "Iteration 23, loss = 0.44581575\n",
      "Iteration 24, loss = 0.44634853\n",
      "Iteration 25, loss = 0.44546811\n",
      "Iteration 26, loss = 0.44754582\n",
      "Iteration 27, loss = 0.44460159\n",
      "Iteration 28, loss = 0.44378753\n",
      "Iteration 29, loss = 0.44245554\n",
      "Iteration 30, loss = 0.44626004\n",
      "Iteration 31, loss = 0.44604353\n",
      "Iteration 32, loss = 0.44421653\n",
      "Iteration 33, loss = 0.44629583\n",
      "Iteration 34, loss = 0.44304016\n",
      "Iteration 35, loss = 0.44205974\n",
      "Iteration 36, loss = 0.44278397\n",
      "Iteration 37, loss = 0.44330005\n",
      "Iteration 38, loss = 0.44303267\n",
      "Iteration 39, loss = 0.44527090\n",
      "Iteration 40, loss = 0.44061948\n",
      "Iteration 41, loss = 0.44175989\n",
      "Iteration 42, loss = 0.44464966\n",
      "Iteration 43, loss = 0.44231577\n",
      "Iteration 44, loss = 0.44055285\n",
      "Iteration 45, loss = 0.44240921\n",
      "Iteration 46, loss = 0.44095847\n",
      "Iteration 47, loss = 0.44050358\n",
      "Iteration 48, loss = 0.43982698\n",
      "Iteration 49, loss = 0.43905644\n",
      "Iteration 50, loss = 0.44055478\n",
      "Iteration 51, loss = 0.44061974\n",
      "Iteration 52, loss = 0.44040054\n",
      "Iteration 53, loss = 0.43885886\n",
      "Iteration 54, loss = 0.43829462\n",
      "Iteration 55, loss = 0.44036718\n",
      "Iteration 56, loss = 0.44020169\n",
      "Iteration 57, loss = 0.44008677\n",
      "Iteration 58, loss = 0.43744206\n",
      "Iteration 59, loss = 0.43947125\n",
      "Iteration 60, loss = 0.43873363\n",
      "Iteration 61, loss = 0.43919539\n",
      "Iteration 62, loss = 0.43650285\n",
      "Iteration 63, loss = 0.43624589\n",
      "Iteration 64, loss = 0.43620871\n",
      "Iteration 65, loss = 0.43813671\n",
      "Iteration 66, loss = 0.43714868\n",
      "Iteration 67, loss = 0.43838828\n",
      "Iteration 68, loss = 0.43753968\n",
      "Iteration 69, loss = 0.43466585\n",
      "Iteration 70, loss = 0.43822511\n",
      "Iteration 71, loss = 0.43895940\n",
      "Iteration 72, loss = 0.43653137\n",
      "Iteration 73, loss = 0.43605858\n",
      "Iteration 74, loss = 0.43621285\n",
      "Iteration 75, loss = 0.43591744\n",
      "Iteration 76, loss = 0.43379766\n",
      "Iteration 77, loss = 0.43375442\n",
      "Iteration 78, loss = 0.43675172\n",
      "Iteration 79, loss = 0.43726578\n",
      "Iteration 80, loss = 0.43470043\n",
      "Iteration 81, loss = 0.43634741\n",
      "Iteration 82, loss = 0.43440185\n",
      "Iteration 83, loss = 0.43236367\n",
      "Iteration 84, loss = 0.43256137\n",
      "Iteration 85, loss = 0.43117492\n",
      "Iteration 86, loss = 0.43287238\n",
      "Iteration 87, loss = 0.43216583\n",
      "Iteration 88, loss = 0.43380840\n",
      "Iteration 89, loss = 0.43069001\n",
      "Iteration 90, loss = 0.43578058\n",
      "Iteration 91, loss = 0.43252951\n",
      "Iteration 92, loss = 0.43485137\n",
      "Iteration 93, loss = 0.43340738\n",
      "Iteration 94, loss = 0.43189631\n",
      "Iteration 95, loss = 0.43207403\n",
      "Iteration 96, loss = 0.43224246\n",
      "Iteration 97, loss = 0.43189249\n",
      "Iteration 98, loss = 0.43189785\n",
      "Iteration 99, loss = 0.43068877\n",
      "Iteration 100, loss = 0.43086340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52764136\n",
      "Iteration 2, loss = 0.49277523\n",
      "Iteration 3, loss = 0.47560445\n",
      "Iteration 4, loss = 0.47074146\n",
      "Iteration 5, loss = 0.46705206\n",
      "Iteration 6, loss = 0.46613608\n",
      "Iteration 7, loss = 0.46504614\n",
      "Iteration 8, loss = 0.46323029\n",
      "Iteration 9, loss = 0.46117446\n",
      "Iteration 10, loss = 0.46032478\n",
      "Iteration 11, loss = 0.45898428\n",
      "Iteration 12, loss = 0.45732982\n",
      "Iteration 13, loss = 0.45735088\n",
      "Iteration 14, loss = 0.45459373\n",
      "Iteration 15, loss = 0.45580027\n",
      "Iteration 16, loss = 0.45332286\n",
      "Iteration 17, loss = 0.45202762\n",
      "Iteration 18, loss = 0.45124954\n",
      "Iteration 19, loss = 0.44942915\n",
      "Iteration 20, loss = 0.44995758\n",
      "Iteration 21, loss = 0.44868885\n",
      "Iteration 22, loss = 0.44719114\n",
      "Iteration 23, loss = 0.44836324\n",
      "Iteration 24, loss = 0.44721842\n",
      "Iteration 25, loss = 0.44595879\n",
      "Iteration 26, loss = 0.44789751\n",
      "Iteration 27, loss = 0.44504275\n",
      "Iteration 28, loss = 0.44559996\n",
      "Iteration 29, loss = 0.44494091\n",
      "Iteration 30, loss = 0.44372389\n",
      "Iteration 31, loss = 0.44251743\n",
      "Iteration 32, loss = 0.44250217\n",
      "Iteration 33, loss = 0.44142205\n",
      "Iteration 34, loss = 0.44103684\n",
      "Iteration 35, loss = 0.44122089\n",
      "Iteration 36, loss = 0.44104214\n",
      "Iteration 37, loss = 0.44281473\n",
      "Iteration 38, loss = 0.44139930\n",
      "Iteration 39, loss = 0.44293245\n",
      "Iteration 40, loss = 0.44177360\n",
      "Iteration 41, loss = 0.44113406\n",
      "Iteration 42, loss = 0.44071417\n",
      "Iteration 43, loss = 0.44187243\n",
      "Iteration 44, loss = 0.44065319\n",
      "Iteration 45, loss = 0.44249454\n",
      "Iteration 46, loss = 0.44064358\n",
      "Iteration 47, loss = 0.44053738\n",
      "Iteration 48, loss = 0.44004665\n",
      "Iteration 49, loss = 0.44175529\n",
      "Iteration 50, loss = 0.43981524\n",
      "Iteration 51, loss = 0.44081328\n",
      "Iteration 52, loss = 0.44043848\n",
      "Iteration 53, loss = 0.44086594\n",
      "Iteration 54, loss = 0.44050170\n",
      "Iteration 55, loss = 0.44105690\n",
      "Iteration 56, loss = 0.44013922\n",
      "Iteration 57, loss = 0.44197490\n",
      "Iteration 58, loss = 0.44057144\n",
      "Iteration 59, loss = 0.44174034\n",
      "Iteration 60, loss = 0.44027921\n",
      "Iteration 61, loss = 0.44010434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52350985\n",
      "Iteration 2, loss = 0.48607317\n",
      "Iteration 3, loss = 0.47631462\n",
      "Iteration 4, loss = 0.47029904\n",
      "Iteration 5, loss = 0.46699813\n",
      "Iteration 6, loss = 0.46430001\n",
      "Iteration 7, loss = 0.46393065\n",
      "Iteration 8, loss = 0.46211435\n",
      "Iteration 9, loss = 0.46045861\n",
      "Iteration 10, loss = 0.45846835\n",
      "Iteration 11, loss = 0.45404626\n",
      "Iteration 12, loss = 0.45299001\n",
      "Iteration 13, loss = 0.45421622\n",
      "Iteration 14, loss = 0.45000690\n",
      "Iteration 15, loss = 0.45024492\n",
      "Iteration 16, loss = 0.44661817\n",
      "Iteration 17, loss = 0.44870072\n",
      "Iteration 18, loss = 0.44728074\n",
      "Iteration 19, loss = 0.44612958\n",
      "Iteration 20, loss = 0.44359943\n",
      "Iteration 21, loss = 0.44582611\n",
      "Iteration 22, loss = 0.44377672\n",
      "Iteration 23, loss = 0.44303858\n",
      "Iteration 24, loss = 0.44231393\n",
      "Iteration 25, loss = 0.44225720\n",
      "Iteration 26, loss = 0.44381953\n",
      "Iteration 27, loss = 0.44294244\n",
      "Iteration 28, loss = 0.44106512\n",
      "Iteration 29, loss = 0.44134273\n",
      "Iteration 30, loss = 0.44022379\n",
      "Iteration 31, loss = 0.44091715\n",
      "Iteration 32, loss = 0.44093466\n",
      "Iteration 33, loss = 0.44137563\n",
      "Iteration 34, loss = 0.44164330\n",
      "Iteration 35, loss = 0.44088662\n",
      "Iteration 36, loss = 0.44007107\n",
      "Iteration 37, loss = 0.43864095\n",
      "Iteration 38, loss = 0.44009116\n",
      "Iteration 39, loss = 0.44001918\n",
      "Iteration 40, loss = 0.43959754\n",
      "Iteration 41, loss = 0.43951533\n",
      "Iteration 42, loss = 0.43936140\n",
      "Iteration 43, loss = 0.44036910\n",
      "Iteration 44, loss = 0.44021342\n",
      "Iteration 45, loss = 0.43927154\n",
      "Iteration 46, loss = 0.43852276\n",
      "Iteration 47, loss = 0.43926421\n",
      "Iteration 48, loss = 0.44066198\n",
      "Iteration 49, loss = 0.43888105\n",
      "Iteration 50, loss = 0.43797922\n",
      "Iteration 51, loss = 0.43924163\n",
      "Iteration 52, loss = 0.44144729\n",
      "Iteration 53, loss = 0.43972448\n",
      "Iteration 54, loss = 0.44217767\n",
      "Iteration 55, loss = 0.43855350\n",
      "Iteration 56, loss = 0.44070412\n",
      "Iteration 57, loss = 0.43985108\n",
      "Iteration 58, loss = 0.44051631\n",
      "Iteration 59, loss = 0.43865538\n",
      "Iteration 60, loss = 0.43961993\n",
      "Iteration 61, loss = 0.43989193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52768464\n",
      "Iteration 2, loss = 0.49307631\n",
      "Iteration 3, loss = 0.48066999\n",
      "Iteration 4, loss = 0.47318199\n",
      "Iteration 5, loss = 0.46919364\n",
      "Iteration 6, loss = 0.46610379\n",
      "Iteration 7, loss = 0.46378510\n",
      "Iteration 8, loss = 0.46144617\n",
      "Iteration 9, loss = 0.46001015\n",
      "Iteration 10, loss = 0.45661839\n",
      "Iteration 11, loss = 0.45410550\n",
      "Iteration 12, loss = 0.45301168\n",
      "Iteration 13, loss = 0.45125400\n",
      "Iteration 14, loss = 0.45071516\n",
      "Iteration 15, loss = 0.44974771\n",
      "Iteration 16, loss = 0.44694043\n",
      "Iteration 17, loss = 0.44504034\n",
      "Iteration 18, loss = 0.44588714\n",
      "Iteration 19, loss = 0.44240783\n",
      "Iteration 20, loss = 0.44292204\n",
      "Iteration 21, loss = 0.44102377\n",
      "Iteration 22, loss = 0.43833979\n",
      "Iteration 23, loss = 0.43820151\n",
      "Iteration 24, loss = 0.43866392\n",
      "Iteration 25, loss = 0.43660091\n",
      "Iteration 26, loss = 0.43724195\n",
      "Iteration 27, loss = 0.43790612\n",
      "Iteration 28, loss = 0.43623567\n",
      "Iteration 29, loss = 0.43660820\n",
      "Iteration 30, loss = 0.44124707\n",
      "Iteration 31, loss = 0.43812174\n",
      "Iteration 32, loss = 0.43640650\n",
      "Iteration 33, loss = 0.43584805\n",
      "Iteration 34, loss = 0.43710862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.43679167\n",
      "Iteration 36, loss = 0.43776750\n",
      "Iteration 37, loss = 0.43734035\n",
      "Iteration 38, loss = 0.43562184\n",
      "Iteration 39, loss = 0.43635053\n",
      "Iteration 40, loss = 0.43461093\n",
      "Iteration 41, loss = 0.43558708\n",
      "Iteration 42, loss = 0.43682327\n",
      "Iteration 43, loss = 0.43468222\n",
      "Iteration 44, loss = 0.43501573\n",
      "Iteration 45, loss = 0.43422806\n",
      "Iteration 46, loss = 0.43393192\n",
      "Iteration 47, loss = 0.43362923\n",
      "Iteration 48, loss = 0.43621974\n",
      "Iteration 49, loss = 0.43343352\n",
      "Iteration 50, loss = 0.43336504\n",
      "Iteration 51, loss = 0.43401070\n",
      "Iteration 52, loss = 0.43336822\n",
      "Iteration 53, loss = 0.43240284\n",
      "Iteration 54, loss = 0.43354683\n",
      "Iteration 55, loss = 0.43568754\n",
      "Iteration 56, loss = 0.43416125\n",
      "Iteration 57, loss = 0.43404083\n",
      "Iteration 58, loss = 0.43444961\n",
      "Iteration 59, loss = 0.43299872\n",
      "Iteration 60, loss = 0.43586860\n",
      "Iteration 61, loss = 0.43366176\n",
      "Iteration 62, loss = 0.43258795\n",
      "Iteration 63, loss = 0.43289655\n",
      "Iteration 64, loss = 0.43287067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52993133\n",
      "Iteration 2, loss = 0.49122145\n",
      "Iteration 3, loss = 0.47297610\n",
      "Iteration 4, loss = 0.46989010\n",
      "Iteration 5, loss = 0.46620424\n",
      "Iteration 6, loss = 0.46580577\n",
      "Iteration 7, loss = 0.46198941\n",
      "Iteration 8, loss = 0.46028773\n",
      "Iteration 9, loss = 0.45784587\n",
      "Iteration 10, loss = 0.45789763\n",
      "Iteration 11, loss = 0.45540139\n",
      "Iteration 12, loss = 0.45199330\n",
      "Iteration 13, loss = 0.45093796\n",
      "Iteration 14, loss = 0.44757082\n",
      "Iteration 15, loss = 0.44862691\n",
      "Iteration 16, loss = 0.44485368\n",
      "Iteration 17, loss = 0.44284404\n",
      "Iteration 18, loss = 0.44336899\n",
      "Iteration 19, loss = 0.44155696\n",
      "Iteration 20, loss = 0.44187850\n",
      "Iteration 21, loss = 0.44123279\n",
      "Iteration 22, loss = 0.44126037\n",
      "Iteration 23, loss = 0.44199814\n",
      "Iteration 24, loss = 0.44072739\n",
      "Iteration 25, loss = 0.44000058\n",
      "Iteration 26, loss = 0.44049940\n",
      "Iteration 27, loss = 0.43935113\n",
      "Iteration 28, loss = 0.44001964\n",
      "Iteration 29, loss = 0.44010940\n",
      "Iteration 30, loss = 0.43934388\n",
      "Iteration 31, loss = 0.43836851\n",
      "Iteration 32, loss = 0.43963744\n",
      "Iteration 33, loss = 0.43872155\n",
      "Iteration 34, loss = 0.44015497\n",
      "Iteration 35, loss = 0.43992624\n",
      "Iteration 36, loss = 0.44086327\n",
      "Iteration 37, loss = 0.43939765\n",
      "Iteration 38, loss = 0.43853521\n",
      "Iteration 39, loss = 0.43920204\n",
      "Iteration 40, loss = 0.43973891\n",
      "Iteration 41, loss = 0.43817011\n",
      "Iteration 42, loss = 0.43835313\n",
      "Iteration 43, loss = 0.43887413\n",
      "Iteration 44, loss = 0.43876824\n",
      "Iteration 45, loss = 0.43926033\n",
      "Iteration 46, loss = 0.43788251\n",
      "Iteration 47, loss = 0.43798792\n",
      "Iteration 48, loss = 0.43751895\n",
      "Iteration 49, loss = 0.43751747\n",
      "Iteration 50, loss = 0.43903846\n",
      "Iteration 51, loss = 0.43817221\n",
      "Iteration 52, loss = 0.43744433\n",
      "Iteration 53, loss = 0.43939678\n",
      "Iteration 54, loss = 0.43892349\n",
      "Iteration 55, loss = 0.43907238\n",
      "Iteration 56, loss = 0.43787593\n",
      "Iteration 57, loss = 0.43918343\n",
      "Iteration 58, loss = 0.43769068\n",
      "Iteration 59, loss = 0.43806723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52619501\n",
      "Iteration 2, loss = 0.48822847\n",
      "Iteration 3, loss = 0.47432708\n",
      "Iteration 4, loss = 0.46964647\n",
      "Iteration 5, loss = 0.46656955\n",
      "Iteration 6, loss = 0.46401721\n",
      "Iteration 7, loss = 0.46217417\n",
      "Iteration 8, loss = 0.45802427\n",
      "Iteration 9, loss = 0.45591215\n",
      "Iteration 10, loss = 0.45491127\n",
      "Iteration 11, loss = 0.45195430\n",
      "Iteration 12, loss = 0.44985770\n",
      "Iteration 13, loss = 0.45012426\n",
      "Iteration 14, loss = 0.44822988\n",
      "Iteration 15, loss = 0.44762930\n",
      "Iteration 16, loss = 0.44581168\n",
      "Iteration 17, loss = 0.44812828\n",
      "Iteration 18, loss = 0.44494592\n",
      "Iteration 19, loss = 0.44503633\n",
      "Iteration 20, loss = 0.44438351\n",
      "Iteration 21, loss = 0.44566675\n",
      "Iteration 22, loss = 0.44328828\n",
      "Iteration 23, loss = 0.44350290\n",
      "Iteration 24, loss = 0.44276505\n",
      "Iteration 25, loss = 0.44189185\n",
      "Iteration 26, loss = 0.44223419\n",
      "Iteration 27, loss = 0.44222963\n",
      "Iteration 28, loss = 0.44075006\n",
      "Iteration 29, loss = 0.44135530\n",
      "Iteration 30, loss = 0.44056187\n",
      "Iteration 31, loss = 0.44159371\n",
      "Iteration 32, loss = 0.44140418\n",
      "Iteration 33, loss = 0.44093849\n",
      "Iteration 34, loss = 0.44114907\n",
      "Iteration 35, loss = 0.44121520\n",
      "Iteration 36, loss = 0.43993815\n",
      "Iteration 37, loss = 0.44001792\n",
      "Iteration 38, loss = 0.44013483\n",
      "Iteration 39, loss = 0.44174695\n",
      "Iteration 40, loss = 0.44042017\n",
      "Iteration 41, loss = 0.44006522\n",
      "Iteration 42, loss = 0.43927202\n",
      "Iteration 43, loss = 0.43912586\n",
      "Iteration 44, loss = 0.43923597\n",
      "Iteration 45, loss = 0.43910518\n",
      "Iteration 46, loss = 0.43922847\n",
      "Iteration 47, loss = 0.43939989\n",
      "Iteration 48, loss = 0.43994225\n",
      "Iteration 49, loss = 0.43862346\n",
      "Iteration 50, loss = 0.43894758\n",
      "Iteration 51, loss = 0.43976697\n",
      "Iteration 52, loss = 0.43926616\n",
      "Iteration 53, loss = 0.43913737\n",
      "Iteration 54, loss = 0.44053065\n",
      "Iteration 55, loss = 0.44237881\n",
      "Iteration 56, loss = 0.44139609\n",
      "Iteration 57, loss = 0.43968478\n",
      "Iteration 58, loss = 0.44086228\n",
      "Iteration 59, loss = 0.43940611\n",
      "Iteration 60, loss = 0.43856306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52710219\n",
      "Iteration 2, loss = 0.48984415\n",
      "Iteration 3, loss = 0.48120806\n",
      "Iteration 4, loss = 0.47504950\n",
      "Iteration 5, loss = 0.46979442\n",
      "Iteration 6, loss = 0.46413497\n",
      "Iteration 7, loss = 0.46160279\n",
      "Iteration 8, loss = 0.45633984\n",
      "Iteration 9, loss = 0.45343963\n",
      "Iteration 10, loss = 0.45040394\n",
      "Iteration 11, loss = 0.44982780\n",
      "Iteration 12, loss = 0.44810020\n",
      "Iteration 13, loss = 0.44865187\n",
      "Iteration 14, loss = 0.44667243\n",
      "Iteration 15, loss = 0.44532214\n",
      "Iteration 16, loss = 0.44442256\n",
      "Iteration 17, loss = 0.44268215\n",
      "Iteration 18, loss = 0.44258136\n",
      "Iteration 19, loss = 0.44289507\n",
      "Iteration 20, loss = 0.44064172\n",
      "Iteration 21, loss = 0.44004163\n",
      "Iteration 22, loss = 0.43877190\n",
      "Iteration 23, loss = 0.43845596\n",
      "Iteration 24, loss = 0.43942543\n",
      "Iteration 25, loss = 0.43789323\n",
      "Iteration 26, loss = 0.43918013\n",
      "Iteration 27, loss = 0.43773188\n",
      "Iteration 28, loss = 0.43723878\n",
      "Iteration 29, loss = 0.43565427\n",
      "Iteration 30, loss = 0.43796061\n",
      "Iteration 31, loss = 0.43767107\n",
      "Iteration 32, loss = 0.43366537\n",
      "Iteration 33, loss = 0.43511991\n",
      "Iteration 34, loss = 0.43629879\n",
      "Iteration 35, loss = 0.43627032\n",
      "Iteration 36, loss = 0.43489044\n",
      "Iteration 37, loss = 0.43500142\n",
      "Iteration 38, loss = 0.43451374\n",
      "Iteration 39, loss = 0.43591218\n",
      "Iteration 40, loss = 0.43227251\n",
      "Iteration 41, loss = 0.43531818\n",
      "Iteration 42, loss = 0.43447902\n",
      "Iteration 43, loss = 0.43101164\n",
      "Iteration 44, loss = 0.43121091\n",
      "Iteration 45, loss = 0.43267106\n",
      "Iteration 46, loss = 0.43121567\n",
      "Iteration 47, loss = 0.42954505\n",
      "Iteration 48, loss = 0.43131685\n",
      "Iteration 49, loss = 0.43086890\n",
      "Iteration 50, loss = 0.42921602\n",
      "Iteration 51, loss = 0.43223030\n",
      "Iteration 52, loss = 0.42986554\n",
      "Iteration 53, loss = 0.42928782\n",
      "Iteration 54, loss = 0.42916238\n",
      "Iteration 55, loss = 0.43127151\n",
      "Iteration 56, loss = 0.42883749\n",
      "Iteration 57, loss = 0.42814625\n",
      "Iteration 58, loss = 0.42991220\n",
      "Iteration 59, loss = 0.43028645\n",
      "Iteration 60, loss = 0.43207274\n",
      "Iteration 61, loss = 0.42965655\n",
      "Iteration 62, loss = 0.42923803\n",
      "Iteration 63, loss = 0.42858033\n",
      "Iteration 64, loss = 0.42856703\n",
      "Iteration 65, loss = 0.43051533\n",
      "Iteration 66, loss = 0.42859936\n",
      "Iteration 67, loss = 0.42987793\n",
      "Iteration 68, loss = 0.43094444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52930953\n",
      "Iteration 2, loss = 0.48823671\n",
      "Iteration 3, loss = 0.47058715\n",
      "Iteration 4, loss = 0.46631053\n",
      "Iteration 5, loss = 0.46418576\n",
      "Iteration 6, loss = 0.46210754\n",
      "Iteration 7, loss = 0.46158073\n",
      "Iteration 8, loss = 0.45809946\n",
      "Iteration 9, loss = 0.45501621\n",
      "Iteration 10, loss = 0.45467462\n",
      "Iteration 11, loss = 0.45097453\n",
      "Iteration 12, loss = 0.44944747\n",
      "Iteration 13, loss = 0.45249304\n",
      "Iteration 14, loss = 0.44542164\n",
      "Iteration 15, loss = 0.44624224\n",
      "Iteration 16, loss = 0.44374826\n",
      "Iteration 17, loss = 0.44210100\n",
      "Iteration 18, loss = 0.44072894\n",
      "Iteration 19, loss = 0.44034965\n",
      "Iteration 20, loss = 0.44012899\n",
      "Iteration 21, loss = 0.44028005\n",
      "Iteration 22, loss = 0.44032668\n",
      "Iteration 23, loss = 0.43932611\n",
      "Iteration 24, loss = 0.44021438\n",
      "Iteration 25, loss = 0.43970457\n",
      "Iteration 26, loss = 0.43894636\n",
      "Iteration 27, loss = 0.43828483\n",
      "Iteration 28, loss = 0.43817960\n",
      "Iteration 29, loss = 0.43701741\n",
      "Iteration 30, loss = 0.43713168\n",
      "Iteration 31, loss = 0.43821003\n",
      "Iteration 32, loss = 0.43786482\n",
      "Iteration 33, loss = 0.43749534\n",
      "Iteration 34, loss = 0.43602147\n",
      "Iteration 35, loss = 0.43685212\n",
      "Iteration 36, loss = 0.43778035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 37, loss = 0.43794845\n",
      "Iteration 38, loss = 0.43593740\n",
      "Iteration 39, loss = 0.43772643\n",
      "Iteration 40, loss = 0.43586017\n",
      "Iteration 41, loss = 0.43482616\n",
      "Iteration 42, loss = 0.43773682\n",
      "Iteration 43, loss = 0.43609180\n",
      "Iteration 44, loss = 0.43548742\n",
      "Iteration 45, loss = 0.43512145\n",
      "Iteration 46, loss = 0.43384710\n",
      "Iteration 47, loss = 0.43427724\n",
      "Iteration 48, loss = 0.43383185\n",
      "Iteration 49, loss = 0.43461532\n",
      "Iteration 50, loss = 0.43613494\n",
      "Iteration 51, loss = 0.43546058\n",
      "Iteration 52, loss = 0.43363333\n",
      "Iteration 53, loss = 0.43552651\n",
      "Iteration 54, loss = 0.43557829\n",
      "Iteration 55, loss = 0.43568221\n",
      "Iteration 56, loss = 0.43506555\n",
      "Iteration 57, loss = 0.43381100\n",
      "Iteration 58, loss = 0.43361210\n",
      "Iteration 59, loss = 0.43612272\n",
      "Iteration 60, loss = 0.43471619\n",
      "Iteration 61, loss = 0.43441120\n",
      "Iteration 62, loss = 0.43351911\n",
      "Iteration 63, loss = 0.43361653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52610444\n",
      "Iteration 2, loss = 0.48597633\n",
      "Iteration 3, loss = 0.47516474\n",
      "Iteration 4, loss = 0.46932332\n",
      "Iteration 5, loss = 0.46570045\n",
      "Iteration 6, loss = 0.46420353\n",
      "Iteration 7, loss = 0.46201098\n",
      "Iteration 8, loss = 0.45940564\n",
      "Iteration 9, loss = 0.45742213\n",
      "Iteration 10, loss = 0.45611269\n",
      "Iteration 11, loss = 0.45336803\n",
      "Iteration 12, loss = 0.45164872\n",
      "Iteration 13, loss = 0.45159976\n",
      "Iteration 14, loss = 0.44926626\n",
      "Iteration 15, loss = 0.44740833\n",
      "Iteration 16, loss = 0.44598114\n",
      "Iteration 17, loss = 0.44693789\n",
      "Iteration 18, loss = 0.44565080\n",
      "Iteration 19, loss = 0.44431893\n",
      "Iteration 20, loss = 0.44527371\n",
      "Iteration 21, loss = 0.44452994\n",
      "Iteration 22, loss = 0.44321393\n",
      "Iteration 23, loss = 0.44233323\n",
      "Iteration 24, loss = 0.44241298\n",
      "Iteration 25, loss = 0.44334234\n",
      "Iteration 26, loss = 0.44182720\n",
      "Iteration 27, loss = 0.44263202\n",
      "Iteration 28, loss = 0.44138517\n",
      "Iteration 29, loss = 0.44255107\n",
      "Iteration 30, loss = 0.44144821\n",
      "Iteration 31, loss = 0.44282889\n",
      "Iteration 32, loss = 0.44271123\n",
      "Iteration 33, loss = 0.44139508\n",
      "Iteration 34, loss = 0.44145481\n",
      "Iteration 35, loss = 0.44141310\n",
      "Iteration 36, loss = 0.44121668\n",
      "Iteration 37, loss = 0.44008773\n",
      "Iteration 38, loss = 0.44109032\n",
      "Iteration 39, loss = 0.44064791\n",
      "Iteration 40, loss = 0.43982587\n",
      "Iteration 41, loss = 0.44078744\n",
      "Iteration 42, loss = 0.44011676\n",
      "Iteration 43, loss = 0.44000752\n",
      "Iteration 44, loss = 0.44143704\n",
      "Iteration 45, loss = 0.43934579\n",
      "Iteration 46, loss = 0.43956703\n",
      "Iteration 47, loss = 0.43909784\n",
      "Iteration 48, loss = 0.44026962\n",
      "Iteration 49, loss = 0.43920054\n",
      "Iteration 50, loss = 0.43923662\n",
      "Iteration 51, loss = 0.43894070\n",
      "Iteration 52, loss = 0.44146886\n",
      "Iteration 53, loss = 0.44139630\n",
      "Iteration 54, loss = 0.44272799\n",
      "Iteration 55, loss = 0.43951543\n",
      "Iteration 56, loss = 0.43873426\n",
      "Iteration 57, loss = 0.43940200\n",
      "Iteration 58, loss = 0.43985124\n",
      "Iteration 59, loss = 0.43904657\n",
      "Iteration 60, loss = 0.44143617\n",
      "Iteration 61, loss = 0.43994864\n",
      "Iteration 62, loss = 0.43874883\n",
      "Iteration 63, loss = 0.43906274\n",
      "Iteration 64, loss = 0.43842172\n",
      "Iteration 65, loss = 0.43876624\n",
      "Iteration 66, loss = 0.43871749\n",
      "Iteration 67, loss = 0.43965747\n",
      "Iteration 68, loss = 0.43932754\n",
      "Iteration 69, loss = 0.43838464\n",
      "Iteration 70, loss = 0.44097998\n",
      "Iteration 71, loss = 0.43912573\n",
      "Iteration 72, loss = 0.43874068\n",
      "Iteration 73, loss = 0.44032549\n",
      "Iteration 74, loss = 0.43966331\n",
      "Iteration 75, loss = 0.43827160\n",
      "Iteration 76, loss = 0.43779258\n",
      "Iteration 77, loss = 0.43869685\n",
      "Iteration 78, loss = 0.43806672\n",
      "Iteration 79, loss = 0.43820928\n",
      "Iteration 80, loss = 0.43806812\n",
      "Iteration 81, loss = 0.43892191\n",
      "Iteration 82, loss = 0.43799104\n",
      "Iteration 83, loss = 0.43811327\n",
      "Iteration 84, loss = 0.43833403\n",
      "Iteration 85, loss = 0.43772866\n",
      "Iteration 86, loss = 0.43800845\n",
      "Iteration 87, loss = 0.43850044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60050106\n",
      "Iteration 2, loss = 0.54587240\n",
      "Iteration 3, loss = 0.53519524\n",
      "Iteration 4, loss = 0.52279830\n",
      "Iteration 5, loss = 0.51669683\n",
      "Iteration 6, loss = 0.51326030\n",
      "Iteration 7, loss = 0.50733033\n",
      "Iteration 8, loss = 0.50205737\n",
      "Iteration 9, loss = 0.48326504\n",
      "Iteration 10, loss = 0.47648675\n",
      "Iteration 11, loss = 0.47353153\n",
      "Iteration 12, loss = 0.47159089\n",
      "Iteration 13, loss = 0.46974176\n",
      "Iteration 14, loss = 0.46667525\n",
      "Iteration 15, loss = 0.46335170\n",
      "Iteration 16, loss = 0.46252706\n",
      "Iteration 17, loss = 0.46398075\n",
      "Iteration 18, loss = 0.46156297\n",
      "Iteration 19, loss = 0.46037568\n",
      "Iteration 20, loss = 0.45857475\n",
      "Iteration 21, loss = 0.45861104\n",
      "Iteration 22, loss = 0.45849896\n",
      "Iteration 23, loss = 0.45724223\n",
      "Iteration 24, loss = 0.45896061\n",
      "Iteration 25, loss = 0.45526032\n",
      "Iteration 26, loss = 0.45667647\n",
      "Iteration 27, loss = 0.45601871\n",
      "Iteration 28, loss = 0.45511226\n",
      "Iteration 29, loss = 0.45351142\n",
      "Iteration 30, loss = 0.45478357\n",
      "Iteration 31, loss = 0.45468449\n",
      "Iteration 32, loss = 0.45205826\n",
      "Iteration 33, loss = 0.45186055\n",
      "Iteration 34, loss = 0.45224856\n",
      "Iteration 35, loss = 0.45127886\n",
      "Iteration 36, loss = 0.45571376\n",
      "Iteration 37, loss = 0.45521607\n",
      "Iteration 38, loss = 0.45425211\n",
      "Iteration 39, loss = 0.45184225\n",
      "Iteration 40, loss = 0.45017295\n",
      "Iteration 41, loss = 0.44954724\n",
      "Iteration 42, loss = 0.45190371\n",
      "Iteration 43, loss = 0.45026869\n",
      "Iteration 44, loss = 0.44995892\n",
      "Iteration 45, loss = 0.45052138\n",
      "Iteration 46, loss = 0.45110097\n",
      "Iteration 47, loss = 0.44943006\n",
      "Iteration 48, loss = 0.45085873\n",
      "Iteration 49, loss = 0.44813521\n",
      "Iteration 50, loss = 0.44805974\n",
      "Iteration 51, loss = 0.45121276\n",
      "Iteration 52, loss = 0.44893464\n",
      "Iteration 53, loss = 0.45051274\n",
      "Iteration 54, loss = 0.44961655\n",
      "Iteration 55, loss = 0.44924429\n",
      "Iteration 56, loss = 0.45027541\n",
      "Iteration 57, loss = 0.45213837\n",
      "Iteration 58, loss = 0.45003157\n",
      "Iteration 59, loss = 0.44798742\n",
      "Iteration 60, loss = 0.44894909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60573625\n",
      "Iteration 2, loss = 0.54435760\n",
      "Iteration 3, loss = 0.53688148\n",
      "Iteration 4, loss = 0.52361891\n",
      "Iteration 5, loss = 0.51511631\n",
      "Iteration 6, loss = 0.51188640\n",
      "Iteration 7, loss = 0.51010452\n",
      "Iteration 8, loss = 0.50601770\n",
      "Iteration 9, loss = 0.49856482\n",
      "Iteration 10, loss = 0.49667372\n",
      "Iteration 11, loss = 0.49401785\n",
      "Iteration 12, loss = 0.49158980\n",
      "Iteration 13, loss = 0.49005524\n",
      "Iteration 14, loss = 0.48912800\n",
      "Iteration 15, loss = 0.48917064\n",
      "Iteration 16, loss = 0.48702745\n",
      "Iteration 17, loss = 0.48326081\n",
      "Iteration 18, loss = 0.46966060\n",
      "Iteration 19, loss = 0.46685778\n",
      "Iteration 20, loss = 0.46491089\n",
      "Iteration 21, loss = 0.46332336\n",
      "Iteration 22, loss = 0.46360393\n",
      "Iteration 23, loss = 0.46304607\n",
      "Iteration 24, loss = 0.46077022\n",
      "Iteration 25, loss = 0.45821595\n",
      "Iteration 26, loss = 0.45813162\n",
      "Iteration 27, loss = 0.45949031\n",
      "Iteration 28, loss = 0.45776649\n",
      "Iteration 29, loss = 0.45704948\n",
      "Iteration 30, loss = 0.45383586\n",
      "Iteration 31, loss = 0.45269409\n",
      "Iteration 32, loss = 0.45252491\n",
      "Iteration 33, loss = 0.45166946\n",
      "Iteration 34, loss = 0.45136933\n",
      "Iteration 35, loss = 0.45192577\n",
      "Iteration 36, loss = 0.45046918\n",
      "Iteration 37, loss = 0.45093028\n",
      "Iteration 38, loss = 0.45002848\n",
      "Iteration 39, loss = 0.45047160\n",
      "Iteration 40, loss = 0.44931218\n",
      "Iteration 41, loss = 0.45103295\n",
      "Iteration 42, loss = 0.44799891\n",
      "Iteration 43, loss = 0.44866745\n",
      "Iteration 44, loss = 0.44821445\n",
      "Iteration 45, loss = 0.44836346\n",
      "Iteration 46, loss = 0.44747993\n",
      "Iteration 47, loss = 0.44700854\n",
      "Iteration 48, loss = 0.44648037\n",
      "Iteration 49, loss = 0.45106498\n",
      "Iteration 50, loss = 0.44890131\n",
      "Iteration 51, loss = 0.44741522\n",
      "Iteration 52, loss = 0.44798039\n",
      "Iteration 53, loss = 0.44814789\n",
      "Iteration 54, loss = 0.44695238\n",
      "Iteration 55, loss = 0.44854930\n",
      "Iteration 56, loss = 0.44698145\n",
      "Iteration 57, loss = 0.44656709\n",
      "Iteration 58, loss = 0.44682022\n",
      "Iteration 59, loss = 0.44773887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59947023\n",
      "Iteration 2, loss = 0.55122580\n",
      "Iteration 3, loss = 0.54186761\n",
      "Iteration 4, loss = 0.52524256\n",
      "Iteration 5, loss = 0.51883773\n",
      "Iteration 6, loss = 0.51602460\n",
      "Iteration 7, loss = 0.51058077\n",
      "Iteration 8, loss = 0.49916230\n",
      "Iteration 9, loss = 0.49237227\n",
      "Iteration 10, loss = 0.48406086\n",
      "Iteration 11, loss = 0.47572242\n",
      "Iteration 12, loss = 0.46602807\n",
      "Iteration 13, loss = 0.46155196\n",
      "Iteration 14, loss = 0.45743728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.45526109\n",
      "Iteration 16, loss = 0.45406483\n",
      "Iteration 17, loss = 0.45717779\n",
      "Iteration 18, loss = 0.45368826\n",
      "Iteration 19, loss = 0.45442713\n",
      "Iteration 20, loss = 0.45256414\n",
      "Iteration 21, loss = 0.45429432\n",
      "Iteration 22, loss = 0.45118114\n",
      "Iteration 23, loss = 0.45101948\n",
      "Iteration 24, loss = 0.45374873\n",
      "Iteration 25, loss = 0.45419473\n",
      "Iteration 26, loss = 0.45231689\n",
      "Iteration 27, loss = 0.45187118\n",
      "Iteration 28, loss = 0.45022767\n",
      "Iteration 29, loss = 0.45034605\n",
      "Iteration 30, loss = 0.45168973\n",
      "Iteration 31, loss = 0.45016362\n",
      "Iteration 32, loss = 0.44895918\n",
      "Iteration 33, loss = 0.45019309\n",
      "Iteration 34, loss = 0.44911582\n",
      "Iteration 35, loss = 0.44926757\n",
      "Iteration 36, loss = 0.44778253\n",
      "Iteration 37, loss = 0.44679725\n",
      "Iteration 38, loss = 0.44662546\n",
      "Iteration 39, loss = 0.44632175\n",
      "Iteration 40, loss = 0.44675446\n",
      "Iteration 41, loss = 0.44652424\n",
      "Iteration 42, loss = 0.44557161\n",
      "Iteration 43, loss = 0.44539959\n",
      "Iteration 44, loss = 0.44382845\n",
      "Iteration 45, loss = 0.44360502\n",
      "Iteration 46, loss = 0.44375512\n",
      "Iteration 47, loss = 0.44396190\n",
      "Iteration 48, loss = 0.44716334\n",
      "Iteration 49, loss = 0.44371101\n",
      "Iteration 50, loss = 0.44369925\n",
      "Iteration 51, loss = 0.44299262\n",
      "Iteration 52, loss = 0.44520952\n",
      "Iteration 53, loss = 0.44646679\n",
      "Iteration 54, loss = 0.44109679\n",
      "Iteration 55, loss = 0.44405346\n",
      "Iteration 56, loss = 0.44221223\n",
      "Iteration 57, loss = 0.44402348\n",
      "Iteration 58, loss = 0.44160366\n",
      "Iteration 59, loss = 0.44274952\n",
      "Iteration 60, loss = 0.44115150\n",
      "Iteration 61, loss = 0.44200121\n",
      "Iteration 62, loss = 0.43978776\n",
      "Iteration 63, loss = 0.44083683\n",
      "Iteration 64, loss = 0.44116117\n",
      "Iteration 65, loss = 0.44101081\n",
      "Iteration 66, loss = 0.44147445\n",
      "Iteration 67, loss = 0.44032953\n",
      "Iteration 68, loss = 0.43895540\n",
      "Iteration 69, loss = 0.44053221\n",
      "Iteration 70, loss = 0.43934438\n",
      "Iteration 71, loss = 0.43893315\n",
      "Iteration 72, loss = 0.43814191\n",
      "Iteration 73, loss = 0.43853015\n",
      "Iteration 74, loss = 0.43895288\n",
      "Iteration 75, loss = 0.43788376\n",
      "Iteration 76, loss = 0.43862209\n",
      "Iteration 77, loss = 0.43831328\n",
      "Iteration 78, loss = 0.43900435\n",
      "Iteration 79, loss = 0.44038839\n",
      "Iteration 80, loss = 0.43771251\n",
      "Iteration 81, loss = 0.43741344\n",
      "Iteration 82, loss = 0.43817554\n",
      "Iteration 83, loss = 0.43694388\n",
      "Iteration 84, loss = 0.43678275\n",
      "Iteration 85, loss = 0.43511918\n",
      "Iteration 86, loss = 0.43453376\n",
      "Iteration 87, loss = 0.43343064\n",
      "Iteration 88, loss = 0.43413083\n",
      "Iteration 89, loss = 0.43423169\n",
      "Iteration 90, loss = 0.43461700\n",
      "Iteration 91, loss = 0.43658279\n",
      "Iteration 92, loss = 0.43266355\n",
      "Iteration 93, loss = 0.43545795\n",
      "Iteration 94, loss = 0.43590548\n",
      "Iteration 95, loss = 0.43400119\n",
      "Iteration 96, loss = 0.43425771\n",
      "Iteration 97, loss = 0.43333049\n",
      "Iteration 98, loss = 0.43370372\n",
      "Iteration 99, loss = 0.43323100\n",
      "Iteration 100, loss = 0.43359987\n",
      "Iteration 101, loss = 0.43382740\n",
      "Iteration 102, loss = 0.43303825\n",
      "Iteration 103, loss = 0.43310130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59857024\n",
      "Iteration 2, loss = 0.54488832\n",
      "Iteration 3, loss = 0.53781549\n",
      "Iteration 4, loss = 0.53258762\n",
      "Iteration 5, loss = 0.53004495\n",
      "Iteration 6, loss = 0.52651293\n",
      "Iteration 7, loss = 0.52491090\n",
      "Iteration 8, loss = 0.52382481\n",
      "Iteration 9, loss = 0.52210614\n",
      "Iteration 10, loss = 0.52244319\n",
      "Iteration 11, loss = 0.52260495\n",
      "Iteration 12, loss = 0.52178340\n",
      "Iteration 13, loss = 0.52108608\n",
      "Iteration 14, loss = 0.52144958\n",
      "Iteration 15, loss = 0.52023121\n",
      "Iteration 16, loss = 0.52157313\n",
      "Iteration 17, loss = 0.52036377\n",
      "Iteration 18, loss = 0.52042045\n",
      "Iteration 19, loss = 0.51884684\n",
      "Iteration 20, loss = 0.51852013\n",
      "Iteration 21, loss = 0.51964343\n",
      "Iteration 22, loss = 0.51959844\n",
      "Iteration 23, loss = 0.51909193\n",
      "Iteration 24, loss = 0.51818911\n",
      "Iteration 25, loss = 0.51780430\n",
      "Iteration 26, loss = 0.51747870\n",
      "Iteration 27, loss = 0.51843446\n",
      "Iteration 28, loss = 0.51593534\n",
      "Iteration 29, loss = 0.51518473\n",
      "Iteration 30, loss = 0.51487865\n",
      "Iteration 31, loss = 0.51445885\n",
      "Iteration 32, loss = 0.51177209\n",
      "Iteration 33, loss = 0.51233125\n",
      "Iteration 34, loss = 0.51037994\n",
      "Iteration 35, loss = 0.51240447\n",
      "Iteration 36, loss = 0.51086039\n",
      "Iteration 37, loss = 0.51136743\n",
      "Iteration 38, loss = 0.51082596\n",
      "Iteration 39, loss = 0.50945476\n",
      "Iteration 40, loss = 0.50873384\n",
      "Iteration 41, loss = 0.50746695\n",
      "Iteration 42, loss = 0.50777456\n",
      "Iteration 43, loss = 0.50610553\n",
      "Iteration 44, loss = 0.50568262\n",
      "Iteration 45, loss = 0.50637213\n",
      "Iteration 46, loss = 0.50680951\n",
      "Iteration 47, loss = 0.50520633\n",
      "Iteration 48, loss = 0.50567655\n",
      "Iteration 49, loss = 0.50512275\n",
      "Iteration 50, loss = 0.50576609\n",
      "Iteration 51, loss = 0.50664348\n",
      "Iteration 52, loss = 0.50533672\n",
      "Iteration 53, loss = 0.50613623\n",
      "Iteration 54, loss = 0.50394870\n",
      "Iteration 55, loss = 0.50315463\n",
      "Iteration 56, loss = 0.50457127\n",
      "Iteration 57, loss = 0.50591555\n",
      "Iteration 58, loss = 0.50578898\n",
      "Iteration 59, loss = 0.50543078\n",
      "Iteration 60, loss = 0.50451067\n",
      "Iteration 61, loss = 0.50384749\n",
      "Iteration 62, loss = 0.50283968\n",
      "Iteration 63, loss = 0.50492703\n",
      "Iteration 64, loss = 0.50279022\n",
      "Iteration 65, loss = 0.50531826\n",
      "Iteration 66, loss = 0.50334846\n",
      "Iteration 67, loss = 0.50270573\n",
      "Iteration 68, loss = 0.50411330\n",
      "Iteration 69, loss = 0.50299335\n",
      "Iteration 70, loss = 0.50417424\n",
      "Iteration 71, loss = 0.50447513\n",
      "Iteration 72, loss = 0.50356583\n",
      "Iteration 73, loss = 0.50330221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60338816\n",
      "Iteration 2, loss = 0.54259580\n",
      "Iteration 3, loss = 0.53790001\n",
      "Iteration 4, loss = 0.53164550\n",
      "Iteration 5, loss = 0.52664795\n",
      "Iteration 6, loss = 0.52388009\n",
      "Iteration 7, loss = 0.52214965\n",
      "Iteration 8, loss = 0.52105659\n",
      "Iteration 9, loss = 0.51939702\n",
      "Iteration 10, loss = 0.52003534\n",
      "Iteration 11, loss = 0.51991168\n",
      "Iteration 12, loss = 0.51876069\n",
      "Iteration 13, loss = 0.51898353\n",
      "Iteration 14, loss = 0.51827653\n",
      "Iteration 15, loss = 0.51860502\n",
      "Iteration 16, loss = 0.51811227\n",
      "Iteration 17, loss = 0.51821071\n",
      "Iteration 18, loss = 0.51800604\n",
      "Iteration 19, loss = 0.51820695\n",
      "Iteration 20, loss = 0.51841365\n",
      "Iteration 21, loss = 0.51784190\n",
      "Iteration 22, loss = 0.51887564\n",
      "Iteration 23, loss = 0.51800157\n",
      "Iteration 24, loss = 0.51800037\n",
      "Iteration 25, loss = 0.51778460\n",
      "Iteration 26, loss = 0.51881192\n",
      "Iteration 27, loss = 0.51762110\n",
      "Iteration 28, loss = 0.51829206\n",
      "Iteration 29, loss = 0.51795483\n",
      "Iteration 30, loss = 0.51857488\n",
      "Iteration 31, loss = 0.51682394\n",
      "Iteration 32, loss = 0.51676360\n",
      "Iteration 33, loss = 0.51685446\n",
      "Iteration 34, loss = 0.51703532\n",
      "Iteration 35, loss = 0.51617096\n",
      "Iteration 36, loss = 0.51699233\n",
      "Iteration 37, loss = 0.51741644\n",
      "Iteration 38, loss = 0.51619415\n",
      "Iteration 39, loss = 0.51761863\n",
      "Iteration 40, loss = 0.51524324\n",
      "Iteration 41, loss = 0.51522237\n",
      "Iteration 42, loss = 0.51516695\n",
      "Iteration 43, loss = 0.51455101\n",
      "Iteration 44, loss = 0.51440267\n",
      "Iteration 45, loss = 0.51415855\n",
      "Iteration 46, loss = 0.51403829\n",
      "Iteration 47, loss = 0.51392487\n",
      "Iteration 48, loss = 0.51369422\n",
      "Iteration 49, loss = 0.51211915\n",
      "Iteration 50, loss = 0.51331874\n",
      "Iteration 51, loss = 0.51244893\n",
      "Iteration 52, loss = 0.51217281\n",
      "Iteration 53, loss = 0.51214577\n",
      "Iteration 54, loss = 0.51167697\n",
      "Iteration 55, loss = 0.51227367\n",
      "Iteration 56, loss = 0.51097163\n",
      "Iteration 57, loss = 0.51002199\n",
      "Iteration 58, loss = 0.51167771\n",
      "Iteration 59, loss = 0.50980692\n",
      "Iteration 60, loss = 0.50951739\n",
      "Iteration 61, loss = 0.50960058\n",
      "Iteration 62, loss = 0.50989629\n",
      "Iteration 63, loss = 0.51082033\n",
      "Iteration 64, loss = 0.50984090\n",
      "Iteration 65, loss = 0.50944599\n",
      "Iteration 66, loss = 0.50855617\n",
      "Iteration 67, loss = 0.50830941\n",
      "Iteration 68, loss = 0.50798055\n",
      "Iteration 69, loss = 0.50771475\n",
      "Iteration 70, loss = 0.50784971\n",
      "Iteration 71, loss = 0.50835216\n",
      "Iteration 72, loss = 0.51000057\n",
      "Iteration 73, loss = 0.50951672\n",
      "Iteration 74, loss = 0.50831362\n",
      "Iteration 75, loss = 0.50816125\n",
      "Iteration 76, loss = 0.50746717\n",
      "Iteration 77, loss = 0.50755560\n",
      "Iteration 78, loss = 0.50697538\n",
      "Iteration 79, loss = 0.50706797\n",
      "Iteration 80, loss = 0.50780729\n",
      "Iteration 81, loss = 0.50717853\n",
      "Iteration 82, loss = 0.50671084\n",
      "Iteration 83, loss = 0.50711273\n",
      "Iteration 84, loss = 0.50762244\n",
      "Iteration 85, loss = 0.50745319\n",
      "Iteration 86, loss = 0.50747905\n",
      "Iteration 87, loss = 0.50745454\n",
      "Iteration 88, loss = 0.50746086\n",
      "Iteration 89, loss = 0.50704955\n",
      "Iteration 90, loss = 0.50720500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 91, loss = 0.50614380\n",
      "Iteration 92, loss = 0.50658062\n",
      "Iteration 93, loss = 0.50695724\n",
      "Iteration 94, loss = 0.50732631\n",
      "Iteration 95, loss = 0.50800467\n",
      "Iteration 96, loss = 0.50644974\n",
      "Iteration 97, loss = 0.50645996\n",
      "Iteration 98, loss = 0.50839438\n",
      "Iteration 99, loss = 0.50635508\n",
      "Iteration 100, loss = 0.50710056\n",
      "Iteration 101, loss = 0.50726447\n",
      "Iteration 102, loss = 0.50660849\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59798232\n",
      "Iteration 2, loss = 0.54590070\n",
      "Iteration 3, loss = 0.54076231\n",
      "Iteration 4, loss = 0.53606713\n",
      "Iteration 5, loss = 0.53179987\n",
      "Iteration 6, loss = 0.52879864\n",
      "Iteration 7, loss = 0.52671964\n",
      "Iteration 8, loss = 0.52599469\n",
      "Iteration 9, loss = 0.52524594\n",
      "Iteration 10, loss = 0.52412405\n",
      "Iteration 11, loss = 0.52337487\n",
      "Iteration 12, loss = 0.52285531\n",
      "Iteration 13, loss = 0.52199639\n",
      "Iteration 14, loss = 0.52283119\n",
      "Iteration 15, loss = 0.52137126\n",
      "Iteration 16, loss = 0.52097699\n",
      "Iteration 17, loss = 0.52117757\n",
      "Iteration 18, loss = 0.52117513\n",
      "Iteration 19, loss = 0.52079262\n",
      "Iteration 20, loss = 0.52138383\n",
      "Iteration 21, loss = 0.52048410\n",
      "Iteration 22, loss = 0.51992065\n",
      "Iteration 23, loss = 0.52107673\n",
      "Iteration 24, loss = 0.51890737\n",
      "Iteration 25, loss = 0.52039073\n",
      "Iteration 26, loss = 0.51923436\n",
      "Iteration 27, loss = 0.51909948\n",
      "Iteration 28, loss = 0.51899831\n",
      "Iteration 29, loss = 0.51832500\n",
      "Iteration 30, loss = 0.51862002\n",
      "Iteration 31, loss = 0.51745987\n",
      "Iteration 32, loss = 0.51802253\n",
      "Iteration 33, loss = 0.51878490\n",
      "Iteration 34, loss = 0.51739508\n",
      "Iteration 35, loss = 0.51755329\n",
      "Iteration 36, loss = 0.51742193\n",
      "Iteration 37, loss = 0.51675636\n",
      "Iteration 38, loss = 0.51706081\n",
      "Iteration 39, loss = 0.51676767\n",
      "Iteration 40, loss = 0.51660460\n",
      "Iteration 41, loss = 0.51644425\n",
      "Iteration 42, loss = 0.51538364\n",
      "Iteration 43, loss = 0.51585699\n",
      "Iteration 44, loss = 0.51494358\n",
      "Iteration 45, loss = 0.51505904\n",
      "Iteration 46, loss = 0.51530656\n",
      "Iteration 47, loss = 0.51485945\n",
      "Iteration 48, loss = 0.51527372\n",
      "Iteration 49, loss = 0.51434491\n",
      "Iteration 50, loss = 0.51511679\n",
      "Iteration 51, loss = 0.51391772\n",
      "Iteration 52, loss = 0.51574502\n",
      "Iteration 53, loss = 0.51471562\n",
      "Iteration 54, loss = 0.51428208\n",
      "Iteration 55, loss = 0.51459250\n",
      "Iteration 56, loss = 0.51398579\n",
      "Iteration 57, loss = 0.51471787\n",
      "Iteration 58, loss = 0.51335660\n",
      "Iteration 59, loss = 0.51475036\n",
      "Iteration 60, loss = 0.51362447\n",
      "Iteration 61, loss = 0.51309783\n",
      "Iteration 62, loss = 0.51290037\n",
      "Iteration 63, loss = 0.51381835\n",
      "Iteration 64, loss = 0.51247497\n",
      "Iteration 65, loss = 0.51322168\n",
      "Iteration 66, loss = 0.51390947\n",
      "Iteration 67, loss = 0.51271475\n",
      "Iteration 68, loss = 0.51265130\n",
      "Iteration 69, loss = 0.51307622\n",
      "Iteration 70, loss = 0.51329192\n",
      "Iteration 71, loss = 0.51176199\n",
      "Iteration 72, loss = 0.51225773\n",
      "Iteration 73, loss = 0.51205609\n",
      "Iteration 74, loss = 0.51306899\n",
      "Iteration 75, loss = 0.51229137\n",
      "Iteration 76, loss = 0.51168905\n",
      "Iteration 77, loss = 0.51151997\n",
      "Iteration 78, loss = 0.51184708\n",
      "Iteration 79, loss = 0.51313225\n",
      "Iteration 80, loss = 0.51167712\n",
      "Iteration 81, loss = 0.51203261\n",
      "Iteration 82, loss = 0.51131879\n",
      "Iteration 83, loss = 0.51296077\n",
      "Iteration 84, loss = 0.51135530\n",
      "Iteration 85, loss = 0.51182538\n",
      "Iteration 86, loss = 0.51199813\n",
      "Iteration 87, loss = 0.51081925\n",
      "Iteration 88, loss = 0.51052737\n",
      "Iteration 89, loss = 0.51193857\n",
      "Iteration 90, loss = 0.51257736\n",
      "Iteration 91, loss = 0.51338537\n",
      "Iteration 92, loss = 0.51090444\n",
      "Iteration 93, loss = 0.51285534\n",
      "Iteration 94, loss = 0.51234591\n",
      "Iteration 95, loss = 0.51239121\n",
      "Iteration 96, loss = 0.51141137\n",
      "Iteration 97, loss = 0.51077085\n",
      "Iteration 98, loss = 0.51100028\n",
      "Iteration 99, loss = 0.51109381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59749914\n",
      "Iteration 2, loss = 0.54119534\n",
      "Iteration 3, loss = 0.52663432\n",
      "Iteration 4, loss = 0.51857983\n",
      "Iteration 5, loss = 0.51069053\n",
      "Iteration 6, loss = 0.50544123\n",
      "Iteration 7, loss = 0.50355653\n",
      "Iteration 8, loss = 0.50322599\n",
      "Iteration 9, loss = 0.50049076\n",
      "Iteration 10, loss = 0.49869999\n",
      "Iteration 11, loss = 0.49812157\n",
      "Iteration 12, loss = 0.49702201\n",
      "Iteration 13, loss = 0.49664625\n",
      "Iteration 14, loss = 0.49638258\n",
      "Iteration 15, loss = 0.49446232\n",
      "Iteration 16, loss = 0.49503250\n",
      "Iteration 17, loss = 0.49441594\n",
      "Iteration 18, loss = 0.49197441\n",
      "Iteration 19, loss = 0.49076137\n",
      "Iteration 20, loss = 0.49083488\n",
      "Iteration 21, loss = 0.49119869\n",
      "Iteration 22, loss = 0.49189227\n",
      "Iteration 23, loss = 0.49026946\n",
      "Iteration 24, loss = 0.48931276\n",
      "Iteration 25, loss = 0.48868195\n",
      "Iteration 26, loss = 0.48837650\n",
      "Iteration 27, loss = 0.48885143\n",
      "Iteration 28, loss = 0.48763252\n",
      "Iteration 29, loss = 0.48617761\n",
      "Iteration 30, loss = 0.48853477\n",
      "Iteration 31, loss = 0.48601624\n",
      "Iteration 32, loss = 0.48474418\n",
      "Iteration 33, loss = 0.48570195\n",
      "Iteration 34, loss = 0.48566983\n",
      "Iteration 35, loss = 0.48552762\n",
      "Iteration 36, loss = 0.48584084\n",
      "Iteration 37, loss = 0.48696392\n",
      "Iteration 38, loss = 0.48681364\n",
      "Iteration 39, loss = 0.48479064\n",
      "Iteration 40, loss = 0.48343309\n",
      "Iteration 41, loss = 0.48290371\n",
      "Iteration 42, loss = 0.48305516\n",
      "Iteration 43, loss = 0.48260708\n",
      "Iteration 44, loss = 0.48242381\n",
      "Iteration 45, loss = 0.48190918\n",
      "Iteration 46, loss = 0.48344819\n",
      "Iteration 47, loss = 0.48214818\n",
      "Iteration 48, loss = 0.48215387\n",
      "Iteration 49, loss = 0.48165560\n",
      "Iteration 50, loss = 0.48251018\n",
      "Iteration 51, loss = 0.48380772\n",
      "Iteration 52, loss = 0.48179809\n",
      "Iteration 53, loss = 0.48128292\n",
      "Iteration 54, loss = 0.48396178\n",
      "Iteration 55, loss = 0.48345137\n",
      "Iteration 56, loss = 0.48276508\n",
      "Iteration 57, loss = 0.48504026\n",
      "Iteration 58, loss = 0.48221122\n",
      "Iteration 59, loss = 0.48197630\n",
      "Iteration 60, loss = 0.48353183\n",
      "Iteration 61, loss = 0.48359155\n",
      "Iteration 62, loss = 0.48153629\n",
      "Iteration 63, loss = 0.48158425\n",
      "Iteration 64, loss = 0.48201828\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60221842\n",
      "Iteration 2, loss = 0.53933277\n",
      "Iteration 3, loss = 0.52705557\n",
      "Iteration 4, loss = 0.51251193\n",
      "Iteration 5, loss = 0.50482778\n",
      "Iteration 6, loss = 0.50154295\n",
      "Iteration 7, loss = 0.50045862\n",
      "Iteration 8, loss = 0.49859795\n",
      "Iteration 9, loss = 0.49763061\n",
      "Iteration 10, loss = 0.49653310\n",
      "Iteration 11, loss = 0.49553251\n",
      "Iteration 12, loss = 0.49451708\n",
      "Iteration 13, loss = 0.49447347\n",
      "Iteration 14, loss = 0.49309117\n",
      "Iteration 15, loss = 0.49331555\n",
      "Iteration 16, loss = 0.49256161\n",
      "Iteration 17, loss = 0.49299391\n",
      "Iteration 18, loss = 0.49060266\n",
      "Iteration 19, loss = 0.49194015\n",
      "Iteration 20, loss = 0.49205596\n",
      "Iteration 21, loss = 0.49006819\n",
      "Iteration 22, loss = 0.49195303\n",
      "Iteration 23, loss = 0.49135799\n",
      "Iteration 24, loss = 0.48935752\n",
      "Iteration 25, loss = 0.48833407\n",
      "Iteration 26, loss = 0.48913144\n",
      "Iteration 27, loss = 0.48844108\n",
      "Iteration 28, loss = 0.48958005\n",
      "Iteration 29, loss = 0.48929980\n",
      "Iteration 30, loss = 0.48893913\n",
      "Iteration 31, loss = 0.48881166\n",
      "Iteration 32, loss = 0.48845504\n",
      "Iteration 33, loss = 0.48871853\n",
      "Iteration 34, loss = 0.48959970\n",
      "Iteration 35, loss = 0.48843395\n",
      "Iteration 36, loss = 0.49096485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59570546\n",
      "Iteration 2, loss = 0.53734578\n",
      "Iteration 3, loss = 0.51626366\n",
      "Iteration 4, loss = 0.50686217\n",
      "Iteration 5, loss = 0.50415363\n",
      "Iteration 6, loss = 0.50414731\n",
      "Iteration 7, loss = 0.50304028\n",
      "Iteration 8, loss = 0.50156207\n",
      "Iteration 9, loss = 0.50280518\n",
      "Iteration 10, loss = 0.50055481\n",
      "Iteration 11, loss = 0.49960765\n",
      "Iteration 12, loss = 0.49887756\n",
      "Iteration 13, loss = 0.49816292\n",
      "Iteration 14, loss = 0.49876120\n",
      "Iteration 15, loss = 0.49690263\n",
      "Iteration 16, loss = 0.49829217\n",
      "Iteration 17, loss = 0.49616204\n",
      "Iteration 18, loss = 0.49427034\n",
      "Iteration 19, loss = 0.49677214\n",
      "Iteration 20, loss = 0.49601288\n",
      "Iteration 21, loss = 0.49569373\n",
      "Iteration 22, loss = 0.49273614\n",
      "Iteration 23, loss = 0.49352849\n",
      "Iteration 24, loss = 0.49239398\n",
      "Iteration 25, loss = 0.49468437\n",
      "Iteration 26, loss = 0.49312140\n",
      "Iteration 27, loss = 0.49406156\n",
      "Iteration 28, loss = 0.49321522\n",
      "Iteration 29, loss = 0.49340136\n",
      "Iteration 30, loss = 0.49324213\n",
      "Iteration 31, loss = 0.49092315\n",
      "Iteration 32, loss = 0.49191829\n",
      "Iteration 33, loss = 0.49137656\n",
      "Iteration 34, loss = 0.49192618\n",
      "Iteration 35, loss = 0.49171801\n",
      "Iteration 36, loss = 0.49140334\n",
      "Iteration 37, loss = 0.49149926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.49113539\n",
      "Iteration 39, loss = 0.49079304\n",
      "Iteration 40, loss = 0.49280107\n",
      "Iteration 41, loss = 0.49080867\n",
      "Iteration 42, loss = 0.48935791\n",
      "Iteration 43, loss = 0.49102427\n",
      "Iteration 44, loss = 0.49178935\n",
      "Iteration 45, loss = 0.49168590\n",
      "Iteration 46, loss = 0.49144615\n",
      "Iteration 47, loss = 0.49023090\n",
      "Iteration 48, loss = 0.48998796\n",
      "Iteration 49, loss = 0.48976546\n",
      "Iteration 50, loss = 0.48890626\n",
      "Iteration 51, loss = 0.48931334\n",
      "Iteration 52, loss = 0.49023842\n",
      "Iteration 53, loss = 0.49224430\n",
      "Iteration 54, loss = 0.48715172\n",
      "Iteration 55, loss = 0.48814044\n",
      "Iteration 56, loss = 0.48910459\n",
      "Iteration 57, loss = 0.48951076\n",
      "Iteration 58, loss = 0.48845241\n",
      "Iteration 59, loss = 0.49030570\n",
      "Iteration 60, loss = 0.48937859\n",
      "Iteration 61, loss = 0.48836234\n",
      "Iteration 62, loss = 0.48808293\n",
      "Iteration 63, loss = 0.48907566\n",
      "Iteration 64, loss = 0.48849806\n",
      "Iteration 65, loss = 0.48888880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54149901\n",
      "Iteration 2, loss = 0.51909663\n",
      "Iteration 3, loss = 0.50786329\n",
      "Iteration 4, loss = 0.49419540\n",
      "Iteration 5, loss = 0.48695938\n",
      "Iteration 6, loss = 0.48426771\n",
      "Iteration 7, loss = 0.48133618\n",
      "Iteration 8, loss = 0.47894088\n",
      "Iteration 9, loss = 0.47661065\n",
      "Iteration 10, loss = 0.47313878\n",
      "Iteration 11, loss = 0.47140340\n",
      "Iteration 12, loss = 0.47040013\n",
      "Iteration 13, loss = 0.47077654\n",
      "Iteration 14, loss = 0.46891588\n",
      "Iteration 15, loss = 0.46804960\n",
      "Iteration 16, loss = 0.46625394\n",
      "Iteration 17, loss = 0.46456318\n",
      "Iteration 18, loss = 0.46377265\n",
      "Iteration 19, loss = 0.46212958\n",
      "Iteration 20, loss = 0.46288788\n",
      "Iteration 21, loss = 0.46405993\n",
      "Iteration 22, loss = 0.46097038\n",
      "Iteration 23, loss = 0.46034885\n",
      "Iteration 24, loss = 0.46104261\n",
      "Iteration 25, loss = 0.45918876\n",
      "Iteration 26, loss = 0.46142797\n",
      "Iteration 27, loss = 0.46196474\n",
      "Iteration 28, loss = 0.46053288\n",
      "Iteration 29, loss = 0.45942334\n",
      "Iteration 30, loss = 0.46116941\n",
      "Iteration 31, loss = 0.46048844\n",
      "Iteration 32, loss = 0.45829056\n",
      "Iteration 33, loss = 0.45885666\n",
      "Iteration 34, loss = 0.45840345\n",
      "Iteration 35, loss = 0.45741539\n",
      "Iteration 36, loss = 0.45848307\n",
      "Iteration 37, loss = 0.45918912\n",
      "Iteration 38, loss = 0.45936355\n",
      "Iteration 39, loss = 0.45934789\n",
      "Iteration 40, loss = 0.45592494\n",
      "Iteration 41, loss = 0.45544046\n",
      "Iteration 42, loss = 0.45832701\n",
      "Iteration 43, loss = 0.45528230\n",
      "Iteration 44, loss = 0.45486401\n",
      "Iteration 45, loss = 0.45539781\n",
      "Iteration 46, loss = 0.45886151\n",
      "Iteration 47, loss = 0.45504390\n",
      "Iteration 48, loss = 0.45506850\n",
      "Iteration 49, loss = 0.45441427\n",
      "Iteration 50, loss = 0.45520742\n",
      "Iteration 51, loss = 0.45423904\n",
      "Iteration 52, loss = 0.45598155\n",
      "Iteration 53, loss = 0.45358542\n",
      "Iteration 54, loss = 0.45689602\n",
      "Iteration 55, loss = 0.45659119\n",
      "Iteration 56, loss = 0.45579668\n",
      "Iteration 57, loss = 0.45691558\n",
      "Iteration 58, loss = 0.45477726\n",
      "Iteration 59, loss = 0.45394972\n",
      "Iteration 60, loss = 0.45559443\n",
      "Iteration 61, loss = 0.45505119\n",
      "Iteration 62, loss = 0.45574702\n",
      "Iteration 63, loss = 0.45241045\n",
      "Iteration 64, loss = 0.45217844\n",
      "Iteration 65, loss = 0.45611330\n",
      "Iteration 66, loss = 0.45326430\n",
      "Iteration 67, loss = 0.45321682\n",
      "Iteration 68, loss = 0.45468442\n",
      "Iteration 69, loss = 0.45118993\n",
      "Iteration 70, loss = 0.45556538\n",
      "Iteration 71, loss = 0.45343747\n",
      "Iteration 72, loss = 0.45214110\n",
      "Iteration 73, loss = 0.45482517\n",
      "Iteration 74, loss = 0.45136004\n",
      "Iteration 75, loss = 0.45091765\n",
      "Iteration 76, loss = 0.45109438\n",
      "Iteration 77, loss = 0.45040578\n",
      "Iteration 78, loss = 0.45125670\n",
      "Iteration 79, loss = 0.45255793\n",
      "Iteration 80, loss = 0.45035544\n",
      "Iteration 81, loss = 0.45267739\n",
      "Iteration 82, loss = 0.45008142\n",
      "Iteration 83, loss = 0.45131455\n",
      "Iteration 84, loss = 0.44985872\n",
      "Iteration 85, loss = 0.45087426\n",
      "Iteration 86, loss = 0.45099113\n",
      "Iteration 87, loss = 0.44951161\n",
      "Iteration 88, loss = 0.44937616\n",
      "Iteration 89, loss = 0.44920156\n",
      "Iteration 90, loss = 0.45026546\n",
      "Iteration 91, loss = 0.44980811\n",
      "Iteration 92, loss = 0.44950652\n",
      "Iteration 93, loss = 0.45021780\n",
      "Iteration 94, loss = 0.45111736\n",
      "Iteration 95, loss = 0.44886728\n",
      "Iteration 96, loss = 0.45042207\n",
      "Iteration 97, loss = 0.44894536\n",
      "Iteration 98, loss = 0.45038151\n",
      "Iteration 99, loss = 0.44998460\n",
      "Iteration 100, loss = 0.44794011\n",
      "Iteration 101, loss = 0.44952108\n",
      "Iteration 102, loss = 0.44899384\n",
      "Iteration 103, loss = 0.45580493\n",
      "Iteration 104, loss = 0.45359111\n",
      "Iteration 105, loss = 0.45280315\n",
      "Iteration 106, loss = 0.45607926\n",
      "Iteration 107, loss = 0.45539098\n",
      "Iteration 108, loss = 0.45582698\n",
      "Iteration 109, loss = 0.45218830\n",
      "Iteration 110, loss = 0.45236623\n",
      "Iteration 111, loss = 0.45330475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54168497\n",
      "Iteration 2, loss = 0.51899400\n",
      "Iteration 3, loss = 0.51220148\n",
      "Iteration 4, loss = 0.50858437\n",
      "Iteration 5, loss = 0.50250646\n",
      "Iteration 6, loss = 0.49755807\n",
      "Iteration 7, loss = 0.49472483\n",
      "Iteration 8, loss = 0.49169885\n",
      "Iteration 9, loss = 0.48505582\n",
      "Iteration 10, loss = 0.48255903\n",
      "Iteration 11, loss = 0.47996519\n",
      "Iteration 12, loss = 0.47779298\n",
      "Iteration 13, loss = 0.47710273\n",
      "Iteration 14, loss = 0.47429928\n",
      "Iteration 15, loss = 0.47312774\n",
      "Iteration 16, loss = 0.47169685\n",
      "Iteration 17, loss = 0.46962232\n",
      "Iteration 18, loss = 0.46960371\n",
      "Iteration 19, loss = 0.47023712\n",
      "Iteration 20, loss = 0.46884960\n",
      "Iteration 21, loss = 0.47056013\n",
      "Iteration 22, loss = 0.46837052\n",
      "Iteration 23, loss = 0.46614881\n",
      "Iteration 24, loss = 0.46749161\n",
      "Iteration 25, loss = 0.46611281\n",
      "Iteration 26, loss = 0.46598211\n",
      "Iteration 27, loss = 0.46426623\n",
      "Iteration 28, loss = 0.46444490\n",
      "Iteration 29, loss = 0.46519550\n",
      "Iteration 30, loss = 0.46315247\n",
      "Iteration 31, loss = 0.46291509\n",
      "Iteration 32, loss = 0.46435596\n",
      "Iteration 33, loss = 0.46373613\n",
      "Iteration 34, loss = 0.46277850\n",
      "Iteration 35, loss = 0.46502853\n",
      "Iteration 36, loss = 0.46482100\n",
      "Iteration 37, loss = 0.46337664\n",
      "Iteration 38, loss = 0.46433907\n",
      "Iteration 39, loss = 0.46240965\n",
      "Iteration 40, loss = 0.46232503\n",
      "Iteration 41, loss = 0.46291060\n",
      "Iteration 42, loss = 0.46382070\n",
      "Iteration 43, loss = 0.46299139\n",
      "Iteration 44, loss = 0.46305781\n",
      "Iteration 45, loss = 0.46430202\n",
      "Iteration 46, loss = 0.46123069\n",
      "Iteration 47, loss = 0.46104322\n",
      "Iteration 48, loss = 0.46148883\n",
      "Iteration 49, loss = 0.46194899\n",
      "Iteration 50, loss = 0.46042264\n",
      "Iteration 51, loss = 0.46155882\n",
      "Iteration 52, loss = 0.45957218\n",
      "Iteration 53, loss = 0.46263970\n",
      "Iteration 54, loss = 0.46232609\n",
      "Iteration 55, loss = 0.46203842\n",
      "Iteration 56, loss = 0.46020526\n",
      "Iteration 57, loss = 0.45915176\n",
      "Iteration 58, loss = 0.45883133\n",
      "Iteration 59, loss = 0.45806012\n",
      "Iteration 60, loss = 0.45852174\n",
      "Iteration 61, loss = 0.45973600\n",
      "Iteration 62, loss = 0.45934965\n",
      "Iteration 63, loss = 0.45704398\n",
      "Iteration 64, loss = 0.45862401\n",
      "Iteration 65, loss = 0.45836805\n",
      "Iteration 66, loss = 0.45755412\n",
      "Iteration 67, loss = 0.45715604\n",
      "Iteration 68, loss = 0.45806205\n",
      "Iteration 69, loss = 0.45729573\n",
      "Iteration 70, loss = 0.45543156\n",
      "Iteration 71, loss = 0.45560628\n",
      "Iteration 72, loss = 0.45596844\n",
      "Iteration 73, loss = 0.45699661\n",
      "Iteration 74, loss = 0.45640558\n",
      "Iteration 75, loss = 0.45613847\n",
      "Iteration 76, loss = 0.45459503\n",
      "Iteration 77, loss = 0.45361684\n",
      "Iteration 78, loss = 0.45474087\n",
      "Iteration 79, loss = 0.45473655\n",
      "Iteration 80, loss = 0.45818947\n",
      "Iteration 81, loss = 0.45613086\n",
      "Iteration 82, loss = 0.45498143\n",
      "Iteration 83, loss = 0.45503264\n",
      "Iteration 84, loss = 0.45394232\n",
      "Iteration 85, loss = 0.45432406\n",
      "Iteration 86, loss = 0.45333351\n",
      "Iteration 87, loss = 0.45341134\n",
      "Iteration 88, loss = 0.45662824\n",
      "Iteration 89, loss = 0.45259441\n",
      "Iteration 90, loss = 0.45699654\n",
      "Iteration 91, loss = 0.45269735\n",
      "Iteration 92, loss = 0.45537443\n",
      "Iteration 93, loss = 0.45415366\n",
      "Iteration 94, loss = 0.45496546\n",
      "Iteration 95, loss = 0.45253150\n",
      "Iteration 96, loss = 0.45567440\n",
      "Iteration 97, loss = 0.45495347\n",
      "Iteration 98, loss = 0.45078520\n",
      "Iteration 99, loss = 0.45193615\n",
      "Iteration 100, loss = 0.45173032\n",
      "Iteration 101, loss = 0.45494273\n",
      "Iteration 102, loss = 0.45342154\n",
      "Iteration 103, loss = 0.45285080\n",
      "Iteration 104, loss = 0.45358586\n",
      "Iteration 105, loss = 0.45404816\n",
      "Iteration 106, loss = 0.45450657\n",
      "Iteration 107, loss = 0.45229740\n",
      "Iteration 108, loss = 0.45387517\n",
      "Iteration 109, loss = 0.45488779\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54104656\n",
      "Iteration 2, loss = 0.51898927\n",
      "Iteration 3, loss = 0.50295979\n",
      "Iteration 4, loss = 0.48696210\n",
      "Iteration 5, loss = 0.48143967\n",
      "Iteration 6, loss = 0.47856112\n",
      "Iteration 7, loss = 0.47792961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.47530536\n",
      "Iteration 9, loss = 0.47542522\n",
      "Iteration 10, loss = 0.47385917\n",
      "Iteration 11, loss = 0.47264636\n",
      "Iteration 12, loss = 0.46986775\n",
      "Iteration 13, loss = 0.46916787\n",
      "Iteration 14, loss = 0.46878398\n",
      "Iteration 15, loss = 0.46950949\n",
      "Iteration 16, loss = 0.46665268\n",
      "Iteration 17, loss = 0.46837495\n",
      "Iteration 18, loss = 0.46574979\n",
      "Iteration 19, loss = 0.46640046\n",
      "Iteration 20, loss = 0.46658128\n",
      "Iteration 21, loss = 0.46658645\n",
      "Iteration 22, loss = 0.46486224\n",
      "Iteration 23, loss = 0.46446675\n",
      "Iteration 24, loss = 0.46427067\n",
      "Iteration 25, loss = 0.46448909\n",
      "Iteration 26, loss = 0.46422145\n",
      "Iteration 27, loss = 0.46539135\n",
      "Iteration 28, loss = 0.46437309\n",
      "Iteration 29, loss = 0.46462046\n",
      "Iteration 30, loss = 0.46319643\n",
      "Iteration 31, loss = 0.46352793\n",
      "Iteration 32, loss = 0.46369274\n",
      "Iteration 33, loss = 0.46313550\n",
      "Iteration 34, loss = 0.46353099\n",
      "Iteration 35, loss = 0.46415970\n",
      "Iteration 36, loss = 0.46431181\n",
      "Iteration 37, loss = 0.46391271\n",
      "Iteration 38, loss = 0.46348118\n",
      "Iteration 39, loss = 0.46345797\n",
      "Iteration 40, loss = 0.46284344\n",
      "Iteration 41, loss = 0.46237249\n",
      "Iteration 42, loss = 0.46352227\n",
      "Iteration 43, loss = 0.46216471\n",
      "Iteration 44, loss = 0.46303084\n",
      "Iteration 45, loss = 0.46283370\n",
      "Iteration 46, loss = 0.46275158\n",
      "Iteration 47, loss = 0.46258151\n",
      "Iteration 48, loss = 0.46363985\n",
      "Iteration 49, loss = 0.46285578\n",
      "Iteration 50, loss = 0.46299199\n",
      "Iteration 51, loss = 0.46242092\n",
      "Iteration 52, loss = 0.46386813\n",
      "Iteration 53, loss = 0.46349319\n",
      "Iteration 54, loss = 0.46312327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53900192\n",
      "Iteration 2, loss = 0.51256204\n",
      "Iteration 3, loss = 0.49080409\n",
      "Iteration 4, loss = 0.48094025\n",
      "Iteration 5, loss = 0.47545577\n",
      "Iteration 6, loss = 0.47369444\n",
      "Iteration 7, loss = 0.47105547\n",
      "Iteration 8, loss = 0.47078152\n",
      "Iteration 9, loss = 0.47054371\n",
      "Iteration 10, loss = 0.46816514\n",
      "Iteration 11, loss = 0.46790662\n",
      "Iteration 12, loss = 0.46784555\n",
      "Iteration 13, loss = 0.46714174\n",
      "Iteration 14, loss = 0.46781985\n",
      "Iteration 15, loss = 0.46490404\n",
      "Iteration 16, loss = 0.46431156\n",
      "Iteration 17, loss = 0.46445382\n",
      "Iteration 18, loss = 0.46657293\n",
      "Iteration 19, loss = 0.46269582\n",
      "Iteration 20, loss = 0.46255160\n",
      "Iteration 21, loss = 0.46513445\n",
      "Iteration 22, loss = 0.46072444\n",
      "Iteration 23, loss = 0.46095393\n",
      "Iteration 24, loss = 0.46220174\n",
      "Iteration 25, loss = 0.46103509\n",
      "Iteration 26, loss = 0.46171271\n",
      "Iteration 27, loss = 0.45927264\n",
      "Iteration 28, loss = 0.46042159\n",
      "Iteration 29, loss = 0.45982561\n",
      "Iteration 30, loss = 0.46093879\n",
      "Iteration 31, loss = 0.46303729\n",
      "Iteration 32, loss = 0.45775781\n",
      "Iteration 33, loss = 0.45845960\n",
      "Iteration 34, loss = 0.45868321\n",
      "Iteration 35, loss = 0.45904592\n",
      "Iteration 36, loss = 0.45735660\n",
      "Iteration 37, loss = 0.45859056\n",
      "Iteration 38, loss = 0.45636103\n",
      "Iteration 39, loss = 0.45537635\n",
      "Iteration 40, loss = 0.45303476\n",
      "Iteration 41, loss = 0.45280266\n",
      "Iteration 42, loss = 0.45671401\n",
      "Iteration 43, loss = 0.45569531\n",
      "Iteration 44, loss = 0.45390843\n",
      "Iteration 45, loss = 0.45485621\n",
      "Iteration 46, loss = 0.45576095\n",
      "Iteration 47, loss = 0.45331383\n",
      "Iteration 48, loss = 0.45298351\n",
      "Iteration 49, loss = 0.45317015\n",
      "Iteration 50, loss = 0.45463680\n",
      "Iteration 51, loss = 0.45216793\n",
      "Iteration 52, loss = 0.45241170\n",
      "Iteration 53, loss = 0.45227077\n",
      "Iteration 54, loss = 0.45285151\n",
      "Iteration 55, loss = 0.45235513\n",
      "Iteration 56, loss = 0.45762076\n",
      "Iteration 57, loss = 0.45227902\n",
      "Iteration 58, loss = 0.45050453\n",
      "Iteration 59, loss = 0.44909980\n",
      "Iteration 60, loss = 0.45524518\n",
      "Iteration 61, loss = 0.44926835\n",
      "Iteration 62, loss = 0.45098307\n",
      "Iteration 63, loss = 0.45008450\n",
      "Iteration 64, loss = 0.44908076\n",
      "Iteration 65, loss = 0.44832309\n",
      "Iteration 66, loss = 0.44748842\n",
      "Iteration 67, loss = 0.44737449\n",
      "Iteration 68, loss = 0.44773117\n",
      "Iteration 69, loss = 0.44601546\n",
      "Iteration 70, loss = 0.44943723\n",
      "Iteration 71, loss = 0.45349562\n",
      "Iteration 72, loss = 0.44834301\n",
      "Iteration 73, loss = 0.44809406\n",
      "Iteration 74, loss = 0.44593646\n",
      "Iteration 75, loss = 0.44835900\n",
      "Iteration 76, loss = 0.44645512\n",
      "Iteration 77, loss = 0.44504540\n",
      "Iteration 78, loss = 0.44627962\n",
      "Iteration 79, loss = 0.44711876\n",
      "Iteration 80, loss = 0.44372314\n",
      "Iteration 81, loss = 0.44341577\n",
      "Iteration 82, loss = 0.44391027\n",
      "Iteration 83, loss = 0.44542922\n",
      "Iteration 84, loss = 0.44621797\n",
      "Iteration 85, loss = 0.44655471\n",
      "Iteration 86, loss = 0.44541054\n",
      "Iteration 87, loss = 0.44433576\n",
      "Iteration 88, loss = 0.44712470\n",
      "Iteration 89, loss = 0.44366196\n",
      "Iteration 90, loss = 0.44554205\n",
      "Iteration 91, loss = 0.44273025\n",
      "Iteration 92, loss = 0.44349866\n",
      "Iteration 93, loss = 0.44446616\n",
      "Iteration 94, loss = 0.44304958\n",
      "Iteration 95, loss = 0.44565597\n",
      "Iteration 96, loss = 0.44492469\n",
      "Iteration 97, loss = 0.44158727\n",
      "Iteration 98, loss = 0.44256414\n",
      "Iteration 99, loss = 0.44361264\n",
      "Iteration 100, loss = 0.44359097\n",
      "Iteration 101, loss = 0.44240570\n",
      "Iteration 102, loss = 0.44250629\n",
      "Iteration 103, loss = 0.44448463\n",
      "Iteration 104, loss = 0.44203728\n",
      "Iteration 105, loss = 0.44312291\n",
      "Iteration 106, loss = 0.44368861\n",
      "Iteration 107, loss = 0.44228590\n",
      "Iteration 108, loss = 0.44454778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53933002\n",
      "Iteration 2, loss = 0.51266442\n",
      "Iteration 3, loss = 0.50056694\n",
      "Iteration 4, loss = 0.48787482\n",
      "Iteration 5, loss = 0.48034010\n",
      "Iteration 6, loss = 0.47890776\n",
      "Iteration 7, loss = 0.47711165\n",
      "Iteration 8, loss = 0.47322717\n",
      "Iteration 9, loss = 0.47085399\n",
      "Iteration 10, loss = 0.47294829\n",
      "Iteration 11, loss = 0.46999770\n",
      "Iteration 12, loss = 0.47044386\n",
      "Iteration 13, loss = 0.47268568\n",
      "Iteration 14, loss = 0.47026685\n",
      "Iteration 15, loss = 0.47152106\n",
      "Iteration 16, loss = 0.46939001\n",
      "Iteration 17, loss = 0.46886683\n",
      "Iteration 18, loss = 0.46749715\n",
      "Iteration 19, loss = 0.46575075\n",
      "Iteration 20, loss = 0.46676321\n",
      "Iteration 21, loss = 0.46622359\n",
      "Iteration 22, loss = 0.46548456\n",
      "Iteration 23, loss = 0.46604644\n",
      "Iteration 24, loss = 0.46845048\n",
      "Iteration 25, loss = 0.46429188\n",
      "Iteration 26, loss = 0.46301207\n",
      "Iteration 27, loss = 0.46175372\n",
      "Iteration 28, loss = 0.45871678\n",
      "Iteration 29, loss = 0.46012005\n",
      "Iteration 30, loss = 0.45998077\n",
      "Iteration 31, loss = 0.45782883\n",
      "Iteration 32, loss = 0.45692182\n",
      "Iteration 33, loss = 0.45588673\n",
      "Iteration 34, loss = 0.45403583\n",
      "Iteration 35, loss = 0.45326412\n",
      "Iteration 36, loss = 0.45282970\n",
      "Iteration 37, loss = 0.45494548\n",
      "Iteration 38, loss = 0.45213914\n",
      "Iteration 39, loss = 0.45129322\n",
      "Iteration 40, loss = 0.45020348\n",
      "Iteration 41, loss = 0.45076025\n",
      "Iteration 42, loss = 0.45169745\n",
      "Iteration 43, loss = 0.44816895\n",
      "Iteration 44, loss = 0.44934005\n",
      "Iteration 45, loss = 0.44854566\n",
      "Iteration 46, loss = 0.44590085\n",
      "Iteration 47, loss = 0.44755098\n",
      "Iteration 48, loss = 0.44543137\n",
      "Iteration 49, loss = 0.44851963\n",
      "Iteration 50, loss = 0.44902295\n",
      "Iteration 51, loss = 0.44545243\n",
      "Iteration 52, loss = 0.44451994\n",
      "Iteration 53, loss = 0.44598985\n",
      "Iteration 54, loss = 0.44794888\n",
      "Iteration 55, loss = 0.44620074\n",
      "Iteration 56, loss = 0.44544930\n",
      "Iteration 57, loss = 0.44393023\n",
      "Iteration 58, loss = 0.44327911\n",
      "Iteration 59, loss = 0.44383252\n",
      "Iteration 60, loss = 0.44370237\n",
      "Iteration 61, loss = 0.44277870\n",
      "Iteration 62, loss = 0.44216453\n",
      "Iteration 63, loss = 0.44333976\n",
      "Iteration 64, loss = 0.44390278\n",
      "Iteration 65, loss = 0.44828663\n",
      "Iteration 66, loss = 0.44423704\n",
      "Iteration 67, loss = 0.44602665\n",
      "Iteration 68, loss = 0.44636988\n",
      "Iteration 69, loss = 0.44624462\n",
      "Iteration 70, loss = 0.44298510\n",
      "Iteration 71, loss = 0.44286989\n",
      "Iteration 72, loss = 0.44329177\n",
      "Iteration 73, loss = 0.44502193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54004962\n",
      "Iteration 2, loss = 0.51322949\n",
      "Iteration 3, loss = 0.49102350\n",
      "Iteration 4, loss = 0.47681319\n",
      "Iteration 5, loss = 0.47199771\n",
      "Iteration 6, loss = 0.46861550\n",
      "Iteration 7, loss = 0.46737768\n",
      "Iteration 8, loss = 0.46388781\n",
      "Iteration 9, loss = 0.46376460\n",
      "Iteration 10, loss = 0.46352926\n",
      "Iteration 11, loss = 0.46267734\n",
      "Iteration 12, loss = 0.46134751\n",
      "Iteration 13, loss = 0.46126277\n",
      "Iteration 14, loss = 0.46044583\n",
      "Iteration 15, loss = 0.46143463\n",
      "Iteration 16, loss = 0.45932380\n",
      "Iteration 17, loss = 0.46067371\n",
      "Iteration 18, loss = 0.45867165\n",
      "Iteration 19, loss = 0.45814592\n",
      "Iteration 20, loss = 0.45943811\n",
      "Iteration 21, loss = 0.46057796\n",
      "Iteration 22, loss = 0.45641224\n",
      "Iteration 23, loss = 0.45645136\n",
      "Iteration 24, loss = 0.45743254\n",
      "Iteration 25, loss = 0.45850443\n",
      "Iteration 26, loss = 0.45830556\n",
      "Iteration 27, loss = 0.45590635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.45375380\n",
      "Iteration 29, loss = 0.45391616\n",
      "Iteration 30, loss = 0.45443471\n",
      "Iteration 31, loss = 0.45610636\n",
      "Iteration 32, loss = 0.45384633\n",
      "Iteration 33, loss = 0.45252231\n",
      "Iteration 34, loss = 0.45309334\n",
      "Iteration 35, loss = 0.45460827\n",
      "Iteration 36, loss = 0.45300688\n",
      "Iteration 37, loss = 0.45171935\n",
      "Iteration 38, loss = 0.45166548\n",
      "Iteration 39, loss = 0.44992315\n",
      "Iteration 40, loss = 0.45126125\n",
      "Iteration 41, loss = 0.45514841\n",
      "Iteration 42, loss = 0.45042653\n",
      "Iteration 43, loss = 0.45298673\n",
      "Iteration 44, loss = 0.45043848\n",
      "Iteration 45, loss = 0.44929141\n",
      "Iteration 46, loss = 0.44931575\n",
      "Iteration 47, loss = 0.44811528\n",
      "Iteration 48, loss = 0.44883691\n",
      "Iteration 49, loss = 0.44926544\n",
      "Iteration 50, loss = 0.44797072\n",
      "Iteration 51, loss = 0.44783879\n",
      "Iteration 52, loss = 0.45012601\n",
      "Iteration 53, loss = 0.44801766\n",
      "Iteration 54, loss = 0.44869051\n",
      "Iteration 55, loss = 0.44745420\n",
      "Iteration 56, loss = 0.44982141\n",
      "Iteration 57, loss = 0.45125089\n",
      "Iteration 58, loss = 0.44981652\n",
      "Iteration 59, loss = 0.44798879\n",
      "Iteration 60, loss = 0.44781691\n",
      "Iteration 61, loss = 0.44772066\n",
      "Iteration 62, loss = 0.44683498\n",
      "Iteration 63, loss = 0.44863277\n",
      "Iteration 64, loss = 0.44665897\n",
      "Iteration 65, loss = 0.44669216\n",
      "Iteration 66, loss = 0.44747784\n",
      "Iteration 67, loss = 0.44522344\n",
      "Iteration 68, loss = 0.44764528\n",
      "Iteration 69, loss = 0.44669212\n",
      "Iteration 70, loss = 0.44774685\n",
      "Iteration 71, loss = 0.44721056\n",
      "Iteration 72, loss = 0.44642692\n",
      "Iteration 73, loss = 0.44803439\n",
      "Iteration 74, loss = 0.44827395\n",
      "Iteration 75, loss = 0.44563150\n",
      "Iteration 76, loss = 0.44646111\n",
      "Iteration 77, loss = 0.44988612\n",
      "Iteration 78, loss = 0.44663384\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55577948\n",
      "Iteration 2, loss = 0.53707473\n",
      "Iteration 3, loss = 0.53106271\n",
      "Iteration 4, loss = 0.52424534\n",
      "Iteration 5, loss = 0.51934153\n",
      "Iteration 6, loss = 0.51762380\n",
      "Iteration 7, loss = 0.51667007\n",
      "Iteration 8, loss = 0.51568777\n",
      "Iteration 9, loss = 0.51415728\n",
      "Iteration 10, loss = 0.51361014\n",
      "Iteration 11, loss = 0.51433855\n",
      "Iteration 12, loss = 0.51233084\n",
      "Iteration 13, loss = 0.51315054\n",
      "Iteration 14, loss = 0.51347047\n",
      "Iteration 15, loss = 0.51314769\n",
      "Iteration 16, loss = 0.51110937\n",
      "Iteration 17, loss = 0.51047126\n",
      "Iteration 18, loss = 0.51008340\n",
      "Iteration 19, loss = 0.51199091\n",
      "Iteration 20, loss = 0.51018976\n",
      "Iteration 21, loss = 0.50942484\n",
      "Iteration 22, loss = 0.51012166\n",
      "Iteration 23, loss = 0.51073036\n",
      "Iteration 24, loss = 0.51042417\n",
      "Iteration 25, loss = 0.50910576\n",
      "Iteration 26, loss = 0.50997408\n",
      "Iteration 27, loss = 0.51035186\n",
      "Iteration 28, loss = 0.50897112\n",
      "Iteration 29, loss = 0.50939386\n",
      "Iteration 30, loss = 0.51102847\n",
      "Iteration 31, loss = 0.51138259\n",
      "Iteration 32, loss = 0.50873140\n",
      "Iteration 33, loss = 0.50871205\n",
      "Iteration 34, loss = 0.50929888\n",
      "Iteration 35, loss = 0.50980709\n",
      "Iteration 36, loss = 0.50976439\n",
      "Iteration 37, loss = 0.51010593\n",
      "Iteration 38, loss = 0.50994273\n",
      "Iteration 39, loss = 0.50855159\n",
      "Iteration 40, loss = 0.50856594\n",
      "Iteration 41, loss = 0.50857676\n",
      "Iteration 42, loss = 0.51190414\n",
      "Iteration 43, loss = 0.50822623\n",
      "Iteration 44, loss = 0.50794606\n",
      "Iteration 45, loss = 0.50735965\n",
      "Iteration 46, loss = 0.50855479\n",
      "Iteration 47, loss = 0.50898528\n",
      "Iteration 48, loss = 0.50849056\n",
      "Iteration 49, loss = 0.50767743\n",
      "Iteration 50, loss = 0.50784901\n",
      "Iteration 51, loss = 0.50899760\n",
      "Iteration 52, loss = 0.50681400\n",
      "Iteration 53, loss = 0.50767178\n",
      "Iteration 54, loss = 0.50812080\n",
      "Iteration 55, loss = 0.50861346\n",
      "Iteration 56, loss = 0.50749834\n",
      "Iteration 57, loss = 0.50978707\n",
      "Iteration 58, loss = 0.50728913\n",
      "Iteration 59, loss = 0.50643304\n",
      "Iteration 60, loss = 0.50858972\n",
      "Iteration 61, loss = 0.50843226\n",
      "Iteration 62, loss = 0.50706891\n",
      "Iteration 63, loss = 0.50656215\n",
      "Iteration 64, loss = 0.50820154\n",
      "Iteration 65, loss = 0.50922202\n",
      "Iteration 66, loss = 0.50701265\n",
      "Iteration 67, loss = 0.50751499\n",
      "Iteration 68, loss = 0.50771374\n",
      "Iteration 69, loss = 0.50675264\n",
      "Iteration 70, loss = 0.50893871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55788656\n",
      "Iteration 2, loss = 0.53765882\n",
      "Iteration 3, loss = 0.53272481\n",
      "Iteration 4, loss = 0.52759856\n",
      "Iteration 5, loss = 0.51535876\n",
      "Iteration 6, loss = 0.51089263\n",
      "Iteration 7, loss = 0.51058637\n",
      "Iteration 8, loss = 0.50712933\n",
      "Iteration 9, loss = 0.50497699\n",
      "Iteration 10, loss = 0.50327406\n",
      "Iteration 11, loss = 0.50468279\n",
      "Iteration 12, loss = 0.50001806\n",
      "Iteration 13, loss = 0.50144740\n",
      "Iteration 14, loss = 0.49897331\n",
      "Iteration 15, loss = 0.49913890\n",
      "Iteration 16, loss = 0.49755465\n",
      "Iteration 17, loss = 0.49999014\n",
      "Iteration 18, loss = 0.49791971\n",
      "Iteration 19, loss = 0.49759311\n",
      "Iteration 20, loss = 0.49711083\n",
      "Iteration 21, loss = 0.49799870\n",
      "Iteration 22, loss = 0.49615881\n",
      "Iteration 23, loss = 0.49705169\n",
      "Iteration 24, loss = 0.49600397\n",
      "Iteration 25, loss = 0.49653253\n",
      "Iteration 26, loss = 0.49761733\n",
      "Iteration 27, loss = 0.49704396\n",
      "Iteration 28, loss = 0.49427941\n",
      "Iteration 29, loss = 0.49631273\n",
      "Iteration 30, loss = 0.49922679\n",
      "Iteration 31, loss = 0.49509730\n",
      "Iteration 32, loss = 0.49502832\n",
      "Iteration 33, loss = 0.49772914\n",
      "Iteration 34, loss = 0.49486724\n",
      "Iteration 35, loss = 0.49833886\n",
      "Iteration 36, loss = 0.49662938\n",
      "Iteration 37, loss = 0.49573252\n",
      "Iteration 38, loss = 0.49523383\n",
      "Iteration 39, loss = 0.49418803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55633061\n",
      "Iteration 2, loss = 0.53957752\n",
      "Iteration 3, loss = 0.53214200\n",
      "Iteration 4, loss = 0.52272994\n",
      "Iteration 5, loss = 0.51972503\n",
      "Iteration 6, loss = 0.51833677\n",
      "Iteration 7, loss = 0.51830164\n",
      "Iteration 8, loss = 0.51604506\n",
      "Iteration 9, loss = 0.51616760\n",
      "Iteration 10, loss = 0.51470930\n",
      "Iteration 11, loss = 0.51556466\n",
      "Iteration 12, loss = 0.51370857\n",
      "Iteration 13, loss = 0.51302411\n",
      "Iteration 14, loss = 0.51206073\n",
      "Iteration 15, loss = 0.51050874\n",
      "Iteration 16, loss = 0.51065132\n",
      "Iteration 17, loss = 0.51043127\n",
      "Iteration 18, loss = 0.51079096\n",
      "Iteration 19, loss = 0.50955792\n",
      "Iteration 20, loss = 0.50901888\n",
      "Iteration 21, loss = 0.51043779\n",
      "Iteration 22, loss = 0.50730864\n",
      "Iteration 23, loss = 0.50630213\n",
      "Iteration 24, loss = 0.50684632\n",
      "Iteration 25, loss = 0.50536568\n",
      "Iteration 26, loss = 0.50594844\n",
      "Iteration 27, loss = 0.50558839\n",
      "Iteration 28, loss = 0.50389799\n",
      "Iteration 29, loss = 0.50286441\n",
      "Iteration 30, loss = 0.50427761\n",
      "Iteration 31, loss = 0.50338009\n",
      "Iteration 32, loss = 0.50478975\n",
      "Iteration 33, loss = 0.50161365\n",
      "Iteration 34, loss = 0.50264828\n",
      "Iteration 35, loss = 0.50261154\n",
      "Iteration 36, loss = 0.50051457\n",
      "Iteration 37, loss = 0.50033754\n",
      "Iteration 38, loss = 0.49899161\n",
      "Iteration 39, loss = 0.49853319\n",
      "Iteration 40, loss = 0.49915416\n",
      "Iteration 41, loss = 0.49775411\n",
      "Iteration 42, loss = 0.49749047\n",
      "Iteration 43, loss = 0.49818107\n",
      "Iteration 44, loss = 0.49849448\n",
      "Iteration 45, loss = 0.49954861\n",
      "Iteration 46, loss = 0.49809261\n",
      "Iteration 47, loss = 0.49556225\n",
      "Iteration 48, loss = 0.49526348\n",
      "Iteration 49, loss = 0.49592525\n",
      "Iteration 50, loss = 0.49533921\n",
      "Iteration 51, loss = 0.49564630\n",
      "Iteration 52, loss = 0.49531574\n",
      "Iteration 53, loss = 0.49553578\n",
      "Iteration 54, loss = 0.49458862\n",
      "Iteration 55, loss = 0.49428897\n",
      "Iteration 56, loss = 0.49354589\n",
      "Iteration 57, loss = 0.49624702\n",
      "Iteration 58, loss = 0.49527789\n",
      "Iteration 59, loss = 0.49479466\n",
      "Iteration 60, loss = 0.49569324\n",
      "Iteration 61, loss = 0.49594409\n",
      "Iteration 62, loss = 0.49218210\n",
      "Iteration 63, loss = 0.49476103\n",
      "Iteration 64, loss = 0.49527809\n",
      "Iteration 65, loss = 0.49272160\n",
      "Iteration 66, loss = 0.49493658\n",
      "Iteration 67, loss = 0.49364017\n",
      "Iteration 68, loss = 0.49314416\n",
      "Iteration 69, loss = 0.49204290\n",
      "Iteration 70, loss = 0.49311434\n",
      "Iteration 71, loss = 0.49221274\n",
      "Iteration 72, loss = 0.49424899\n",
      "Iteration 73, loss = 0.49149914\n",
      "Iteration 74, loss = 0.49484249\n",
      "Iteration 75, loss = 0.49342027\n",
      "Iteration 76, loss = 0.49298803\n",
      "Iteration 77, loss = 0.49303434\n",
      "Iteration 78, loss = 0.49136259\n",
      "Iteration 79, loss = 0.49425512\n",
      "Iteration 80, loss = 0.49254356\n",
      "Iteration 81, loss = 0.49265309\n",
      "Iteration 82, loss = 0.49124644\n",
      "Iteration 83, loss = 0.49223292\n",
      "Iteration 84, loss = 0.49137126\n",
      "Iteration 85, loss = 0.49091525\n",
      "Iteration 86, loss = 0.49083370\n",
      "Iteration 87, loss = 0.49313531\n",
      "Iteration 88, loss = 0.49289659\n",
      "Iteration 89, loss = 0.49238590\n",
      "Iteration 90, loss = 0.49211716\n",
      "Iteration 91, loss = 0.49583421\n",
      "Iteration 92, loss = 0.49265338\n",
      "Iteration 93, loss = 0.49098512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 0.49137614\n",
      "Iteration 95, loss = 0.49061302\n",
      "Iteration 96, loss = 0.49099713\n",
      "Iteration 97, loss = 0.49016437\n",
      "Iteration 98, loss = 0.49134047\n",
      "Iteration 99, loss = 0.49207697\n",
      "Iteration 100, loss = 0.49110622\n",
      "Iteration 101, loss = 0.49152165\n",
      "Iteration 102, loss = 0.49076906\n",
      "Iteration 103, loss = 0.49111650\n",
      "Iteration 104, loss = 0.49191427\n",
      "Iteration 105, loss = 0.49152374\n",
      "Iteration 106, loss = 0.49204011\n",
      "Iteration 107, loss = 0.49175781\n",
      "Iteration 108, loss = 0.49115937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59286755\n",
      "Iteration 2, loss = 0.48764473\n",
      "Iteration 3, loss = 0.47060670\n",
      "Iteration 4, loss = 0.46366895\n",
      "Iteration 5, loss = 0.45665020\n",
      "Iteration 6, loss = 0.45475360\n",
      "Iteration 7, loss = 0.45218123\n",
      "Iteration 8, loss = 0.45151541\n",
      "Iteration 9, loss = 0.45044942\n",
      "Iteration 10, loss = 0.44839351\n",
      "Iteration 11, loss = 0.44693360\n",
      "Iteration 12, loss = 0.44555414\n",
      "Iteration 13, loss = 0.44655028\n",
      "Iteration 14, loss = 0.44376646\n",
      "Iteration 15, loss = 0.43934016\n",
      "Iteration 16, loss = 0.43624027\n",
      "Iteration 17, loss = 0.43695322\n",
      "Iteration 18, loss = 0.43306622\n",
      "Iteration 19, loss = 0.43238398\n",
      "Iteration 20, loss = 0.43309378\n",
      "Iteration 21, loss = 0.43141034\n",
      "Iteration 22, loss = 0.43216440\n",
      "Iteration 23, loss = 0.43141127\n",
      "Iteration 24, loss = 0.43016416\n",
      "Iteration 25, loss = 0.42798557\n",
      "Iteration 26, loss = 0.42855798\n",
      "Iteration 27, loss = 0.42762127\n",
      "Iteration 28, loss = 0.42971543\n",
      "Iteration 29, loss = 0.42759693\n",
      "Iteration 30, loss = 0.42946293\n",
      "Iteration 31, loss = 0.42870283\n",
      "Iteration 32, loss = 0.42663751\n",
      "Iteration 33, loss = 0.42544317\n",
      "Iteration 34, loss = 0.42676305\n",
      "Iteration 35, loss = 0.42779496\n",
      "Iteration 36, loss = 0.42733464\n",
      "Iteration 37, loss = 0.42770960\n",
      "Iteration 38, loss = 0.42779174\n",
      "Iteration 39, loss = 0.42785073\n",
      "Iteration 40, loss = 0.42472825\n",
      "Iteration 41, loss = 0.42283269\n",
      "Iteration 42, loss = 0.42536654\n",
      "Iteration 43, loss = 0.42386654\n",
      "Iteration 44, loss = 0.42325050\n",
      "Iteration 45, loss = 0.42377359\n",
      "Iteration 46, loss = 0.42328623\n",
      "Iteration 47, loss = 0.42267958\n",
      "Iteration 48, loss = 0.42337512\n",
      "Iteration 49, loss = 0.42195240\n",
      "Iteration 50, loss = 0.42234007\n",
      "Iteration 51, loss = 0.42300920\n",
      "Iteration 52, loss = 0.42279118\n",
      "Iteration 53, loss = 0.42201402\n",
      "Iteration 54, loss = 0.42337960\n",
      "Iteration 55, loss = 0.42411492\n",
      "Iteration 56, loss = 0.42278903\n",
      "Iteration 57, loss = 0.42380440\n",
      "Iteration 58, loss = 0.42226400\n",
      "Iteration 59, loss = 0.42287507\n",
      "Iteration 60, loss = 0.42335469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59816182\n",
      "Iteration 2, loss = 0.48480444\n",
      "Iteration 3, loss = 0.46798209\n",
      "Iteration 4, loss = 0.46134442\n",
      "Iteration 5, loss = 0.45491258\n",
      "Iteration 6, loss = 0.45217658\n",
      "Iteration 7, loss = 0.45020302\n",
      "Iteration 8, loss = 0.44780165\n",
      "Iteration 9, loss = 0.44727748\n",
      "Iteration 10, loss = 0.44555027\n",
      "Iteration 11, loss = 0.44671706\n",
      "Iteration 12, loss = 0.44491533\n",
      "Iteration 13, loss = 0.44472742\n",
      "Iteration 14, loss = 0.44320976\n",
      "Iteration 15, loss = 0.44254941\n",
      "Iteration 16, loss = 0.44081112\n",
      "Iteration 17, loss = 0.44240407\n",
      "Iteration 18, loss = 0.43960328\n",
      "Iteration 19, loss = 0.44000399\n",
      "Iteration 20, loss = 0.44064713\n",
      "Iteration 21, loss = 0.44128598\n",
      "Iteration 22, loss = 0.44175488\n",
      "Iteration 23, loss = 0.44008956\n",
      "Iteration 24, loss = 0.43848489\n",
      "Iteration 25, loss = 0.44114629\n",
      "Iteration 26, loss = 0.43903759\n",
      "Iteration 27, loss = 0.43632160\n",
      "Iteration 28, loss = 0.43702350\n",
      "Iteration 29, loss = 0.43552857\n",
      "Iteration 30, loss = 0.43811597\n",
      "Iteration 31, loss = 0.43835364\n",
      "Iteration 32, loss = 0.43681895\n",
      "Iteration 33, loss = 0.43735498\n",
      "Iteration 34, loss = 0.43757983\n",
      "Iteration 35, loss = 0.43781071\n",
      "Iteration 36, loss = 0.43670076\n",
      "Iteration 37, loss = 0.44042184\n",
      "Iteration 38, loss = 0.43540350\n",
      "Iteration 39, loss = 0.43884930\n",
      "Iteration 40, loss = 0.43650760\n",
      "Iteration 41, loss = 0.43557332\n",
      "Iteration 42, loss = 0.43642698\n",
      "Iteration 43, loss = 0.43263753\n",
      "Iteration 44, loss = 0.43560623\n",
      "Iteration 45, loss = 0.43489692\n",
      "Iteration 46, loss = 0.43233323\n",
      "Iteration 47, loss = 0.43388644\n",
      "Iteration 48, loss = 0.43520758\n",
      "Iteration 49, loss = 0.43372642\n",
      "Iteration 50, loss = 0.43326235\n",
      "Iteration 51, loss = 0.43250830\n",
      "Iteration 52, loss = 0.43200449\n",
      "Iteration 53, loss = 0.43137708\n",
      "Iteration 54, loss = 0.43192789\n",
      "Iteration 55, loss = 0.43224671\n",
      "Iteration 56, loss = 0.43138086\n",
      "Iteration 57, loss = 0.43168834\n",
      "Iteration 58, loss = 0.42960973\n",
      "Iteration 59, loss = 0.42941847\n",
      "Iteration 60, loss = 0.43029993\n",
      "Iteration 61, loss = 0.43095183\n",
      "Iteration 62, loss = 0.42902487\n",
      "Iteration 63, loss = 0.42943826\n",
      "Iteration 64, loss = 0.43048771\n",
      "Iteration 65, loss = 0.43119909\n",
      "Iteration 66, loss = 0.42986781\n",
      "Iteration 67, loss = 0.43259160\n",
      "Iteration 68, loss = 0.42876182\n",
      "Iteration 69, loss = 0.42833539\n",
      "Iteration 70, loss = 0.42765132\n",
      "Iteration 71, loss = 0.42858953\n",
      "Iteration 72, loss = 0.42627357\n",
      "Iteration 73, loss = 0.42736036\n",
      "Iteration 74, loss = 0.42622922\n",
      "Iteration 75, loss = 0.42892273\n",
      "Iteration 76, loss = 0.42836512\n",
      "Iteration 77, loss = 0.42522589\n",
      "Iteration 78, loss = 0.42808367\n",
      "Iteration 79, loss = 0.42822643\n",
      "Iteration 80, loss = 0.42618039\n",
      "Iteration 81, loss = 0.42632317\n",
      "Iteration 82, loss = 0.42438501\n",
      "Iteration 83, loss = 0.42362992\n",
      "Iteration 84, loss = 0.42602653\n",
      "Iteration 85, loss = 0.42592724\n",
      "Iteration 86, loss = 0.42933455\n",
      "Iteration 87, loss = 0.42866498\n",
      "Iteration 88, loss = 0.42324700\n",
      "Iteration 89, loss = 0.42493090\n",
      "Iteration 90, loss = 0.42632031\n",
      "Iteration 91, loss = 0.42392493\n",
      "Iteration 92, loss = 0.42260976\n",
      "Iteration 93, loss = 0.42382284\n",
      "Iteration 94, loss = 0.42330090\n",
      "Iteration 95, loss = 0.42484469\n",
      "Iteration 96, loss = 0.42555358\n",
      "Iteration 97, loss = 0.42120034\n",
      "Iteration 98, loss = 0.42284314\n",
      "Iteration 99, loss = 0.42264227\n",
      "Iteration 100, loss = 0.42221345\n",
      "Iteration 101, loss = 0.42340874\n",
      "Iteration 102, loss = 0.42469280\n",
      "Iteration 103, loss = 0.42405679\n",
      "Iteration 104, loss = 0.42444480\n",
      "Iteration 105, loss = 0.42185899\n",
      "Iteration 106, loss = 0.42627643\n",
      "Iteration 107, loss = 0.42255857\n",
      "Iteration 108, loss = 0.42163025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59208402\n",
      "Iteration 2, loss = 0.48739558\n",
      "Iteration 3, loss = 0.46967177\n",
      "Iteration 4, loss = 0.46056158\n",
      "Iteration 5, loss = 0.45622032\n",
      "Iteration 6, loss = 0.45534000\n",
      "Iteration 7, loss = 0.45326740\n",
      "Iteration 8, loss = 0.44943222\n",
      "Iteration 9, loss = 0.45271509\n",
      "Iteration 10, loss = 0.44603654\n",
      "Iteration 11, loss = 0.44508279\n",
      "Iteration 12, loss = 0.44180555\n",
      "Iteration 13, loss = 0.44156479\n",
      "Iteration 14, loss = 0.43907172\n",
      "Iteration 15, loss = 0.43678051\n",
      "Iteration 16, loss = 0.43792977\n",
      "Iteration 17, loss = 0.43640398\n",
      "Iteration 18, loss = 0.43649904\n",
      "Iteration 19, loss = 0.43572197\n",
      "Iteration 20, loss = 0.43308142\n",
      "Iteration 21, loss = 0.43622288\n",
      "Iteration 22, loss = 0.43213403\n",
      "Iteration 23, loss = 0.43034574\n",
      "Iteration 24, loss = 0.42987439\n",
      "Iteration 25, loss = 0.43224089\n",
      "Iteration 26, loss = 0.43418703\n",
      "Iteration 27, loss = 0.43062172\n",
      "Iteration 28, loss = 0.42972996\n",
      "Iteration 29, loss = 0.43033925\n",
      "Iteration 30, loss = 0.42945538\n",
      "Iteration 31, loss = 0.42843259\n",
      "Iteration 32, loss = 0.42871671\n",
      "Iteration 33, loss = 0.42769978\n",
      "Iteration 34, loss = 0.42810931\n",
      "Iteration 35, loss = 0.42800932\n",
      "Iteration 36, loss = 0.42936917\n",
      "Iteration 37, loss = 0.42736367\n",
      "Iteration 38, loss = 0.42859672\n",
      "Iteration 39, loss = 0.42953101\n",
      "Iteration 40, loss = 0.43025448\n",
      "Iteration 41, loss = 0.42645435\n",
      "Iteration 42, loss = 0.42709871\n",
      "Iteration 43, loss = 0.42608133\n",
      "Iteration 44, loss = 0.42816881\n",
      "Iteration 45, loss = 0.42702209\n",
      "Iteration 46, loss = 0.42722067\n",
      "Iteration 47, loss = 0.42555061\n",
      "Iteration 48, loss = 0.42563740\n",
      "Iteration 49, loss = 0.42439814\n",
      "Iteration 50, loss = 0.42319928\n",
      "Iteration 51, loss = 0.42537628\n",
      "Iteration 52, loss = 0.42669020\n",
      "Iteration 53, loss = 0.42900678\n",
      "Iteration 54, loss = 0.42388900\n",
      "Iteration 55, loss = 0.42419932\n",
      "Iteration 56, loss = 0.42386037\n",
      "Iteration 57, loss = 0.42387416\n",
      "Iteration 58, loss = 0.42618834\n",
      "Iteration 59, loss = 0.42402244\n",
      "Iteration 60, loss = 0.42207057\n",
      "Iteration 61, loss = 0.42732310\n",
      "Iteration 62, loss = 0.42219428\n",
      "Iteration 63, loss = 0.42246808\n",
      "Iteration 64, loss = 0.42387030\n",
      "Iteration 65, loss = 0.42360617\n",
      "Iteration 66, loss = 0.42309957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 67, loss = 0.42340747\n",
      "Iteration 68, loss = 0.42211308\n",
      "Iteration 69, loss = 0.42510032\n",
      "Iteration 70, loss = 0.42375059\n",
      "Iteration 71, loss = 0.42339388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59299413\n",
      "Iteration 2, loss = 0.48758437\n",
      "Iteration 3, loss = 0.46823103\n",
      "Iteration 4, loss = 0.46242153\n",
      "Iteration 5, loss = 0.45812099\n",
      "Iteration 6, loss = 0.45507838\n",
      "Iteration 7, loss = 0.45192918\n",
      "Iteration 8, loss = 0.45164877\n",
      "Iteration 9, loss = 0.44936585\n",
      "Iteration 10, loss = 0.44624571\n",
      "Iteration 11, loss = 0.44569908\n",
      "Iteration 12, loss = 0.44317467\n",
      "Iteration 13, loss = 0.44591139\n",
      "Iteration 14, loss = 0.44291651\n",
      "Iteration 15, loss = 0.43914151\n",
      "Iteration 16, loss = 0.43903646\n",
      "Iteration 17, loss = 0.43707093\n",
      "Iteration 18, loss = 0.43602299\n",
      "Iteration 19, loss = 0.43414431\n",
      "Iteration 20, loss = 0.43351013\n",
      "Iteration 21, loss = 0.43205972\n",
      "Iteration 22, loss = 0.43205660\n",
      "Iteration 23, loss = 0.43229500\n",
      "Iteration 24, loss = 0.43282956\n",
      "Iteration 25, loss = 0.43113060\n",
      "Iteration 26, loss = 0.43079768\n",
      "Iteration 27, loss = 0.42842628\n",
      "Iteration 28, loss = 0.43336751\n",
      "Iteration 29, loss = 0.43023144\n",
      "Iteration 30, loss = 0.43221121\n",
      "Iteration 31, loss = 0.42818517\n",
      "Iteration 32, loss = 0.42800707\n",
      "Iteration 33, loss = 0.42721355\n",
      "Iteration 34, loss = 0.42769362\n",
      "Iteration 35, loss = 0.42742675\n",
      "Iteration 36, loss = 0.42767635\n",
      "Iteration 37, loss = 0.42782124\n",
      "Iteration 38, loss = 0.43102408\n",
      "Iteration 39, loss = 0.42710068\n",
      "Iteration 40, loss = 0.42708443\n",
      "Iteration 41, loss = 0.42563825\n",
      "Iteration 42, loss = 0.42661731\n",
      "Iteration 43, loss = 0.42526828\n",
      "Iteration 44, loss = 0.42448850\n",
      "Iteration 45, loss = 0.42545270\n",
      "Iteration 46, loss = 0.42527578\n",
      "Iteration 47, loss = 0.42402091\n",
      "Iteration 48, loss = 0.42718958\n",
      "Iteration 49, loss = 0.42474593\n",
      "Iteration 50, loss = 0.42575058\n",
      "Iteration 51, loss = 0.42989549\n",
      "Iteration 52, loss = 0.42404354\n",
      "Iteration 53, loss = 0.42407195\n",
      "Iteration 54, loss = 0.42587380\n",
      "Iteration 55, loss = 0.42772372\n",
      "Iteration 56, loss = 0.42643636\n",
      "Iteration 57, loss = 0.42436876\n",
      "Iteration 58, loss = 0.42510532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59812741\n",
      "Iteration 2, loss = 0.48703040\n",
      "Iteration 3, loss = 0.46494264\n",
      "Iteration 4, loss = 0.45909936\n",
      "Iteration 5, loss = 0.45582815\n",
      "Iteration 6, loss = 0.45191427\n",
      "Iteration 7, loss = 0.45106591\n",
      "Iteration 8, loss = 0.45006381\n",
      "Iteration 9, loss = 0.44866444\n",
      "Iteration 10, loss = 0.44766034\n",
      "Iteration 11, loss = 0.44867201\n",
      "Iteration 12, loss = 0.44498768\n",
      "Iteration 13, loss = 0.44605937\n",
      "Iteration 14, loss = 0.44565692\n",
      "Iteration 15, loss = 0.44460834\n",
      "Iteration 16, loss = 0.44451773\n",
      "Iteration 17, loss = 0.44378902\n",
      "Iteration 18, loss = 0.44129679\n",
      "Iteration 19, loss = 0.44204004\n",
      "Iteration 20, loss = 0.44110438\n",
      "Iteration 21, loss = 0.44137090\n",
      "Iteration 22, loss = 0.44398940\n",
      "Iteration 23, loss = 0.44087095\n",
      "Iteration 24, loss = 0.43867243\n",
      "Iteration 25, loss = 0.43888626\n",
      "Iteration 26, loss = 0.43971453\n",
      "Iteration 27, loss = 0.43693489\n",
      "Iteration 28, loss = 0.43710597\n",
      "Iteration 29, loss = 0.43719175\n",
      "Iteration 30, loss = 0.43694920\n",
      "Iteration 31, loss = 0.43567003\n",
      "Iteration 32, loss = 0.43540739\n",
      "Iteration 33, loss = 0.43442847\n",
      "Iteration 34, loss = 0.43740302\n",
      "Iteration 35, loss = 0.43854566\n",
      "Iteration 36, loss = 0.43542557\n",
      "Iteration 37, loss = 0.44006041\n",
      "Iteration 38, loss = 0.43590484\n",
      "Iteration 39, loss = 0.43339788\n",
      "Iteration 40, loss = 0.43230313\n",
      "Iteration 41, loss = 0.43536423\n",
      "Iteration 42, loss = 0.43356723\n",
      "Iteration 43, loss = 0.43237561\n",
      "Iteration 44, loss = 0.43214303\n",
      "Iteration 45, loss = 0.43367233\n",
      "Iteration 46, loss = 0.43281060\n",
      "Iteration 47, loss = 0.43481614\n",
      "Iteration 48, loss = 0.43477779\n",
      "Iteration 49, loss = 0.43146358\n",
      "Iteration 50, loss = 0.42976855\n",
      "Iteration 51, loss = 0.43111346\n",
      "Iteration 52, loss = 0.43429434\n",
      "Iteration 53, loss = 0.43456181\n",
      "Iteration 54, loss = 0.43175549\n",
      "Iteration 55, loss = 0.43037306\n",
      "Iteration 56, loss = 0.42974821\n",
      "Iteration 57, loss = 0.43122201\n",
      "Iteration 58, loss = 0.42850660\n",
      "Iteration 59, loss = 0.43006045\n",
      "Iteration 60, loss = 0.42956923\n",
      "Iteration 61, loss = 0.42935742\n",
      "Iteration 62, loss = 0.42846624\n",
      "Iteration 63, loss = 0.42761323\n",
      "Iteration 64, loss = 0.43168556\n",
      "Iteration 65, loss = 0.42783729\n",
      "Iteration 66, loss = 0.42971530\n",
      "Iteration 67, loss = 0.43021201\n",
      "Iteration 68, loss = 0.43028586\n",
      "Iteration 69, loss = 0.42713234\n",
      "Iteration 70, loss = 0.42631834\n",
      "Iteration 71, loss = 0.42602307\n",
      "Iteration 72, loss = 0.42526228\n",
      "Iteration 73, loss = 0.42472107\n",
      "Iteration 74, loss = 0.42734408\n",
      "Iteration 75, loss = 0.42545946\n",
      "Iteration 76, loss = 0.42685169\n",
      "Iteration 77, loss = 0.42511115\n",
      "Iteration 78, loss = 0.42567521\n",
      "Iteration 79, loss = 0.42430099\n",
      "Iteration 80, loss = 0.42261556\n",
      "Iteration 81, loss = 0.42172348\n",
      "Iteration 82, loss = 0.42014219\n",
      "Iteration 83, loss = 0.42011230\n",
      "Iteration 84, loss = 0.42113039\n",
      "Iteration 85, loss = 0.42027686\n",
      "Iteration 86, loss = 0.42125829\n",
      "Iteration 87, loss = 0.42092633\n",
      "Iteration 88, loss = 0.42204534\n",
      "Iteration 89, loss = 0.42304876\n",
      "Iteration 90, loss = 0.42001087\n",
      "Iteration 91, loss = 0.41856607\n",
      "Iteration 92, loss = 0.42000307\n",
      "Iteration 93, loss = 0.42104095\n",
      "Iteration 94, loss = 0.42057874\n",
      "Iteration 95, loss = 0.41900793\n",
      "Iteration 96, loss = 0.42083111\n",
      "Iteration 97, loss = 0.41999637\n",
      "Iteration 98, loss = 0.41891669\n",
      "Iteration 99, loss = 0.42079965\n",
      "Iteration 100, loss = 0.41849292\n",
      "Iteration 101, loss = 0.42068841\n",
      "Iteration 102, loss = 0.41902402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59093050\n",
      "Iteration 2, loss = 0.48558467\n",
      "Iteration 3, loss = 0.46778272\n",
      "Iteration 4, loss = 0.46096445\n",
      "Iteration 5, loss = 0.45786640\n",
      "Iteration 6, loss = 0.45436833\n",
      "Iteration 7, loss = 0.45408807\n",
      "Iteration 8, loss = 0.45127386\n",
      "Iteration 9, loss = 0.45263680\n",
      "Iteration 10, loss = 0.44896434\n",
      "Iteration 11, loss = 0.44822186\n",
      "Iteration 12, loss = 0.44681998\n",
      "Iteration 13, loss = 0.44593153\n",
      "Iteration 14, loss = 0.44511513\n",
      "Iteration 15, loss = 0.44475393\n",
      "Iteration 16, loss = 0.44454389\n",
      "Iteration 17, loss = 0.44376905\n",
      "Iteration 18, loss = 0.44295932\n",
      "Iteration 19, loss = 0.44244824\n",
      "Iteration 20, loss = 0.44134054\n",
      "Iteration 21, loss = 0.44262871\n",
      "Iteration 22, loss = 0.43890187\n",
      "Iteration 23, loss = 0.43574296\n",
      "Iteration 24, loss = 0.43466385\n",
      "Iteration 25, loss = 0.43633468\n",
      "Iteration 26, loss = 0.43977621\n",
      "Iteration 27, loss = 0.43404180\n",
      "Iteration 28, loss = 0.43374210\n",
      "Iteration 29, loss = 0.43165819\n",
      "Iteration 30, loss = 0.43153336\n",
      "Iteration 31, loss = 0.43068529\n",
      "Iteration 32, loss = 0.42962701\n",
      "Iteration 33, loss = 0.42795851\n",
      "Iteration 34, loss = 0.42837892\n",
      "Iteration 35, loss = 0.42619827\n",
      "Iteration 36, loss = 0.42637155\n",
      "Iteration 37, loss = 0.42405052\n",
      "Iteration 38, loss = 0.42661712\n",
      "Iteration 39, loss = 0.42615545\n",
      "Iteration 40, loss = 0.42544595\n",
      "Iteration 41, loss = 0.42554688\n",
      "Iteration 42, loss = 0.42393782\n",
      "Iteration 43, loss = 0.42342002\n",
      "Iteration 44, loss = 0.42388974\n",
      "Iteration 45, loss = 0.42408275\n",
      "Iteration 46, loss = 0.42454965\n",
      "Iteration 47, loss = 0.42242467\n",
      "Iteration 48, loss = 0.42287055\n",
      "Iteration 49, loss = 0.42197210\n",
      "Iteration 50, loss = 0.42320772\n",
      "Iteration 51, loss = 0.42363443\n",
      "Iteration 52, loss = 0.42664001\n",
      "Iteration 53, loss = 0.42757029\n",
      "Iteration 54, loss = 0.42407183\n",
      "Iteration 55, loss = 0.42275854\n",
      "Iteration 56, loss = 0.42229791\n",
      "Iteration 57, loss = 0.42201176\n",
      "Iteration 58, loss = 0.42252300\n",
      "Iteration 59, loss = 0.42176123\n",
      "Iteration 60, loss = 0.42094573\n",
      "Iteration 61, loss = 0.42733721\n",
      "Iteration 62, loss = 0.42199181\n",
      "Iteration 63, loss = 0.42097493\n",
      "Iteration 64, loss = 0.42103397\n",
      "Iteration 65, loss = 0.42203802\n",
      "Iteration 66, loss = 0.42106237\n",
      "Iteration 67, loss = 0.42233937\n",
      "Iteration 68, loss = 0.42027522\n",
      "Iteration 69, loss = 0.42225347\n",
      "Iteration 70, loss = 0.42081966\n",
      "Iteration 71, loss = 0.41884000\n",
      "Iteration 72, loss = 0.41877095\n",
      "Iteration 73, loss = 0.41976692\n",
      "Iteration 74, loss = 0.42202733\n",
      "Iteration 75, loss = 0.42082920\n",
      "Iteration 76, loss = 0.41965903\n",
      "Iteration 77, loss = 0.42235302\n",
      "Iteration 78, loss = 0.42044110\n",
      "Iteration 79, loss = 0.42075973\n",
      "Iteration 80, loss = 0.41820574\n",
      "Iteration 81, loss = 0.41955572\n",
      "Iteration 82, loss = 0.41896487\n",
      "Iteration 83, loss = 0.41799993\n",
      "Iteration 84, loss = 0.42021060\n",
      "Iteration 85, loss = 0.41682140\n",
      "Iteration 86, loss = 0.41935066\n",
      "Iteration 87, loss = 0.41791817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 88, loss = 0.41697698\n",
      "Iteration 89, loss = 0.41807972\n",
      "Iteration 90, loss = 0.41914027\n",
      "Iteration 91, loss = 0.41711431\n",
      "Iteration 92, loss = 0.41696644\n",
      "Iteration 93, loss = 0.41933505\n",
      "Iteration 94, loss = 0.41774241\n",
      "Iteration 95, loss = 0.41922392\n",
      "Iteration 96, loss = 0.42288177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59049986\n",
      "Iteration 2, loss = 0.48369454\n",
      "Iteration 3, loss = 0.46755753\n",
      "Iteration 4, loss = 0.46192974\n",
      "Iteration 5, loss = 0.45769628\n",
      "Iteration 6, loss = 0.45562357\n",
      "Iteration 7, loss = 0.45262388\n",
      "Iteration 8, loss = 0.45143702\n",
      "Iteration 9, loss = 0.44948795\n",
      "Iteration 10, loss = 0.44774147\n",
      "Iteration 11, loss = 0.44616189\n",
      "Iteration 12, loss = 0.44456049\n",
      "Iteration 13, loss = 0.44629966\n",
      "Iteration 14, loss = 0.44417271\n",
      "Iteration 15, loss = 0.43981008\n",
      "Iteration 16, loss = 0.43813788\n",
      "Iteration 17, loss = 0.43974313\n",
      "Iteration 18, loss = 0.43738989\n",
      "Iteration 19, loss = 0.43577111\n",
      "Iteration 20, loss = 0.43675974\n",
      "Iteration 21, loss = 0.43432063\n",
      "Iteration 22, loss = 0.43525054\n",
      "Iteration 23, loss = 0.43719393\n",
      "Iteration 24, loss = 0.43393878\n",
      "Iteration 25, loss = 0.43218649\n",
      "Iteration 26, loss = 0.43268045\n",
      "Iteration 27, loss = 0.43148682\n",
      "Iteration 28, loss = 0.43306193\n",
      "Iteration 29, loss = 0.43031069\n",
      "Iteration 30, loss = 0.43268669\n",
      "Iteration 31, loss = 0.43125346\n",
      "Iteration 32, loss = 0.43017481\n",
      "Iteration 33, loss = 0.43003440\n",
      "Iteration 34, loss = 0.43144242\n",
      "Iteration 35, loss = 0.43021011\n",
      "Iteration 36, loss = 0.43109802\n",
      "Iteration 37, loss = 0.42756428\n",
      "Iteration 38, loss = 0.43000961\n",
      "Iteration 39, loss = 0.42673849\n",
      "Iteration 40, loss = 0.42641379\n",
      "Iteration 41, loss = 0.42364698\n",
      "Iteration 42, loss = 0.42670556\n",
      "Iteration 43, loss = 0.42364910\n",
      "Iteration 44, loss = 0.42318565\n",
      "Iteration 45, loss = 0.42354953\n",
      "Iteration 46, loss = 0.42446003\n",
      "Iteration 47, loss = 0.42177002\n",
      "Iteration 48, loss = 0.42575340\n",
      "Iteration 49, loss = 0.42367265\n",
      "Iteration 50, loss = 0.42352279\n",
      "Iteration 51, loss = 0.42575264\n",
      "Iteration 52, loss = 0.42477307\n",
      "Iteration 53, loss = 0.42176270\n",
      "Iteration 54, loss = 0.42517378\n",
      "Iteration 55, loss = 0.42428016\n",
      "Iteration 56, loss = 0.42321733\n",
      "Iteration 57, loss = 0.42463434\n",
      "Iteration 58, loss = 0.42661459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59632274\n",
      "Iteration 2, loss = 0.48178682\n",
      "Iteration 3, loss = 0.46227920\n",
      "Iteration 4, loss = 0.45886132\n",
      "Iteration 5, loss = 0.45370429\n",
      "Iteration 6, loss = 0.45111774\n",
      "Iteration 7, loss = 0.45005718\n",
      "Iteration 8, loss = 0.44924016\n",
      "Iteration 9, loss = 0.44722705\n",
      "Iteration 10, loss = 0.44659796\n",
      "Iteration 11, loss = 0.44724601\n",
      "Iteration 12, loss = 0.44180728\n",
      "Iteration 13, loss = 0.44059960\n",
      "Iteration 14, loss = 0.43845580\n",
      "Iteration 15, loss = 0.43729611\n",
      "Iteration 16, loss = 0.43550790\n",
      "Iteration 17, loss = 0.43451782\n",
      "Iteration 18, loss = 0.43193609\n",
      "Iteration 19, loss = 0.43247934\n",
      "Iteration 20, loss = 0.43034372\n",
      "Iteration 21, loss = 0.43127379\n",
      "Iteration 22, loss = 0.43295152\n",
      "Iteration 23, loss = 0.42987196\n",
      "Iteration 24, loss = 0.42844896\n",
      "Iteration 25, loss = 0.42956645\n",
      "Iteration 26, loss = 0.42902391\n",
      "Iteration 27, loss = 0.42740933\n",
      "Iteration 28, loss = 0.42653274\n",
      "Iteration 29, loss = 0.42733970\n",
      "Iteration 30, loss = 0.42748693\n",
      "Iteration 31, loss = 0.42550927\n",
      "Iteration 32, loss = 0.42581940\n",
      "Iteration 33, loss = 0.42574396\n",
      "Iteration 34, loss = 0.42639212\n",
      "Iteration 35, loss = 0.42492121\n",
      "Iteration 36, loss = 0.42188123\n",
      "Iteration 37, loss = 0.42577562\n",
      "Iteration 38, loss = 0.42480168\n",
      "Iteration 39, loss = 0.42239354\n",
      "Iteration 40, loss = 0.42419147\n",
      "Iteration 41, loss = 0.42354886\n",
      "Iteration 42, loss = 0.42370380\n",
      "Iteration 43, loss = 0.41974221\n",
      "Iteration 44, loss = 0.42014796\n",
      "Iteration 45, loss = 0.42043156\n",
      "Iteration 46, loss = 0.42002033\n",
      "Iteration 47, loss = 0.42382046\n",
      "Iteration 48, loss = 0.42108770\n",
      "Iteration 49, loss = 0.42046564\n",
      "Iteration 50, loss = 0.41895453\n",
      "Iteration 51, loss = 0.42383131\n",
      "Iteration 52, loss = 0.42006064\n",
      "Iteration 53, loss = 0.42168775\n",
      "Iteration 54, loss = 0.42204363\n",
      "Iteration 55, loss = 0.41891088\n",
      "Iteration 56, loss = 0.41957686\n",
      "Iteration 57, loss = 0.41995175\n",
      "Iteration 58, loss = 0.41914767\n",
      "Iteration 59, loss = 0.41939864\n",
      "Iteration 60, loss = 0.42072190\n",
      "Iteration 61, loss = 0.42066308\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58921064\n",
      "Iteration 2, loss = 0.48361062\n",
      "Iteration 3, loss = 0.47003214\n",
      "Iteration 4, loss = 0.46236844\n",
      "Iteration 5, loss = 0.45867738\n",
      "Iteration 6, loss = 0.45776562\n",
      "Iteration 7, loss = 0.45533630\n",
      "Iteration 8, loss = 0.44964301\n",
      "Iteration 9, loss = 0.44859774\n",
      "Iteration 10, loss = 0.44725732\n",
      "Iteration 11, loss = 0.44906673\n",
      "Iteration 12, loss = 0.44467103\n",
      "Iteration 13, loss = 0.44357310\n",
      "Iteration 14, loss = 0.44183132\n",
      "Iteration 15, loss = 0.44228697\n",
      "Iteration 16, loss = 0.44339622\n",
      "Iteration 17, loss = 0.44126140\n",
      "Iteration 18, loss = 0.43997445\n",
      "Iteration 19, loss = 0.43936070\n",
      "Iteration 20, loss = 0.43709793\n",
      "Iteration 21, loss = 0.43636941\n",
      "Iteration 22, loss = 0.43314474\n",
      "Iteration 23, loss = 0.43300082\n",
      "Iteration 24, loss = 0.43056072\n",
      "Iteration 25, loss = 0.42975946\n",
      "Iteration 26, loss = 0.43249881\n",
      "Iteration 27, loss = 0.43222478\n",
      "Iteration 28, loss = 0.42964138\n",
      "Iteration 29, loss = 0.42730210\n",
      "Iteration 30, loss = 0.42876879\n",
      "Iteration 31, loss = 0.42796191\n",
      "Iteration 32, loss = 0.42779926\n",
      "Iteration 33, loss = 0.42489126\n",
      "Iteration 34, loss = 0.42480172\n",
      "Iteration 35, loss = 0.42348190\n",
      "Iteration 36, loss = 0.42368101\n",
      "Iteration 37, loss = 0.42342134\n",
      "Iteration 38, loss = 0.42403849\n",
      "Iteration 39, loss = 0.42341691\n",
      "Iteration 40, loss = 0.42363479\n",
      "Iteration 41, loss = 0.42101762\n",
      "Iteration 42, loss = 0.42028239\n",
      "Iteration 43, loss = 0.42300470\n",
      "Iteration 44, loss = 0.42033297\n",
      "Iteration 45, loss = 0.42069014\n",
      "Iteration 46, loss = 0.42097533\n",
      "Iteration 47, loss = 0.41989595\n",
      "Iteration 48, loss = 0.42083785\n",
      "Iteration 49, loss = 0.41860203\n",
      "Iteration 50, loss = 0.41966789\n",
      "Iteration 51, loss = 0.41988531\n",
      "Iteration 52, loss = 0.42128220\n",
      "Iteration 53, loss = 0.42317353\n",
      "Iteration 54, loss = 0.41995514\n",
      "Iteration 55, loss = 0.41865909\n",
      "Iteration 56, loss = 0.42194803\n",
      "Iteration 57, loss = 0.42117444\n",
      "Iteration 58, loss = 0.42052781\n",
      "Iteration 59, loss = 0.42094902\n",
      "Iteration 60, loss = 0.41872141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52040856\n",
      "Iteration 2, loss = 0.48447005\n",
      "Iteration 3, loss = 0.47498800\n",
      "Iteration 4, loss = 0.47203033\n",
      "Iteration 5, loss = 0.46999083\n",
      "Iteration 6, loss = 0.46995860\n",
      "Iteration 7, loss = 0.46962277\n",
      "Iteration 8, loss = 0.46801126\n",
      "Iteration 9, loss = 0.46794104\n",
      "Iteration 10, loss = 0.46586125\n",
      "Iteration 11, loss = 0.46223079\n",
      "Iteration 12, loss = 0.46108510\n",
      "Iteration 13, loss = 0.46062002\n",
      "Iteration 14, loss = 0.45810402\n",
      "Iteration 15, loss = 0.45629349\n",
      "Iteration 16, loss = 0.45598267\n",
      "Iteration 17, loss = 0.45410579\n",
      "Iteration 18, loss = 0.45330039\n",
      "Iteration 19, loss = 0.45102061\n",
      "Iteration 20, loss = 0.45236532\n",
      "Iteration 21, loss = 0.45029535\n",
      "Iteration 22, loss = 0.44954147\n",
      "Iteration 23, loss = 0.44870841\n",
      "Iteration 24, loss = 0.44900773\n",
      "Iteration 25, loss = 0.44692555\n",
      "Iteration 26, loss = 0.44686810\n",
      "Iteration 27, loss = 0.44590516\n",
      "Iteration 28, loss = 0.44896429\n",
      "Iteration 29, loss = 0.44662093\n",
      "Iteration 30, loss = 0.44923393\n",
      "Iteration 31, loss = 0.44684521\n",
      "Iteration 32, loss = 0.44625278\n",
      "Iteration 33, loss = 0.44550673\n",
      "Iteration 34, loss = 0.44457273\n",
      "Iteration 35, loss = 0.44567443\n",
      "Iteration 36, loss = 0.44703976\n",
      "Iteration 37, loss = 0.44714506\n",
      "Iteration 38, loss = 0.44813145\n",
      "Iteration 39, loss = 0.44616162\n",
      "Iteration 40, loss = 0.44406766\n",
      "Iteration 41, loss = 0.44350686\n",
      "Iteration 42, loss = 0.44767660\n",
      "Iteration 43, loss = 0.44340658\n",
      "Iteration 44, loss = 0.44479398\n",
      "Iteration 45, loss = 0.44410608\n",
      "Iteration 46, loss = 0.44458753\n",
      "Iteration 47, loss = 0.44437081\n",
      "Iteration 48, loss = 0.44474830\n",
      "Iteration 49, loss = 0.44364353\n",
      "Iteration 50, loss = 0.44414583\n",
      "Iteration 51, loss = 0.44278738\n",
      "Iteration 52, loss = 0.44342337\n",
      "Iteration 53, loss = 0.44304689\n",
      "Iteration 54, loss = 0.44416303\n",
      "Iteration 55, loss = 0.44470361\n",
      "Iteration 56, loss = 0.44534385\n",
      "Iteration 57, loss = 0.44509273\n",
      "Iteration 58, loss = 0.44335147\n",
      "Iteration 59, loss = 0.44309288\n",
      "Iteration 60, loss = 0.44343067\n",
      "Iteration 61, loss = 0.44493270\n",
      "Iteration 62, loss = 0.44435562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51912472\n",
      "Iteration 2, loss = 0.48031367\n",
      "Iteration 3, loss = 0.47118046\n",
      "Iteration 4, loss = 0.47323248\n",
      "Iteration 5, loss = 0.46765377\n",
      "Iteration 6, loss = 0.46554002\n",
      "Iteration 7, loss = 0.46528205\n",
      "Iteration 8, loss = 0.46145616\n",
      "Iteration 9, loss = 0.45861949\n",
      "Iteration 10, loss = 0.45770997\n",
      "Iteration 11, loss = 0.45703240\n",
      "Iteration 12, loss = 0.45542057\n",
      "Iteration 13, loss = 0.45760626\n",
      "Iteration 14, loss = 0.45355050\n",
      "Iteration 15, loss = 0.45408159\n",
      "Iteration 16, loss = 0.45201548\n",
      "Iteration 17, loss = 0.45240299\n",
      "Iteration 18, loss = 0.45075688\n",
      "Iteration 19, loss = 0.45006089\n",
      "Iteration 20, loss = 0.44961570\n",
      "Iteration 21, loss = 0.45315865\n",
      "Iteration 22, loss = 0.45194267\n",
      "Iteration 23, loss = 0.45006019\n",
      "Iteration 24, loss = 0.45180283\n",
      "Iteration 25, loss = 0.45098937\n",
      "Iteration 26, loss = 0.44934297\n",
      "Iteration 27, loss = 0.44912819\n",
      "Iteration 28, loss = 0.44868828\n",
      "Iteration 29, loss = 0.44719410\n",
      "Iteration 30, loss = 0.44851659\n",
      "Iteration 31, loss = 0.44974076\n",
      "Iteration 32, loss = 0.45050084\n",
      "Iteration 33, loss = 0.44976176\n",
      "Iteration 34, loss = 0.44728393\n",
      "Iteration 35, loss = 0.44913585\n",
      "Iteration 36, loss = 0.44957926\n",
      "Iteration 37, loss = 0.44962219\n",
      "Iteration 38, loss = 0.44752476\n",
      "Iteration 39, loss = 0.44877814\n",
      "Iteration 40, loss = 0.44801507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51552412\n",
      "Iteration 2, loss = 0.47974369\n",
      "Iteration 3, loss = 0.47304931\n",
      "Iteration 4, loss = 0.46964690\n",
      "Iteration 5, loss = 0.46859789\n",
      "Iteration 6, loss = 0.46908939\n",
      "Iteration 7, loss = 0.46860660\n",
      "Iteration 8, loss = 0.46656112\n",
      "Iteration 9, loss = 0.46668512\n",
      "Iteration 10, loss = 0.46476131\n",
      "Iteration 11, loss = 0.46389624\n",
      "Iteration 12, loss = 0.46289981\n",
      "Iteration 13, loss = 0.46420409\n",
      "Iteration 14, loss = 0.46231426\n",
      "Iteration 15, loss = 0.46438162\n",
      "Iteration 16, loss = 0.46195213\n",
      "Iteration 17, loss = 0.46343467\n",
      "Iteration 18, loss = 0.46323002\n",
      "Iteration 19, loss = 0.46160424\n",
      "Iteration 20, loss = 0.46296357\n",
      "Iteration 21, loss = 0.46262520\n",
      "Iteration 22, loss = 0.46026034\n",
      "Iteration 23, loss = 0.46057285\n",
      "Iteration 24, loss = 0.45762916\n",
      "Iteration 25, loss = 0.45646361\n",
      "Iteration 26, loss = 0.45574789\n",
      "Iteration 27, loss = 0.45542405\n",
      "Iteration 28, loss = 0.45188346\n",
      "Iteration 29, loss = 0.45194318\n",
      "Iteration 30, loss = 0.45118227\n",
      "Iteration 31, loss = 0.45096324\n",
      "Iteration 32, loss = 0.44946942\n",
      "Iteration 33, loss = 0.44996508\n",
      "Iteration 34, loss = 0.44934421\n",
      "Iteration 35, loss = 0.44779584\n",
      "Iteration 36, loss = 0.44901163\n",
      "Iteration 37, loss = 0.44777895\n",
      "Iteration 38, loss = 0.44748555\n",
      "Iteration 39, loss = 0.44758167\n",
      "Iteration 40, loss = 0.44844384\n",
      "Iteration 41, loss = 0.44755976\n",
      "Iteration 42, loss = 0.44624135\n",
      "Iteration 43, loss = 0.44641556\n",
      "Iteration 44, loss = 0.44726653\n",
      "Iteration 45, loss = 0.44533383\n",
      "Iteration 46, loss = 0.44504662\n",
      "Iteration 47, loss = 0.44448068\n",
      "Iteration 48, loss = 0.44602130\n",
      "Iteration 49, loss = 0.44519181\n",
      "Iteration 50, loss = 0.44468005\n",
      "Iteration 51, loss = 0.44595919\n",
      "Iteration 52, loss = 0.44567835\n",
      "Iteration 53, loss = 0.44553495\n",
      "Iteration 54, loss = 0.44423103\n",
      "Iteration 55, loss = 0.44465339\n",
      "Iteration 56, loss = 0.44395796\n",
      "Iteration 57, loss = 0.44460614\n",
      "Iteration 58, loss = 0.44593266\n",
      "Iteration 59, loss = 0.44532269\n",
      "Iteration 60, loss = 0.44437427\n",
      "Iteration 61, loss = 0.44691262\n",
      "Iteration 62, loss = 0.44428139\n",
      "Iteration 63, loss = 0.44478480\n",
      "Iteration 64, loss = 0.44485709\n",
      "Iteration 65, loss = 0.44572900\n",
      "Iteration 66, loss = 0.44340240\n",
      "Iteration 67, loss = 0.44477917\n",
      "Iteration 68, loss = 0.44445429\n",
      "Iteration 69, loss = 0.44400048\n",
      "Iteration 70, loss = 0.44447383\n",
      "Iteration 71, loss = 0.44360538\n",
      "Iteration 72, loss = 0.44364367\n",
      "Iteration 73, loss = 0.44650464\n",
      "Iteration 74, loss = 0.44531869\n",
      "Iteration 75, loss = 0.44396407\n",
      "Iteration 76, loss = 0.44354627\n",
      "Iteration 77, loss = 0.44566869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51706443\n",
      "Iteration 2, loss = 0.48084048\n",
      "Iteration 3, loss = 0.47457806\n",
      "Iteration 4, loss = 0.47243577\n",
      "Iteration 5, loss = 0.46972406\n",
      "Iteration 6, loss = 0.46967013\n",
      "Iteration 7, loss = 0.46878158\n",
      "Iteration 8, loss = 0.46666801\n",
      "Iteration 9, loss = 0.46671267\n",
      "Iteration 10, loss = 0.46455487\n",
      "Iteration 11, loss = 0.46325365\n",
      "Iteration 12, loss = 0.46249530\n",
      "Iteration 13, loss = 0.46263874\n",
      "Iteration 14, loss = 0.46096531\n",
      "Iteration 15, loss = 0.45682375\n",
      "Iteration 16, loss = 0.45745662\n",
      "Iteration 17, loss = 0.45546455\n",
      "Iteration 18, loss = 0.45541721\n",
      "Iteration 19, loss = 0.45205391\n",
      "Iteration 20, loss = 0.45281369\n",
      "Iteration 21, loss = 0.45179442\n",
      "Iteration 22, loss = 0.45085903\n",
      "Iteration 23, loss = 0.44960098\n",
      "Iteration 24, loss = 0.45023176\n",
      "Iteration 25, loss = 0.44897965\n",
      "Iteration 26, loss = 0.44823764\n",
      "Iteration 27, loss = 0.44871473\n",
      "Iteration 28, loss = 0.45079658\n",
      "Iteration 29, loss = 0.44711269\n",
      "Iteration 30, loss = 0.44916554\n",
      "Iteration 31, loss = 0.44904010\n",
      "Iteration 32, loss = 0.44687015\n",
      "Iteration 33, loss = 0.44701697\n",
      "Iteration 34, loss = 0.44646774\n",
      "Iteration 35, loss = 0.44757823\n",
      "Iteration 36, loss = 0.44848930\n",
      "Iteration 37, loss = 0.44737214\n",
      "Iteration 38, loss = 0.44825814\n",
      "Iteration 39, loss = 0.44444590\n",
      "Iteration 40, loss = 0.44425268\n",
      "Iteration 41, loss = 0.44358796\n",
      "Iteration 42, loss = 0.44598043\n",
      "Iteration 43, loss = 0.44375798\n",
      "Iteration 44, loss = 0.44409396\n",
      "Iteration 45, loss = 0.44546958\n",
      "Iteration 46, loss = 0.44306475\n",
      "Iteration 47, loss = 0.44436035\n",
      "Iteration 48, loss = 0.44363741\n",
      "Iteration 49, loss = 0.44329962\n",
      "Iteration 50, loss = 0.44337808\n",
      "Iteration 51, loss = 0.44304515\n",
      "Iteration 52, loss = 0.44302312\n",
      "Iteration 53, loss = 0.44360807\n",
      "Iteration 54, loss = 0.44383707\n",
      "Iteration 55, loss = 0.44385142\n",
      "Iteration 56, loss = 0.44319710\n",
      "Iteration 57, loss = 0.44305136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51675546\n",
      "Iteration 2, loss = 0.47773437\n",
      "Iteration 3, loss = 0.47100896\n",
      "Iteration 4, loss = 0.47143883\n",
      "Iteration 5, loss = 0.46738264\n",
      "Iteration 6, loss = 0.46579241\n",
      "Iteration 7, loss = 0.46457217\n",
      "Iteration 8, loss = 0.46083855\n",
      "Iteration 9, loss = 0.45787890\n",
      "Iteration 10, loss = 0.45783084\n",
      "Iteration 11, loss = 0.45597901\n",
      "Iteration 12, loss = 0.45482816\n",
      "Iteration 13, loss = 0.45724561\n",
      "Iteration 14, loss = 0.45366777\n",
      "Iteration 15, loss = 0.45372206\n",
      "Iteration 16, loss = 0.45084799\n",
      "Iteration 17, loss = 0.45110894\n",
      "Iteration 18, loss = 0.44966097\n",
      "Iteration 19, loss = 0.44938642\n",
      "Iteration 20, loss = 0.44846649\n",
      "Iteration 21, loss = 0.45138455\n",
      "Iteration 22, loss = 0.45092331\n",
      "Iteration 23, loss = 0.45002370\n",
      "Iteration 24, loss = 0.45242107\n",
      "Iteration 25, loss = 0.45034228\n",
      "Iteration 26, loss = 0.44877965\n",
      "Iteration 27, loss = 0.44947898\n",
      "Iteration 28, loss = 0.44880083\n",
      "Iteration 29, loss = 0.44731062\n",
      "Iteration 30, loss = 0.44871974\n",
      "Iteration 31, loss = 0.44883123\n",
      "Iteration 32, loss = 0.45034374\n",
      "Iteration 33, loss = 0.44898804\n",
      "Iteration 34, loss = 0.44771135\n",
      "Iteration 35, loss = 0.44890019\n",
      "Iteration 36, loss = 0.45013911\n",
      "Iteration 37, loss = 0.44930890\n",
      "Iteration 38, loss = 0.44825913\n",
      "Iteration 39, loss = 0.44839249\n",
      "Iteration 40, loss = 0.44910333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51306984\n",
      "Iteration 2, loss = 0.47724828\n",
      "Iteration 3, loss = 0.47469527\n",
      "Iteration 4, loss = 0.47040390\n",
      "Iteration 5, loss = 0.46835073\n",
      "Iteration 6, loss = 0.46813240\n",
      "Iteration 7, loss = 0.46733337\n",
      "Iteration 8, loss = 0.46554389\n",
      "Iteration 9, loss = 0.46534066\n",
      "Iteration 10, loss = 0.46368850\n",
      "Iteration 11, loss = 0.46233938\n",
      "Iteration 12, loss = 0.46084363\n",
      "Iteration 13, loss = 0.45982824\n",
      "Iteration 14, loss = 0.45668558\n",
      "Iteration 15, loss = 0.45890267\n",
      "Iteration 16, loss = 0.45466684\n",
      "Iteration 17, loss = 0.45551741\n",
      "Iteration 18, loss = 0.45524954\n",
      "Iteration 19, loss = 0.45362486\n",
      "Iteration 20, loss = 0.45228536\n",
      "Iteration 21, loss = 0.45254996\n",
      "Iteration 22, loss = 0.45004753\n",
      "Iteration 23, loss = 0.45074041\n",
      "Iteration 24, loss = 0.44948767\n",
      "Iteration 25, loss = 0.44863414\n",
      "Iteration 26, loss = 0.44843559\n",
      "Iteration 27, loss = 0.44938649\n",
      "Iteration 28, loss = 0.44632693\n",
      "Iteration 29, loss = 0.44707760\n",
      "Iteration 30, loss = 0.44687210\n",
      "Iteration 31, loss = 0.44674678\n",
      "Iteration 32, loss = 0.44637264\n",
      "Iteration 33, loss = 0.44610109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.44596021\n",
      "Iteration 35, loss = 0.44524488\n",
      "Iteration 36, loss = 0.44626630\n",
      "Iteration 37, loss = 0.44550317\n",
      "Iteration 38, loss = 0.44565419\n",
      "Iteration 39, loss = 0.44657912\n",
      "Iteration 40, loss = 0.44585301\n",
      "Iteration 41, loss = 0.44586383\n",
      "Iteration 42, loss = 0.44453778\n",
      "Iteration 43, loss = 0.44443254\n",
      "Iteration 44, loss = 0.44658883\n",
      "Iteration 45, loss = 0.44444422\n",
      "Iteration 46, loss = 0.44474868\n",
      "Iteration 47, loss = 0.44372899\n",
      "Iteration 48, loss = 0.44479612\n",
      "Iteration 49, loss = 0.44330270\n",
      "Iteration 50, loss = 0.44408415\n",
      "Iteration 51, loss = 0.44584350\n",
      "Iteration 52, loss = 0.44570276\n",
      "Iteration 53, loss = 0.44524348\n",
      "Iteration 54, loss = 0.44491825\n",
      "Iteration 55, loss = 0.44434198\n",
      "Iteration 56, loss = 0.44331767\n",
      "Iteration 57, loss = 0.44562832\n",
      "Iteration 58, loss = 0.44325014\n",
      "Iteration 59, loss = 0.44412521\n",
      "Iteration 60, loss = 0.44437190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51928900\n",
      "Iteration 2, loss = 0.47619126\n",
      "Iteration 3, loss = 0.47075414\n",
      "Iteration 4, loss = 0.46833613\n",
      "Iteration 5, loss = 0.46622836\n",
      "Iteration 6, loss = 0.46490891\n",
      "Iteration 7, loss = 0.46428225\n",
      "Iteration 8, loss = 0.46235886\n",
      "Iteration 9, loss = 0.46160932\n",
      "Iteration 10, loss = 0.46027486\n",
      "Iteration 11, loss = 0.45929162\n",
      "Iteration 12, loss = 0.46002796\n",
      "Iteration 13, loss = 0.45898654\n",
      "Iteration 14, loss = 0.45861425\n",
      "Iteration 15, loss = 0.45572878\n",
      "Iteration 16, loss = 0.45657007\n",
      "Iteration 17, loss = 0.45378002\n",
      "Iteration 18, loss = 0.45451362\n",
      "Iteration 19, loss = 0.45149273\n",
      "Iteration 20, loss = 0.45250405\n",
      "Iteration 21, loss = 0.45217088\n",
      "Iteration 22, loss = 0.45105299\n",
      "Iteration 23, loss = 0.45009296\n",
      "Iteration 24, loss = 0.45170375\n",
      "Iteration 25, loss = 0.45059387\n",
      "Iteration 26, loss = 0.45005479\n",
      "Iteration 27, loss = 0.44892033\n",
      "Iteration 28, loss = 0.45386564\n",
      "Iteration 29, loss = 0.44974120\n",
      "Iteration 30, loss = 0.45226042\n",
      "Iteration 31, loss = 0.45197035\n",
      "Iteration 32, loss = 0.44960629\n",
      "Iteration 33, loss = 0.44812042\n",
      "Iteration 34, loss = 0.44859213\n",
      "Iteration 35, loss = 0.44957525\n",
      "Iteration 36, loss = 0.45070311\n",
      "Iteration 37, loss = 0.45093839\n",
      "Iteration 38, loss = 0.45309469\n",
      "Iteration 39, loss = 0.44937024\n",
      "Iteration 40, loss = 0.44786043\n",
      "Iteration 41, loss = 0.44722479\n",
      "Iteration 42, loss = 0.45059056\n",
      "Iteration 43, loss = 0.44757184\n",
      "Iteration 44, loss = 0.44815773\n",
      "Iteration 45, loss = 0.44729204\n",
      "Iteration 46, loss = 0.44795389\n",
      "Iteration 47, loss = 0.44805152\n",
      "Iteration 48, loss = 0.44926421\n",
      "Iteration 49, loss = 0.44720384\n",
      "Iteration 50, loss = 0.44739715\n",
      "Iteration 51, loss = 0.44782860\n",
      "Iteration 52, loss = 0.44718316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52209846\n",
      "Iteration 2, loss = 0.47791695\n",
      "Iteration 3, loss = 0.47043019\n",
      "Iteration 4, loss = 0.46992162\n",
      "Iteration 5, loss = 0.46694158\n",
      "Iteration 6, loss = 0.46581211\n",
      "Iteration 7, loss = 0.46543590\n",
      "Iteration 8, loss = 0.46309698\n",
      "Iteration 9, loss = 0.46125407\n",
      "Iteration 10, loss = 0.46062030\n",
      "Iteration 11, loss = 0.45951554\n",
      "Iteration 12, loss = 0.45844387\n",
      "Iteration 13, loss = 0.45955344\n",
      "Iteration 14, loss = 0.45667985\n",
      "Iteration 15, loss = 0.45769464\n",
      "Iteration 16, loss = 0.45609707\n",
      "Iteration 17, loss = 0.45484264\n",
      "Iteration 18, loss = 0.45406308\n",
      "Iteration 19, loss = 0.45369874\n",
      "Iteration 20, loss = 0.45316290\n",
      "Iteration 21, loss = 0.45514932\n",
      "Iteration 22, loss = 0.45420114\n",
      "Iteration 23, loss = 0.45306356\n",
      "Iteration 24, loss = 0.45571400\n",
      "Iteration 25, loss = 0.45312489\n",
      "Iteration 26, loss = 0.45122022\n",
      "Iteration 27, loss = 0.45102357\n",
      "Iteration 28, loss = 0.45242886\n",
      "Iteration 29, loss = 0.45044081\n",
      "Iteration 30, loss = 0.45140616\n",
      "Iteration 31, loss = 0.45125083\n",
      "Iteration 32, loss = 0.45071707\n",
      "Iteration 33, loss = 0.45069607\n",
      "Iteration 34, loss = 0.44897435\n",
      "Iteration 35, loss = 0.44971221\n",
      "Iteration 36, loss = 0.45066237\n",
      "Iteration 37, loss = 0.45193007\n",
      "Iteration 38, loss = 0.45065586\n",
      "Iteration 39, loss = 0.44991971\n",
      "Iteration 40, loss = 0.44912535\n",
      "Iteration 41, loss = 0.44884601\n",
      "Iteration 42, loss = 0.45040082\n",
      "Iteration 43, loss = 0.44933868\n",
      "Iteration 44, loss = 0.44889321\n",
      "Iteration 45, loss = 0.44903718\n",
      "Iteration 46, loss = 0.44886107\n",
      "Iteration 47, loss = 0.44889798\n",
      "Iteration 48, loss = 0.44841946\n",
      "Iteration 49, loss = 0.44910571\n",
      "Iteration 50, loss = 0.44908881\n",
      "Iteration 51, loss = 0.44879821\n",
      "Iteration 52, loss = 0.44797570\n",
      "Iteration 53, loss = 0.44861980\n",
      "Iteration 54, loss = 0.44903890\n",
      "Iteration 55, loss = 0.44948422\n",
      "Iteration 56, loss = 0.44864440\n",
      "Iteration 57, loss = 0.44787456\n",
      "Iteration 58, loss = 0.44714902\n",
      "Iteration 59, loss = 0.44721284\n",
      "Iteration 60, loss = 0.44810394\n",
      "Iteration 61, loss = 0.44911328\n",
      "Iteration 62, loss = 0.44797557\n",
      "Iteration 63, loss = 0.44820437\n",
      "Iteration 64, loss = 0.44771168\n",
      "Iteration 65, loss = 0.44770759\n",
      "Iteration 66, loss = 0.44815698\n",
      "Iteration 67, loss = 0.44729786\n",
      "Iteration 68, loss = 0.44837892\n",
      "Iteration 69, loss = 0.44737744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51598104\n",
      "Iteration 2, loss = 0.47731743\n",
      "Iteration 3, loss = 0.47150104\n",
      "Iteration 4, loss = 0.46793483\n",
      "Iteration 5, loss = 0.46626063\n",
      "Iteration 6, loss = 0.46590413\n",
      "Iteration 7, loss = 0.46566774\n",
      "Iteration 8, loss = 0.46349713\n",
      "Iteration 9, loss = 0.46336612\n",
      "Iteration 10, loss = 0.46155238\n",
      "Iteration 11, loss = 0.46014669\n",
      "Iteration 12, loss = 0.45837210\n",
      "Iteration 13, loss = 0.45867277\n",
      "Iteration 14, loss = 0.45670781\n",
      "Iteration 15, loss = 0.45800369\n",
      "Iteration 16, loss = 0.45450920\n",
      "Iteration 17, loss = 0.45645234\n",
      "Iteration 18, loss = 0.45391595\n",
      "Iteration 19, loss = 0.45289990\n",
      "Iteration 20, loss = 0.45402101\n",
      "Iteration 21, loss = 0.45404614\n",
      "Iteration 22, loss = 0.45129183\n",
      "Iteration 23, loss = 0.45158462\n",
      "Iteration 24, loss = 0.45051638\n",
      "Iteration 25, loss = 0.45085622\n",
      "Iteration 26, loss = 0.45056537\n",
      "Iteration 27, loss = 0.45103144\n",
      "Iteration 28, loss = 0.44848047\n",
      "Iteration 29, loss = 0.44959472\n",
      "Iteration 30, loss = 0.44878615\n",
      "Iteration 31, loss = 0.44926097\n",
      "Iteration 32, loss = 0.44799609\n",
      "Iteration 33, loss = 0.44969198\n",
      "Iteration 34, loss = 0.44872224\n",
      "Iteration 35, loss = 0.44779449\n",
      "Iteration 36, loss = 0.44831362\n",
      "Iteration 37, loss = 0.44723928\n",
      "Iteration 38, loss = 0.44763302\n",
      "Iteration 39, loss = 0.44769484\n",
      "Iteration 40, loss = 0.44825044\n",
      "Iteration 41, loss = 0.44819758\n",
      "Iteration 42, loss = 0.44668650\n",
      "Iteration 43, loss = 0.44657477\n",
      "Iteration 44, loss = 0.44782481\n",
      "Iteration 45, loss = 0.44666882\n",
      "Iteration 46, loss = 0.44658855\n",
      "Iteration 47, loss = 0.44646309\n",
      "Iteration 48, loss = 0.44740642\n",
      "Iteration 49, loss = 0.44621715\n",
      "Iteration 50, loss = 0.44580517\n",
      "Iteration 51, loss = 0.44637023\n",
      "Iteration 52, loss = 0.44799623\n",
      "Iteration 53, loss = 0.44715548\n",
      "Iteration 54, loss = 0.44638289\n",
      "Iteration 55, loss = 0.44586470\n",
      "Iteration 56, loss = 0.44517480\n",
      "Iteration 57, loss = 0.44623344\n",
      "Iteration 58, loss = 0.44582194\n",
      "Iteration 59, loss = 0.44558108\n",
      "Iteration 60, loss = 0.44662255\n",
      "Iteration 61, loss = 0.44834067\n",
      "Iteration 62, loss = 0.44545562\n",
      "Iteration 63, loss = 0.44575310\n",
      "Iteration 64, loss = 0.44587603\n",
      "Iteration 65, loss = 0.44608940\n",
      "Iteration 66, loss = 0.44531027\n",
      "Iteration 67, loss = 0.44568911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55196643\n",
      "Iteration 2, loss = 0.52478169\n",
      "Iteration 3, loss = 0.51336895\n",
      "Iteration 4, loss = 0.50461058\n",
      "Iteration 5, loss = 0.49701528\n",
      "Iteration 6, loss = 0.49117672\n",
      "Iteration 7, loss = 0.48475273\n",
      "Iteration 8, loss = 0.48388554\n",
      "Iteration 9, loss = 0.47868113\n",
      "Iteration 10, loss = 0.47600095\n",
      "Iteration 11, loss = 0.47366261\n",
      "Iteration 12, loss = 0.47146821\n",
      "Iteration 13, loss = 0.46937836\n",
      "Iteration 14, loss = 0.46705671\n",
      "Iteration 15, loss = 0.46696300\n",
      "Iteration 16, loss = 0.46245759\n",
      "Iteration 17, loss = 0.46095155\n",
      "Iteration 18, loss = 0.45644560\n",
      "Iteration 19, loss = 0.45424089\n",
      "Iteration 20, loss = 0.45441449\n",
      "Iteration 21, loss = 0.45363449\n",
      "Iteration 22, loss = 0.45472148\n",
      "Iteration 23, loss = 0.45162277\n",
      "Iteration 24, loss = 0.45202054\n",
      "Iteration 25, loss = 0.45170171\n",
      "Iteration 26, loss = 0.45084363\n",
      "Iteration 27, loss = 0.44879775\n",
      "Iteration 28, loss = 0.45279396\n",
      "Iteration 29, loss = 0.44898361\n",
      "Iteration 30, loss = 0.44908572\n",
      "Iteration 31, loss = 0.44799281\n",
      "Iteration 32, loss = 0.44644866\n",
      "Iteration 33, loss = 0.44551142\n",
      "Iteration 34, loss = 0.44698521\n",
      "Iteration 35, loss = 0.44779051\n",
      "Iteration 36, loss = 0.44674110\n",
      "Iteration 37, loss = 0.44510061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.44741430\n",
      "Iteration 39, loss = 0.44594720\n",
      "Iteration 40, loss = 0.44291546\n",
      "Iteration 41, loss = 0.44221150\n",
      "Iteration 42, loss = 0.44574072\n",
      "Iteration 43, loss = 0.44422467\n",
      "Iteration 44, loss = 0.44195220\n",
      "Iteration 45, loss = 0.44269692\n",
      "Iteration 46, loss = 0.44255519\n",
      "Iteration 47, loss = 0.44011576\n",
      "Iteration 48, loss = 0.44241736\n",
      "Iteration 49, loss = 0.44083267\n",
      "Iteration 50, loss = 0.44096316\n",
      "Iteration 51, loss = 0.44341554\n",
      "Iteration 52, loss = 0.44046258\n",
      "Iteration 53, loss = 0.44160344\n",
      "Iteration 54, loss = 0.44341459\n",
      "Iteration 55, loss = 0.44287898\n",
      "Iteration 56, loss = 0.44120541\n",
      "Iteration 57, loss = 0.44269318\n",
      "Iteration 58, loss = 0.43998166\n",
      "Iteration 59, loss = 0.43889367\n",
      "Iteration 60, loss = 0.44170718\n",
      "Iteration 61, loss = 0.44045357\n",
      "Iteration 62, loss = 0.43946776\n",
      "Iteration 63, loss = 0.43956222\n",
      "Iteration 64, loss = 0.43863518\n",
      "Iteration 65, loss = 0.44184249\n",
      "Iteration 66, loss = 0.43862907\n",
      "Iteration 67, loss = 0.44113654\n",
      "Iteration 68, loss = 0.44143405\n",
      "Iteration 69, loss = 0.43821473\n",
      "Iteration 70, loss = 0.44089656\n",
      "Iteration 71, loss = 0.43991879\n",
      "Iteration 72, loss = 0.44175013\n",
      "Iteration 73, loss = 0.43898746\n",
      "Iteration 74, loss = 0.43800058\n",
      "Iteration 75, loss = 0.43802675\n",
      "Iteration 76, loss = 0.43768435\n",
      "Iteration 77, loss = 0.44139358\n",
      "Iteration 78, loss = 0.43907402\n",
      "Iteration 79, loss = 0.43917489\n",
      "Iteration 80, loss = 0.43681029\n",
      "Iteration 81, loss = 0.43801585\n",
      "Iteration 82, loss = 0.43787513\n",
      "Iteration 83, loss = 0.43802770\n",
      "Iteration 84, loss = 0.43716397\n",
      "Iteration 85, loss = 0.43785797\n",
      "Iteration 86, loss = 0.43611369\n",
      "Iteration 87, loss = 0.43877459\n",
      "Iteration 88, loss = 0.44183001\n",
      "Iteration 89, loss = 0.43857269\n",
      "Iteration 90, loss = 0.43701791\n",
      "Iteration 91, loss = 0.43686856\n",
      "Iteration 92, loss = 0.43577107\n",
      "Iteration 93, loss = 0.43612653\n",
      "Iteration 94, loss = 0.43761683\n",
      "Iteration 95, loss = 0.43886365\n",
      "Iteration 96, loss = 0.43512291\n",
      "Iteration 97, loss = 0.43443847\n",
      "Iteration 98, loss = 0.43647052\n",
      "Iteration 99, loss = 0.43271859\n",
      "Iteration 100, loss = 0.43338057\n",
      "Iteration 101, loss = 0.43491089\n",
      "Iteration 102, loss = 0.43524730\n",
      "Iteration 103, loss = 0.43175414\n",
      "Iteration 104, loss = 0.43134297\n",
      "Iteration 105, loss = 0.43756061\n",
      "Iteration 106, loss = 0.43637385\n",
      "Iteration 107, loss = 0.43401299\n",
      "Iteration 108, loss = 0.43480139\n",
      "Iteration 109, loss = 0.43244296\n",
      "Iteration 110, loss = 0.43279682\n",
      "Iteration 111, loss = 0.43297821\n",
      "Iteration 112, loss = 0.43191308\n",
      "Iteration 113, loss = 0.43249721\n",
      "Iteration 114, loss = 0.43048318\n",
      "Iteration 115, loss = 0.43185228\n",
      "Iteration 116, loss = 0.43293129\n",
      "Iteration 117, loss = 0.43066134\n",
      "Iteration 118, loss = 0.43190261\n",
      "Iteration 119, loss = 0.43102556\n",
      "Iteration 120, loss = 0.43407983\n",
      "Iteration 121, loss = 0.43172281\n",
      "Iteration 122, loss = 0.43168073\n",
      "Iteration 123, loss = 0.43395387\n",
      "Iteration 124, loss = 0.43405567\n",
      "Iteration 125, loss = 0.43361149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55014986\n",
      "Iteration 2, loss = 0.51953101\n",
      "Iteration 3, loss = 0.50832889\n",
      "Iteration 4, loss = 0.50376076\n",
      "Iteration 5, loss = 0.49512576\n",
      "Iteration 6, loss = 0.48882872\n",
      "Iteration 7, loss = 0.48696353\n",
      "Iteration 8, loss = 0.48005898\n",
      "Iteration 9, loss = 0.47934441\n",
      "Iteration 10, loss = 0.47740025\n",
      "Iteration 11, loss = 0.47620625\n",
      "Iteration 12, loss = 0.47319458\n",
      "Iteration 13, loss = 0.47376718\n",
      "Iteration 14, loss = 0.47101256\n",
      "Iteration 15, loss = 0.47253728\n",
      "Iteration 16, loss = 0.47155042\n",
      "Iteration 17, loss = 0.47077119\n",
      "Iteration 18, loss = 0.47074468\n",
      "Iteration 19, loss = 0.46914629\n",
      "Iteration 20, loss = 0.46882100\n",
      "Iteration 21, loss = 0.46945194\n",
      "Iteration 22, loss = 0.46996839\n",
      "Iteration 23, loss = 0.46806189\n",
      "Iteration 24, loss = 0.46826935\n",
      "Iteration 25, loss = 0.46827003\n",
      "Iteration 26, loss = 0.47011455\n",
      "Iteration 27, loss = 0.46796317\n",
      "Iteration 28, loss = 0.46767132\n",
      "Iteration 29, loss = 0.46814808\n",
      "Iteration 30, loss = 0.46410347\n",
      "Iteration 31, loss = 0.46622199\n",
      "Iteration 32, loss = 0.46792972\n",
      "Iteration 33, loss = 0.46421938\n",
      "Iteration 34, loss = 0.46609061\n",
      "Iteration 35, loss = 0.46477829\n",
      "Iteration 36, loss = 0.46533913\n",
      "Iteration 37, loss = 0.46870048\n",
      "Iteration 38, loss = 0.46316749\n",
      "Iteration 39, loss = 0.46424089\n",
      "Iteration 40, loss = 0.46458975\n",
      "Iteration 41, loss = 0.46148410\n",
      "Iteration 42, loss = 0.45989696\n",
      "Iteration 43, loss = 0.45901675\n",
      "Iteration 44, loss = 0.45722395\n",
      "Iteration 45, loss = 0.45734110\n",
      "Iteration 46, loss = 0.45465143\n",
      "Iteration 47, loss = 0.45501743\n",
      "Iteration 48, loss = 0.45205295\n",
      "Iteration 49, loss = 0.45071461\n",
      "Iteration 50, loss = 0.44822727\n",
      "Iteration 51, loss = 0.44697627\n",
      "Iteration 52, loss = 0.44661962\n",
      "Iteration 53, loss = 0.44885028\n",
      "Iteration 54, loss = 0.44518583\n",
      "Iteration 55, loss = 0.44438409\n",
      "Iteration 56, loss = 0.44527914\n",
      "Iteration 57, loss = 0.44550538\n",
      "Iteration 58, loss = 0.44529227\n",
      "Iteration 59, loss = 0.44387165\n",
      "Iteration 60, loss = 0.44476406\n",
      "Iteration 61, loss = 0.44499650\n",
      "Iteration 62, loss = 0.44258755\n",
      "Iteration 63, loss = 0.44411479\n",
      "Iteration 64, loss = 0.44307730\n",
      "Iteration 65, loss = 0.44163141\n",
      "Iteration 66, loss = 0.44024883\n",
      "Iteration 67, loss = 0.44026799\n",
      "Iteration 68, loss = 0.44332651\n",
      "Iteration 69, loss = 0.44240286\n",
      "Iteration 70, loss = 0.43625206\n",
      "Iteration 71, loss = 0.43782844\n",
      "Iteration 72, loss = 0.43496289\n",
      "Iteration 73, loss = 0.43595057\n",
      "Iteration 74, loss = 0.43597780\n",
      "Iteration 75, loss = 0.43549877\n",
      "Iteration 76, loss = 0.43287576\n",
      "Iteration 77, loss = 0.43499755\n",
      "Iteration 78, loss = 0.43618417\n",
      "Iteration 79, loss = 0.43311735\n",
      "Iteration 80, loss = 0.43145176\n",
      "Iteration 81, loss = 0.43476551\n",
      "Iteration 82, loss = 0.43150839\n",
      "Iteration 83, loss = 0.43316004\n",
      "Iteration 84, loss = 0.43093981\n",
      "Iteration 85, loss = 0.43253058\n",
      "Iteration 86, loss = 0.43396494\n",
      "Iteration 87, loss = 0.43501851\n",
      "Iteration 88, loss = 0.44104390\n",
      "Iteration 89, loss = 0.43356764\n",
      "Iteration 90, loss = 0.43341523\n",
      "Iteration 91, loss = 0.43057886\n",
      "Iteration 92, loss = 0.43189955\n",
      "Iteration 93, loss = 0.43288559\n",
      "Iteration 94, loss = 0.43449149\n",
      "Iteration 95, loss = 0.43560540\n",
      "Iteration 96, loss = 0.43425600\n",
      "Iteration 97, loss = 0.43116658\n",
      "Iteration 98, loss = 0.43063393\n",
      "Iteration 99, loss = 0.43238697\n",
      "Iteration 100, loss = 0.42884717\n",
      "Iteration 101, loss = 0.43186249\n",
      "Iteration 102, loss = 0.43147478\n",
      "Iteration 103, loss = 0.42881553\n",
      "Iteration 104, loss = 0.43287575\n",
      "Iteration 105, loss = 0.43244669\n",
      "Iteration 106, loss = 0.43197829\n",
      "Iteration 107, loss = 0.42946695\n",
      "Iteration 108, loss = 0.43126993\n",
      "Iteration 109, loss = 0.43145047\n",
      "Iteration 110, loss = 0.42938125\n",
      "Iteration 111, loss = 0.43189168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54582547\n",
      "Iteration 2, loss = 0.52217740\n",
      "Iteration 3, loss = 0.51170553\n",
      "Iteration 4, loss = 0.50436293\n",
      "Iteration 5, loss = 0.49983681\n",
      "Iteration 6, loss = 0.49757108\n",
      "Iteration 7, loss = 0.49515154\n",
      "Iteration 8, loss = 0.48971244\n",
      "Iteration 9, loss = 0.48802186\n",
      "Iteration 10, loss = 0.48554781\n",
      "Iteration 11, loss = 0.48062539\n",
      "Iteration 12, loss = 0.47652219\n",
      "Iteration 13, loss = 0.47509135\n",
      "Iteration 14, loss = 0.47321796\n",
      "Iteration 15, loss = 0.47092803\n",
      "Iteration 16, loss = 0.46908409\n",
      "Iteration 17, loss = 0.46724216\n",
      "Iteration 18, loss = 0.46473136\n",
      "Iteration 19, loss = 0.46568552\n",
      "Iteration 20, loss = 0.46472294\n",
      "Iteration 21, loss = 0.46107043\n",
      "Iteration 22, loss = 0.45862114\n",
      "Iteration 23, loss = 0.45622495\n",
      "Iteration 24, loss = 0.45668708\n",
      "Iteration 25, loss = 0.45619539\n",
      "Iteration 26, loss = 0.45926404\n",
      "Iteration 27, loss = 0.45627203\n",
      "Iteration 28, loss = 0.45422567\n",
      "Iteration 29, loss = 0.45014862\n",
      "Iteration 30, loss = 0.44917141\n",
      "Iteration 31, loss = 0.45013337\n",
      "Iteration 32, loss = 0.45108822\n",
      "Iteration 33, loss = 0.44882875\n",
      "Iteration 34, loss = 0.45066868\n",
      "Iteration 35, loss = 0.44940932\n",
      "Iteration 36, loss = 0.44848383\n",
      "Iteration 37, loss = 0.44736673\n",
      "Iteration 38, loss = 0.45162393\n",
      "Iteration 39, loss = 0.44639896\n",
      "Iteration 40, loss = 0.45079651\n",
      "Iteration 41, loss = 0.44889347\n",
      "Iteration 42, loss = 0.44742590\n",
      "Iteration 43, loss = 0.44695008\n",
      "Iteration 44, loss = 0.44579744\n",
      "Iteration 45, loss = 0.44689140\n",
      "Iteration 46, loss = 0.44991938\n",
      "Iteration 47, loss = 0.44526410\n",
      "Iteration 48, loss = 0.44512990\n",
      "Iteration 49, loss = 0.44527993\n",
      "Iteration 50, loss = 0.44494519\n",
      "Iteration 51, loss = 0.44464323\n",
      "Iteration 52, loss = 0.44913923\n",
      "Iteration 53, loss = 0.44396052\n",
      "Iteration 54, loss = 0.44489057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, loss = 0.44721592\n",
      "Iteration 56, loss = 0.44415666\n",
      "Iteration 57, loss = 0.44222608\n",
      "Iteration 58, loss = 0.44414772\n",
      "Iteration 59, loss = 0.44318500\n",
      "Iteration 60, loss = 0.44509336\n",
      "Iteration 61, loss = 0.45076691\n",
      "Iteration 62, loss = 0.44574310\n",
      "Iteration 63, loss = 0.44290427\n",
      "Iteration 64, loss = 0.44631185\n",
      "Iteration 65, loss = 0.44220422\n",
      "Iteration 66, loss = 0.44564356\n",
      "Iteration 67, loss = 0.44052044\n",
      "Iteration 68, loss = 0.44074439\n",
      "Iteration 69, loss = 0.44380943\n",
      "Iteration 70, loss = 0.44165563\n",
      "Iteration 71, loss = 0.44296417\n",
      "Iteration 72, loss = 0.43906521\n",
      "Iteration 73, loss = 0.44075500\n",
      "Iteration 74, loss = 0.44193707\n",
      "Iteration 75, loss = 0.44390836\n",
      "Iteration 76, loss = 0.43962780\n",
      "Iteration 77, loss = 0.44281981\n",
      "Iteration 78, loss = 0.44056063\n",
      "Iteration 79, loss = 0.44129143\n",
      "Iteration 80, loss = 0.43972502\n",
      "Iteration 81, loss = 0.43988668\n",
      "Iteration 82, loss = 0.44298282\n",
      "Iteration 83, loss = 0.43935285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54969793\n",
      "Iteration 2, loss = 0.51624301\n",
      "Iteration 3, loss = 0.49583129\n",
      "Iteration 4, loss = 0.48372666\n",
      "Iteration 5, loss = 0.47441982\n",
      "Iteration 6, loss = 0.47062915\n",
      "Iteration 7, loss = 0.46619469\n",
      "Iteration 8, loss = 0.46562761\n",
      "Iteration 9, loss = 0.46350132\n",
      "Iteration 10, loss = 0.45984257\n",
      "Iteration 11, loss = 0.45960827\n",
      "Iteration 12, loss = 0.45509943\n",
      "Iteration 13, loss = 0.45789914\n",
      "Iteration 14, loss = 0.45410838\n",
      "Iteration 15, loss = 0.45221289\n",
      "Iteration 16, loss = 0.45207931\n",
      "Iteration 17, loss = 0.45241444\n",
      "Iteration 18, loss = 0.44993909\n",
      "Iteration 19, loss = 0.44885945\n",
      "Iteration 20, loss = 0.45082366\n",
      "Iteration 21, loss = 0.44795102\n",
      "Iteration 22, loss = 0.45043762\n",
      "Iteration 23, loss = 0.44855477\n",
      "Iteration 24, loss = 0.44822692\n",
      "Iteration 25, loss = 0.44634556\n",
      "Iteration 26, loss = 0.44575663\n",
      "Iteration 27, loss = 0.44503975\n",
      "Iteration 28, loss = 0.44677978\n",
      "Iteration 29, loss = 0.44280484\n",
      "Iteration 30, loss = 0.44508839\n",
      "Iteration 31, loss = 0.44840335\n",
      "Iteration 32, loss = 0.44282927\n",
      "Iteration 33, loss = 0.44103414\n",
      "Iteration 34, loss = 0.44154020\n",
      "Iteration 35, loss = 0.44131244\n",
      "Iteration 36, loss = 0.44121849\n",
      "Iteration 37, loss = 0.43972930\n",
      "Iteration 38, loss = 0.44109185\n",
      "Iteration 39, loss = 0.43998479\n",
      "Iteration 40, loss = 0.43997691\n",
      "Iteration 41, loss = 0.43789888\n",
      "Iteration 42, loss = 0.44238954\n",
      "Iteration 43, loss = 0.43906407\n",
      "Iteration 44, loss = 0.43695086\n",
      "Iteration 45, loss = 0.43839674\n",
      "Iteration 46, loss = 0.43796859\n",
      "Iteration 47, loss = 0.43679217\n",
      "Iteration 48, loss = 0.43725670\n",
      "Iteration 49, loss = 0.43878957\n",
      "Iteration 50, loss = 0.43751320\n",
      "Iteration 51, loss = 0.44072475\n",
      "Iteration 52, loss = 0.43616240\n",
      "Iteration 53, loss = 0.43758470\n",
      "Iteration 54, loss = 0.43659182\n",
      "Iteration 55, loss = 0.44017684\n",
      "Iteration 56, loss = 0.43782998\n",
      "Iteration 57, loss = 0.43702695\n",
      "Iteration 58, loss = 0.43848923\n",
      "Iteration 59, loss = 0.43501229\n",
      "Iteration 60, loss = 0.43690787\n",
      "Iteration 61, loss = 0.43681031\n",
      "Iteration 62, loss = 0.43624314\n",
      "Iteration 63, loss = 0.43591465\n",
      "Iteration 64, loss = 0.43514304\n",
      "Iteration 65, loss = 0.43626789\n",
      "Iteration 66, loss = 0.43469935\n",
      "Iteration 67, loss = 0.43802137\n",
      "Iteration 68, loss = 0.43783132\n",
      "Iteration 69, loss = 0.43463099\n",
      "Iteration 70, loss = 0.43565466\n",
      "Iteration 71, loss = 0.43601765\n",
      "Iteration 72, loss = 0.43526561\n",
      "Iteration 73, loss = 0.43550438\n",
      "Iteration 74, loss = 0.43544330\n",
      "Iteration 75, loss = 0.43281629\n",
      "Iteration 76, loss = 0.43231227\n",
      "Iteration 77, loss = 0.43296679\n",
      "Iteration 78, loss = 0.43216054\n",
      "Iteration 79, loss = 0.43163525\n",
      "Iteration 80, loss = 0.42926358\n",
      "Iteration 81, loss = 0.42901377\n",
      "Iteration 82, loss = 0.43012186\n",
      "Iteration 83, loss = 0.42775722\n",
      "Iteration 84, loss = 0.42673364\n",
      "Iteration 85, loss = 0.43021630\n",
      "Iteration 86, loss = 0.42634560\n",
      "Iteration 87, loss = 0.42707203\n",
      "Iteration 88, loss = 0.42754320\n",
      "Iteration 89, loss = 0.43008880\n",
      "Iteration 90, loss = 0.42727125\n",
      "Iteration 91, loss = 0.42314281\n",
      "Iteration 92, loss = 0.42447871\n",
      "Iteration 93, loss = 0.42242592\n",
      "Iteration 94, loss = 0.42215426\n",
      "Iteration 95, loss = 0.42484096\n",
      "Iteration 96, loss = 0.42409616\n",
      "Iteration 97, loss = 0.42267408\n",
      "Iteration 98, loss = 0.42818007\n",
      "Iteration 99, loss = 0.42230773\n",
      "Iteration 100, loss = 0.42039505\n",
      "Iteration 101, loss = 0.42334852\n",
      "Iteration 102, loss = 0.42197778\n",
      "Iteration 103, loss = 0.42321308\n",
      "Iteration 104, loss = 0.42045818\n",
      "Iteration 105, loss = 0.42152663\n",
      "Iteration 106, loss = 0.42059294\n",
      "Iteration 107, loss = 0.42302328\n",
      "Iteration 108, loss = 0.42366154\n",
      "Iteration 109, loss = 0.42288552\n",
      "Iteration 110, loss = 0.42354125\n",
      "Iteration 111, loss = 0.42120559\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54856731\n",
      "Iteration 2, loss = 0.51309423\n",
      "Iteration 3, loss = 0.49254592\n",
      "Iteration 4, loss = 0.48303367\n",
      "Iteration 5, loss = 0.47130341\n",
      "Iteration 6, loss = 0.46693441\n",
      "Iteration 7, loss = 0.46429725\n",
      "Iteration 8, loss = 0.46052633\n",
      "Iteration 9, loss = 0.45920050\n",
      "Iteration 10, loss = 0.45800797\n",
      "Iteration 11, loss = 0.45744121\n",
      "Iteration 12, loss = 0.45267538\n",
      "Iteration 13, loss = 0.45373181\n",
      "Iteration 14, loss = 0.45225218\n",
      "Iteration 15, loss = 0.44945252\n",
      "Iteration 16, loss = 0.44721895\n",
      "Iteration 17, loss = 0.44734197\n",
      "Iteration 18, loss = 0.44615677\n",
      "Iteration 19, loss = 0.44372553\n",
      "Iteration 20, loss = 0.44315310\n",
      "Iteration 21, loss = 0.44502273\n",
      "Iteration 22, loss = 0.44284432\n",
      "Iteration 23, loss = 0.43998891\n",
      "Iteration 24, loss = 0.44233142\n",
      "Iteration 25, loss = 0.44164410\n",
      "Iteration 26, loss = 0.44037878\n",
      "Iteration 27, loss = 0.43869685\n",
      "Iteration 28, loss = 0.43596745\n",
      "Iteration 29, loss = 0.43940351\n",
      "Iteration 30, loss = 0.43797377\n",
      "Iteration 31, loss = 0.43807378\n",
      "Iteration 32, loss = 0.43682895\n",
      "Iteration 33, loss = 0.43656155\n",
      "Iteration 34, loss = 0.43662550\n",
      "Iteration 35, loss = 0.43955427\n",
      "Iteration 36, loss = 0.43998703\n",
      "Iteration 37, loss = 0.44164234\n",
      "Iteration 38, loss = 0.43585944\n",
      "Iteration 39, loss = 0.43715073\n",
      "Iteration 40, loss = 0.43798042\n",
      "Iteration 41, loss = 0.43482505\n",
      "Iteration 42, loss = 0.43594197\n",
      "Iteration 43, loss = 0.43536402\n",
      "Iteration 44, loss = 0.43627541\n",
      "Iteration 45, loss = 0.43677968\n",
      "Iteration 46, loss = 0.43333202\n",
      "Iteration 47, loss = 0.43495448\n",
      "Iteration 48, loss = 0.43258736\n",
      "Iteration 49, loss = 0.43414490\n",
      "Iteration 50, loss = 0.43277012\n",
      "Iteration 51, loss = 0.43496716\n",
      "Iteration 52, loss = 0.43421360\n",
      "Iteration 53, loss = 0.43252168\n",
      "Iteration 54, loss = 0.43364875\n",
      "Iteration 55, loss = 0.43165933\n",
      "Iteration 56, loss = 0.43300619\n",
      "Iteration 57, loss = 0.43423365\n",
      "Iteration 58, loss = 0.43176653\n",
      "Iteration 59, loss = 0.43240707\n",
      "Iteration 60, loss = 0.43123986\n",
      "Iteration 61, loss = 0.43427959\n",
      "Iteration 62, loss = 0.43168425\n",
      "Iteration 63, loss = 0.43012254\n",
      "Iteration 64, loss = 0.43304125\n",
      "Iteration 65, loss = 0.43062675\n",
      "Iteration 66, loss = 0.43075632\n",
      "Iteration 67, loss = 0.42982015\n",
      "Iteration 68, loss = 0.43233962\n",
      "Iteration 69, loss = 0.43637626\n",
      "Iteration 70, loss = 0.43088044\n",
      "Iteration 71, loss = 0.43177702\n",
      "Iteration 72, loss = 0.43057061\n",
      "Iteration 73, loss = 0.42894934\n",
      "Iteration 74, loss = 0.42931273\n",
      "Iteration 75, loss = 0.42744350\n",
      "Iteration 76, loss = 0.42789390\n",
      "Iteration 77, loss = 0.42756823\n",
      "Iteration 78, loss = 0.42925383\n",
      "Iteration 79, loss = 0.42842305\n",
      "Iteration 80, loss = 0.42773728\n",
      "Iteration 81, loss = 0.42832291\n",
      "Iteration 82, loss = 0.42707497\n",
      "Iteration 83, loss = 0.42973598\n",
      "Iteration 84, loss = 0.42530593\n",
      "Iteration 85, loss = 0.42769499\n",
      "Iteration 86, loss = 0.42947946\n",
      "Iteration 87, loss = 0.42851349\n",
      "Iteration 88, loss = 0.42765687\n",
      "Iteration 89, loss = 0.42650562\n",
      "Iteration 90, loss = 0.42750449\n",
      "Iteration 91, loss = 0.42745899\n",
      "Iteration 92, loss = 0.42898721\n",
      "Iteration 93, loss = 0.42705058\n",
      "Iteration 94, loss = 0.42783130\n",
      "Iteration 95, loss = 0.42762325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54331296\n",
      "Iteration 2, loss = 0.50796723\n",
      "Iteration 3, loss = 0.48820881\n",
      "Iteration 4, loss = 0.47540408\n",
      "Iteration 5, loss = 0.46906641\n",
      "Iteration 6, loss = 0.46664982\n",
      "Iteration 7, loss = 0.46460061\n",
      "Iteration 8, loss = 0.45957641\n",
      "Iteration 9, loss = 0.45755734\n",
      "Iteration 10, loss = 0.45427911\n",
      "Iteration 11, loss = 0.45669510\n",
      "Iteration 12, loss = 0.45355893\n",
      "Iteration 13, loss = 0.45285911\n",
      "Iteration 14, loss = 0.45033193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.44973586\n",
      "Iteration 16, loss = 0.44859796\n",
      "Iteration 17, loss = 0.44884189\n",
      "Iteration 18, loss = 0.44770454\n",
      "Iteration 19, loss = 0.44712782\n",
      "Iteration 20, loss = 0.44567767\n",
      "Iteration 21, loss = 0.44773572\n",
      "Iteration 22, loss = 0.44520211\n",
      "Iteration 23, loss = 0.44523634\n",
      "Iteration 24, loss = 0.44649044\n",
      "Iteration 25, loss = 0.44715242\n",
      "Iteration 26, loss = 0.44793116\n",
      "Iteration 27, loss = 0.44745987\n",
      "Iteration 28, loss = 0.44666747\n",
      "Iteration 29, loss = 0.44481051\n",
      "Iteration 30, loss = 0.44385577\n",
      "Iteration 31, loss = 0.44075065\n",
      "Iteration 32, loss = 0.44200687\n",
      "Iteration 33, loss = 0.44012552\n",
      "Iteration 34, loss = 0.43761931\n",
      "Iteration 35, loss = 0.43938785\n",
      "Iteration 36, loss = 0.43699955\n",
      "Iteration 37, loss = 0.43716744\n",
      "Iteration 38, loss = 0.43566759\n",
      "Iteration 39, loss = 0.43674049\n",
      "Iteration 40, loss = 0.43748604\n",
      "Iteration 41, loss = 0.43816114\n",
      "Iteration 42, loss = 0.43698659\n",
      "Iteration 43, loss = 0.43704200\n",
      "Iteration 44, loss = 0.43742126\n",
      "Iteration 45, loss = 0.43766990\n",
      "Iteration 46, loss = 0.43933378\n",
      "Iteration 47, loss = 0.43620621\n",
      "Iteration 48, loss = 0.43853821\n",
      "Iteration 49, loss = 0.43840316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55898575\n",
      "Iteration 2, loss = 0.52861380\n",
      "Iteration 3, loss = 0.51511312\n",
      "Iteration 4, loss = 0.51057472\n",
      "Iteration 5, loss = 0.50553934\n",
      "Iteration 6, loss = 0.50359390\n",
      "Iteration 7, loss = 0.50112691\n",
      "Iteration 8, loss = 0.50380418\n",
      "Iteration 9, loss = 0.50042663\n",
      "Iteration 10, loss = 0.49852230\n",
      "Iteration 11, loss = 0.49714245\n",
      "Iteration 12, loss = 0.49510713\n",
      "Iteration 13, loss = 0.49414917\n",
      "Iteration 14, loss = 0.49438074\n",
      "Iteration 15, loss = 0.49240538\n",
      "Iteration 16, loss = 0.49175258\n",
      "Iteration 17, loss = 0.49331985\n",
      "Iteration 18, loss = 0.49104657\n",
      "Iteration 19, loss = 0.48869436\n",
      "Iteration 20, loss = 0.48862385\n",
      "Iteration 21, loss = 0.48785715\n",
      "Iteration 22, loss = 0.48816785\n",
      "Iteration 23, loss = 0.48680939\n",
      "Iteration 24, loss = 0.48711522\n",
      "Iteration 25, loss = 0.48502850\n",
      "Iteration 26, loss = 0.48601097\n",
      "Iteration 27, loss = 0.48578186\n",
      "Iteration 28, loss = 0.48958323\n",
      "Iteration 29, loss = 0.48446066\n",
      "Iteration 30, loss = 0.48415069\n",
      "Iteration 31, loss = 0.48597256\n",
      "Iteration 32, loss = 0.48334485\n",
      "Iteration 33, loss = 0.48247274\n",
      "Iteration 34, loss = 0.48460041\n",
      "Iteration 35, loss = 0.48284440\n",
      "Iteration 36, loss = 0.48483789\n",
      "Iteration 37, loss = 0.48950300\n",
      "Iteration 38, loss = 0.48569266\n",
      "Iteration 39, loss = 0.48361178\n",
      "Iteration 40, loss = 0.48246041\n",
      "Iteration 41, loss = 0.48113854\n",
      "Iteration 42, loss = 0.48435697\n",
      "Iteration 43, loss = 0.48376597\n",
      "Iteration 44, loss = 0.48058035\n",
      "Iteration 45, loss = 0.48245012\n",
      "Iteration 46, loss = 0.48200967\n",
      "Iteration 47, loss = 0.48084031\n",
      "Iteration 48, loss = 0.48352033\n",
      "Iteration 49, loss = 0.48157371\n",
      "Iteration 50, loss = 0.47992236\n",
      "Iteration 51, loss = 0.48186319\n",
      "Iteration 52, loss = 0.48025402\n",
      "Iteration 53, loss = 0.48039059\n",
      "Iteration 54, loss = 0.48162853\n",
      "Iteration 55, loss = 0.48229564\n",
      "Iteration 56, loss = 0.48007741\n",
      "Iteration 57, loss = 0.48462519\n",
      "Iteration 58, loss = 0.48029236\n",
      "Iteration 59, loss = 0.47856157\n",
      "Iteration 60, loss = 0.48123464\n",
      "Iteration 61, loss = 0.48013474\n",
      "Iteration 62, loss = 0.48044530\n",
      "Iteration 63, loss = 0.47896854\n",
      "Iteration 64, loss = 0.47885573\n",
      "Iteration 65, loss = 0.48020904\n",
      "Iteration 66, loss = 0.47926871\n",
      "Iteration 67, loss = 0.48125656\n",
      "Iteration 68, loss = 0.47952104\n",
      "Iteration 69, loss = 0.47880402\n",
      "Iteration 70, loss = 0.47853723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55986960\n",
      "Iteration 2, loss = 0.52553336\n",
      "Iteration 3, loss = 0.50855902\n",
      "Iteration 4, loss = 0.50382018\n",
      "Iteration 5, loss = 0.50085340\n",
      "Iteration 6, loss = 0.49649676\n",
      "Iteration 7, loss = 0.49430475\n",
      "Iteration 8, loss = 0.49106905\n",
      "Iteration 9, loss = 0.48975387\n",
      "Iteration 10, loss = 0.48750401\n",
      "Iteration 11, loss = 0.48896028\n",
      "Iteration 12, loss = 0.48519458\n",
      "Iteration 13, loss = 0.48523690\n",
      "Iteration 14, loss = 0.48319652\n",
      "Iteration 15, loss = 0.48477570\n",
      "Iteration 16, loss = 0.48428134\n",
      "Iteration 17, loss = 0.48382285\n",
      "Iteration 18, loss = 0.48492914\n",
      "Iteration 19, loss = 0.48367695\n",
      "Iteration 20, loss = 0.48354640\n",
      "Iteration 21, loss = 0.48244181\n",
      "Iteration 22, loss = 0.48633348\n",
      "Iteration 23, loss = 0.48180561\n",
      "Iteration 24, loss = 0.48124522\n",
      "Iteration 25, loss = 0.48314766\n",
      "Iteration 26, loss = 0.48518827\n",
      "Iteration 27, loss = 0.48401310\n",
      "Iteration 28, loss = 0.48302390\n",
      "Iteration 29, loss = 0.48174025\n",
      "Iteration 30, loss = 0.47884084\n",
      "Iteration 31, loss = 0.48344120\n",
      "Iteration 32, loss = 0.48343672\n",
      "Iteration 33, loss = 0.47885650\n",
      "Iteration 34, loss = 0.47993453\n",
      "Iteration 35, loss = 0.48331881\n",
      "Iteration 36, loss = 0.48554926\n",
      "Iteration 37, loss = 0.48242898\n",
      "Iteration 38, loss = 0.47808485\n",
      "Iteration 39, loss = 0.47934683\n",
      "Iteration 40, loss = 0.48006411\n",
      "Iteration 41, loss = 0.47867447\n",
      "Iteration 42, loss = 0.47678227\n",
      "Iteration 43, loss = 0.47699047\n",
      "Iteration 44, loss = 0.47722546\n",
      "Iteration 45, loss = 0.47853042\n",
      "Iteration 46, loss = 0.47474026\n",
      "Iteration 47, loss = 0.47597437\n",
      "Iteration 48, loss = 0.47543640\n",
      "Iteration 49, loss = 0.47561782\n",
      "Iteration 50, loss = 0.47463634\n",
      "Iteration 51, loss = 0.47447065\n",
      "Iteration 52, loss = 0.47382471\n",
      "Iteration 53, loss = 0.47694434\n",
      "Iteration 54, loss = 0.47648935\n",
      "Iteration 55, loss = 0.47279455\n",
      "Iteration 56, loss = 0.47443173\n",
      "Iteration 57, loss = 0.47435259\n",
      "Iteration 58, loss = 0.47336880\n",
      "Iteration 59, loss = 0.47303376\n",
      "Iteration 60, loss = 0.47433634\n",
      "Iteration 61, loss = 0.47470087\n",
      "Iteration 62, loss = 0.47319649\n",
      "Iteration 63, loss = 0.47477108\n",
      "Iteration 64, loss = 0.47603818\n",
      "Iteration 65, loss = 0.47441287\n",
      "Iteration 66, loss = 0.47388740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55403638\n",
      "Iteration 2, loss = 0.52656227\n",
      "Iteration 3, loss = 0.51209559\n",
      "Iteration 4, loss = 0.50428949\n",
      "Iteration 5, loss = 0.50006256\n",
      "Iteration 6, loss = 0.49858188\n",
      "Iteration 7, loss = 0.49716033\n",
      "Iteration 8, loss = 0.49382657\n",
      "Iteration 9, loss = 0.49612179\n",
      "Iteration 10, loss = 0.49237240\n",
      "Iteration 11, loss = 0.49295662\n",
      "Iteration 12, loss = 0.49008446\n",
      "Iteration 13, loss = 0.48985193\n",
      "Iteration 14, loss = 0.48944517\n",
      "Iteration 15, loss = 0.48834444\n",
      "Iteration 16, loss = 0.48881110\n",
      "Iteration 17, loss = 0.48848830\n",
      "Iteration 18, loss = 0.48800817\n",
      "Iteration 19, loss = 0.48908222\n",
      "Iteration 20, loss = 0.48767482\n",
      "Iteration 21, loss = 0.48893679\n",
      "Iteration 22, loss = 0.48727370\n",
      "Iteration 23, loss = 0.48767646\n",
      "Iteration 24, loss = 0.48672553\n",
      "Iteration 25, loss = 0.48773103\n",
      "Iteration 26, loss = 0.48784547\n",
      "Iteration 27, loss = 0.48619016\n",
      "Iteration 28, loss = 0.48935921\n",
      "Iteration 29, loss = 0.48854949\n",
      "Iteration 30, loss = 0.48560586\n",
      "Iteration 31, loss = 0.48617543\n",
      "Iteration 32, loss = 0.48585556\n",
      "Iteration 33, loss = 0.48463596\n",
      "Iteration 34, loss = 0.48746228\n",
      "Iteration 35, loss = 0.48617329\n",
      "Iteration 36, loss = 0.48575305\n",
      "Iteration 37, loss = 0.48568066\n",
      "Iteration 38, loss = 0.48461385\n",
      "Iteration 39, loss = 0.48469405\n",
      "Iteration 40, loss = 0.48550206\n",
      "Iteration 41, loss = 0.48653181\n",
      "Iteration 42, loss = 0.48515778\n",
      "Iteration 43, loss = 0.48589166\n",
      "Iteration 44, loss = 0.48673104\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53251933\n",
      "Iteration 2, loss = 0.49735330\n",
      "Iteration 3, loss = 0.48829348\n",
      "Iteration 4, loss = 0.48454812\n",
      "Iteration 5, loss = 0.48155802\n",
      "Iteration 6, loss = 0.48038793\n",
      "Iteration 7, loss = 0.47906765\n",
      "Iteration 8, loss = 0.47695618\n",
      "Iteration 9, loss = 0.47551042\n",
      "Iteration 10, loss = 0.47348446\n",
      "Iteration 11, loss = 0.47225083\n",
      "Iteration 12, loss = 0.47123467\n",
      "Iteration 13, loss = 0.47218935\n",
      "Iteration 14, loss = 0.47133539\n",
      "Iteration 15, loss = 0.47002507\n",
      "Iteration 16, loss = 0.46997805\n",
      "Iteration 17, loss = 0.46631724\n",
      "Iteration 18, loss = 0.46732584\n",
      "Iteration 19, loss = 0.46490591\n",
      "Iteration 20, loss = 0.46571338\n",
      "Iteration 21, loss = 0.46474235\n",
      "Iteration 22, loss = 0.46332706\n",
      "Iteration 23, loss = 0.46316803\n",
      "Iteration 24, loss = 0.46408385\n",
      "Iteration 25, loss = 0.46350516\n",
      "Iteration 26, loss = 0.46233064\n",
      "Iteration 27, loss = 0.46301198\n",
      "Iteration 28, loss = 0.46471736\n",
      "Iteration 29, loss = 0.46118869\n",
      "Iteration 30, loss = 0.46275340\n",
      "Iteration 31, loss = 0.46339617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.46127220\n",
      "Iteration 33, loss = 0.45927461\n",
      "Iteration 34, loss = 0.46053517\n",
      "Iteration 35, loss = 0.46039788\n",
      "Iteration 36, loss = 0.46033144\n",
      "Iteration 37, loss = 0.45942016\n",
      "Iteration 38, loss = 0.46153078\n",
      "Iteration 39, loss = 0.46060387\n",
      "Iteration 40, loss = 0.45892212\n",
      "Iteration 41, loss = 0.45796901\n",
      "Iteration 42, loss = 0.46053417\n",
      "Iteration 43, loss = 0.45802476\n",
      "Iteration 44, loss = 0.45775453\n",
      "Iteration 45, loss = 0.45865214\n",
      "Iteration 46, loss = 0.45714744\n",
      "Iteration 47, loss = 0.45779514\n",
      "Iteration 48, loss = 0.45801634\n",
      "Iteration 49, loss = 0.45720213\n",
      "Iteration 50, loss = 0.45802074\n",
      "Iteration 51, loss = 0.45758889\n",
      "Iteration 52, loss = 0.45594410\n",
      "Iteration 53, loss = 0.45642475\n",
      "Iteration 54, loss = 0.45777827\n",
      "Iteration 55, loss = 0.45742687\n",
      "Iteration 56, loss = 0.45678100\n",
      "Iteration 57, loss = 0.45724120\n",
      "Iteration 58, loss = 0.45646034\n",
      "Iteration 59, loss = 0.45629560\n",
      "Iteration 60, loss = 0.45717299\n",
      "Iteration 61, loss = 0.45730279\n",
      "Iteration 62, loss = 0.45761337\n",
      "Iteration 63, loss = 0.45561368\n",
      "Iteration 64, loss = 0.45543382\n",
      "Iteration 65, loss = 0.45731913\n",
      "Iteration 66, loss = 0.45584154\n",
      "Iteration 67, loss = 0.45753254\n",
      "Iteration 68, loss = 0.45551732\n",
      "Iteration 69, loss = 0.45470220\n",
      "Iteration 70, loss = 0.45651934\n",
      "Iteration 71, loss = 0.45631345\n",
      "Iteration 72, loss = 0.45833523\n",
      "Iteration 73, loss = 0.45615978\n",
      "Iteration 74, loss = 0.45549234\n",
      "Iteration 75, loss = 0.45751194\n",
      "Iteration 76, loss = 0.45539666\n",
      "Iteration 77, loss = 0.45504814\n",
      "Iteration 78, loss = 0.45629878\n",
      "Iteration 79, loss = 0.45806192\n",
      "Iteration 80, loss = 0.45597539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.53312470\n",
      "Iteration 2, loss = 0.49585165\n",
      "Iteration 3, loss = 0.48581498\n",
      "Iteration 4, loss = 0.48388378\n",
      "Iteration 5, loss = 0.47916278\n",
      "Iteration 6, loss = 0.47709338\n",
      "Iteration 7, loss = 0.47632499\n",
      "Iteration 8, loss = 0.47384708\n",
      "Iteration 9, loss = 0.47085629\n",
      "Iteration 10, loss = 0.47069697\n",
      "Iteration 11, loss = 0.46853596\n",
      "Iteration 12, loss = 0.46649503\n",
      "Iteration 13, loss = 0.46791760\n",
      "Iteration 14, loss = 0.46435214\n",
      "Iteration 15, loss = 0.46540913\n",
      "Iteration 16, loss = 0.46466137\n",
      "Iteration 17, loss = 0.46420747\n",
      "Iteration 18, loss = 0.46252410\n",
      "Iteration 19, loss = 0.46259013\n",
      "Iteration 20, loss = 0.46166276\n",
      "Iteration 21, loss = 0.46151465\n",
      "Iteration 22, loss = 0.46113873\n",
      "Iteration 23, loss = 0.45906160\n",
      "Iteration 24, loss = 0.46193620\n",
      "Iteration 25, loss = 0.46000528\n",
      "Iteration 26, loss = 0.45647651\n",
      "Iteration 27, loss = 0.45719438\n",
      "Iteration 28, loss = 0.45655149\n",
      "Iteration 29, loss = 0.45664800\n",
      "Iteration 30, loss = 0.45593602\n",
      "Iteration 31, loss = 0.45354432\n",
      "Iteration 32, loss = 0.45565009\n",
      "Iteration 33, loss = 0.45653128\n",
      "Iteration 34, loss = 0.45196287\n",
      "Iteration 35, loss = 0.45491004\n",
      "Iteration 36, loss = 0.45693551\n",
      "Iteration 37, loss = 0.45433838\n",
      "Iteration 38, loss = 0.45247348\n",
      "Iteration 39, loss = 0.45437575\n",
      "Iteration 40, loss = 0.45451631\n",
      "Iteration 41, loss = 0.45213184\n",
      "Iteration 42, loss = 0.45245763\n",
      "Iteration 43, loss = 0.45290499\n",
      "Iteration 44, loss = 0.45240593\n",
      "Iteration 45, loss = 0.45262733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52775675\n",
      "Iteration 2, loss = 0.49608865\n",
      "Iteration 3, loss = 0.48813935\n",
      "Iteration 4, loss = 0.48506518\n",
      "Iteration 5, loss = 0.48221914\n",
      "Iteration 6, loss = 0.48076285\n",
      "Iteration 7, loss = 0.48115492\n",
      "Iteration 8, loss = 0.47864355\n",
      "Iteration 9, loss = 0.47814261\n",
      "Iteration 10, loss = 0.47580994\n",
      "Iteration 11, loss = 0.47482580\n",
      "Iteration 12, loss = 0.47288728\n",
      "Iteration 13, loss = 0.47272466\n",
      "Iteration 14, loss = 0.47083870\n",
      "Iteration 15, loss = 0.47164009\n",
      "Iteration 16, loss = 0.47003662\n",
      "Iteration 17, loss = 0.47090624\n",
      "Iteration 18, loss = 0.46943011\n",
      "Iteration 19, loss = 0.46867146\n",
      "Iteration 20, loss = 0.47012605\n",
      "Iteration 21, loss = 0.46872453\n",
      "Iteration 22, loss = 0.46480309\n",
      "Iteration 23, loss = 0.46419954\n",
      "Iteration 24, loss = 0.46282758\n",
      "Iteration 25, loss = 0.46208754\n",
      "Iteration 26, loss = 0.46256419\n",
      "Iteration 27, loss = 0.46290808\n",
      "Iteration 28, loss = 0.45973112\n",
      "Iteration 29, loss = 0.45737618\n",
      "Iteration 30, loss = 0.45719827\n",
      "Iteration 31, loss = 0.45662560\n",
      "Iteration 32, loss = 0.45605069\n",
      "Iteration 33, loss = 0.45639565\n",
      "Iteration 34, loss = 0.45583267\n",
      "Iteration 35, loss = 0.45486328\n",
      "Iteration 36, loss = 0.45523018\n",
      "Iteration 37, loss = 0.45345225\n",
      "Iteration 38, loss = 0.45358320\n",
      "Iteration 39, loss = 0.45413951\n",
      "Iteration 40, loss = 0.45483344\n",
      "Iteration 41, loss = 0.45393299\n",
      "Iteration 42, loss = 0.45234394\n",
      "Iteration 43, loss = 0.45217393\n",
      "Iteration 44, loss = 0.45238784\n",
      "Iteration 45, loss = 0.45145039\n",
      "Iteration 46, loss = 0.45165127\n",
      "Iteration 47, loss = 0.45173676\n",
      "Iteration 48, loss = 0.45290117\n",
      "Iteration 49, loss = 0.45131884\n",
      "Iteration 50, loss = 0.45153774\n",
      "Iteration 51, loss = 0.45136183\n",
      "Iteration 52, loss = 0.45221713\n",
      "Iteration 53, loss = 0.45243128\n",
      "Iteration 54, loss = 0.45455964\n",
      "Iteration 55, loss = 0.45186966\n",
      "Iteration 56, loss = 0.45104677\n",
      "Iteration 57, loss = 0.45314964\n",
      "Iteration 58, loss = 0.45069111\n",
      "Iteration 59, loss = 0.45137999\n",
      "Iteration 60, loss = 0.45142455\n",
      "Iteration 61, loss = 0.45584370\n",
      "Iteration 62, loss = 0.45126075\n",
      "Iteration 63, loss = 0.45115462\n",
      "Iteration 64, loss = 0.45033251\n",
      "Iteration 65, loss = 0.45066795\n",
      "Iteration 66, loss = 0.45146556\n",
      "Iteration 67, loss = 0.45027094\n",
      "Iteration 68, loss = 0.45102774\n",
      "Iteration 69, loss = 0.45026890\n",
      "Iteration 70, loss = 0.45169885\n",
      "Iteration 71, loss = 0.45123074\n",
      "Iteration 72, loss = 0.45062352\n",
      "Iteration 73, loss = 0.45278867\n",
      "Iteration 74, loss = 0.44954420\n",
      "Iteration 75, loss = 0.45043722\n",
      "Iteration 76, loss = 0.45034123\n",
      "Iteration 77, loss = 0.45205191\n",
      "Iteration 78, loss = 0.45023993\n",
      "Iteration 79, loss = 0.45123546\n",
      "Iteration 80, loss = 0.45166854\n",
      "Iteration 81, loss = 0.44997564\n",
      "Iteration 82, loss = 0.45058597\n",
      "Iteration 83, loss = 0.45181954\n",
      "Iteration 84, loss = 0.45054537\n",
      "Iteration 85, loss = 0.45040295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59145844\n",
      "Iteration 2, loss = 0.49563364\n",
      "Iteration 3, loss = 0.47704535\n",
      "Iteration 4, loss = 0.47006404\n",
      "Iteration 5, loss = 0.46369485\n",
      "Iteration 6, loss = 0.45996660\n",
      "Iteration 7, loss = 0.45653608\n",
      "Iteration 8, loss = 0.45618909\n",
      "Iteration 9, loss = 0.45185134\n",
      "Iteration 10, loss = 0.45054713\n",
      "Iteration 11, loss = 0.44922942\n",
      "Iteration 12, loss = 0.44706361\n",
      "Iteration 13, loss = 0.44690222\n",
      "Iteration 14, loss = 0.44632611\n",
      "Iteration 15, loss = 0.44169913\n",
      "Iteration 16, loss = 0.44004317\n",
      "Iteration 17, loss = 0.44082321\n",
      "Iteration 18, loss = 0.43587513\n",
      "Iteration 19, loss = 0.43471971\n",
      "Iteration 20, loss = 0.43591121\n",
      "Iteration 21, loss = 0.43341262\n",
      "Iteration 22, loss = 0.43317403\n",
      "Iteration 23, loss = 0.43223737\n",
      "Iteration 24, loss = 0.43406071\n",
      "Iteration 25, loss = 0.42936920\n",
      "Iteration 26, loss = 0.43085683\n",
      "Iteration 27, loss = 0.43001083\n",
      "Iteration 28, loss = 0.42939853\n",
      "Iteration 29, loss = 0.42714110\n",
      "Iteration 30, loss = 0.42875572\n",
      "Iteration 31, loss = 0.43000044\n",
      "Iteration 32, loss = 0.42650507\n",
      "Iteration 33, loss = 0.42459496\n",
      "Iteration 34, loss = 0.42560587\n",
      "Iteration 35, loss = 0.42769565\n",
      "Iteration 36, loss = 0.42965213\n",
      "Iteration 37, loss = 0.42605952\n",
      "Iteration 38, loss = 0.42615328\n",
      "Iteration 39, loss = 0.42703866\n",
      "Iteration 40, loss = 0.42255554\n",
      "Iteration 41, loss = 0.42217410\n",
      "Iteration 42, loss = 0.42734235\n",
      "Iteration 43, loss = 0.42186213\n",
      "Iteration 44, loss = 0.42034013\n",
      "Iteration 45, loss = 0.42360798\n",
      "Iteration 46, loss = 0.42285256\n",
      "Iteration 47, loss = 0.42081592\n",
      "Iteration 48, loss = 0.42184688\n",
      "Iteration 49, loss = 0.41994577\n",
      "Iteration 50, loss = 0.42140045\n",
      "Iteration 51, loss = 0.42061943\n",
      "Iteration 52, loss = 0.42200961\n",
      "Iteration 53, loss = 0.41875832\n",
      "Iteration 54, loss = 0.42063803\n",
      "Iteration 55, loss = 0.42149346\n",
      "Iteration 56, loss = 0.41934939\n",
      "Iteration 57, loss = 0.42291978\n",
      "Iteration 58, loss = 0.42037299\n",
      "Iteration 59, loss = 0.41824622\n",
      "Iteration 60, loss = 0.42013990\n",
      "Iteration 61, loss = 0.41996109\n",
      "Iteration 62, loss = 0.41786083\n",
      "Iteration 63, loss = 0.41806586\n",
      "Iteration 64, loss = 0.41755285\n",
      "Iteration 65, loss = 0.41930648\n",
      "Iteration 66, loss = 0.41908385\n",
      "Iteration 67, loss = 0.41930921\n",
      "Iteration 68, loss = 0.41914141\n",
      "Iteration 69, loss = 0.41891147\n",
      "Iteration 70, loss = 0.41860696\n",
      "Iteration 71, loss = 0.41904767\n",
      "Iteration 72, loss = 0.41859630\n",
      "Iteration 73, loss = 0.41842166\n",
      "Iteration 74, loss = 0.41972349\n",
      "Iteration 75, loss = 0.41806407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59212599\n",
      "Iteration 2, loss = 0.49019043\n",
      "Iteration 3, loss = 0.47371112\n",
      "Iteration 4, loss = 0.46336886\n",
      "Iteration 5, loss = 0.45841684\n",
      "Iteration 6, loss = 0.45573169\n",
      "Iteration 7, loss = 0.45504167\n",
      "Iteration 8, loss = 0.45093480\n",
      "Iteration 9, loss = 0.44905758\n",
      "Iteration 10, loss = 0.44895972\n",
      "Iteration 11, loss = 0.44965190\n",
      "Iteration 12, loss = 0.44563050\n",
      "Iteration 13, loss = 0.44641736\n",
      "Iteration 14, loss = 0.44502540\n",
      "Iteration 15, loss = 0.44343546\n",
      "Iteration 16, loss = 0.44067731\n",
      "Iteration 17, loss = 0.44309016\n",
      "Iteration 18, loss = 0.44075459\n",
      "Iteration 19, loss = 0.43964466\n",
      "Iteration 20, loss = 0.43911529\n",
      "Iteration 21, loss = 0.43812463\n",
      "Iteration 22, loss = 0.43712981\n",
      "Iteration 23, loss = 0.43616442\n",
      "Iteration 24, loss = 0.43653688\n",
      "Iteration 25, loss = 0.43278351\n",
      "Iteration 26, loss = 0.43505096\n",
      "Iteration 27, loss = 0.43185494\n",
      "Iteration 28, loss = 0.43108573\n",
      "Iteration 29, loss = 0.43150690\n",
      "Iteration 30, loss = 0.43244788\n",
      "Iteration 31, loss = 0.43055918\n",
      "Iteration 32, loss = 0.42963951\n",
      "Iteration 33, loss = 0.42853250\n",
      "Iteration 34, loss = 0.42958092\n",
      "Iteration 35, loss = 0.43132182\n",
      "Iteration 36, loss = 0.43356377\n",
      "Iteration 37, loss = 0.42949511\n",
      "Iteration 38, loss = 0.42832525\n",
      "Iteration 39, loss = 0.42784659\n",
      "Iteration 40, loss = 0.42727411\n",
      "Iteration 41, loss = 0.42501880\n",
      "Iteration 42, loss = 0.42508995\n",
      "Iteration 43, loss = 0.42799805\n",
      "Iteration 44, loss = 0.42559728\n",
      "Iteration 45, loss = 0.42538678\n",
      "Iteration 46, loss = 0.42457559\n",
      "Iteration 47, loss = 0.42628496\n",
      "Iteration 48, loss = 0.42971290\n",
      "Iteration 49, loss = 0.43072898\n",
      "Iteration 50, loss = 0.42663283\n",
      "Iteration 51, loss = 0.42561605\n",
      "Iteration 52, loss = 0.42752235\n",
      "Iteration 53, loss = 0.42508848\n",
      "Iteration 54, loss = 0.42278838\n",
      "Iteration 55, loss = 0.42169908\n",
      "Iteration 56, loss = 0.42200017\n",
      "Iteration 57, loss = 0.42615552\n",
      "Iteration 58, loss = 0.42456807\n",
      "Iteration 59, loss = 0.42585256\n",
      "Iteration 60, loss = 0.42210923\n",
      "Iteration 61, loss = 0.42677729\n",
      "Iteration 62, loss = 0.42736807\n",
      "Iteration 63, loss = 0.42674938\n",
      "Iteration 64, loss = 0.42675246\n",
      "Iteration 65, loss = 0.42771814\n",
      "Iteration 66, loss = 0.42783186\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59029079\n",
      "Iteration 2, loss = 0.49337032\n",
      "Iteration 3, loss = 0.47526470\n",
      "Iteration 4, loss = 0.46531178\n",
      "Iteration 5, loss = 0.46080300\n",
      "Iteration 6, loss = 0.46057061\n",
      "Iteration 7, loss = 0.46051031\n",
      "Iteration 8, loss = 0.45502999\n",
      "Iteration 9, loss = 0.45493366\n",
      "Iteration 10, loss = 0.45260798\n",
      "Iteration 11, loss = 0.45146584\n",
      "Iteration 12, loss = 0.44921143\n",
      "Iteration 13, loss = 0.44933779\n",
      "Iteration 14, loss = 0.44776233\n",
      "Iteration 15, loss = 0.44763303\n",
      "Iteration 16, loss = 0.44636375\n",
      "Iteration 17, loss = 0.44275881\n",
      "Iteration 18, loss = 0.44234197\n",
      "Iteration 19, loss = 0.44198127\n",
      "Iteration 20, loss = 0.43883831\n",
      "Iteration 21, loss = 0.44023992\n",
      "Iteration 22, loss = 0.43724220\n",
      "Iteration 23, loss = 0.43528419\n",
      "Iteration 24, loss = 0.43313629\n",
      "Iteration 25, loss = 0.43265565\n",
      "Iteration 26, loss = 0.43340965\n",
      "Iteration 27, loss = 0.43312505\n",
      "Iteration 28, loss = 0.43111173\n",
      "Iteration 29, loss = 0.42918723\n",
      "Iteration 30, loss = 0.43115886\n",
      "Iteration 31, loss = 0.42912240\n",
      "Iteration 32, loss = 0.42750968\n",
      "Iteration 33, loss = 0.42962951\n",
      "Iteration 34, loss = 0.42771828\n",
      "Iteration 35, loss = 0.42778402\n",
      "Iteration 36, loss = 0.42847602\n",
      "Iteration 37, loss = 0.42679493\n",
      "Iteration 38, loss = 0.42674465\n",
      "Iteration 39, loss = 0.42552579\n",
      "Iteration 40, loss = 0.42694068\n",
      "Iteration 41, loss = 0.42608403\n",
      "Iteration 42, loss = 0.42449956\n",
      "Iteration 43, loss = 0.42592284\n",
      "Iteration 44, loss = 0.42499677\n",
      "Iteration 45, loss = 0.42395511\n",
      "Iteration 46, loss = 0.42440457\n",
      "Iteration 47, loss = 0.42591146\n",
      "Iteration 48, loss = 0.42469795\n",
      "Iteration 49, loss = 0.42324016\n",
      "Iteration 50, loss = 0.42292576\n",
      "Iteration 51, loss = 0.42331856\n",
      "Iteration 52, loss = 0.42329553\n",
      "Iteration 53, loss = 0.42586530\n",
      "Iteration 54, loss = 0.42354602\n",
      "Iteration 55, loss = 0.42493473\n",
      "Iteration 56, loss = 0.42302339\n",
      "Iteration 57, loss = 0.42434488\n",
      "Iteration 58, loss = 0.42364978\n",
      "Iteration 59, loss = 0.42609143\n",
      "Iteration 60, loss = 0.42372285\n",
      "Iteration 61, loss = 0.42415652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59552466\n",
      "Iteration 2, loss = 0.49755145\n",
      "Iteration 3, loss = 0.47824229\n",
      "Iteration 4, loss = 0.47170068\n",
      "Iteration 5, loss = 0.46461863\n",
      "Iteration 6, loss = 0.46176239\n",
      "Iteration 7, loss = 0.45880336\n",
      "Iteration 8, loss = 0.45640958\n",
      "Iteration 9, loss = 0.45421363\n",
      "Iteration 10, loss = 0.45356175\n",
      "Iteration 11, loss = 0.45282071\n",
      "Iteration 12, loss = 0.45002753\n",
      "Iteration 13, loss = 0.45277361\n",
      "Iteration 14, loss = 0.45007948\n",
      "Iteration 15, loss = 0.45011716\n",
      "Iteration 16, loss = 0.44788627\n",
      "Iteration 17, loss = 0.44522281\n",
      "Iteration 18, loss = 0.44464735\n",
      "Iteration 19, loss = 0.44246650\n",
      "Iteration 20, loss = 0.44329098\n",
      "Iteration 21, loss = 0.44135430\n",
      "Iteration 22, loss = 0.44195036\n",
      "Iteration 23, loss = 0.44048728\n",
      "Iteration 24, loss = 0.44061895\n",
      "Iteration 25, loss = 0.44346903\n",
      "Iteration 26, loss = 0.43975595\n",
      "Iteration 27, loss = 0.43766689\n",
      "Iteration 28, loss = 0.43772706\n",
      "Iteration 29, loss = 0.43529049\n",
      "Iteration 30, loss = 0.43897766\n",
      "Iteration 31, loss = 0.43897778\n",
      "Iteration 32, loss = 0.43910114\n",
      "Iteration 33, loss = 0.43735438\n",
      "Iteration 34, loss = 0.43693356\n",
      "Iteration 35, loss = 0.43605774\n",
      "Iteration 36, loss = 0.43880712\n",
      "Iteration 37, loss = 0.43551771\n",
      "Iteration 38, loss = 0.43539826\n",
      "Iteration 39, loss = 0.43409527\n",
      "Iteration 40, loss = 0.43390295\n",
      "Iteration 41, loss = 0.43454129\n",
      "Iteration 42, loss = 0.43696698\n",
      "Iteration 43, loss = 0.43173869\n",
      "Iteration 44, loss = 0.43221265\n",
      "Iteration 45, loss = 0.43426562\n",
      "Iteration 46, loss = 0.43566954\n",
      "Iteration 47, loss = 0.43356662\n",
      "Iteration 48, loss = 0.43398291\n",
      "Iteration 49, loss = 0.43274852\n",
      "Iteration 50, loss = 0.43501669\n",
      "Iteration 51, loss = 0.43432060\n",
      "Iteration 52, loss = 0.43221675\n",
      "Iteration 53, loss = 0.43185649\n",
      "Iteration 54, loss = 0.43181373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59445122\n",
      "Iteration 2, loss = 0.49928086\n",
      "Iteration 3, loss = 0.47822386\n",
      "Iteration 4, loss = 0.46333260\n",
      "Iteration 5, loss = 0.45469356\n",
      "Iteration 6, loss = 0.45280854\n",
      "Iteration 7, loss = 0.45156296\n",
      "Iteration 8, loss = 0.44925868\n",
      "Iteration 9, loss = 0.44624840\n",
      "Iteration 10, loss = 0.44647484\n",
      "Iteration 11, loss = 0.44830775\n",
      "Iteration 12, loss = 0.44291839\n",
      "Iteration 13, loss = 0.44316197\n",
      "Iteration 14, loss = 0.44095447\n",
      "Iteration 15, loss = 0.44000200\n",
      "Iteration 16, loss = 0.43725943\n",
      "Iteration 17, loss = 0.43743733\n",
      "Iteration 18, loss = 0.43654938\n",
      "Iteration 19, loss = 0.43478153\n",
      "Iteration 20, loss = 0.43427559\n",
      "Iteration 21, loss = 0.43352459\n",
      "Iteration 22, loss = 0.43344622\n",
      "Iteration 23, loss = 0.43043695\n",
      "Iteration 24, loss = 0.43077864\n",
      "Iteration 25, loss = 0.42865536\n",
      "Iteration 26, loss = 0.43018344\n",
      "Iteration 27, loss = 0.42929746\n",
      "Iteration 28, loss = 0.42709421\n",
      "Iteration 29, loss = 0.42650002\n",
      "Iteration 30, loss = 0.42767031\n",
      "Iteration 31, loss = 0.42642618\n",
      "Iteration 32, loss = 0.42499832\n",
      "Iteration 33, loss = 0.42490141\n",
      "Iteration 34, loss = 0.42699452\n",
      "Iteration 35, loss = 0.42543028\n",
      "Iteration 36, loss = 0.42536494\n",
      "Iteration 37, loss = 0.42747726\n",
      "Iteration 38, loss = 0.42373395\n",
      "Iteration 39, loss = 0.42595118\n",
      "Iteration 40, loss = 0.42515829\n",
      "Iteration 41, loss = 0.42427739\n",
      "Iteration 42, loss = 0.42415345\n",
      "Iteration 43, loss = 0.42391006\n",
      "Iteration 44, loss = 0.42313779\n",
      "Iteration 45, loss = 0.42317763\n",
      "Iteration 46, loss = 0.42351334\n",
      "Iteration 47, loss = 0.42502350\n",
      "Iteration 48, loss = 0.42234460\n",
      "Iteration 49, loss = 0.42398509\n",
      "Iteration 50, loss = 0.42255671\n",
      "Iteration 51, loss = 0.42317044\n",
      "Iteration 52, loss = 0.42311380\n",
      "Iteration 53, loss = 0.42261861\n",
      "Iteration 54, loss = 0.42272964\n",
      "Iteration 55, loss = 0.42227465\n",
      "Iteration 56, loss = 0.42227502\n",
      "Iteration 57, loss = 0.42374397\n",
      "Iteration 58, loss = 0.42139135\n",
      "Iteration 59, loss = 0.42219382\n",
      "Iteration 60, loss = 0.42072832\n",
      "Iteration 61, loss = 0.42235511\n",
      "Iteration 62, loss = 0.42255279\n",
      "Iteration 63, loss = 0.42314653\n",
      "Iteration 64, loss = 0.42180212\n",
      "Iteration 65, loss = 0.42251832\n",
      "Iteration 66, loss = 0.42206709\n",
      "Iteration 67, loss = 0.42101420\n",
      "Iteration 68, loss = 0.42201917\n",
      "Iteration 69, loss = 0.42186603\n",
      "Iteration 70, loss = 0.42075171\n",
      "Iteration 71, loss = 0.42174632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59008570\n",
      "Iteration 2, loss = 0.49617074\n",
      "Iteration 3, loss = 0.47579858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.46885144\n",
      "Iteration 5, loss = 0.46365587\n",
      "Iteration 6, loss = 0.45921181\n",
      "Iteration 7, loss = 0.45894174\n",
      "Iteration 8, loss = 0.45324389\n",
      "Iteration 9, loss = 0.45400499\n",
      "Iteration 10, loss = 0.44986938\n",
      "Iteration 11, loss = 0.44972632\n",
      "Iteration 12, loss = 0.44698184\n",
      "Iteration 13, loss = 0.44803856\n",
      "Iteration 14, loss = 0.44636478\n",
      "Iteration 15, loss = 0.44467697\n",
      "Iteration 16, loss = 0.44540702\n",
      "Iteration 17, loss = 0.44353134\n",
      "Iteration 18, loss = 0.44317792\n",
      "Iteration 19, loss = 0.44219886\n",
      "Iteration 20, loss = 0.43915889\n",
      "Iteration 21, loss = 0.44269157\n",
      "Iteration 22, loss = 0.44128555\n",
      "Iteration 23, loss = 0.43856428\n",
      "Iteration 24, loss = 0.43856164\n",
      "Iteration 25, loss = 0.43865768\n",
      "Iteration 26, loss = 0.43828741\n",
      "Iteration 27, loss = 0.43526087\n",
      "Iteration 28, loss = 0.43542497\n",
      "Iteration 29, loss = 0.43312592\n",
      "Iteration 30, loss = 0.43407288\n",
      "Iteration 31, loss = 0.43473534\n",
      "Iteration 32, loss = 0.43303325\n",
      "Iteration 33, loss = 0.43208372\n",
      "Iteration 34, loss = 0.43192917\n",
      "Iteration 35, loss = 0.43423758\n",
      "Iteration 36, loss = 0.43258159\n",
      "Iteration 37, loss = 0.43083443\n",
      "Iteration 38, loss = 0.42960634\n",
      "Iteration 39, loss = 0.42802780\n",
      "Iteration 40, loss = 0.42979956\n",
      "Iteration 41, loss = 0.42824107\n",
      "Iteration 42, loss = 0.42708144\n",
      "Iteration 43, loss = 0.42877897\n",
      "Iteration 44, loss = 0.42655857\n",
      "Iteration 45, loss = 0.42657618\n",
      "Iteration 46, loss = 0.42598060\n",
      "Iteration 47, loss = 0.42734958\n",
      "Iteration 48, loss = 0.42754532\n",
      "Iteration 49, loss = 0.42511399\n",
      "Iteration 50, loss = 0.42552782\n",
      "Iteration 51, loss = 0.42486720\n",
      "Iteration 52, loss = 0.42540126\n",
      "Iteration 53, loss = 0.42929014\n",
      "Iteration 54, loss = 0.42687898\n",
      "Iteration 55, loss = 0.42594575\n",
      "Iteration 56, loss = 0.42499322\n",
      "Iteration 57, loss = 0.42786146\n",
      "Iteration 58, loss = 0.42705694\n",
      "Iteration 59, loss = 0.42637130\n",
      "Iteration 60, loss = 0.42616755\n",
      "Iteration 61, loss = 0.42456621\n",
      "Iteration 62, loss = 0.42307555\n",
      "Iteration 63, loss = 0.42621661\n",
      "Iteration 64, loss = 0.42596790\n",
      "Iteration 65, loss = 0.42347821\n",
      "Iteration 66, loss = 0.42317804\n",
      "Iteration 67, loss = 0.42781052\n",
      "Iteration 68, loss = 0.42391546\n",
      "Iteration 69, loss = 0.42566582\n",
      "Iteration 70, loss = 0.42532576\n",
      "Iteration 71, loss = 0.42382572\n",
      "Iteration 72, loss = 0.42324534\n",
      "Iteration 73, loss = 0.42471283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59175252\n",
      "Iteration 2, loss = 0.49256407\n",
      "Iteration 3, loss = 0.47321265\n",
      "Iteration 4, loss = 0.46672167\n",
      "Iteration 5, loss = 0.46214647\n",
      "Iteration 6, loss = 0.45960729\n",
      "Iteration 7, loss = 0.45606486\n",
      "Iteration 8, loss = 0.45471323\n",
      "Iteration 9, loss = 0.45298555\n",
      "Iteration 10, loss = 0.45394445\n",
      "Iteration 11, loss = 0.45186384\n",
      "Iteration 12, loss = 0.45096789\n",
      "Iteration 13, loss = 0.45300320\n",
      "Iteration 14, loss = 0.45243544\n",
      "Iteration 15, loss = 0.44979838\n",
      "Iteration 16, loss = 0.44775829\n",
      "Iteration 17, loss = 0.44561234\n",
      "Iteration 18, loss = 0.44671237\n",
      "Iteration 19, loss = 0.44371920\n",
      "Iteration 20, loss = 0.44481846\n",
      "Iteration 21, loss = 0.44330189\n",
      "Iteration 22, loss = 0.44287234\n",
      "Iteration 23, loss = 0.44222149\n",
      "Iteration 24, loss = 0.44281659\n",
      "Iteration 25, loss = 0.44346578\n",
      "Iteration 26, loss = 0.44116141\n",
      "Iteration 27, loss = 0.44061939\n",
      "Iteration 28, loss = 0.43837279\n",
      "Iteration 29, loss = 0.43697928\n",
      "Iteration 30, loss = 0.43987273\n",
      "Iteration 31, loss = 0.44129304\n",
      "Iteration 32, loss = 0.43861829\n",
      "Iteration 33, loss = 0.43656919\n",
      "Iteration 34, loss = 0.43602722\n",
      "Iteration 35, loss = 0.43654514\n",
      "Iteration 36, loss = 0.44014343\n",
      "Iteration 37, loss = 0.43808518\n",
      "Iteration 38, loss = 0.43525980\n",
      "Iteration 39, loss = 0.43605783\n",
      "Iteration 40, loss = 0.43427076\n",
      "Iteration 41, loss = 0.43470545\n",
      "Iteration 42, loss = 0.43759495\n",
      "Iteration 43, loss = 0.43340032\n",
      "Iteration 44, loss = 0.43218773\n",
      "Iteration 45, loss = 0.43643725\n",
      "Iteration 46, loss = 0.43648875\n",
      "Iteration 47, loss = 0.43553209\n",
      "Iteration 48, loss = 0.43456743\n",
      "Iteration 49, loss = 0.43281243\n",
      "Iteration 50, loss = 0.43279649\n",
      "Iteration 51, loss = 0.43343657\n",
      "Iteration 52, loss = 0.43290120\n",
      "Iteration 53, loss = 0.43252782\n",
      "Iteration 54, loss = 0.43179159\n",
      "Iteration 55, loss = 0.43202204\n",
      "Iteration 56, loss = 0.43072573\n",
      "Iteration 57, loss = 0.43238909\n",
      "Iteration 58, loss = 0.43278224\n",
      "Iteration 59, loss = 0.43243563\n",
      "Iteration 60, loss = 0.43147201\n",
      "Iteration 61, loss = 0.43324761\n",
      "Iteration 62, loss = 0.43294436\n",
      "Iteration 63, loss = 0.42946692\n",
      "Iteration 64, loss = 0.43053297\n",
      "Iteration 65, loss = 0.43130287\n",
      "Iteration 66, loss = 0.43032179\n",
      "Iteration 67, loss = 0.43263245\n",
      "Iteration 68, loss = 0.43222725\n",
      "Iteration 69, loss = 0.42977071\n",
      "Iteration 70, loss = 0.43139940\n",
      "Iteration 71, loss = 0.43011258\n",
      "Iteration 72, loss = 0.43055959\n",
      "Iteration 73, loss = 0.43220061\n",
      "Iteration 74, loss = 0.43283985\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59291206\n",
      "Iteration 2, loss = 0.49309670\n",
      "Iteration 3, loss = 0.47021198\n",
      "Iteration 4, loss = 0.46047058\n",
      "Iteration 5, loss = 0.45319754\n",
      "Iteration 6, loss = 0.45271268\n",
      "Iteration 7, loss = 0.45154357\n",
      "Iteration 8, loss = 0.44901454\n",
      "Iteration 9, loss = 0.44630971\n",
      "Iteration 10, loss = 0.44621967\n",
      "Iteration 11, loss = 0.44609150\n",
      "Iteration 12, loss = 0.44315252\n",
      "Iteration 13, loss = 0.44441073\n",
      "Iteration 14, loss = 0.44300894\n",
      "Iteration 15, loss = 0.44303674\n",
      "Iteration 16, loss = 0.43982793\n",
      "Iteration 17, loss = 0.44019115\n",
      "Iteration 18, loss = 0.43974495\n",
      "Iteration 19, loss = 0.43818401\n",
      "Iteration 20, loss = 0.43715083\n",
      "Iteration 21, loss = 0.43701744\n",
      "Iteration 22, loss = 0.43602687\n",
      "Iteration 23, loss = 0.43504555\n",
      "Iteration 24, loss = 0.43474119\n",
      "Iteration 25, loss = 0.43589633\n",
      "Iteration 26, loss = 0.43331418\n",
      "Iteration 27, loss = 0.43259986\n",
      "Iteration 28, loss = 0.43018120\n",
      "Iteration 29, loss = 0.42988775\n",
      "Iteration 30, loss = 0.42985749\n",
      "Iteration 31, loss = 0.42935986\n",
      "Iteration 32, loss = 0.42776889\n",
      "Iteration 33, loss = 0.42655580\n",
      "Iteration 34, loss = 0.42776078\n",
      "Iteration 35, loss = 0.42843723\n",
      "Iteration 36, loss = 0.43203316\n",
      "Iteration 37, loss = 0.42838137\n",
      "Iteration 38, loss = 0.42620775\n",
      "Iteration 39, loss = 0.42631617\n",
      "Iteration 40, loss = 0.42448518\n",
      "Iteration 41, loss = 0.42478880\n",
      "Iteration 42, loss = 0.42537203\n",
      "Iteration 43, loss = 0.42653645\n",
      "Iteration 44, loss = 0.42373437\n",
      "Iteration 45, loss = 0.42354229\n",
      "Iteration 46, loss = 0.42415677\n",
      "Iteration 47, loss = 0.42363282\n",
      "Iteration 48, loss = 0.42362727\n",
      "Iteration 49, loss = 0.42323119\n",
      "Iteration 50, loss = 0.42365090\n",
      "Iteration 51, loss = 0.42310104\n",
      "Iteration 52, loss = 0.42221983\n",
      "Iteration 53, loss = 0.42177198\n",
      "Iteration 54, loss = 0.42247858\n",
      "Iteration 55, loss = 0.42235908\n",
      "Iteration 56, loss = 0.42162500\n",
      "Iteration 57, loss = 0.42239004\n",
      "Iteration 58, loss = 0.42051491\n",
      "Iteration 59, loss = 0.42087992\n",
      "Iteration 60, loss = 0.42177333\n",
      "Iteration 61, loss = 0.42264144\n",
      "Iteration 62, loss = 0.42015676\n",
      "Iteration 63, loss = 0.42310047\n",
      "Iteration 64, loss = 0.42148814\n",
      "Iteration 65, loss = 0.42218853\n",
      "Iteration 66, loss = 0.42074839\n",
      "Iteration 67, loss = 0.42164781\n",
      "Iteration 68, loss = 0.42122361\n",
      "Iteration 69, loss = 0.42183738\n",
      "Iteration 70, loss = 0.42117361\n",
      "Iteration 71, loss = 0.42122420\n",
      "Iteration 72, loss = 0.42231459\n",
      "Iteration 73, loss = 0.42069526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58642139\n",
      "Iteration 2, loss = 0.49255375\n",
      "Iteration 3, loss = 0.47338251\n",
      "Iteration 4, loss = 0.46762471\n",
      "Iteration 5, loss = 0.46339715\n",
      "Iteration 6, loss = 0.45899436\n",
      "Iteration 7, loss = 0.45613329\n",
      "Iteration 8, loss = 0.45165783\n",
      "Iteration 9, loss = 0.45322486\n",
      "Iteration 10, loss = 0.45078828\n",
      "Iteration 11, loss = 0.44968775\n",
      "Iteration 12, loss = 0.44775175\n",
      "Iteration 13, loss = 0.44907532\n",
      "Iteration 14, loss = 0.44670759\n",
      "Iteration 15, loss = 0.44507107\n",
      "Iteration 16, loss = 0.44750695\n",
      "Iteration 17, loss = 0.44450298\n",
      "Iteration 18, loss = 0.44423623\n",
      "Iteration 19, loss = 0.44291599\n",
      "Iteration 20, loss = 0.44129225\n",
      "Iteration 21, loss = 0.44400454\n",
      "Iteration 22, loss = 0.44195817\n",
      "Iteration 23, loss = 0.43988536\n",
      "Iteration 24, loss = 0.43861190\n",
      "Iteration 25, loss = 0.43916796\n",
      "Iteration 26, loss = 0.43831627\n",
      "Iteration 27, loss = 0.43907845\n",
      "Iteration 28, loss = 0.43875094\n",
      "Iteration 29, loss = 0.43550130\n",
      "Iteration 30, loss = 0.43662164\n",
      "Iteration 31, loss = 0.43887969\n",
      "Iteration 32, loss = 0.43422837\n",
      "Iteration 33, loss = 0.43494510\n",
      "Iteration 34, loss = 0.43409263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.43315639\n",
      "Iteration 36, loss = 0.43426757\n",
      "Iteration 37, loss = 0.43339552\n",
      "Iteration 38, loss = 0.43110491\n",
      "Iteration 39, loss = 0.42975550\n",
      "Iteration 40, loss = 0.42924936\n",
      "Iteration 41, loss = 0.43148730\n",
      "Iteration 42, loss = 0.42843233\n",
      "Iteration 43, loss = 0.42964627\n",
      "Iteration 44, loss = 0.42782142\n",
      "Iteration 45, loss = 0.42868681\n",
      "Iteration 46, loss = 0.42743426\n",
      "Iteration 47, loss = 0.42767765\n",
      "Iteration 48, loss = 0.42702404\n",
      "Iteration 49, loss = 0.42574755\n",
      "Iteration 50, loss = 0.42536890\n",
      "Iteration 51, loss = 0.42489458\n",
      "Iteration 52, loss = 0.42522310\n",
      "Iteration 53, loss = 0.42898271\n",
      "Iteration 54, loss = 0.42607644\n",
      "Iteration 55, loss = 0.42541848\n",
      "Iteration 56, loss = 0.42422084\n",
      "Iteration 57, loss = 0.42463862\n",
      "Iteration 58, loss = 0.42378257\n",
      "Iteration 59, loss = 0.42513349\n",
      "Iteration 60, loss = 0.42354949\n",
      "Iteration 61, loss = 0.42340508\n",
      "Iteration 62, loss = 0.42149371\n",
      "Iteration 63, loss = 0.42665828\n",
      "Iteration 64, loss = 0.42376031\n",
      "Iteration 65, loss = 0.42130930\n",
      "Iteration 66, loss = 0.42351043\n",
      "Iteration 67, loss = 0.42296875\n",
      "Iteration 68, loss = 0.42297225\n",
      "Iteration 69, loss = 0.42356009\n",
      "Iteration 70, loss = 0.42526003\n",
      "Iteration 71, loss = 0.42278237\n",
      "Iteration 72, loss = 0.42158968\n",
      "Iteration 73, loss = 0.42264625\n",
      "Iteration 74, loss = 0.42486804\n",
      "Iteration 75, loss = 0.42256057\n",
      "Iteration 76, loss = 0.42119670\n",
      "Iteration 77, loss = 0.42324092\n",
      "Iteration 78, loss = 0.42317931\n",
      "Iteration 79, loss = 0.42425249\n",
      "Iteration 80, loss = 0.42388859\n",
      "Iteration 81, loss = 0.42452608\n",
      "Iteration 82, loss = 0.42252197\n",
      "Iteration 83, loss = 0.42007442\n",
      "Iteration 84, loss = 0.42259899\n",
      "Iteration 85, loss = 0.42094433\n",
      "Iteration 86, loss = 0.42220089\n",
      "Iteration 87, loss = 0.42321203\n",
      "Iteration 88, loss = 0.42334974\n",
      "Iteration 89, loss = 0.42161861\n",
      "Iteration 90, loss = 0.42354457\n",
      "Iteration 91, loss = 0.42100909\n",
      "Iteration 92, loss = 0.42097287\n",
      "Iteration 93, loss = 0.42405208\n",
      "Iteration 94, loss = 0.42127266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55290450\n",
      "Iteration 2, loss = 0.50096319\n",
      "Iteration 3, loss = 0.48091784\n",
      "Iteration 4, loss = 0.47199997\n",
      "Iteration 5, loss = 0.46948647\n",
      "Iteration 6, loss = 0.46808113\n",
      "Iteration 7, loss = 0.46704476\n",
      "Iteration 8, loss = 0.46906839\n",
      "Iteration 9, loss = 0.46758408\n",
      "Iteration 10, loss = 0.46758689\n",
      "Iteration 11, loss = 0.46719161\n",
      "Iteration 12, loss = 0.46571281\n",
      "Iteration 13, loss = 0.46721115\n",
      "Iteration 14, loss = 0.46705373\n",
      "Iteration 15, loss = 0.46482592\n",
      "Iteration 16, loss = 0.46606174\n",
      "Iteration 17, loss = 0.46408774\n",
      "Iteration 18, loss = 0.46422054\n",
      "Iteration 19, loss = 0.46391273\n",
      "Iteration 20, loss = 0.46431341\n",
      "Iteration 21, loss = 0.46328240\n",
      "Iteration 22, loss = 0.46260290\n",
      "Iteration 23, loss = 0.46271424\n",
      "Iteration 24, loss = 0.46403340\n",
      "Iteration 25, loss = 0.46313240\n",
      "Iteration 26, loss = 0.46364417\n",
      "Iteration 27, loss = 0.46216955\n",
      "Iteration 28, loss = 0.46202760\n",
      "Iteration 29, loss = 0.46069555\n",
      "Iteration 30, loss = 0.46240622\n",
      "Iteration 31, loss = 0.46100661\n",
      "Iteration 32, loss = 0.45940910\n",
      "Iteration 33, loss = 0.46013731\n",
      "Iteration 34, loss = 0.45928967\n",
      "Iteration 35, loss = 0.45998645\n",
      "Iteration 36, loss = 0.45930156\n",
      "Iteration 37, loss = 0.45642545\n",
      "Iteration 38, loss = 0.45655621\n",
      "Iteration 39, loss = 0.45248570\n",
      "Iteration 40, loss = 0.44884055\n",
      "Iteration 41, loss = 0.44898133\n",
      "Iteration 42, loss = 0.44946717\n",
      "Iteration 43, loss = 0.44708623\n",
      "Iteration 44, loss = 0.44763174\n",
      "Iteration 45, loss = 0.44711845\n",
      "Iteration 46, loss = 0.44565612\n",
      "Iteration 47, loss = 0.44416339\n",
      "Iteration 48, loss = 0.44457822\n",
      "Iteration 49, loss = 0.44413586\n",
      "Iteration 50, loss = 0.44310292\n",
      "Iteration 51, loss = 0.44383448\n",
      "Iteration 52, loss = 0.44223028\n",
      "Iteration 53, loss = 0.44162934\n",
      "Iteration 54, loss = 0.44163529\n",
      "Iteration 55, loss = 0.44207536\n",
      "Iteration 56, loss = 0.44100525\n",
      "Iteration 57, loss = 0.44184482\n",
      "Iteration 58, loss = 0.44145698\n",
      "Iteration 59, loss = 0.43993627\n",
      "Iteration 60, loss = 0.43997321\n",
      "Iteration 61, loss = 0.44151042\n",
      "Iteration 62, loss = 0.43998068\n",
      "Iteration 63, loss = 0.43851277\n",
      "Iteration 64, loss = 0.43923105\n",
      "Iteration 65, loss = 0.44046053\n",
      "Iteration 66, loss = 0.43849698\n",
      "Iteration 67, loss = 0.44048700\n",
      "Iteration 68, loss = 0.43720159\n",
      "Iteration 69, loss = 0.43655990\n",
      "Iteration 70, loss = 0.43787532\n",
      "Iteration 71, loss = 0.43782835\n",
      "Iteration 72, loss = 0.43718258\n",
      "Iteration 73, loss = 0.43657417\n",
      "Iteration 74, loss = 0.43662582\n",
      "Iteration 75, loss = 0.43591730\n",
      "Iteration 76, loss = 0.43672362\n",
      "Iteration 77, loss = 0.43625206\n",
      "Iteration 78, loss = 0.43421638\n",
      "Iteration 79, loss = 0.43961580\n",
      "Iteration 80, loss = 0.43657343\n",
      "Iteration 81, loss = 0.43461950\n",
      "Iteration 82, loss = 0.43541069\n",
      "Iteration 83, loss = 0.43720127\n",
      "Iteration 84, loss = 0.43633997\n",
      "Iteration 85, loss = 0.43523037\n",
      "Iteration 86, loss = 0.43987525\n",
      "Iteration 87, loss = 0.43625961\n",
      "Iteration 88, loss = 0.43549713\n",
      "Iteration 89, loss = 0.43325813\n",
      "Iteration 90, loss = 0.43749742\n",
      "Iteration 91, loss = 0.43373809\n",
      "Iteration 92, loss = 0.43408044\n",
      "Iteration 93, loss = 0.43592799\n",
      "Iteration 94, loss = 0.43437008\n",
      "Iteration 95, loss = 0.43373920\n",
      "Iteration 96, loss = 0.43278388\n",
      "Iteration 97, loss = 0.43285156\n",
      "Iteration 98, loss = 0.43457381\n",
      "Iteration 99, loss = 0.43421907\n",
      "Iteration 100, loss = 0.43268558\n",
      "Iteration 101, loss = 0.43458697\n",
      "Iteration 102, loss = 0.43311970\n",
      "Iteration 103, loss = 0.43474180\n",
      "Iteration 104, loss = 0.43339695\n",
      "Iteration 105, loss = 0.43267831\n",
      "Iteration 106, loss = 0.43331680\n",
      "Iteration 107, loss = 0.43606419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54881459\n",
      "Iteration 2, loss = 0.49903956\n",
      "Iteration 3, loss = 0.47639438\n",
      "Iteration 4, loss = 0.47113039\n",
      "Iteration 5, loss = 0.46609621\n",
      "Iteration 6, loss = 0.46311143\n",
      "Iteration 7, loss = 0.46287987\n",
      "Iteration 8, loss = 0.46218835\n",
      "Iteration 9, loss = 0.46128819\n",
      "Iteration 10, loss = 0.46300247\n",
      "Iteration 11, loss = 0.46304056\n",
      "Iteration 12, loss = 0.46215550\n",
      "Iteration 13, loss = 0.46340457\n",
      "Iteration 14, loss = 0.46197969\n",
      "Iteration 15, loss = 0.46109461\n",
      "Iteration 16, loss = 0.45951378\n",
      "Iteration 17, loss = 0.46044016\n",
      "Iteration 18, loss = 0.45920715\n",
      "Iteration 19, loss = 0.45939819\n",
      "Iteration 20, loss = 0.45921837\n",
      "Iteration 21, loss = 0.46042602\n",
      "Iteration 22, loss = 0.45850046\n",
      "Iteration 23, loss = 0.45823314\n",
      "Iteration 24, loss = 0.45785237\n",
      "Iteration 25, loss = 0.45837806\n",
      "Iteration 26, loss = 0.45868566\n",
      "Iteration 27, loss = 0.45675197\n",
      "Iteration 28, loss = 0.45533175\n",
      "Iteration 29, loss = 0.45618203\n",
      "Iteration 30, loss = 0.45561269\n",
      "Iteration 31, loss = 0.45583516\n",
      "Iteration 32, loss = 0.45550685\n",
      "Iteration 33, loss = 0.45462015\n",
      "Iteration 34, loss = 0.45534801\n",
      "Iteration 35, loss = 0.45791358\n",
      "Iteration 36, loss = 0.45647467\n",
      "Iteration 37, loss = 0.45514050\n",
      "Iteration 38, loss = 0.45619208\n",
      "Iteration 39, loss = 0.45577218\n",
      "Iteration 40, loss = 0.45409593\n",
      "Iteration 41, loss = 0.45386231\n",
      "Iteration 42, loss = 0.45669384\n",
      "Iteration 43, loss = 0.45564093\n",
      "Iteration 44, loss = 0.45467110\n",
      "Iteration 45, loss = 0.45610927\n",
      "Iteration 46, loss = 0.45281007\n",
      "Iteration 47, loss = 0.45225913\n",
      "Iteration 48, loss = 0.45368029\n",
      "Iteration 49, loss = 0.45411927\n",
      "Iteration 50, loss = 0.45262323\n",
      "Iteration 51, loss = 0.45272396\n",
      "Iteration 52, loss = 0.45072952\n",
      "Iteration 53, loss = 0.45162589\n",
      "Iteration 54, loss = 0.44830730\n",
      "Iteration 55, loss = 0.44782112\n",
      "Iteration 56, loss = 0.44590796\n",
      "Iteration 57, loss = 0.44816442\n",
      "Iteration 58, loss = 0.44429416\n",
      "Iteration 59, loss = 0.44420867\n",
      "Iteration 60, loss = 0.44395918\n",
      "Iteration 61, loss = 0.44396667\n",
      "Iteration 62, loss = 0.44284953\n",
      "Iteration 63, loss = 0.44256791\n",
      "Iteration 64, loss = 0.44393964\n",
      "Iteration 65, loss = 0.44277787\n",
      "Iteration 66, loss = 0.44167221\n",
      "Iteration 67, loss = 0.44194667\n",
      "Iteration 68, loss = 0.43939202\n",
      "Iteration 69, loss = 0.44055033\n",
      "Iteration 70, loss = 0.44085981\n",
      "Iteration 71, loss = 0.43899407\n",
      "Iteration 72, loss = 0.43851317\n",
      "Iteration 73, loss = 0.43938315\n",
      "Iteration 74, loss = 0.43943187\n",
      "Iteration 75, loss = 0.44022528\n",
      "Iteration 76, loss = 0.43912209\n",
      "Iteration 77, loss = 0.43762628\n",
      "Iteration 78, loss = 0.43860624\n",
      "Iteration 79, loss = 0.43788272\n",
      "Iteration 80, loss = 0.44105311\n",
      "Iteration 81, loss = 0.43868013\n",
      "Iteration 82, loss = 0.43692350\n",
      "Iteration 83, loss = 0.43857991\n",
      "Iteration 84, loss = 0.43789965\n",
      "Iteration 85, loss = 0.43806288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86, loss = 0.43791688\n",
      "Iteration 87, loss = 0.43931876\n",
      "Iteration 88, loss = 0.43840512\n",
      "Iteration 89, loss = 0.43620851\n",
      "Iteration 90, loss = 0.43665682\n",
      "Iteration 91, loss = 0.43457609\n",
      "Iteration 92, loss = 0.43424801\n",
      "Iteration 93, loss = 0.43529318\n",
      "Iteration 94, loss = 0.43465235\n",
      "Iteration 95, loss = 0.43502113\n",
      "Iteration 96, loss = 0.43411256\n",
      "Iteration 97, loss = 0.43383848\n",
      "Iteration 98, loss = 0.43186933\n",
      "Iteration 99, loss = 0.43292047\n",
      "Iteration 100, loss = 0.43238396\n",
      "Iteration 101, loss = 0.43398931\n",
      "Iteration 102, loss = 0.43379429\n",
      "Iteration 103, loss = 0.43341127\n",
      "Iteration 104, loss = 0.43416254\n",
      "Iteration 105, loss = 0.43329969\n",
      "Iteration 106, loss = 0.43301361\n",
      "Iteration 107, loss = 0.43367429\n",
      "Iteration 108, loss = 0.43392024\n",
      "Iteration 109, loss = 0.43299771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55585229\n",
      "Iteration 2, loss = 0.50493148\n",
      "Iteration 3, loss = 0.48364013\n",
      "Iteration 4, loss = 0.47190932\n",
      "Iteration 5, loss = 0.46933497\n",
      "Iteration 6, loss = 0.46805755\n",
      "Iteration 7, loss = 0.46755572\n",
      "Iteration 8, loss = 0.46506532\n",
      "Iteration 9, loss = 0.46671202\n",
      "Iteration 10, loss = 0.46513318\n",
      "Iteration 11, loss = 0.46528477\n",
      "Iteration 12, loss = 0.46382651\n",
      "Iteration 13, loss = 0.46414460\n",
      "Iteration 14, loss = 0.46183742\n",
      "Iteration 15, loss = 0.46263755\n",
      "Iteration 16, loss = 0.45958657\n",
      "Iteration 17, loss = 0.45989411\n",
      "Iteration 18, loss = 0.45919576\n",
      "Iteration 19, loss = 0.45684778\n",
      "Iteration 20, loss = 0.45607541\n",
      "Iteration 21, loss = 0.45702884\n",
      "Iteration 22, loss = 0.45511370\n",
      "Iteration 23, loss = 0.45252308\n",
      "Iteration 24, loss = 0.45105311\n",
      "Iteration 25, loss = 0.45083510\n",
      "Iteration 26, loss = 0.44952505\n",
      "Iteration 27, loss = 0.44832050\n",
      "Iteration 28, loss = 0.44704497\n",
      "Iteration 29, loss = 0.44631977\n",
      "Iteration 30, loss = 0.44509278\n",
      "Iteration 31, loss = 0.44592784\n",
      "Iteration 32, loss = 0.44539497\n",
      "Iteration 33, loss = 0.44416610\n",
      "Iteration 34, loss = 0.44386632\n",
      "Iteration 35, loss = 0.44458696\n",
      "Iteration 36, loss = 0.44228613\n",
      "Iteration 37, loss = 0.44137220\n",
      "Iteration 38, loss = 0.44147927\n",
      "Iteration 39, loss = 0.44033974\n",
      "Iteration 40, loss = 0.44114809\n",
      "Iteration 41, loss = 0.44004957\n",
      "Iteration 42, loss = 0.43878949\n",
      "Iteration 43, loss = 0.44095471\n",
      "Iteration 44, loss = 0.43973433\n",
      "Iteration 45, loss = 0.44009739\n",
      "Iteration 46, loss = 0.43837843\n",
      "Iteration 47, loss = 0.43844918\n",
      "Iteration 48, loss = 0.43955707\n",
      "Iteration 49, loss = 0.43734870\n",
      "Iteration 50, loss = 0.43744110\n",
      "Iteration 51, loss = 0.43632423\n",
      "Iteration 52, loss = 0.43761415\n",
      "Iteration 53, loss = 0.43966702\n",
      "Iteration 54, loss = 0.43980541\n",
      "Iteration 55, loss = 0.43640133\n",
      "Iteration 56, loss = 0.43573497\n",
      "Iteration 57, loss = 0.43796324\n",
      "Iteration 58, loss = 0.43682022\n",
      "Iteration 59, loss = 0.43737269\n",
      "Iteration 60, loss = 0.43863629\n",
      "Iteration 61, loss = 0.43891917\n",
      "Iteration 62, loss = 0.43508608\n",
      "Iteration 63, loss = 0.43741779\n",
      "Iteration 64, loss = 0.43810092\n",
      "Iteration 65, loss = 0.43556157\n",
      "Iteration 66, loss = 0.43591482\n",
      "Iteration 67, loss = 0.43735906\n",
      "Iteration 68, loss = 0.43551119\n",
      "Iteration 69, loss = 0.43453999\n",
      "Iteration 70, loss = 0.43475085\n",
      "Iteration 71, loss = 0.43431490\n",
      "Iteration 72, loss = 0.43492218\n",
      "Iteration 73, loss = 0.43577362\n",
      "Iteration 74, loss = 0.43557797\n",
      "Iteration 75, loss = 0.43437263\n",
      "Iteration 76, loss = 0.43435097\n",
      "Iteration 77, loss = 0.43558206\n",
      "Iteration 78, loss = 0.43774596\n",
      "Iteration 79, loss = 0.43583522\n",
      "Iteration 80, loss = 0.43600216\n",
      "Iteration 81, loss = 0.43538642\n",
      "Iteration 82, loss = 0.43267946\n",
      "Iteration 83, loss = 0.43250792\n",
      "Iteration 84, loss = 0.43350602\n",
      "Iteration 85, loss = 0.43467347\n",
      "Iteration 86, loss = 0.43250654\n",
      "Iteration 87, loss = 0.43347390\n",
      "Iteration 88, loss = 0.43692638\n",
      "Iteration 89, loss = 0.43473814\n",
      "Iteration 90, loss = 0.43587945\n",
      "Iteration 91, loss = 0.43529112\n",
      "Iteration 92, loss = 0.43385214\n",
      "Iteration 93, loss = 0.43323508\n",
      "Iteration 94, loss = 0.43247410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54781972\n",
      "Iteration 2, loss = 0.49401403\n",
      "Iteration 3, loss = 0.47897250\n",
      "Iteration 4, loss = 0.47129024\n",
      "Iteration 5, loss = 0.46830288\n",
      "Iteration 6, loss = 0.46657877\n",
      "Iteration 7, loss = 0.46492419\n",
      "Iteration 8, loss = 0.46790605\n",
      "Iteration 9, loss = 0.46427052\n",
      "Iteration 10, loss = 0.46333614\n",
      "Iteration 11, loss = 0.46316642\n",
      "Iteration 12, loss = 0.46164453\n",
      "Iteration 13, loss = 0.46201409\n",
      "Iteration 14, loss = 0.46277449\n",
      "Iteration 15, loss = 0.45926215\n",
      "Iteration 16, loss = 0.46000454\n",
      "Iteration 17, loss = 0.45801050\n",
      "Iteration 18, loss = 0.45764208\n",
      "Iteration 19, loss = 0.45615588\n",
      "Iteration 20, loss = 0.45613798\n",
      "Iteration 21, loss = 0.45392958\n",
      "Iteration 22, loss = 0.45225939\n",
      "Iteration 23, loss = 0.45091414\n",
      "Iteration 24, loss = 0.45168022\n",
      "Iteration 25, loss = 0.45084830\n",
      "Iteration 26, loss = 0.44865924\n",
      "Iteration 27, loss = 0.44789290\n",
      "Iteration 28, loss = 0.44849431\n",
      "Iteration 29, loss = 0.44707486\n",
      "Iteration 30, loss = 0.44825942\n",
      "Iteration 31, loss = 0.45028510\n",
      "Iteration 32, loss = 0.44540691\n",
      "Iteration 33, loss = 0.44560028\n",
      "Iteration 34, loss = 0.44429001\n",
      "Iteration 35, loss = 0.44660939\n",
      "Iteration 36, loss = 0.44640977\n",
      "Iteration 37, loss = 0.44380476\n",
      "Iteration 38, loss = 0.44427840\n",
      "Iteration 39, loss = 0.44391923\n",
      "Iteration 40, loss = 0.43956984\n",
      "Iteration 41, loss = 0.43993957\n",
      "Iteration 42, loss = 0.44155821\n",
      "Iteration 43, loss = 0.43879008\n",
      "Iteration 44, loss = 0.43834349\n",
      "Iteration 45, loss = 0.44094598\n",
      "Iteration 46, loss = 0.43995578\n",
      "Iteration 47, loss = 0.43723159\n",
      "Iteration 48, loss = 0.43786167\n",
      "Iteration 49, loss = 0.43789026\n",
      "Iteration 50, loss = 0.43599474\n",
      "Iteration 51, loss = 0.43714556\n",
      "Iteration 52, loss = 0.43629109\n",
      "Iteration 53, loss = 0.43793168\n",
      "Iteration 54, loss = 0.43563358\n",
      "Iteration 55, loss = 0.43664532\n",
      "Iteration 56, loss = 0.43549554\n",
      "Iteration 57, loss = 0.43499941\n",
      "Iteration 58, loss = 0.43589793\n",
      "Iteration 59, loss = 0.43366915\n",
      "Iteration 60, loss = 0.43397492\n",
      "Iteration 61, loss = 0.43748651\n",
      "Iteration 62, loss = 0.43450777\n",
      "Iteration 63, loss = 0.43336695\n",
      "Iteration 64, loss = 0.43416765\n",
      "Iteration 65, loss = 0.43533518\n",
      "Iteration 66, loss = 0.43402734\n",
      "Iteration 67, loss = 0.43803949\n",
      "Iteration 68, loss = 0.43365224\n",
      "Iteration 69, loss = 0.43203478\n",
      "Iteration 70, loss = 0.43420460\n",
      "Iteration 71, loss = 0.43424086\n",
      "Iteration 72, loss = 0.43293057\n",
      "Iteration 73, loss = 0.43268682\n",
      "Iteration 74, loss = 0.43427384\n",
      "Iteration 75, loss = 0.43249872\n",
      "Iteration 76, loss = 0.43348105\n",
      "Iteration 77, loss = 0.43415232\n",
      "Iteration 78, loss = 0.43235044\n",
      "Iteration 79, loss = 0.43426446\n",
      "Iteration 80, loss = 0.43174301\n",
      "Iteration 81, loss = 0.43265967\n",
      "Iteration 82, loss = 0.43154386\n",
      "Iteration 83, loss = 0.43456288\n",
      "Iteration 84, loss = 0.43218970\n",
      "Iteration 85, loss = 0.43068110\n",
      "Iteration 86, loss = 0.43521097\n",
      "Iteration 87, loss = 0.43103055\n",
      "Iteration 88, loss = 0.43320536\n",
      "Iteration 89, loss = 0.42966381\n",
      "Iteration 90, loss = 0.43374672\n",
      "Iteration 91, loss = 0.43004222\n",
      "Iteration 92, loss = 0.43220611\n",
      "Iteration 93, loss = 0.43418194\n",
      "Iteration 94, loss = 0.43287188\n",
      "Iteration 95, loss = 0.43065014\n",
      "Iteration 96, loss = 0.43049264\n",
      "Iteration 97, loss = 0.43037786\n",
      "Iteration 98, loss = 0.43246610\n",
      "Iteration 99, loss = 0.43054383\n",
      "Iteration 100, loss = 0.42892938\n",
      "Iteration 101, loss = 0.43090056\n",
      "Iteration 102, loss = 0.43010185\n",
      "Iteration 103, loss = 0.43131843\n",
      "Iteration 104, loss = 0.42954961\n",
      "Iteration 105, loss = 0.42828455\n",
      "Iteration 106, loss = 0.43039414\n",
      "Iteration 107, loss = 0.43119637\n",
      "Iteration 108, loss = 0.43445822\n",
      "Iteration 109, loss = 0.43115722\n",
      "Iteration 110, loss = 0.42898490\n",
      "Iteration 111, loss = 0.42965862\n",
      "Iteration 112, loss = 0.42958112\n",
      "Iteration 113, loss = 0.43193537\n",
      "Iteration 114, loss = 0.42867787\n",
      "Iteration 115, loss = 0.43021223\n",
      "Iteration 116, loss = 0.42828220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54391100\n",
      "Iteration 2, loss = 0.49240125\n",
      "Iteration 3, loss = 0.47313936\n",
      "Iteration 4, loss = 0.46791191\n",
      "Iteration 5, loss = 0.46523209\n",
      "Iteration 6, loss = 0.46315898\n",
      "Iteration 7, loss = 0.46245244\n",
      "Iteration 8, loss = 0.46189474\n",
      "Iteration 9, loss = 0.46117571\n",
      "Iteration 10, loss = 0.46302380\n",
      "Iteration 11, loss = 0.46176456\n",
      "Iteration 12, loss = 0.46085799\n",
      "Iteration 13, loss = 0.46232883\n",
      "Iteration 14, loss = 0.46169778\n",
      "Iteration 15, loss = 0.45898501\n",
      "Iteration 16, loss = 0.45775171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.45853608\n",
      "Iteration 18, loss = 0.45627602\n",
      "Iteration 19, loss = 0.45557283\n",
      "Iteration 20, loss = 0.45547907\n",
      "Iteration 21, loss = 0.45653574\n",
      "Iteration 22, loss = 0.45677149\n",
      "Iteration 23, loss = 0.45489952\n",
      "Iteration 24, loss = 0.45428806\n",
      "Iteration 25, loss = 0.45416817\n",
      "Iteration 26, loss = 0.45481910\n",
      "Iteration 27, loss = 0.45054778\n",
      "Iteration 28, loss = 0.44864927\n",
      "Iteration 29, loss = 0.44825313\n",
      "Iteration 30, loss = 0.44680246\n",
      "Iteration 31, loss = 0.44665209\n",
      "Iteration 32, loss = 0.44558848\n",
      "Iteration 33, loss = 0.44447601\n",
      "Iteration 34, loss = 0.44449686\n",
      "Iteration 35, loss = 0.44745563\n",
      "Iteration 36, loss = 0.44464761\n",
      "Iteration 37, loss = 0.44253076\n",
      "Iteration 38, loss = 0.44224576\n",
      "Iteration 39, loss = 0.44416086\n",
      "Iteration 40, loss = 0.44071086\n",
      "Iteration 41, loss = 0.44001079\n",
      "Iteration 42, loss = 0.44061628\n",
      "Iteration 43, loss = 0.43915909\n",
      "Iteration 44, loss = 0.43931193\n",
      "Iteration 45, loss = 0.43791148\n",
      "Iteration 46, loss = 0.43632922\n",
      "Iteration 47, loss = 0.43745903\n",
      "Iteration 48, loss = 0.44009683\n",
      "Iteration 49, loss = 0.43881180\n",
      "Iteration 50, loss = 0.43754926\n",
      "Iteration 51, loss = 0.43662622\n",
      "Iteration 52, loss = 0.43410394\n",
      "Iteration 53, loss = 0.43903863\n",
      "Iteration 54, loss = 0.43515224\n",
      "Iteration 55, loss = 0.43722559\n",
      "Iteration 56, loss = 0.43515338\n",
      "Iteration 57, loss = 0.43524248\n",
      "Iteration 58, loss = 0.43262867\n",
      "Iteration 59, loss = 0.43228335\n",
      "Iteration 60, loss = 0.43284787\n",
      "Iteration 61, loss = 0.43289969\n",
      "Iteration 62, loss = 0.43247134\n",
      "Iteration 63, loss = 0.43228832\n",
      "Iteration 64, loss = 0.43421233\n",
      "Iteration 65, loss = 0.43820761\n",
      "Iteration 66, loss = 0.43548580\n",
      "Iteration 67, loss = 0.43447583\n",
      "Iteration 68, loss = 0.43184715\n",
      "Iteration 69, loss = 0.43447465\n",
      "Iteration 70, loss = 0.43227241\n",
      "Iteration 71, loss = 0.43166621\n",
      "Iteration 72, loss = 0.43141221\n",
      "Iteration 73, loss = 0.43220575\n",
      "Iteration 74, loss = 0.43247090\n",
      "Iteration 75, loss = 0.43224963\n",
      "Iteration 76, loss = 0.43284184\n",
      "Iteration 77, loss = 0.42996330\n",
      "Iteration 78, loss = 0.43279551\n",
      "Iteration 79, loss = 0.43082393\n",
      "Iteration 80, loss = 0.43620814\n",
      "Iteration 81, loss = 0.43246376\n",
      "Iteration 82, loss = 0.42933098\n",
      "Iteration 83, loss = 0.43273896\n",
      "Iteration 84, loss = 0.43095192\n",
      "Iteration 85, loss = 0.43052666\n",
      "Iteration 86, loss = 0.43010104\n",
      "Iteration 87, loss = 0.43122388\n",
      "Iteration 88, loss = 0.43156671\n",
      "Iteration 89, loss = 0.43075709\n",
      "Iteration 90, loss = 0.43502498\n",
      "Iteration 91, loss = 0.42902811\n",
      "Iteration 92, loss = 0.42987861\n",
      "Iteration 93, loss = 0.42978006\n",
      "Iteration 94, loss = 0.42976684\n",
      "Iteration 95, loss = 0.43061607\n",
      "Iteration 96, loss = 0.43111391\n",
      "Iteration 97, loss = 0.42861168\n",
      "Iteration 98, loss = 0.42725054\n",
      "Iteration 99, loss = 0.42847959\n",
      "Iteration 100, loss = 0.42900483\n",
      "Iteration 101, loss = 0.43145225\n",
      "Iteration 102, loss = 0.42970783\n",
      "Iteration 103, loss = 0.42963366\n",
      "Iteration 104, loss = 0.42990903\n",
      "Iteration 105, loss = 0.42846184\n",
      "Iteration 106, loss = 0.43187615\n",
      "Iteration 107, loss = 0.42793630\n",
      "Iteration 108, loss = 0.42945004\n",
      "Iteration 109, loss = 0.43090811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54856727\n",
      "Iteration 2, loss = 0.49294761\n",
      "Iteration 3, loss = 0.47693580\n",
      "Iteration 4, loss = 0.46991849\n",
      "Iteration 5, loss = 0.46688245\n",
      "Iteration 6, loss = 0.46563831\n",
      "Iteration 7, loss = 0.46506454\n",
      "Iteration 8, loss = 0.46161505\n",
      "Iteration 9, loss = 0.46369436\n",
      "Iteration 10, loss = 0.46112937\n",
      "Iteration 11, loss = 0.46145405\n",
      "Iteration 12, loss = 0.45942638\n",
      "Iteration 13, loss = 0.46092981\n",
      "Iteration 14, loss = 0.45854337\n",
      "Iteration 15, loss = 0.45863550\n",
      "Iteration 16, loss = 0.45802412\n",
      "Iteration 17, loss = 0.45850543\n",
      "Iteration 18, loss = 0.45594561\n",
      "Iteration 19, loss = 0.45509951\n",
      "Iteration 20, loss = 0.45319120\n",
      "Iteration 21, loss = 0.45458887\n",
      "Iteration 22, loss = 0.45749285\n",
      "Iteration 23, loss = 0.45223544\n",
      "Iteration 24, loss = 0.45189361\n",
      "Iteration 25, loss = 0.45266629\n",
      "Iteration 26, loss = 0.45107113\n",
      "Iteration 27, loss = 0.44816079\n",
      "Iteration 28, loss = 0.44605757\n",
      "Iteration 29, loss = 0.44492695\n",
      "Iteration 30, loss = 0.44514105\n",
      "Iteration 31, loss = 0.44469129\n",
      "Iteration 32, loss = 0.44226478\n",
      "Iteration 33, loss = 0.44048305\n",
      "Iteration 34, loss = 0.43925685\n",
      "Iteration 35, loss = 0.44100166\n",
      "Iteration 36, loss = 0.43887740\n",
      "Iteration 37, loss = 0.43862253\n",
      "Iteration 38, loss = 0.44001854\n",
      "Iteration 39, loss = 0.43747700\n",
      "Iteration 40, loss = 0.43746447\n",
      "Iteration 41, loss = 0.43656249\n",
      "Iteration 42, loss = 0.43590825\n",
      "Iteration 43, loss = 0.43621644\n",
      "Iteration 44, loss = 0.43660582\n",
      "Iteration 45, loss = 0.43680021\n",
      "Iteration 46, loss = 0.43528163\n",
      "Iteration 47, loss = 0.43459622\n",
      "Iteration 48, loss = 0.43585238\n",
      "Iteration 49, loss = 0.43489878\n",
      "Iteration 50, loss = 0.43467447\n",
      "Iteration 51, loss = 0.43446439\n",
      "Iteration 52, loss = 0.43420614\n",
      "Iteration 53, loss = 0.43532335\n",
      "Iteration 54, loss = 0.43540340\n",
      "Iteration 55, loss = 0.43636607\n",
      "Iteration 56, loss = 0.43361224\n",
      "Iteration 57, loss = 0.43585386\n",
      "Iteration 58, loss = 0.43522824\n",
      "Iteration 59, loss = 0.43558378\n",
      "Iteration 60, loss = 0.43658335\n",
      "Iteration 61, loss = 0.43725374\n",
      "Iteration 62, loss = 0.43376121\n",
      "Iteration 63, loss = 0.43528304\n",
      "Iteration 64, loss = 0.43537006\n",
      "Iteration 65, loss = 0.43317435\n",
      "Iteration 66, loss = 0.43491315\n",
      "Iteration 67, loss = 0.43726456\n",
      "Iteration 68, loss = 0.43459275\n",
      "Iteration 69, loss = 0.43396358\n",
      "Iteration 70, loss = 0.43477185\n",
      "Iteration 71, loss = 0.43400375\n",
      "Iteration 72, loss = 0.43317392\n",
      "Iteration 73, loss = 0.43363250\n",
      "Iteration 74, loss = 0.43450604\n",
      "Iteration 75, loss = 0.43303226\n",
      "Iteration 76, loss = 0.43346075\n",
      "Iteration 77, loss = 0.43282110\n",
      "Iteration 78, loss = 0.43532152\n",
      "Iteration 79, loss = 0.43412489\n",
      "Iteration 80, loss = 0.43422866\n",
      "Iteration 81, loss = 0.43436631\n",
      "Iteration 82, loss = 0.43230229\n",
      "Iteration 83, loss = 0.43296335\n",
      "Iteration 84, loss = 0.43468303\n",
      "Iteration 85, loss = 0.43402525\n",
      "Iteration 86, loss = 0.43241004\n",
      "Iteration 87, loss = 0.43314712\n",
      "Iteration 88, loss = 0.43329559\n",
      "Iteration 89, loss = 0.43308399\n",
      "Iteration 90, loss = 0.43588270\n",
      "Iteration 91, loss = 0.43445432\n",
      "Iteration 92, loss = 0.43304701\n",
      "Iteration 93, loss = 0.43255556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54963887\n",
      "Iteration 2, loss = 0.49390096\n",
      "Iteration 3, loss = 0.47962012\n",
      "Iteration 4, loss = 0.47284080\n",
      "Iteration 5, loss = 0.46954569\n",
      "Iteration 6, loss = 0.46759843\n",
      "Iteration 7, loss = 0.46594355\n",
      "Iteration 8, loss = 0.46822899\n",
      "Iteration 9, loss = 0.46510936\n",
      "Iteration 10, loss = 0.46439052\n",
      "Iteration 11, loss = 0.46321714\n",
      "Iteration 12, loss = 0.46107230\n",
      "Iteration 13, loss = 0.46221096\n",
      "Iteration 14, loss = 0.46001037\n",
      "Iteration 15, loss = 0.45729761\n",
      "Iteration 16, loss = 0.45687572\n",
      "Iteration 17, loss = 0.45467333\n",
      "Iteration 18, loss = 0.45420939\n",
      "Iteration 19, loss = 0.45352796\n",
      "Iteration 20, loss = 0.45279744\n",
      "Iteration 21, loss = 0.45185090\n",
      "Iteration 22, loss = 0.44998257\n",
      "Iteration 23, loss = 0.44922830\n",
      "Iteration 24, loss = 0.44995150\n",
      "Iteration 25, loss = 0.45110140\n",
      "Iteration 26, loss = 0.44879532\n",
      "Iteration 27, loss = 0.44743843\n",
      "Iteration 28, loss = 0.44808522\n",
      "Iteration 29, loss = 0.44660843\n",
      "Iteration 30, loss = 0.44874511\n",
      "Iteration 31, loss = 0.44837990\n",
      "Iteration 32, loss = 0.44579449\n",
      "Iteration 33, loss = 0.44532044\n",
      "Iteration 34, loss = 0.44543574\n",
      "Iteration 35, loss = 0.44673975\n",
      "Iteration 36, loss = 0.44618343\n",
      "Iteration 37, loss = 0.44492484\n",
      "Iteration 38, loss = 0.44730270\n",
      "Iteration 39, loss = 0.44553595\n",
      "Iteration 40, loss = 0.44246401\n",
      "Iteration 41, loss = 0.44318012\n",
      "Iteration 42, loss = 0.44467687\n",
      "Iteration 43, loss = 0.44261579\n",
      "Iteration 44, loss = 0.44254459\n",
      "Iteration 45, loss = 0.44432039\n",
      "Iteration 46, loss = 0.44289561\n",
      "Iteration 47, loss = 0.44243706\n",
      "Iteration 48, loss = 0.44288808\n",
      "Iteration 49, loss = 0.44265285\n",
      "Iteration 50, loss = 0.44168968\n",
      "Iteration 51, loss = 0.44338272\n",
      "Iteration 52, loss = 0.44191391\n",
      "Iteration 53, loss = 0.44193242\n",
      "Iteration 54, loss = 0.44247576\n",
      "Iteration 55, loss = 0.44326796\n",
      "Iteration 56, loss = 0.44093417\n",
      "Iteration 57, loss = 0.44297820\n",
      "Iteration 58, loss = 0.44184716\n",
      "Iteration 59, loss = 0.44072261\n",
      "Iteration 60, loss = 0.44251024\n",
      "Iteration 61, loss = 0.44308392\n",
      "Iteration 62, loss = 0.44130416\n",
      "Iteration 63, loss = 0.44050364\n",
      "Iteration 64, loss = 0.44135638\n",
      "Iteration 65, loss = 0.44269969\n",
      "Iteration 66, loss = 0.44116393\n",
      "Iteration 67, loss = 0.44432027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 68, loss = 0.44065242\n",
      "Iteration 69, loss = 0.44025336\n",
      "Iteration 70, loss = 0.44141059\n",
      "Iteration 71, loss = 0.44209778\n",
      "Iteration 72, loss = 0.44115869\n",
      "Iteration 73, loss = 0.44120528\n",
      "Iteration 74, loss = 0.44123998\n",
      "Iteration 75, loss = 0.44019002\n",
      "Iteration 76, loss = 0.44016731\n",
      "Iteration 77, loss = 0.43931077\n",
      "Iteration 78, loss = 0.43880000\n",
      "Iteration 79, loss = 0.44220396\n",
      "Iteration 80, loss = 0.43896060\n",
      "Iteration 81, loss = 0.43940106\n",
      "Iteration 82, loss = 0.43840976\n",
      "Iteration 83, loss = 0.43930288\n",
      "Iteration 84, loss = 0.43888050\n",
      "Iteration 85, loss = 0.43810697\n",
      "Iteration 86, loss = 0.44035999\n",
      "Iteration 87, loss = 0.43718788\n",
      "Iteration 88, loss = 0.43890500\n",
      "Iteration 89, loss = 0.43672708\n",
      "Iteration 90, loss = 0.43945783\n",
      "Iteration 91, loss = 0.43703219\n",
      "Iteration 92, loss = 0.43722290\n",
      "Iteration 93, loss = 0.43857490\n",
      "Iteration 94, loss = 0.43722816\n",
      "Iteration 95, loss = 0.43642228\n",
      "Iteration 96, loss = 0.43611383\n",
      "Iteration 97, loss = 0.43576059\n",
      "Iteration 98, loss = 0.43746426\n",
      "Iteration 99, loss = 0.43648989\n",
      "Iteration 100, loss = 0.43528508\n",
      "Iteration 101, loss = 0.43698161\n",
      "Iteration 102, loss = 0.43708928\n",
      "Iteration 103, loss = 0.43651010\n",
      "Iteration 104, loss = 0.43514147\n",
      "Iteration 105, loss = 0.43479917\n",
      "Iteration 106, loss = 0.43566116\n",
      "Iteration 107, loss = 0.43485022\n",
      "Iteration 108, loss = 0.43735159\n",
      "Iteration 109, loss = 0.43477731\n",
      "Iteration 110, loss = 0.43377489\n",
      "Iteration 111, loss = 0.43528928\n",
      "Iteration 112, loss = 0.43412826\n",
      "Iteration 113, loss = 0.43364430\n",
      "Iteration 114, loss = 0.43312443\n",
      "Iteration 115, loss = 0.43388651\n",
      "Iteration 116, loss = 0.43354610\n",
      "Iteration 117, loss = 0.43239915\n",
      "Iteration 118, loss = 0.43309959\n",
      "Iteration 119, loss = 0.43338291\n",
      "Iteration 120, loss = 0.43879058\n",
      "Iteration 121, loss = 0.43127332\n",
      "Iteration 122, loss = 0.43293812\n",
      "Iteration 123, loss = 0.43293445\n",
      "Iteration 124, loss = 0.43305653\n",
      "Iteration 125, loss = 0.43099026\n",
      "Iteration 126, loss = 0.43152306\n",
      "Iteration 127, loss = 0.43433279\n",
      "Iteration 128, loss = 0.43214688\n",
      "Iteration 129, loss = 0.43312671\n",
      "Iteration 130, loss = 0.43383553\n",
      "Iteration 131, loss = 0.43165185\n",
      "Iteration 132, loss = 0.43211327\n",
      "Iteration 133, loss = 0.43173320\n",
      "Iteration 134, loss = 0.43188900\n",
      "Iteration 135, loss = 0.43413724\n",
      "Iteration 136, loss = 0.43300519\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54714882\n",
      "Iteration 2, loss = 0.49365674\n",
      "Iteration 3, loss = 0.47515480\n",
      "Iteration 4, loss = 0.46930260\n",
      "Iteration 5, loss = 0.46400315\n",
      "Iteration 6, loss = 0.46188495\n",
      "Iteration 7, loss = 0.46107141\n",
      "Iteration 8, loss = 0.46007217\n",
      "Iteration 9, loss = 0.45903765\n",
      "Iteration 10, loss = 0.46005575\n",
      "Iteration 11, loss = 0.46033351\n",
      "Iteration 12, loss = 0.45736260\n",
      "Iteration 13, loss = 0.45985483\n",
      "Iteration 14, loss = 0.45765246\n",
      "Iteration 15, loss = 0.45512320\n",
      "Iteration 16, loss = 0.45404862\n",
      "Iteration 17, loss = 0.45460750\n",
      "Iteration 18, loss = 0.45437941\n",
      "Iteration 19, loss = 0.45260076\n",
      "Iteration 20, loss = 0.45276373\n",
      "Iteration 21, loss = 0.45402989\n",
      "Iteration 22, loss = 0.45526714\n",
      "Iteration 23, loss = 0.45267286\n",
      "Iteration 24, loss = 0.45279991\n",
      "Iteration 25, loss = 0.45233347\n",
      "Iteration 26, loss = 0.45174254\n",
      "Iteration 27, loss = 0.44821939\n",
      "Iteration 28, loss = 0.44610150\n",
      "Iteration 29, loss = 0.44608211\n",
      "Iteration 30, loss = 0.44444340\n",
      "Iteration 31, loss = 0.44413229\n",
      "Iteration 32, loss = 0.44319490\n",
      "Iteration 33, loss = 0.44245127\n",
      "Iteration 34, loss = 0.44287915\n",
      "Iteration 35, loss = 0.44650305\n",
      "Iteration 36, loss = 0.44333291\n",
      "Iteration 37, loss = 0.44140517\n",
      "Iteration 38, loss = 0.44183908\n",
      "Iteration 39, loss = 0.44275111\n",
      "Iteration 40, loss = 0.44001923\n",
      "Iteration 41, loss = 0.43858491\n",
      "Iteration 42, loss = 0.44086966\n",
      "Iteration 43, loss = 0.43941984\n",
      "Iteration 44, loss = 0.43919474\n",
      "Iteration 45, loss = 0.43905553\n",
      "Iteration 46, loss = 0.43747270\n",
      "Iteration 47, loss = 0.43678479\n",
      "Iteration 48, loss = 0.43811729\n",
      "Iteration 49, loss = 0.43860564\n",
      "Iteration 50, loss = 0.43696230\n",
      "Iteration 51, loss = 0.43737767\n",
      "Iteration 52, loss = 0.43550506\n",
      "Iteration 53, loss = 0.43841490\n",
      "Iteration 54, loss = 0.43598839\n",
      "Iteration 55, loss = 0.43645265\n",
      "Iteration 56, loss = 0.43519714\n",
      "Iteration 57, loss = 0.43636515\n",
      "Iteration 58, loss = 0.43430100\n",
      "Iteration 59, loss = 0.43367999\n",
      "Iteration 60, loss = 0.43577424\n",
      "Iteration 61, loss = 0.43549235\n",
      "Iteration 62, loss = 0.43355247\n",
      "Iteration 63, loss = 0.43542016\n",
      "Iteration 64, loss = 0.43618155\n",
      "Iteration 65, loss = 0.43648799\n",
      "Iteration 66, loss = 0.43470887\n",
      "Iteration 67, loss = 0.43518617\n",
      "Iteration 68, loss = 0.43286913\n",
      "Iteration 69, loss = 0.43450243\n",
      "Iteration 70, loss = 0.43470293\n",
      "Iteration 71, loss = 0.43355162\n",
      "Iteration 72, loss = 0.43246167\n",
      "Iteration 73, loss = 0.43447775\n",
      "Iteration 74, loss = 0.43504791\n",
      "Iteration 75, loss = 0.43432348\n",
      "Iteration 76, loss = 0.43356417\n",
      "Iteration 77, loss = 0.43181170\n",
      "Iteration 78, loss = 0.43464856\n",
      "Iteration 79, loss = 0.43352654\n",
      "Iteration 80, loss = 0.43683251\n",
      "Iteration 81, loss = 0.43368983\n",
      "Iteration 82, loss = 0.43208437\n",
      "Iteration 83, loss = 0.43414987\n",
      "Iteration 84, loss = 0.43295773\n",
      "Iteration 85, loss = 0.43412549\n",
      "Iteration 86, loss = 0.43298460\n",
      "Iteration 87, loss = 0.43369002\n",
      "Iteration 88, loss = 0.43499049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55339097\n",
      "Iteration 2, loss = 0.49455272\n",
      "Iteration 3, loss = 0.47860305\n",
      "Iteration 4, loss = 0.47159681\n",
      "Iteration 5, loss = 0.46918134\n",
      "Iteration 6, loss = 0.46654859\n",
      "Iteration 7, loss = 0.46568237\n",
      "Iteration 8, loss = 0.46308760\n",
      "Iteration 9, loss = 0.46481974\n",
      "Iteration 10, loss = 0.46174871\n",
      "Iteration 11, loss = 0.46219527\n",
      "Iteration 12, loss = 0.46065211\n",
      "Iteration 13, loss = 0.46084965\n",
      "Iteration 14, loss = 0.45905061\n",
      "Iteration 15, loss = 0.45906241\n",
      "Iteration 16, loss = 0.45698825\n",
      "Iteration 17, loss = 0.45607854\n",
      "Iteration 18, loss = 0.45497649\n",
      "Iteration 19, loss = 0.45325433\n",
      "Iteration 20, loss = 0.45169298\n",
      "Iteration 21, loss = 0.45262360\n",
      "Iteration 22, loss = 0.45378698\n",
      "Iteration 23, loss = 0.44997112\n",
      "Iteration 24, loss = 0.45071049\n",
      "Iteration 25, loss = 0.45072481\n",
      "Iteration 26, loss = 0.44945231\n",
      "Iteration 27, loss = 0.44780556\n",
      "Iteration 28, loss = 0.44682477\n",
      "Iteration 29, loss = 0.44637112\n",
      "Iteration 30, loss = 0.44519265\n",
      "Iteration 31, loss = 0.44646438\n",
      "Iteration 32, loss = 0.44502259\n",
      "Iteration 33, loss = 0.44346918\n",
      "Iteration 34, loss = 0.44298966\n",
      "Iteration 35, loss = 0.44463424\n",
      "Iteration 36, loss = 0.44304660\n",
      "Iteration 37, loss = 0.44232483\n",
      "Iteration 38, loss = 0.44268160\n",
      "Iteration 39, loss = 0.44087617\n",
      "Iteration 40, loss = 0.44253514\n",
      "Iteration 41, loss = 0.44092342\n",
      "Iteration 42, loss = 0.43978438\n",
      "Iteration 43, loss = 0.44066853\n",
      "Iteration 44, loss = 0.44085130\n",
      "Iteration 45, loss = 0.44073542\n",
      "Iteration 46, loss = 0.44061102\n",
      "Iteration 47, loss = 0.43997409\n",
      "Iteration 48, loss = 0.43993911\n",
      "Iteration 49, loss = 0.43919363\n",
      "Iteration 50, loss = 0.43938210\n",
      "Iteration 51, loss = 0.43873357\n",
      "Iteration 52, loss = 0.43936234\n",
      "Iteration 53, loss = 0.44088156\n",
      "Iteration 54, loss = 0.44032077\n",
      "Iteration 55, loss = 0.44002006\n",
      "Iteration 56, loss = 0.43809863\n",
      "Iteration 57, loss = 0.43983247\n",
      "Iteration 58, loss = 0.44088615\n",
      "Iteration 59, loss = 0.44127511\n",
      "Iteration 60, loss = 0.43959293\n",
      "Iteration 61, loss = 0.44141140\n",
      "Iteration 62, loss = 0.43699400\n",
      "Iteration 63, loss = 0.43896622\n",
      "Iteration 64, loss = 0.43890987\n",
      "Iteration 65, loss = 0.43686288\n",
      "Iteration 66, loss = 0.43785871\n",
      "Iteration 67, loss = 0.44008465\n",
      "Iteration 68, loss = 0.43810740\n",
      "Iteration 69, loss = 0.43772604\n",
      "Iteration 70, loss = 0.43754021\n",
      "Iteration 71, loss = 0.43621121\n",
      "Iteration 72, loss = 0.43591491\n",
      "Iteration 73, loss = 0.43706381\n",
      "Iteration 74, loss = 0.43818585\n",
      "Iteration 75, loss = 0.43613744\n",
      "Iteration 76, loss = 0.43647149\n",
      "Iteration 77, loss = 0.43691623\n",
      "Iteration 78, loss = 0.43990159\n",
      "Iteration 79, loss = 0.43927011\n",
      "Iteration 80, loss = 0.43765660\n",
      "Iteration 81, loss = 0.43715079\n",
      "Iteration 82, loss = 0.43474702\n",
      "Iteration 83, loss = 0.43493919\n",
      "Iteration 84, loss = 0.43664164\n",
      "Iteration 85, loss = 0.43647198\n",
      "Iteration 86, loss = 0.43527182\n",
      "Iteration 87, loss = 0.43604850\n",
      "Iteration 88, loss = 0.43590755\n",
      "Iteration 89, loss = 0.43568296\n",
      "Iteration 90, loss = 0.44082179\n",
      "Iteration 91, loss = 0.43553341\n",
      "Iteration 92, loss = 0.43808375\n",
      "Iteration 93, loss = 0.43692729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56675839\n",
      "Iteration 2, loss = 0.52575112\n",
      "Iteration 3, loss = 0.51986047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.51651884\n",
      "Iteration 5, loss = 0.51451452\n",
      "Iteration 6, loss = 0.51220250\n",
      "Iteration 7, loss = 0.50957131\n",
      "Iteration 8, loss = 0.50847812\n",
      "Iteration 9, loss = 0.50592084\n",
      "Iteration 10, loss = 0.50356430\n",
      "Iteration 11, loss = 0.50213290\n",
      "Iteration 12, loss = 0.50050550\n",
      "Iteration 13, loss = 0.50072262\n",
      "Iteration 14, loss = 0.49855004\n",
      "Iteration 15, loss = 0.49547718\n",
      "Iteration 16, loss = 0.49342122\n",
      "Iteration 17, loss = 0.49202021\n",
      "Iteration 18, loss = 0.48916715\n",
      "Iteration 19, loss = 0.48229476\n",
      "Iteration 20, loss = 0.47904644\n",
      "Iteration 21, loss = 0.47572634\n",
      "Iteration 22, loss = 0.47720796\n",
      "Iteration 23, loss = 0.47499809\n",
      "Iteration 24, loss = 0.47570632\n",
      "Iteration 25, loss = 0.47241786\n",
      "Iteration 26, loss = 0.47029504\n",
      "Iteration 27, loss = 0.46342072\n",
      "Iteration 28, loss = 0.46470808\n",
      "Iteration 29, loss = 0.46056381\n",
      "Iteration 30, loss = 0.46215247\n",
      "Iteration 31, loss = 0.46420178\n",
      "Iteration 32, loss = 0.45694079\n",
      "Iteration 33, loss = 0.45622262\n",
      "Iteration 34, loss = 0.46013765\n",
      "Iteration 35, loss = 0.45919824\n",
      "Iteration 36, loss = 0.45797810\n",
      "Iteration 37, loss = 0.45688904\n",
      "Iteration 38, loss = 0.45509939\n",
      "Iteration 39, loss = 0.45581757\n",
      "Iteration 40, loss = 0.45401285\n",
      "Iteration 41, loss = 0.45378019\n",
      "Iteration 42, loss = 0.45657075\n",
      "Iteration 43, loss = 0.45503919\n",
      "Iteration 44, loss = 0.45086119\n",
      "Iteration 45, loss = 0.45511956\n",
      "Iteration 46, loss = 0.45269648\n",
      "Iteration 47, loss = 0.44860083\n",
      "Iteration 48, loss = 0.45002999\n",
      "Iteration 49, loss = 0.45147883\n",
      "Iteration 50, loss = 0.44921448\n",
      "Iteration 51, loss = 0.45325080\n",
      "Iteration 52, loss = 0.44990292\n",
      "Iteration 53, loss = 0.44784733\n",
      "Iteration 54, loss = 0.44713173\n",
      "Iteration 55, loss = 0.44885514\n",
      "Iteration 56, loss = 0.44591384\n",
      "Iteration 57, loss = 0.44666532\n",
      "Iteration 58, loss = 0.44797668\n",
      "Iteration 59, loss = 0.44552436\n",
      "Iteration 60, loss = 0.44533480\n",
      "Iteration 61, loss = 0.44927406\n",
      "Iteration 62, loss = 0.44485745\n",
      "Iteration 63, loss = 0.44602736\n",
      "Iteration 64, loss = 0.44645678\n",
      "Iteration 65, loss = 0.44320864\n",
      "Iteration 66, loss = 0.44520064\n",
      "Iteration 67, loss = 0.44739293\n",
      "Iteration 68, loss = 0.44714974\n",
      "Iteration 69, loss = 0.44513155\n",
      "Iteration 70, loss = 0.44588550\n",
      "Iteration 71, loss = 0.44194700\n",
      "Iteration 72, loss = 0.44552067\n",
      "Iteration 73, loss = 0.44384032\n",
      "Iteration 74, loss = 0.44624750\n",
      "Iteration 75, loss = 0.44269074\n",
      "Iteration 76, loss = 0.44211123\n",
      "Iteration 77, loss = 0.44010393\n",
      "Iteration 78, loss = 0.44168448\n",
      "Iteration 79, loss = 0.44555359\n",
      "Iteration 80, loss = 0.44256424\n",
      "Iteration 81, loss = 0.44023515\n",
      "Iteration 82, loss = 0.44017292\n",
      "Iteration 83, loss = 0.44477917\n",
      "Iteration 84, loss = 0.44118033\n",
      "Iteration 85, loss = 0.44145283\n",
      "Iteration 86, loss = 0.44343686\n",
      "Iteration 87, loss = 0.44175754\n",
      "Iteration 88, loss = 0.44201586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56254289\n",
      "Iteration 2, loss = 0.51883888\n",
      "Iteration 3, loss = 0.51076577\n",
      "Iteration 4, loss = 0.50915239\n",
      "Iteration 5, loss = 0.50757459\n",
      "Iteration 6, loss = 0.50464222\n",
      "Iteration 7, loss = 0.50522055\n",
      "Iteration 8, loss = 0.50370115\n",
      "Iteration 9, loss = 0.50206829\n",
      "Iteration 10, loss = 0.50263945\n",
      "Iteration 11, loss = 0.50097972\n",
      "Iteration 12, loss = 0.50038881\n",
      "Iteration 13, loss = 0.50037584\n",
      "Iteration 14, loss = 0.49875569\n",
      "Iteration 15, loss = 0.49818789\n",
      "Iteration 16, loss = 0.49655306\n",
      "Iteration 17, loss = 0.49588196\n",
      "Iteration 18, loss = 0.49520302\n",
      "Iteration 19, loss = 0.49621753\n",
      "Iteration 20, loss = 0.49430635\n",
      "Iteration 21, loss = 0.49332054\n",
      "Iteration 22, loss = 0.49287722\n",
      "Iteration 23, loss = 0.49127196\n",
      "Iteration 24, loss = 0.48945048\n",
      "Iteration 25, loss = 0.48711591\n",
      "Iteration 26, loss = 0.48821736\n",
      "Iteration 27, loss = 0.48755704\n",
      "Iteration 28, loss = 0.48294900\n",
      "Iteration 29, loss = 0.48202029\n",
      "Iteration 30, loss = 0.47725638\n",
      "Iteration 31, loss = 0.46791916\n",
      "Iteration 32, loss = 0.46322522\n",
      "Iteration 33, loss = 0.46467595\n",
      "Iteration 34, loss = 0.46191920\n",
      "Iteration 35, loss = 0.46150910\n",
      "Iteration 36, loss = 0.46087679\n",
      "Iteration 37, loss = 0.45829576\n",
      "Iteration 38, loss = 0.45653659\n",
      "Iteration 39, loss = 0.45169605\n",
      "Iteration 40, loss = 0.45029826\n",
      "Iteration 41, loss = 0.44737327\n",
      "Iteration 42, loss = 0.44642913\n",
      "Iteration 43, loss = 0.44544788\n",
      "Iteration 44, loss = 0.44406222\n",
      "Iteration 45, loss = 0.44384658\n",
      "Iteration 46, loss = 0.44316737\n",
      "Iteration 47, loss = 0.44282696\n",
      "Iteration 48, loss = 0.44248461\n",
      "Iteration 49, loss = 0.44535422\n",
      "Iteration 50, loss = 0.44291120\n",
      "Iteration 51, loss = 0.44371559\n",
      "Iteration 52, loss = 0.44185578\n",
      "Iteration 53, loss = 0.44536469\n",
      "Iteration 54, loss = 0.44185578\n",
      "Iteration 55, loss = 0.44109612\n",
      "Iteration 56, loss = 0.44019348\n",
      "Iteration 57, loss = 0.44432307\n",
      "Iteration 58, loss = 0.43969630\n",
      "Iteration 59, loss = 0.44704544\n",
      "Iteration 60, loss = 0.43540702\n",
      "Iteration 61, loss = 0.43305177\n",
      "Iteration 62, loss = 0.43398277\n",
      "Iteration 63, loss = 0.43279889\n",
      "Iteration 64, loss = 0.43307764\n",
      "Iteration 65, loss = 0.43543117\n",
      "Iteration 66, loss = 0.43027536\n",
      "Iteration 67, loss = 0.42944568\n",
      "Iteration 68, loss = 0.42867808\n",
      "Iteration 69, loss = 0.42896708\n",
      "Iteration 70, loss = 0.43025799\n",
      "Iteration 71, loss = 0.42966838\n",
      "Iteration 72, loss = 0.42975279\n",
      "Iteration 73, loss = 0.42907560\n",
      "Iteration 74, loss = 0.42771371\n",
      "Iteration 75, loss = 0.42639297\n",
      "Iteration 76, loss = 0.42704607\n",
      "Iteration 77, loss = 0.42776876\n",
      "Iteration 78, loss = 0.42603096\n",
      "Iteration 79, loss = 0.42571005\n",
      "Iteration 80, loss = 0.42530380\n",
      "Iteration 81, loss = 0.42574649\n",
      "Iteration 82, loss = 0.42744916\n",
      "Iteration 83, loss = 0.42728399\n",
      "Iteration 84, loss = 0.42564972\n",
      "Iteration 85, loss = 0.42612201\n",
      "Iteration 86, loss = 0.42600559\n",
      "Iteration 87, loss = 0.42639797\n",
      "Iteration 88, loss = 0.42649944\n",
      "Iteration 89, loss = 0.42626250\n",
      "Iteration 90, loss = 0.42667329\n",
      "Iteration 91, loss = 0.42648217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56108446\n",
      "Iteration 2, loss = 0.52494569\n",
      "Iteration 3, loss = 0.51402365\n",
      "Iteration 4, loss = 0.51012959\n",
      "Iteration 5, loss = 0.50903711\n",
      "Iteration 6, loss = 0.50693239\n",
      "Iteration 7, loss = 0.50502169\n",
      "Iteration 8, loss = 0.49948920\n",
      "Iteration 9, loss = 0.49786354\n",
      "Iteration 10, loss = 0.49298571\n",
      "Iteration 11, loss = 0.48701729\n",
      "Iteration 12, loss = 0.48023207\n",
      "Iteration 13, loss = 0.47484563\n",
      "Iteration 14, loss = 0.46296790\n",
      "Iteration 15, loss = 0.46130098\n",
      "Iteration 16, loss = 0.46002216\n",
      "Iteration 17, loss = 0.45871493\n",
      "Iteration 18, loss = 0.45497964\n",
      "Iteration 19, loss = 0.45348876\n",
      "Iteration 20, loss = 0.45228470\n",
      "Iteration 21, loss = 0.45053651\n",
      "Iteration 22, loss = 0.45297698\n",
      "Iteration 23, loss = 0.44901501\n",
      "Iteration 24, loss = 0.44659355\n",
      "Iteration 25, loss = 0.44739663\n",
      "Iteration 26, loss = 0.44625585\n",
      "Iteration 27, loss = 0.44909676\n",
      "Iteration 28, loss = 0.44672892\n",
      "Iteration 29, loss = 0.44263645\n",
      "Iteration 30, loss = 0.44393665\n",
      "Iteration 31, loss = 0.44164953\n",
      "Iteration 32, loss = 0.43861141\n",
      "Iteration 33, loss = 0.43906343\n",
      "Iteration 34, loss = 0.43999885\n",
      "Iteration 35, loss = 0.44285775\n",
      "Iteration 36, loss = 0.44097288\n",
      "Iteration 37, loss = 0.43997688\n",
      "Iteration 38, loss = 0.43854067\n",
      "Iteration 39, loss = 0.43685619\n",
      "Iteration 40, loss = 0.43734849\n",
      "Iteration 41, loss = 0.43562643\n",
      "Iteration 42, loss = 0.43609751\n",
      "Iteration 43, loss = 0.43946899\n",
      "Iteration 44, loss = 0.43677555\n",
      "Iteration 45, loss = 0.43621865\n",
      "Iteration 46, loss = 0.43510299\n",
      "Iteration 47, loss = 0.43412762\n",
      "Iteration 48, loss = 0.43507583\n",
      "Iteration 49, loss = 0.43675670\n",
      "Iteration 50, loss = 0.43414873\n",
      "Iteration 51, loss = 0.43267824\n",
      "Iteration 52, loss = 0.43441454\n",
      "Iteration 53, loss = 0.43659287\n",
      "Iteration 54, loss = 0.43502966\n",
      "Iteration 55, loss = 0.43410313\n",
      "Iteration 56, loss = 0.43406150\n",
      "Iteration 57, loss = 0.43895487\n",
      "Iteration 58, loss = 0.43472690\n",
      "Iteration 59, loss = 0.43740099\n",
      "Iteration 60, loss = 0.43603225\n",
      "Iteration 61, loss = 0.43455334\n",
      "Iteration 62, loss = 0.43160218\n",
      "Iteration 63, loss = 0.43520828\n",
      "Iteration 64, loss = 0.43336781\n",
      "Iteration 65, loss = 0.43250754\n",
      "Iteration 66, loss = 0.43361041\n",
      "Iteration 67, loss = 0.43088673\n",
      "Iteration 68, loss = 0.43129321\n",
      "Iteration 69, loss = 0.43354255\n",
      "Iteration 70, loss = 0.43163563\n",
      "Iteration 71, loss = 0.43193674\n",
      "Iteration 72, loss = 0.43321176\n",
      "Iteration 73, loss = 0.43316471\n",
      "Iteration 74, loss = 0.43191508\n",
      "Iteration 75, loss = 0.43215877\n",
      "Iteration 76, loss = 0.43048557\n",
      "Iteration 77, loss = 0.43359505\n",
      "Iteration 78, loss = 0.43176352\n",
      "Iteration 79, loss = 0.43153858\n",
      "Iteration 80, loss = 0.43100683\n",
      "Iteration 81, loss = 0.43240680\n",
      "Iteration 82, loss = 0.43034991\n",
      "Iteration 83, loss = 0.43127419\n",
      "Iteration 84, loss = 0.42947397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 85, loss = 0.43144165\n",
      "Iteration 86, loss = 0.43277692\n",
      "Iteration 87, loss = 0.43174890\n",
      "Iteration 88, loss = 0.43206114\n",
      "Iteration 89, loss = 0.43189266\n",
      "Iteration 90, loss = 0.43023691\n",
      "Iteration 91, loss = 0.43355958\n",
      "Iteration 92, loss = 0.43049753\n",
      "Iteration 93, loss = 0.42999907\n",
      "Iteration 94, loss = 0.43039364\n",
      "Iteration 95, loss = 0.42940695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56400585\n",
      "Iteration 2, loss = 0.51936954\n",
      "Iteration 3, loss = 0.51140165\n",
      "Iteration 4, loss = 0.50586652\n",
      "Iteration 5, loss = 0.49760022\n",
      "Iteration 6, loss = 0.48916122\n",
      "Iteration 7, loss = 0.48419199\n",
      "Iteration 8, loss = 0.48351777\n",
      "Iteration 9, loss = 0.47688220\n",
      "Iteration 10, loss = 0.47459546\n",
      "Iteration 11, loss = 0.47616899\n",
      "Iteration 12, loss = 0.46736445\n",
      "Iteration 13, loss = 0.47163845\n",
      "Iteration 14, loss = 0.47084445\n",
      "Iteration 15, loss = 0.46670428\n",
      "Iteration 16, loss = 0.46504013\n",
      "Iteration 17, loss = 0.46709196\n",
      "Iteration 18, loss = 0.46420286\n",
      "Iteration 19, loss = 0.46270026\n",
      "Iteration 20, loss = 0.46273534\n",
      "Iteration 21, loss = 0.46163418\n",
      "Iteration 22, loss = 0.46143086\n",
      "Iteration 23, loss = 0.46105973\n",
      "Iteration 24, loss = 0.46276911\n",
      "Iteration 25, loss = 0.45905997\n",
      "Iteration 26, loss = 0.46015270\n",
      "Iteration 27, loss = 0.45774955\n",
      "Iteration 28, loss = 0.45658700\n",
      "Iteration 29, loss = 0.45308632\n",
      "Iteration 30, loss = 0.45971835\n",
      "Iteration 31, loss = 0.45573069\n",
      "Iteration 32, loss = 0.45181443\n",
      "Iteration 33, loss = 0.45220809\n",
      "Iteration 34, loss = 0.45083852\n",
      "Iteration 35, loss = 0.45334643\n",
      "Iteration 36, loss = 0.45294713\n",
      "Iteration 37, loss = 0.45239814\n",
      "Iteration 38, loss = 0.44999527\n",
      "Iteration 39, loss = 0.45119624\n",
      "Iteration 40, loss = 0.44848805\n",
      "Iteration 41, loss = 0.44669667\n",
      "Iteration 42, loss = 0.45255570\n",
      "Iteration 43, loss = 0.44753566\n",
      "Iteration 44, loss = 0.44705369\n",
      "Iteration 45, loss = 0.44820697\n",
      "Iteration 46, loss = 0.44901967\n",
      "Iteration 47, loss = 0.44581016\n",
      "Iteration 48, loss = 0.44648877\n",
      "Iteration 49, loss = 0.44558540\n",
      "Iteration 50, loss = 0.44466672\n",
      "Iteration 51, loss = 0.44479828\n",
      "Iteration 52, loss = 0.43995121\n",
      "Iteration 53, loss = 0.44126866\n",
      "Iteration 54, loss = 0.44032968\n",
      "Iteration 55, loss = 0.43907499\n",
      "Iteration 56, loss = 0.43673238\n",
      "Iteration 57, loss = 0.43755262\n",
      "Iteration 58, loss = 0.43672177\n",
      "Iteration 59, loss = 0.43356321\n",
      "Iteration 60, loss = 0.43327073\n",
      "Iteration 61, loss = 0.43597141\n",
      "Iteration 62, loss = 0.43352369\n",
      "Iteration 63, loss = 0.43162221\n",
      "Iteration 64, loss = 0.43132087\n",
      "Iteration 65, loss = 0.43485709\n",
      "Iteration 66, loss = 0.43228194\n",
      "Iteration 67, loss = 0.43082936\n",
      "Iteration 68, loss = 0.42874798\n",
      "Iteration 69, loss = 0.43164868\n",
      "Iteration 70, loss = 0.42944633\n",
      "Iteration 71, loss = 0.42619631\n",
      "Iteration 72, loss = 0.42929460\n",
      "Iteration 73, loss = 0.42802088\n",
      "Iteration 74, loss = 0.43056421\n",
      "Iteration 75, loss = 0.42527244\n",
      "Iteration 76, loss = 0.42592127\n",
      "Iteration 77, loss = 0.42961531\n",
      "Iteration 78, loss = 0.43083241\n",
      "Iteration 79, loss = 0.42531602\n",
      "Iteration 80, loss = 0.42665848\n",
      "Iteration 81, loss = 0.42414937\n",
      "Iteration 82, loss = 0.42493818\n",
      "Iteration 83, loss = 0.42606267\n",
      "Iteration 84, loss = 0.42437758\n",
      "Iteration 85, loss = 0.42426278\n",
      "Iteration 86, loss = 0.42815214\n",
      "Iteration 87, loss = 0.42390669\n",
      "Iteration 88, loss = 0.42526897\n",
      "Iteration 89, loss = 0.42326369\n",
      "Iteration 90, loss = 0.42475404\n",
      "Iteration 91, loss = 0.42395388\n",
      "Iteration 92, loss = 0.42360553\n",
      "Iteration 93, loss = 0.42298613\n",
      "Iteration 94, loss = 0.42504660\n",
      "Iteration 95, loss = 0.42364194\n",
      "Iteration 96, loss = 0.42288063\n",
      "Iteration 97, loss = 0.42214478\n",
      "Iteration 98, loss = 0.42543955\n",
      "Iteration 99, loss = 0.42531130\n",
      "Iteration 100, loss = 0.42040046\n",
      "Iteration 101, loss = 0.42226710\n",
      "Iteration 102, loss = 0.41974356\n",
      "Iteration 103, loss = 0.42145557\n",
      "Iteration 104, loss = 0.41862177\n",
      "Iteration 105, loss = 0.42032033\n",
      "Iteration 106, loss = 0.42052057\n",
      "Iteration 107, loss = 0.42364905\n",
      "Iteration 108, loss = 0.42255897\n",
      "Iteration 109, loss = 0.41993238\n",
      "Iteration 110, loss = 0.42177579\n",
      "Iteration 111, loss = 0.42031432\n",
      "Iteration 112, loss = 0.42152708\n",
      "Iteration 113, loss = 0.42102136\n",
      "Iteration 114, loss = 0.41961793\n",
      "Iteration 115, loss = 0.42068612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55970939\n",
      "Iteration 2, loss = 0.51874827\n",
      "Iteration 3, loss = 0.50714945\n",
      "Iteration 4, loss = 0.50078731\n",
      "Iteration 5, loss = 0.48748328\n",
      "Iteration 6, loss = 0.47738206\n",
      "Iteration 7, loss = 0.47377345\n",
      "Iteration 8, loss = 0.47471930\n",
      "Iteration 9, loss = 0.47086696\n",
      "Iteration 10, loss = 0.47112208\n",
      "Iteration 11, loss = 0.47027056\n",
      "Iteration 12, loss = 0.46844489\n",
      "Iteration 13, loss = 0.46739120\n",
      "Iteration 14, loss = 0.46627202\n",
      "Iteration 15, loss = 0.46470978\n",
      "Iteration 16, loss = 0.46240418\n",
      "Iteration 17, loss = 0.46192856\n",
      "Iteration 18, loss = 0.46240847\n",
      "Iteration 19, loss = 0.46199170\n",
      "Iteration 20, loss = 0.45931605\n",
      "Iteration 21, loss = 0.46036151\n",
      "Iteration 22, loss = 0.45915529\n",
      "Iteration 23, loss = 0.45591735\n",
      "Iteration 24, loss = 0.45498724\n",
      "Iteration 25, loss = 0.45497371\n",
      "Iteration 26, loss = 0.45415544\n",
      "Iteration 27, loss = 0.45386561\n",
      "Iteration 28, loss = 0.45129969\n",
      "Iteration 29, loss = 0.45385018\n",
      "Iteration 30, loss = 0.45247110\n",
      "Iteration 31, loss = 0.45398191\n",
      "Iteration 32, loss = 0.45028152\n",
      "Iteration 33, loss = 0.45283848\n",
      "Iteration 34, loss = 0.45363083\n",
      "Iteration 35, loss = 0.45623597\n",
      "Iteration 36, loss = 0.45425309\n",
      "Iteration 37, loss = 0.45739352\n",
      "Iteration 38, loss = 0.44980096\n",
      "Iteration 39, loss = 0.45219138\n",
      "Iteration 40, loss = 0.44855320\n",
      "Iteration 41, loss = 0.44956225\n",
      "Iteration 42, loss = 0.45086998\n",
      "Iteration 43, loss = 0.44882186\n",
      "Iteration 44, loss = 0.44885081\n",
      "Iteration 45, loss = 0.44961739\n",
      "Iteration 46, loss = 0.44786720\n",
      "Iteration 47, loss = 0.44689240\n",
      "Iteration 48, loss = 0.44797453\n",
      "Iteration 49, loss = 0.45022658\n",
      "Iteration 50, loss = 0.44715605\n",
      "Iteration 51, loss = 0.44962832\n",
      "Iteration 52, loss = 0.44598990\n",
      "Iteration 53, loss = 0.44966044\n",
      "Iteration 54, loss = 0.44875964\n",
      "Iteration 55, loss = 0.44634271\n",
      "Iteration 56, loss = 0.44693694\n",
      "Iteration 57, loss = 0.44847500\n",
      "Iteration 58, loss = 0.44543297\n",
      "Iteration 59, loss = 0.44745839\n",
      "Iteration 60, loss = 0.44474529\n",
      "Iteration 61, loss = 0.44779208\n",
      "Iteration 62, loss = 0.44679541\n",
      "Iteration 63, loss = 0.44742663\n",
      "Iteration 64, loss = 0.44682892\n",
      "Iteration 65, loss = 0.44646371\n",
      "Iteration 66, loss = 0.44657531\n",
      "Iteration 67, loss = 0.44817740\n",
      "Iteration 68, loss = 0.44964003\n",
      "Iteration 69, loss = 0.44888317\n",
      "Iteration 70, loss = 0.44448996\n",
      "Iteration 71, loss = 0.44534293\n",
      "Iteration 72, loss = 0.44588223\n",
      "Iteration 73, loss = 0.44593178\n",
      "Iteration 74, loss = 0.44537663\n",
      "Iteration 75, loss = 0.44592343\n",
      "Iteration 76, loss = 0.44639421\n",
      "Iteration 77, loss = 0.44423641\n",
      "Iteration 78, loss = 0.44660350\n",
      "Iteration 79, loss = 0.44686101\n",
      "Iteration 80, loss = 0.44576887\n",
      "Iteration 81, loss = 0.44615612\n",
      "Iteration 82, loss = 0.44393033\n",
      "Iteration 83, loss = 0.44466471\n",
      "Iteration 84, loss = 0.44267183\n",
      "Iteration 85, loss = 0.44480824\n",
      "Iteration 86, loss = 0.44452704\n",
      "Iteration 87, loss = 0.44454106\n",
      "Iteration 88, loss = 0.44798968\n",
      "Iteration 89, loss = 0.44482121\n",
      "Iteration 90, loss = 0.44768106\n",
      "Iteration 91, loss = 0.44301590\n",
      "Iteration 92, loss = 0.44525303\n",
      "Iteration 93, loss = 0.44306754\n",
      "Iteration 94, loss = 0.44307695\n",
      "Iteration 95, loss = 0.44471553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55914718\n",
      "Iteration 2, loss = 0.52007870\n",
      "Iteration 3, loss = 0.50485538\n",
      "Iteration 4, loss = 0.49236447\n",
      "Iteration 5, loss = 0.48402797\n",
      "Iteration 6, loss = 0.47682608\n",
      "Iteration 7, loss = 0.47393968\n",
      "Iteration 8, loss = 0.47245865\n",
      "Iteration 9, loss = 0.47271706\n",
      "Iteration 10, loss = 0.47067011\n",
      "Iteration 11, loss = 0.47131803\n",
      "Iteration 12, loss = 0.46817174\n",
      "Iteration 13, loss = 0.46975015\n",
      "Iteration 14, loss = 0.46590734\n",
      "Iteration 15, loss = 0.46522087\n",
      "Iteration 16, loss = 0.46596959\n",
      "Iteration 17, loss = 0.46502057\n",
      "Iteration 18, loss = 0.46515076\n",
      "Iteration 19, loss = 0.46462790\n",
      "Iteration 20, loss = 0.46138127\n",
      "Iteration 21, loss = 0.46204264\n",
      "Iteration 22, loss = 0.46428229\n",
      "Iteration 23, loss = 0.45955780\n",
      "Iteration 24, loss = 0.45918048\n",
      "Iteration 25, loss = 0.46264098\n",
      "Iteration 26, loss = 0.45910403\n",
      "Iteration 27, loss = 0.46299124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 28, loss = 0.45844728\n",
      "Iteration 29, loss = 0.45730009\n",
      "Iteration 30, loss = 0.45856236\n",
      "Iteration 31, loss = 0.45679635\n",
      "Iteration 32, loss = 0.45550632\n",
      "Iteration 33, loss = 0.45510064\n",
      "Iteration 34, loss = 0.45551352\n",
      "Iteration 35, loss = 0.45867737\n",
      "Iteration 36, loss = 0.45762676\n",
      "Iteration 37, loss = 0.45893141\n",
      "Iteration 38, loss = 0.45610512\n",
      "Iteration 39, loss = 0.45329630\n",
      "Iteration 40, loss = 0.45551388\n",
      "Iteration 41, loss = 0.45250646\n",
      "Iteration 42, loss = 0.45059815\n",
      "Iteration 43, loss = 0.45125999\n",
      "Iteration 44, loss = 0.45043167\n",
      "Iteration 45, loss = 0.45075625\n",
      "Iteration 46, loss = 0.44712818\n",
      "Iteration 47, loss = 0.44745568\n",
      "Iteration 48, loss = 0.44948653\n",
      "Iteration 49, loss = 0.44652194\n",
      "Iteration 50, loss = 0.44613192\n",
      "Iteration 51, loss = 0.44588071\n",
      "Iteration 52, loss = 0.44489245\n",
      "Iteration 53, loss = 0.44923946\n",
      "Iteration 54, loss = 0.44691577\n",
      "Iteration 55, loss = 0.44644502\n",
      "Iteration 56, loss = 0.44415082\n",
      "Iteration 57, loss = 0.44462066\n",
      "Iteration 58, loss = 0.44299874\n",
      "Iteration 59, loss = 0.44462484\n",
      "Iteration 60, loss = 0.44402943\n",
      "Iteration 61, loss = 0.44461138\n",
      "Iteration 62, loss = 0.44111753\n",
      "Iteration 63, loss = 0.44271234\n",
      "Iteration 64, loss = 0.44296678\n",
      "Iteration 65, loss = 0.44031032\n",
      "Iteration 66, loss = 0.44136213\n",
      "Iteration 67, loss = 0.43996444\n",
      "Iteration 68, loss = 0.43865488\n",
      "Iteration 69, loss = 0.44150975\n",
      "Iteration 70, loss = 0.44024713\n",
      "Iteration 71, loss = 0.43763731\n",
      "Iteration 72, loss = 0.43899033\n",
      "Iteration 73, loss = 0.44005350\n",
      "Iteration 74, loss = 0.43980953\n",
      "Iteration 75, loss = 0.43884814\n",
      "Iteration 76, loss = 0.43616824\n",
      "Iteration 77, loss = 0.43948795\n",
      "Iteration 78, loss = 0.43964472\n",
      "Iteration 79, loss = 0.44061662\n",
      "Iteration 80, loss = 0.43547350\n",
      "Iteration 81, loss = 0.43523355\n",
      "Iteration 82, loss = 0.43505375\n",
      "Iteration 83, loss = 0.43407453\n",
      "Iteration 84, loss = 0.43515100\n",
      "Iteration 85, loss = 0.43475117\n",
      "Iteration 86, loss = 0.43440185\n",
      "Iteration 87, loss = 0.43395214\n",
      "Iteration 88, loss = 0.43398442\n",
      "Iteration 89, loss = 0.43377122\n",
      "Iteration 90, loss = 0.43535874\n",
      "Iteration 91, loss = 0.43360902\n",
      "Iteration 92, loss = 0.43302078\n",
      "Iteration 93, loss = 0.43662710\n",
      "Iteration 94, loss = 0.43537737\n",
      "Iteration 95, loss = 0.43608969\n",
      "Iteration 96, loss = 0.43479246\n",
      "Iteration 97, loss = 0.43135592\n",
      "Iteration 98, loss = 0.43190463\n",
      "Iteration 99, loss = 0.43189340\n",
      "Iteration 100, loss = 0.43157224\n",
      "Iteration 101, loss = 0.43540626\n",
      "Iteration 102, loss = 0.43364668\n",
      "Iteration 103, loss = 0.43245073\n",
      "Iteration 104, loss = 0.43241225\n",
      "Iteration 105, loss = 0.43064494\n",
      "Iteration 106, loss = 0.43028007\n",
      "Iteration 107, loss = 0.43208616\n",
      "Iteration 108, loss = 0.43148815\n",
      "Iteration 109, loss = 0.43137780\n",
      "Iteration 110, loss = 0.43032678\n",
      "Iteration 111, loss = 0.43091417\n",
      "Iteration 112, loss = 0.43081793\n",
      "Iteration 113, loss = 0.42888920\n",
      "Iteration 114, loss = 0.42820332\n",
      "Iteration 115, loss = 0.43100259\n",
      "Iteration 116, loss = 0.42821599\n",
      "Iteration 117, loss = 0.42738049\n",
      "Iteration 118, loss = 0.42770292\n",
      "Iteration 119, loss = 0.42824927\n",
      "Iteration 120, loss = 0.42719107\n",
      "Iteration 121, loss = 0.42831937\n",
      "Iteration 122, loss = 0.43014723\n",
      "Iteration 123, loss = 0.42719023\n",
      "Iteration 124, loss = 0.42653270\n",
      "Iteration 125, loss = 0.42799295\n",
      "Iteration 126, loss = 0.42455285\n",
      "Iteration 127, loss = 0.42868698\n",
      "Iteration 128, loss = 0.42795289\n",
      "Iteration 129, loss = 0.42810778\n",
      "Iteration 130, loss = 0.42548099\n",
      "Iteration 131, loss = 0.42624767\n",
      "Iteration 132, loss = 0.42473822\n",
      "Iteration 133, loss = 0.42374198\n",
      "Iteration 134, loss = 0.42526715\n",
      "Iteration 135, loss = 0.42461353\n",
      "Iteration 136, loss = 0.42387527\n",
      "Iteration 137, loss = 0.42151984\n",
      "Iteration 138, loss = 0.42299531\n",
      "Iteration 139, loss = 0.42466564\n",
      "Iteration 140, loss = 0.42459767\n",
      "Iteration 141, loss = 0.42467464\n",
      "Iteration 142, loss = 0.42317204\n",
      "Iteration 143, loss = 0.42337798\n",
      "Iteration 144, loss = 0.42415503\n",
      "Iteration 145, loss = 0.42253505\n",
      "Iteration 146, loss = 0.42394949\n",
      "Iteration 147, loss = 0.42408535\n",
      "Iteration 148, loss = 0.42278653\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57221340\n",
      "Iteration 2, loss = 0.53718038\n",
      "Iteration 3, loss = 0.53327100\n",
      "Iteration 4, loss = 0.52823872\n",
      "Iteration 5, loss = 0.51675688\n",
      "Iteration 6, loss = 0.50719571\n",
      "Iteration 7, loss = 0.50234899\n",
      "Iteration 8, loss = 0.50530680\n",
      "Iteration 9, loss = 0.50042229\n",
      "Iteration 10, loss = 0.49809147\n",
      "Iteration 11, loss = 0.49922605\n",
      "Iteration 12, loss = 0.49697943\n",
      "Iteration 13, loss = 0.50070679\n",
      "Iteration 14, loss = 0.49789486\n",
      "Iteration 15, loss = 0.49344058\n",
      "Iteration 16, loss = 0.49354777\n",
      "Iteration 17, loss = 0.49620588\n",
      "Iteration 18, loss = 0.49292557\n",
      "Iteration 19, loss = 0.49136312\n",
      "Iteration 20, loss = 0.49153031\n",
      "Iteration 21, loss = 0.49042665\n",
      "Iteration 22, loss = 0.49098152\n",
      "Iteration 23, loss = 0.49230842\n",
      "Iteration 24, loss = 0.49233258\n",
      "Iteration 25, loss = 0.49134962\n",
      "Iteration 26, loss = 0.49086199\n",
      "Iteration 27, loss = 0.49058612\n",
      "Iteration 28, loss = 0.49022992\n",
      "Iteration 29, loss = 0.48943553\n",
      "Iteration 30, loss = 0.49057335\n",
      "Iteration 31, loss = 0.49104971\n",
      "Iteration 32, loss = 0.48781035\n",
      "Iteration 33, loss = 0.48781243\n",
      "Iteration 34, loss = 0.48792628\n",
      "Iteration 35, loss = 0.48902384\n",
      "Iteration 36, loss = 0.48956260\n",
      "Iteration 37, loss = 0.48856831\n",
      "Iteration 38, loss = 0.48685213\n",
      "Iteration 39, loss = 0.49091192\n",
      "Iteration 40, loss = 0.48633223\n",
      "Iteration 41, loss = 0.48606427\n",
      "Iteration 42, loss = 0.49171910\n",
      "Iteration 43, loss = 0.48578626\n",
      "Iteration 44, loss = 0.48554287\n",
      "Iteration 45, loss = 0.48650119\n",
      "Iteration 46, loss = 0.48688037\n",
      "Iteration 47, loss = 0.48658499\n",
      "Iteration 48, loss = 0.48727392\n",
      "Iteration 49, loss = 0.48551595\n",
      "Iteration 50, loss = 0.48579298\n",
      "Iteration 51, loss = 0.48760770\n",
      "Iteration 52, loss = 0.48533575\n",
      "Iteration 53, loss = 0.48493742\n",
      "Iteration 54, loss = 0.48573725\n",
      "Iteration 55, loss = 0.48584748\n",
      "Iteration 56, loss = 0.48443829\n",
      "Iteration 57, loss = 0.48919511\n",
      "Iteration 58, loss = 0.48636991\n",
      "Iteration 59, loss = 0.48446975\n",
      "Iteration 60, loss = 0.48702801\n",
      "Iteration 61, loss = 0.48721958\n",
      "Iteration 62, loss = 0.48555801\n",
      "Iteration 63, loss = 0.48523974\n",
      "Iteration 64, loss = 0.48442929\n",
      "Iteration 65, loss = 0.48530658\n",
      "Iteration 66, loss = 0.48434569\n",
      "Iteration 67, loss = 0.48636971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57060597\n",
      "Iteration 2, loss = 0.53480024\n",
      "Iteration 3, loss = 0.52597290\n",
      "Iteration 4, loss = 0.51942317\n",
      "Iteration 5, loss = 0.50877681\n",
      "Iteration 6, loss = 0.49912203\n",
      "Iteration 7, loss = 0.49697146\n",
      "Iteration 8, loss = 0.49611265\n",
      "Iteration 9, loss = 0.49528432\n",
      "Iteration 10, loss = 0.49623556\n",
      "Iteration 11, loss = 0.49705540\n",
      "Iteration 12, loss = 0.49261344\n",
      "Iteration 13, loss = 0.49246693\n",
      "Iteration 14, loss = 0.49287848\n",
      "Iteration 15, loss = 0.49106539\n",
      "Iteration 16, loss = 0.48849466\n",
      "Iteration 17, loss = 0.48858495\n",
      "Iteration 18, loss = 0.49050383\n",
      "Iteration 19, loss = 0.48823536\n",
      "Iteration 20, loss = 0.48633932\n",
      "Iteration 21, loss = 0.48953956\n",
      "Iteration 22, loss = 0.49019087\n",
      "Iteration 23, loss = 0.48802087\n",
      "Iteration 24, loss = 0.48842284\n",
      "Iteration 25, loss = 0.48715653\n",
      "Iteration 26, loss = 0.48836613\n",
      "Iteration 27, loss = 0.48615244\n",
      "Iteration 28, loss = 0.48341833\n",
      "Iteration 29, loss = 0.48333818\n",
      "Iteration 30, loss = 0.48362806\n",
      "Iteration 31, loss = 0.48442644\n",
      "Iteration 32, loss = 0.48289519\n",
      "Iteration 33, loss = 0.48210109\n",
      "Iteration 34, loss = 0.48351313\n",
      "Iteration 35, loss = 0.49054970\n",
      "Iteration 36, loss = 0.48256131\n",
      "Iteration 37, loss = 0.48496202\n",
      "Iteration 38, loss = 0.47957274\n",
      "Iteration 39, loss = 0.48417851\n",
      "Iteration 40, loss = 0.48119162\n",
      "Iteration 41, loss = 0.48176002\n",
      "Iteration 42, loss = 0.48484519\n",
      "Iteration 43, loss = 0.48021217\n",
      "Iteration 44, loss = 0.47975052\n",
      "Iteration 45, loss = 0.48246794\n",
      "Iteration 46, loss = 0.47913577\n",
      "Iteration 47, loss = 0.47963246\n",
      "Iteration 48, loss = 0.48150061\n",
      "Iteration 49, loss = 0.48178977\n",
      "Iteration 50, loss = 0.47961228\n",
      "Iteration 51, loss = 0.48037266\n",
      "Iteration 52, loss = 0.47801127\n",
      "Iteration 53, loss = 0.48124466\n",
      "Iteration 54, loss = 0.48216579\n",
      "Iteration 55, loss = 0.48136860\n",
      "Iteration 56, loss = 0.47941421\n",
      "Iteration 57, loss = 0.47923058\n",
      "Iteration 58, loss = 0.47812797\n",
      "Iteration 59, loss = 0.47867581\n",
      "Iteration 60, loss = 0.47852342\n",
      "Iteration 61, loss = 0.47823155\n",
      "Iteration 62, loss = 0.47827763\n",
      "Iteration 63, loss = 0.48001027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56919431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53596818\n",
      "Iteration 3, loss = 0.52957452\n",
      "Iteration 4, loss = 0.51720555\n",
      "Iteration 5, loss = 0.50926228\n",
      "Iteration 6, loss = 0.50647835\n",
      "Iteration 7, loss = 0.50235908\n",
      "Iteration 8, loss = 0.49797135\n",
      "Iteration 9, loss = 0.50213829\n",
      "Iteration 10, loss = 0.49835722\n",
      "Iteration 11, loss = 0.49886143\n",
      "Iteration 12, loss = 0.49384735\n",
      "Iteration 13, loss = 0.49530673\n",
      "Iteration 14, loss = 0.49341145\n",
      "Iteration 15, loss = 0.49199400\n",
      "Iteration 16, loss = 0.49259459\n",
      "Iteration 17, loss = 0.49242123\n",
      "Iteration 18, loss = 0.49274398\n",
      "Iteration 19, loss = 0.49530279\n",
      "Iteration 20, loss = 0.48930412\n",
      "Iteration 21, loss = 0.49116378\n",
      "Iteration 22, loss = 0.49220833\n",
      "Iteration 23, loss = 0.48930056\n",
      "Iteration 24, loss = 0.48857433\n",
      "Iteration 25, loss = 0.49056162\n",
      "Iteration 26, loss = 0.48940743\n",
      "Iteration 27, loss = 0.48880145\n",
      "Iteration 28, loss = 0.49032416\n",
      "Iteration 29, loss = 0.48858664\n",
      "Iteration 30, loss = 0.48995868\n",
      "Iteration 31, loss = 0.48759662\n",
      "Iteration 32, loss = 0.48917101\n",
      "Iteration 33, loss = 0.48819197\n",
      "Iteration 34, loss = 0.48817263\n",
      "Iteration 35, loss = 0.48891634\n",
      "Iteration 36, loss = 0.48831242\n",
      "Iteration 37, loss = 0.48820985\n",
      "Iteration 38, loss = 0.49022512\n",
      "Iteration 39, loss = 0.48637202\n",
      "Iteration 40, loss = 0.48709277\n",
      "Iteration 41, loss = 0.48633845\n",
      "Iteration 42, loss = 0.48543483\n",
      "Iteration 43, loss = 0.48712085\n",
      "Iteration 44, loss = 0.48706180\n",
      "Iteration 45, loss = 0.48902396\n",
      "Iteration 46, loss = 0.48918932\n",
      "Iteration 47, loss = 0.48682241\n",
      "Iteration 48, loss = 0.48596284\n",
      "Iteration 49, loss = 0.48479862\n",
      "Iteration 50, loss = 0.48437598\n",
      "Iteration 51, loss = 0.48508600\n",
      "Iteration 52, loss = 0.48542701\n",
      "Iteration 53, loss = 0.48672658\n",
      "Iteration 54, loss = 0.48573350\n",
      "Iteration 55, loss = 0.48890597\n",
      "Iteration 56, loss = 0.48357678\n",
      "Iteration 57, loss = 0.48534981\n",
      "Iteration 58, loss = 0.48558673\n",
      "Iteration 59, loss = 0.48497530\n",
      "Iteration 60, loss = 0.48557276\n",
      "Iteration 61, loss = 0.48648535\n",
      "Iteration 62, loss = 0.48359783\n",
      "Iteration 63, loss = 0.48592999\n",
      "Iteration 64, loss = 0.48372686\n",
      "Iteration 65, loss = 0.48309717\n",
      "Iteration 66, loss = 0.48321338\n",
      "Iteration 67, loss = 0.48302426\n",
      "Iteration 68, loss = 0.48432789\n",
      "Iteration 69, loss = 0.48381291\n",
      "Iteration 70, loss = 0.48291606\n",
      "Iteration 71, loss = 0.48265221\n",
      "Iteration 72, loss = 0.48186910\n",
      "Iteration 73, loss = 0.48222856\n",
      "Iteration 74, loss = 0.48514653\n",
      "Iteration 75, loss = 0.48291386\n",
      "Iteration 76, loss = 0.48209066\n",
      "Iteration 77, loss = 0.48135228\n",
      "Iteration 78, loss = 0.48413896\n",
      "Iteration 79, loss = 0.48392074\n",
      "Iteration 80, loss = 0.48210010\n",
      "Iteration 81, loss = 0.48367843\n",
      "Iteration 82, loss = 0.48149154\n",
      "Iteration 83, loss = 0.48211391\n",
      "Iteration 84, loss = 0.48362923\n",
      "Iteration 85, loss = 0.48135525\n",
      "Iteration 86, loss = 0.48182079\n",
      "Iteration 87, loss = 0.48021651\n",
      "Iteration 88, loss = 0.48131930\n",
      "Iteration 89, loss = 0.47972717\n",
      "Iteration 90, loss = 0.48106909\n",
      "Iteration 91, loss = 0.48171760\n",
      "Iteration 92, loss = 0.48067086\n",
      "Iteration 93, loss = 0.48278974\n",
      "Iteration 94, loss = 0.47939701\n",
      "Iteration 95, loss = 0.48174053\n",
      "Iteration 96, loss = 0.48163991\n",
      "Iteration 97, loss = 0.47924866\n",
      "Iteration 98, loss = 0.47753294\n",
      "Iteration 99, loss = 0.47840968\n",
      "Iteration 100, loss = 0.47893597\n",
      "Iteration 101, loss = 0.47879331\n",
      "Iteration 102, loss = 0.48088394\n",
      "Iteration 103, loss = 0.47903214\n",
      "Iteration 104, loss = 0.48281446\n",
      "Iteration 105, loss = 0.48158963\n",
      "Iteration 106, loss = 0.47747049\n",
      "Iteration 107, loss = 0.47799949\n",
      "Iteration 108, loss = 0.47872264\n",
      "Iteration 109, loss = 0.47862377\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55788082\n",
      "Iteration 2, loss = 0.50940436\n",
      "Iteration 3, loss = 0.49775250\n",
      "Iteration 4, loss = 0.49102911\n",
      "Iteration 5, loss = 0.48789425\n",
      "Iteration 6, loss = 0.48535292\n",
      "Iteration 7, loss = 0.48346060\n",
      "Iteration 8, loss = 0.48497406\n",
      "Iteration 9, loss = 0.48060179\n",
      "Iteration 10, loss = 0.47915700\n",
      "Iteration 11, loss = 0.47872845\n",
      "Iteration 12, loss = 0.47566921\n",
      "Iteration 13, loss = 0.47913475\n",
      "Iteration 14, loss = 0.47915648\n",
      "Iteration 15, loss = 0.47376773\n",
      "Iteration 16, loss = 0.47361370\n",
      "Iteration 17, loss = 0.47176279\n",
      "Iteration 18, loss = 0.47126981\n",
      "Iteration 19, loss = 0.47018713\n",
      "Iteration 20, loss = 0.46964576\n",
      "Iteration 21, loss = 0.46953266\n",
      "Iteration 22, loss = 0.46980440\n",
      "Iteration 23, loss = 0.46812497\n",
      "Iteration 24, loss = 0.47081478\n",
      "Iteration 25, loss = 0.46834927\n",
      "Iteration 26, loss = 0.46732673\n",
      "Iteration 27, loss = 0.46596654\n",
      "Iteration 28, loss = 0.46620884\n",
      "Iteration 29, loss = 0.46543373\n",
      "Iteration 30, loss = 0.46806020\n",
      "Iteration 31, loss = 0.47252221\n",
      "Iteration 32, loss = 0.46528801\n",
      "Iteration 33, loss = 0.46474449\n",
      "Iteration 34, loss = 0.46418426\n",
      "Iteration 35, loss = 0.46861823\n",
      "Iteration 36, loss = 0.46751674\n",
      "Iteration 37, loss = 0.46439446\n",
      "Iteration 38, loss = 0.46373617\n",
      "Iteration 39, loss = 0.46448180\n",
      "Iteration 40, loss = 0.46267447\n",
      "Iteration 41, loss = 0.46209248\n",
      "Iteration 42, loss = 0.46811070\n",
      "Iteration 43, loss = 0.46182234\n",
      "Iteration 44, loss = 0.46172251\n",
      "Iteration 45, loss = 0.46323698\n",
      "Iteration 46, loss = 0.46359230\n",
      "Iteration 47, loss = 0.46100483\n",
      "Iteration 48, loss = 0.46331927\n",
      "Iteration 49, loss = 0.46167041\n",
      "Iteration 50, loss = 0.46149835\n",
      "Iteration 51, loss = 0.46456875\n",
      "Iteration 52, loss = 0.46068258\n",
      "Iteration 53, loss = 0.46143616\n",
      "Iteration 54, loss = 0.45929039\n",
      "Iteration 55, loss = 0.46134356\n",
      "Iteration 56, loss = 0.46006544\n",
      "Iteration 57, loss = 0.45948678\n",
      "Iteration 58, loss = 0.46035087\n",
      "Iteration 59, loss = 0.45857925\n",
      "Iteration 60, loss = 0.45931397\n",
      "Iteration 61, loss = 0.46174599\n",
      "Iteration 62, loss = 0.46081022\n",
      "Iteration 63, loss = 0.45845052\n",
      "Iteration 64, loss = 0.46029715\n",
      "Iteration 65, loss = 0.46039735\n",
      "Iteration 66, loss = 0.45943537\n",
      "Iteration 67, loss = 0.46083794\n",
      "Iteration 68, loss = 0.45983390\n",
      "Iteration 69, loss = 0.45926016\n",
      "Iteration 70, loss = 0.45841107\n",
      "Iteration 71, loss = 0.45861729\n",
      "Iteration 72, loss = 0.45881396\n",
      "Iteration 73, loss = 0.45870716\n",
      "Iteration 74, loss = 0.46170351\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55355826\n",
      "Iteration 2, loss = 0.50911988\n",
      "Iteration 3, loss = 0.49208989\n",
      "Iteration 4, loss = 0.48597026\n",
      "Iteration 5, loss = 0.48164938\n",
      "Iteration 6, loss = 0.47831022\n",
      "Iteration 7, loss = 0.47801832\n",
      "Iteration 8, loss = 0.47713691\n",
      "Iteration 9, loss = 0.47595114\n",
      "Iteration 10, loss = 0.47630711\n",
      "Iteration 11, loss = 0.47374985\n",
      "Iteration 12, loss = 0.47209883\n",
      "Iteration 13, loss = 0.47640115\n",
      "Iteration 14, loss = 0.47296723\n",
      "Iteration 15, loss = 0.46972587\n",
      "Iteration 16, loss = 0.46836024\n",
      "Iteration 17, loss = 0.46805521\n",
      "Iteration 18, loss = 0.46848065\n",
      "Iteration 19, loss = 0.46706431\n",
      "Iteration 20, loss = 0.46693861\n",
      "Iteration 21, loss = 0.46984805\n",
      "Iteration 22, loss = 0.47115891\n",
      "Iteration 23, loss = 0.46600845\n",
      "Iteration 24, loss = 0.46866461\n",
      "Iteration 25, loss = 0.46698238\n",
      "Iteration 26, loss = 0.46609247\n",
      "Iteration 27, loss = 0.46518120\n",
      "Iteration 28, loss = 0.46493018\n",
      "Iteration 29, loss = 0.46542700\n",
      "Iteration 30, loss = 0.46334726\n",
      "Iteration 31, loss = 0.46560436\n",
      "Iteration 32, loss = 0.46221680\n",
      "Iteration 33, loss = 0.46332395\n",
      "Iteration 34, loss = 0.46354423\n",
      "Iteration 35, loss = 0.46720340\n",
      "Iteration 36, loss = 0.46288376\n",
      "Iteration 37, loss = 0.46275059\n",
      "Iteration 38, loss = 0.46066449\n",
      "Iteration 39, loss = 0.46342544\n",
      "Iteration 40, loss = 0.46094545\n",
      "Iteration 41, loss = 0.46181699\n",
      "Iteration 42, loss = 0.46209241\n",
      "Iteration 43, loss = 0.46275132\n",
      "Iteration 44, loss = 0.46247018\n",
      "Iteration 45, loss = 0.46153615\n",
      "Iteration 46, loss = 0.45983836\n",
      "Iteration 47, loss = 0.45978262\n",
      "Iteration 48, loss = 0.46290142\n",
      "Iteration 49, loss = 0.46527513\n",
      "Iteration 50, loss = 0.46228175\n",
      "Iteration 51, loss = 0.46083786\n",
      "Iteration 52, loss = 0.45971346\n",
      "Iteration 53, loss = 0.46270635\n",
      "Iteration 54, loss = 0.45984563\n",
      "Iteration 55, loss = 0.46355623\n",
      "Iteration 56, loss = 0.46118978\n",
      "Iteration 57, loss = 0.46230087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56149711\n",
      "Iteration 2, loss = 0.51092798\n",
      "Iteration 3, loss = 0.49576425\n",
      "Iteration 4, loss = 0.48961568\n",
      "Iteration 5, loss = 0.48645478\n",
      "Iteration 6, loss = 0.48458187\n",
      "Iteration 7, loss = 0.48261739\n",
      "Iteration 8, loss = 0.47991240\n",
      "Iteration 9, loss = 0.48002909\n",
      "Iteration 10, loss = 0.47713014\n",
      "Iteration 11, loss = 0.47710765\n",
      "Iteration 12, loss = 0.47597165\n",
      "Iteration 13, loss = 0.47547502\n",
      "Iteration 14, loss = 0.47396678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, loss = 0.47173383\n",
      "Iteration 16, loss = 0.46794077\n",
      "Iteration 17, loss = 0.46694737\n",
      "Iteration 18, loss = 0.46638890\n",
      "Iteration 19, loss = 0.46415044\n",
      "Iteration 20, loss = 0.46284350\n",
      "Iteration 21, loss = 0.46586428\n",
      "Iteration 22, loss = 0.46301507\n",
      "Iteration 23, loss = 0.46147947\n",
      "Iteration 24, loss = 0.46101151\n",
      "Iteration 25, loss = 0.46224632\n",
      "Iteration 26, loss = 0.46023569\n",
      "Iteration 27, loss = 0.45956065\n",
      "Iteration 28, loss = 0.45865384\n",
      "Iteration 29, loss = 0.45965456\n",
      "Iteration 30, loss = 0.45728966\n",
      "Iteration 31, loss = 0.45878994\n",
      "Iteration 32, loss = 0.45625927\n",
      "Iteration 33, loss = 0.45766786\n",
      "Iteration 34, loss = 0.45727067\n",
      "Iteration 35, loss = 0.45788077\n",
      "Iteration 36, loss = 0.45540356\n",
      "Iteration 37, loss = 0.45555514\n",
      "Iteration 38, loss = 0.45565735\n",
      "Iteration 39, loss = 0.45444874\n",
      "Iteration 40, loss = 0.45704001\n",
      "Iteration 41, loss = 0.45551469\n",
      "Iteration 42, loss = 0.45344671\n",
      "Iteration 43, loss = 0.45367086\n",
      "Iteration 44, loss = 0.45288175\n",
      "Iteration 45, loss = 0.45318866\n",
      "Iteration 46, loss = 0.45267635\n",
      "Iteration 47, loss = 0.45252727\n",
      "Iteration 48, loss = 0.45292985\n",
      "Iteration 49, loss = 0.45213635\n",
      "Iteration 50, loss = 0.45262113\n",
      "Iteration 51, loss = 0.45214282\n",
      "Iteration 52, loss = 0.45187920\n",
      "Iteration 53, loss = 0.45809558\n",
      "Iteration 54, loss = 0.45210459\n",
      "Iteration 55, loss = 0.45279944\n",
      "Iteration 56, loss = 0.45285186\n",
      "Iteration 57, loss = 0.45418565\n",
      "Iteration 58, loss = 0.45260025\n",
      "Iteration 59, loss = 0.45300236\n",
      "Iteration 60, loss = 0.45247798\n",
      "Iteration 61, loss = 0.45333590\n",
      "Iteration 62, loss = 0.45144100\n",
      "Iteration 63, loss = 0.45494777\n",
      "Iteration 64, loss = 0.45449087\n",
      "Iteration 65, loss = 0.45087164\n",
      "Iteration 66, loss = 0.45276566\n",
      "Iteration 67, loss = 0.45224782\n",
      "Iteration 68, loss = 0.45213869\n",
      "Iteration 69, loss = 0.45151318\n",
      "Iteration 70, loss = 0.45163847\n",
      "Iteration 71, loss = 0.45151758\n",
      "Iteration 72, loss = 0.45119666\n",
      "Iteration 73, loss = 0.45174493\n",
      "Iteration 74, loss = 0.45390630\n",
      "Iteration 75, loss = 0.45089634\n",
      "Iteration 76, loss = 0.45121248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55585841\n",
      "Iteration 2, loss = 0.50546805\n",
      "Iteration 3, loss = 0.49159082\n",
      "Iteration 4, loss = 0.48248065\n",
      "Iteration 5, loss = 0.47741153\n",
      "Iteration 6, loss = 0.47323056\n",
      "Iteration 7, loss = 0.46944067\n",
      "Iteration 8, loss = 0.46942284\n",
      "Iteration 9, loss = 0.46714187\n",
      "Iteration 10, loss = 0.46358545\n",
      "Iteration 11, loss = 0.46077557\n",
      "Iteration 12, loss = 0.45884092\n",
      "Iteration 13, loss = 0.45609318\n",
      "Iteration 14, loss = 0.45666957\n",
      "Iteration 15, loss = 0.45161861\n",
      "Iteration 16, loss = 0.45098175\n",
      "Iteration 17, loss = 0.44874296\n",
      "Iteration 18, loss = 0.44582280\n",
      "Iteration 19, loss = 0.44351256\n",
      "Iteration 20, loss = 0.44278829\n",
      "Iteration 21, loss = 0.44283147\n",
      "Iteration 22, loss = 0.44134153\n",
      "Iteration 23, loss = 0.43932990\n",
      "Iteration 24, loss = 0.44502509\n",
      "Iteration 25, loss = 0.44016029\n",
      "Iteration 26, loss = 0.43734052\n",
      "Iteration 27, loss = 0.43481867\n",
      "Iteration 28, loss = 0.43358317\n",
      "Iteration 29, loss = 0.43441287\n",
      "Iteration 30, loss = 0.43138410\n",
      "Iteration 31, loss = 0.43037942\n",
      "Iteration 32, loss = 0.42731838\n",
      "Iteration 33, loss = 0.42744975\n",
      "Iteration 34, loss = 0.42505197\n",
      "Iteration 35, loss = 0.42474829\n",
      "Iteration 36, loss = 0.42501095\n",
      "Iteration 37, loss = 0.42819715\n",
      "Iteration 38, loss = 0.42464003\n",
      "Iteration 39, loss = 0.42512472\n",
      "Iteration 40, loss = 0.42141213\n",
      "Iteration 41, loss = 0.42225922\n",
      "Iteration 42, loss = 0.42836635\n",
      "Iteration 43, loss = 0.42094079\n",
      "Iteration 44, loss = 0.41926673\n",
      "Iteration 45, loss = 0.42089113\n",
      "Iteration 46, loss = 0.41977972\n",
      "Iteration 47, loss = 0.41982291\n",
      "Iteration 48, loss = 0.41984650\n",
      "Iteration 49, loss = 0.42065189\n",
      "Iteration 50, loss = 0.42054882\n",
      "Iteration 51, loss = 0.42007535\n",
      "Iteration 52, loss = 0.41957426\n",
      "Iteration 53, loss = 0.41994649\n",
      "Iteration 54, loss = 0.42028426\n",
      "Iteration 55, loss = 0.41987493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55464134\n",
      "Iteration 2, loss = 0.49640648\n",
      "Iteration 3, loss = 0.47752839\n",
      "Iteration 4, loss = 0.46793220\n",
      "Iteration 5, loss = 0.46114510\n",
      "Iteration 6, loss = 0.45665784\n",
      "Iteration 7, loss = 0.45417823\n",
      "Iteration 8, loss = 0.44979875\n",
      "Iteration 9, loss = 0.44686450\n",
      "Iteration 10, loss = 0.44462083\n",
      "Iteration 11, loss = 0.44300588\n",
      "Iteration 12, loss = 0.43971190\n",
      "Iteration 13, loss = 0.44030051\n",
      "Iteration 14, loss = 0.43816977\n",
      "Iteration 15, loss = 0.43776467\n",
      "Iteration 16, loss = 0.43620875\n",
      "Iteration 17, loss = 0.43514145\n",
      "Iteration 18, loss = 0.43361828\n",
      "Iteration 19, loss = 0.43394391\n",
      "Iteration 20, loss = 0.43168214\n",
      "Iteration 21, loss = 0.43226418\n",
      "Iteration 22, loss = 0.43105674\n",
      "Iteration 23, loss = 0.43128222\n",
      "Iteration 24, loss = 0.42758313\n",
      "Iteration 25, loss = 0.42911847\n",
      "Iteration 26, loss = 0.42661518\n",
      "Iteration 27, loss = 0.42694490\n",
      "Iteration 28, loss = 0.42785893\n",
      "Iteration 29, loss = 0.42694520\n",
      "Iteration 30, loss = 0.42585207\n",
      "Iteration 31, loss = 0.42478277\n",
      "Iteration 32, loss = 0.42687092\n",
      "Iteration 33, loss = 0.42711211\n",
      "Iteration 34, loss = 0.42504355\n",
      "Iteration 35, loss = 0.42508021\n",
      "Iteration 36, loss = 0.42707498\n",
      "Iteration 37, loss = 0.42567393\n",
      "Iteration 38, loss = 0.42555679\n",
      "Iteration 39, loss = 0.42419244\n",
      "Iteration 40, loss = 0.42595213\n",
      "Iteration 41, loss = 0.42494014\n",
      "Iteration 42, loss = 0.42255329\n",
      "Iteration 43, loss = 0.42259927\n",
      "Iteration 44, loss = 0.42376856\n",
      "Iteration 45, loss = 0.42657551\n",
      "Iteration 46, loss = 0.42270461\n",
      "Iteration 47, loss = 0.42222960\n",
      "Iteration 48, loss = 0.42398825\n",
      "Iteration 49, loss = 0.42405066\n",
      "Iteration 50, loss = 0.42204795\n",
      "Iteration 51, loss = 0.42178808\n",
      "Iteration 52, loss = 0.42237790\n",
      "Iteration 53, loss = 0.42729013\n",
      "Iteration 54, loss = 0.42259006\n",
      "Iteration 55, loss = 0.42179955\n",
      "Iteration 56, loss = 0.42140518\n",
      "Iteration 57, loss = 0.42200507\n",
      "Iteration 58, loss = 0.42187435\n",
      "Iteration 59, loss = 0.42303558\n",
      "Iteration 60, loss = 0.42144411\n",
      "Iteration 61, loss = 0.42180897\n",
      "Iteration 62, loss = 0.42272271\n",
      "Iteration 63, loss = 0.42277717\n",
      "Iteration 64, loss = 0.42112197\n",
      "Iteration 65, loss = 0.42148515\n",
      "Iteration 66, loss = 0.42059354\n",
      "Iteration 67, loss = 0.42124348\n",
      "Iteration 68, loss = 0.42305227\n",
      "Iteration 69, loss = 0.42385101\n",
      "Iteration 70, loss = 0.41937233\n",
      "Iteration 71, loss = 0.42145697\n",
      "Iteration 72, loss = 0.41971237\n",
      "Iteration 73, loss = 0.42153510\n",
      "Iteration 74, loss = 0.42110267\n",
      "Iteration 75, loss = 0.42017970\n",
      "Iteration 76, loss = 0.42012314\n",
      "Iteration 77, loss = 0.42150008\n",
      "Iteration 78, loss = 0.42204670\n",
      "Iteration 79, loss = 0.42056390\n",
      "Iteration 80, loss = 0.42158894\n",
      "Iteration 81, loss = 0.42034553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54985684\n",
      "Iteration 2, loss = 0.50316137\n",
      "Iteration 3, loss = 0.48571912\n",
      "Iteration 4, loss = 0.47536757\n",
      "Iteration 5, loss = 0.47164200\n",
      "Iteration 6, loss = 0.46867046\n",
      "Iteration 7, loss = 0.46637780\n",
      "Iteration 8, loss = 0.46197855\n",
      "Iteration 9, loss = 0.46461866\n",
      "Iteration 10, loss = 0.46068956\n",
      "Iteration 11, loss = 0.45874395\n",
      "Iteration 12, loss = 0.45536326\n",
      "Iteration 13, loss = 0.45584252\n",
      "Iteration 14, loss = 0.45489637\n",
      "Iteration 15, loss = 0.45510109\n",
      "Iteration 16, loss = 0.45167604\n",
      "Iteration 17, loss = 0.45047618\n",
      "Iteration 18, loss = 0.44732973\n",
      "Iteration 19, loss = 0.44562271\n",
      "Iteration 20, loss = 0.44516782\n",
      "Iteration 21, loss = 0.44356624\n",
      "Iteration 22, loss = 0.44064961\n",
      "Iteration 23, loss = 0.44161548\n",
      "Iteration 24, loss = 0.43763987\n",
      "Iteration 25, loss = 0.43886561\n",
      "Iteration 26, loss = 0.43973915\n",
      "Iteration 27, loss = 0.43837992\n",
      "Iteration 28, loss = 0.43473584\n",
      "Iteration 29, loss = 0.43073539\n",
      "Iteration 30, loss = 0.43030272\n",
      "Iteration 31, loss = 0.43272305\n",
      "Iteration 32, loss = 0.42928810\n",
      "Iteration 33, loss = 0.43056044\n",
      "Iteration 34, loss = 0.42739291\n",
      "Iteration 35, loss = 0.42724321\n",
      "Iteration 36, loss = 0.42715033\n",
      "Iteration 37, loss = 0.42813601\n",
      "Iteration 38, loss = 0.42420077\n",
      "Iteration 39, loss = 0.42561968\n",
      "Iteration 40, loss = 0.42705143\n",
      "Iteration 41, loss = 0.42452278\n",
      "Iteration 42, loss = 0.42456203\n",
      "Iteration 43, loss = 0.42481682\n",
      "Iteration 44, loss = 0.42454656\n",
      "Iteration 45, loss = 0.42404448\n",
      "Iteration 46, loss = 0.42474077\n",
      "Iteration 47, loss = 0.42309893\n",
      "Iteration 48, loss = 0.42293173\n",
      "Iteration 49, loss = 0.42195473\n",
      "Iteration 50, loss = 0.42338829\n",
      "Iteration 51, loss = 0.42503443\n",
      "Iteration 52, loss = 0.42664260\n",
      "Iteration 53, loss = 0.42292706\n",
      "Iteration 54, loss = 0.42189621\n",
      "Iteration 55, loss = 0.42260959\n",
      "Iteration 56, loss = 0.42242193\n",
      "Iteration 57, loss = 0.42466964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 58, loss = 0.42016460\n",
      "Iteration 59, loss = 0.42563792\n",
      "Iteration 60, loss = 0.42160628\n",
      "Iteration 61, loss = 0.42225359\n",
      "Iteration 62, loss = 0.41982434\n",
      "Iteration 63, loss = 0.42031164\n",
      "Iteration 64, loss = 0.42042183\n",
      "Iteration 65, loss = 0.42158033\n",
      "Iteration 66, loss = 0.42194253\n",
      "Iteration 67, loss = 0.42097725\n",
      "Iteration 68, loss = 0.42135886\n",
      "Iteration 69, loss = 0.42208842\n",
      "Iteration 70, loss = 0.42024501\n",
      "Iteration 71, loss = 0.42068312\n",
      "Iteration 72, loss = 0.41879663\n",
      "Iteration 73, loss = 0.41915821\n",
      "Iteration 74, loss = 0.42255647\n",
      "Iteration 75, loss = 0.41850821\n",
      "Iteration 76, loss = 0.41880405\n",
      "Iteration 77, loss = 0.42102446\n",
      "Iteration 78, loss = 0.41887231\n",
      "Iteration 79, loss = 0.41922423\n",
      "Iteration 80, loss = 0.42079963\n",
      "Iteration 81, loss = 0.41779924\n",
      "Iteration 82, loss = 0.41895437\n",
      "Iteration 83, loss = 0.42049931\n",
      "Iteration 84, loss = 0.41928034\n",
      "Iteration 85, loss = 0.41937144\n",
      "Iteration 86, loss = 0.42079280\n",
      "Iteration 87, loss = 0.41853918\n",
      "Iteration 88, loss = 0.41879298\n",
      "Iteration 89, loss = 0.41963376\n",
      "Iteration 90, loss = 0.41922068\n",
      "Iteration 91, loss = 0.41997661\n",
      "Iteration 92, loss = 0.41798147\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55327329\n",
      "Iteration 2, loss = 0.49832969\n",
      "Iteration 3, loss = 0.48355207\n",
      "Iteration 4, loss = 0.47874339\n",
      "Iteration 5, loss = 0.47308811\n",
      "Iteration 6, loss = 0.46882749\n",
      "Iteration 7, loss = 0.46624471\n",
      "Iteration 8, loss = 0.46500355\n",
      "Iteration 9, loss = 0.45739005\n",
      "Iteration 10, loss = 0.44890050\n",
      "Iteration 11, loss = 0.44276862\n",
      "Iteration 12, loss = 0.43905885\n",
      "Iteration 13, loss = 0.43619605\n",
      "Iteration 14, loss = 0.43440106\n",
      "Iteration 15, loss = 0.43527258\n",
      "Iteration 16, loss = 0.43198569\n",
      "Iteration 17, loss = 0.43058468\n",
      "Iteration 18, loss = 0.42523752\n",
      "Iteration 19, loss = 0.42369893\n",
      "Iteration 20, loss = 0.42402027\n",
      "Iteration 21, loss = 0.42409574\n",
      "Iteration 22, loss = 0.42691480\n",
      "Iteration 23, loss = 0.42317895\n",
      "Iteration 24, loss = 0.42227183\n",
      "Iteration 25, loss = 0.42470874\n",
      "Iteration 26, loss = 0.42422508\n",
      "Iteration 27, loss = 0.42083808\n",
      "Iteration 28, loss = 0.42297240\n",
      "Iteration 29, loss = 0.42321361\n",
      "Iteration 30, loss = 0.42324800\n",
      "Iteration 31, loss = 0.42142953\n",
      "Iteration 32, loss = 0.41968515\n",
      "Iteration 33, loss = 0.42075680\n",
      "Iteration 34, loss = 0.42073577\n",
      "Iteration 35, loss = 0.41971234\n",
      "Iteration 36, loss = 0.42037237\n",
      "Iteration 37, loss = 0.42021244\n",
      "Iteration 38, loss = 0.42025164\n",
      "Iteration 39, loss = 0.41925007\n",
      "Iteration 40, loss = 0.41952295\n",
      "Iteration 41, loss = 0.41947066\n",
      "Iteration 42, loss = 0.41957372\n",
      "Iteration 43, loss = 0.41773984\n",
      "Iteration 44, loss = 0.41740731\n",
      "Iteration 45, loss = 0.41903592\n",
      "Iteration 46, loss = 0.41764309\n",
      "Iteration 47, loss = 0.41734311\n",
      "Iteration 48, loss = 0.41907439\n",
      "Iteration 49, loss = 0.41980508\n",
      "Iteration 50, loss = 0.41820675\n",
      "Iteration 51, loss = 0.41968632\n",
      "Iteration 52, loss = 0.41719294\n",
      "Iteration 53, loss = 0.41787377\n",
      "Iteration 54, loss = 0.41764859\n",
      "Iteration 55, loss = 0.42017798\n",
      "Iteration 56, loss = 0.41973291\n",
      "Iteration 57, loss = 0.41740555\n",
      "Iteration 58, loss = 0.41761413\n",
      "Iteration 59, loss = 0.41784111\n",
      "Iteration 60, loss = 0.41981463\n",
      "Iteration 61, loss = 0.41878578\n",
      "Iteration 62, loss = 0.41657634\n",
      "Iteration 63, loss = 0.41673474\n",
      "Iteration 64, loss = 0.41696207\n",
      "Iteration 65, loss = 0.41913166\n",
      "Iteration 66, loss = 0.41971951\n",
      "Iteration 67, loss = 0.42216688\n",
      "Iteration 68, loss = 0.41827927\n",
      "Iteration 69, loss = 0.41802665\n",
      "Iteration 70, loss = 0.42009878\n",
      "Iteration 71, loss = 0.42101388\n",
      "Iteration 72, loss = 0.41952858\n",
      "Iteration 73, loss = 0.41758575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55162842\n",
      "Iteration 2, loss = 0.49267162\n",
      "Iteration 3, loss = 0.47761138\n",
      "Iteration 4, loss = 0.47357329\n",
      "Iteration 5, loss = 0.46838638\n",
      "Iteration 6, loss = 0.46538400\n",
      "Iteration 7, loss = 0.46515873\n",
      "Iteration 8, loss = 0.46336955\n",
      "Iteration 9, loss = 0.46215650\n",
      "Iteration 10, loss = 0.46385676\n",
      "Iteration 11, loss = 0.46044063\n",
      "Iteration 12, loss = 0.45880460\n",
      "Iteration 13, loss = 0.45804103\n",
      "Iteration 14, loss = 0.45701451\n",
      "Iteration 15, loss = 0.45522952\n",
      "Iteration 16, loss = 0.45229328\n",
      "Iteration 17, loss = 0.45535399\n",
      "Iteration 18, loss = 0.44926232\n",
      "Iteration 19, loss = 0.44824167\n",
      "Iteration 20, loss = 0.44523163\n",
      "Iteration 21, loss = 0.44335039\n",
      "Iteration 22, loss = 0.44311642\n",
      "Iteration 23, loss = 0.44096167\n",
      "Iteration 24, loss = 0.44039989\n",
      "Iteration 25, loss = 0.44067687\n",
      "Iteration 26, loss = 0.43935150\n",
      "Iteration 27, loss = 0.43811399\n",
      "Iteration 28, loss = 0.43607775\n",
      "Iteration 29, loss = 0.43734333\n",
      "Iteration 30, loss = 0.44185506\n",
      "Iteration 31, loss = 0.43890510\n",
      "Iteration 32, loss = 0.43361976\n",
      "Iteration 33, loss = 0.43615749\n",
      "Iteration 34, loss = 0.43420238\n",
      "Iteration 35, loss = 0.43746679\n",
      "Iteration 36, loss = 0.44062468\n",
      "Iteration 37, loss = 0.43825644\n",
      "Iteration 38, loss = 0.43308061\n",
      "Iteration 39, loss = 0.43260768\n",
      "Iteration 40, loss = 0.43347452\n",
      "Iteration 41, loss = 0.43329157\n",
      "Iteration 42, loss = 0.43321024\n",
      "Iteration 43, loss = 0.43118479\n",
      "Iteration 44, loss = 0.43146058\n",
      "Iteration 45, loss = 0.43473016\n",
      "Iteration 46, loss = 0.42980762\n",
      "Iteration 47, loss = 0.43058159\n",
      "Iteration 48, loss = 0.43124727\n",
      "Iteration 49, loss = 0.43283375\n",
      "Iteration 50, loss = 0.43109424\n",
      "Iteration 51, loss = 0.42963213\n",
      "Iteration 52, loss = 0.42994745\n",
      "Iteration 53, loss = 0.43243037\n",
      "Iteration 54, loss = 0.42914151\n",
      "Iteration 55, loss = 0.43028444\n",
      "Iteration 56, loss = 0.42987443\n",
      "Iteration 57, loss = 0.42813931\n",
      "Iteration 58, loss = 0.42733072\n",
      "Iteration 59, loss = 0.42904217\n",
      "Iteration 60, loss = 0.42723625\n",
      "Iteration 61, loss = 0.43111858\n",
      "Iteration 62, loss = 0.43100785\n",
      "Iteration 63, loss = 0.42722012\n",
      "Iteration 64, loss = 0.42716803\n",
      "Iteration 65, loss = 0.42959039\n",
      "Iteration 66, loss = 0.42907782\n",
      "Iteration 67, loss = 0.42799734\n",
      "Iteration 68, loss = 0.42761571\n",
      "Iteration 69, loss = 0.42853593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54700724\n",
      "Iteration 2, loss = 0.49285851\n",
      "Iteration 3, loss = 0.47902992\n",
      "Iteration 4, loss = 0.47376380\n",
      "Iteration 5, loss = 0.46848505\n",
      "Iteration 6, loss = 0.46629344\n",
      "Iteration 7, loss = 0.46371715\n",
      "Iteration 8, loss = 0.46167017\n",
      "Iteration 9, loss = 0.46086806\n",
      "Iteration 10, loss = 0.45817208\n",
      "Iteration 11, loss = 0.45813674\n",
      "Iteration 12, loss = 0.45567006\n",
      "Iteration 13, loss = 0.45574499\n",
      "Iteration 14, loss = 0.45238253\n",
      "Iteration 15, loss = 0.45305799\n",
      "Iteration 16, loss = 0.45104226\n",
      "Iteration 17, loss = 0.45112787\n",
      "Iteration 18, loss = 0.44877684\n",
      "Iteration 19, loss = 0.44678362\n",
      "Iteration 20, loss = 0.44373425\n",
      "Iteration 21, loss = 0.44559336\n",
      "Iteration 22, loss = 0.44228097\n",
      "Iteration 23, loss = 0.43886064\n",
      "Iteration 24, loss = 0.43570172\n",
      "Iteration 25, loss = 0.43801584\n",
      "Iteration 26, loss = 0.43778661\n",
      "Iteration 27, loss = 0.43531808\n",
      "Iteration 28, loss = 0.43572128\n",
      "Iteration 29, loss = 0.43296559\n",
      "Iteration 30, loss = 0.43379443\n",
      "Iteration 31, loss = 0.43347861\n",
      "Iteration 32, loss = 0.43194107\n",
      "Iteration 33, loss = 0.43233626\n",
      "Iteration 34, loss = 0.43257715\n",
      "Iteration 35, loss = 0.42968136\n",
      "Iteration 36, loss = 0.43287019\n",
      "Iteration 37, loss = 0.43079323\n",
      "Iteration 38, loss = 0.43039075\n",
      "Iteration 39, loss = 0.42990204\n",
      "Iteration 40, loss = 0.42938538\n",
      "Iteration 41, loss = 0.42954896\n",
      "Iteration 42, loss = 0.42878608\n",
      "Iteration 43, loss = 0.42757672\n",
      "Iteration 44, loss = 0.42738758\n",
      "Iteration 45, loss = 0.42763104\n",
      "Iteration 46, loss = 0.42945232\n",
      "Iteration 47, loss = 0.42733413\n",
      "Iteration 48, loss = 0.42776626\n",
      "Iteration 49, loss = 0.42750685\n",
      "Iteration 50, loss = 0.42596757\n",
      "Iteration 51, loss = 0.42659784\n",
      "Iteration 52, loss = 0.42724975\n",
      "Iteration 53, loss = 0.42802533\n",
      "Iteration 54, loss = 0.42596331\n",
      "Iteration 55, loss = 0.42647336\n",
      "Iteration 56, loss = 0.42755332\n",
      "Iteration 57, loss = 0.42594535\n",
      "Iteration 58, loss = 0.42880760\n",
      "Iteration 59, loss = 0.42631925\n",
      "Iteration 60, loss = 0.42555615\n",
      "Iteration 61, loss = 0.42549026\n",
      "Iteration 62, loss = 0.42475446\n",
      "Iteration 63, loss = 0.42777208\n",
      "Iteration 64, loss = 0.42682616\n",
      "Iteration 65, loss = 0.42657327\n",
      "Iteration 66, loss = 0.42280837\n",
      "Iteration 67, loss = 0.42205192\n",
      "Iteration 68, loss = 0.42210296\n",
      "Iteration 69, loss = 0.42322663\n",
      "Iteration 70, loss = 0.42362476\n",
      "Iteration 71, loss = 0.42224441\n",
      "Iteration 72, loss = 0.42394823\n",
      "Iteration 73, loss = 0.42318258\n",
      "Iteration 74, loss = 0.42100948\n",
      "Iteration 75, loss = 0.42153236\n",
      "Iteration 76, loss = 0.42192402\n",
      "Iteration 77, loss = 0.42504728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78, loss = 0.42119798\n",
      "Iteration 79, loss = 0.42307874\n",
      "Iteration 80, loss = 0.42229761\n",
      "Iteration 81, loss = 0.42241310\n",
      "Iteration 82, loss = 0.42098754\n",
      "Iteration 83, loss = 0.42112831\n",
      "Iteration 84, loss = 0.42311194\n",
      "Iteration 85, loss = 0.42291367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55142912\n",
      "Iteration 2, loss = 0.49191155\n",
      "Iteration 3, loss = 0.47814257\n",
      "Iteration 4, loss = 0.47315580\n",
      "Iteration 5, loss = 0.46758710\n",
      "Iteration 6, loss = 0.46357870\n",
      "Iteration 7, loss = 0.45904836\n",
      "Iteration 8, loss = 0.45786924\n",
      "Iteration 9, loss = 0.45514256\n",
      "Iteration 10, loss = 0.44899753\n",
      "Iteration 11, loss = 0.44737592\n",
      "Iteration 12, loss = 0.44514879\n",
      "Iteration 13, loss = 0.44450500\n",
      "Iteration 14, loss = 0.43929513\n",
      "Iteration 15, loss = 0.43990074\n",
      "Iteration 16, loss = 0.43607412\n",
      "Iteration 17, loss = 0.44046366\n",
      "Iteration 18, loss = 0.43773066\n",
      "Iteration 19, loss = 0.43387997\n",
      "Iteration 20, loss = 0.43282929\n",
      "Iteration 21, loss = 0.43354080\n",
      "Iteration 22, loss = 0.43400518\n",
      "Iteration 23, loss = 0.43242362\n",
      "Iteration 24, loss = 0.43391199\n",
      "Iteration 25, loss = 0.43128432\n",
      "Iteration 26, loss = 0.43125553\n",
      "Iteration 27, loss = 0.42933985\n",
      "Iteration 28, loss = 0.43195928\n",
      "Iteration 29, loss = 0.42898806\n",
      "Iteration 30, loss = 0.43042780\n",
      "Iteration 31, loss = 0.43070540\n",
      "Iteration 32, loss = 0.43013249\n",
      "Iteration 33, loss = 0.42921828\n",
      "Iteration 34, loss = 0.42916490\n",
      "Iteration 35, loss = 0.42716949\n",
      "Iteration 36, loss = 0.42964240\n",
      "Iteration 37, loss = 0.43292777\n",
      "Iteration 38, loss = 0.43071926\n",
      "Iteration 39, loss = 0.42763355\n",
      "Iteration 40, loss = 0.42827597\n",
      "Iteration 41, loss = 0.42684517\n",
      "Iteration 42, loss = 0.42749419\n",
      "Iteration 43, loss = 0.42441820\n",
      "Iteration 44, loss = 0.42481261\n",
      "Iteration 45, loss = 0.42732085\n",
      "Iteration 46, loss = 0.42366854\n",
      "Iteration 47, loss = 0.42462716\n",
      "Iteration 48, loss = 0.42459515\n",
      "Iteration 49, loss = 0.42424752\n",
      "Iteration 50, loss = 0.42586480\n",
      "Iteration 51, loss = 0.42547034\n",
      "Iteration 52, loss = 0.42223721\n",
      "Iteration 53, loss = 0.42245292\n",
      "Iteration 54, loss = 0.42233855\n",
      "Iteration 55, loss = 0.42311159\n",
      "Iteration 56, loss = 0.42315575\n",
      "Iteration 57, loss = 0.42307487\n",
      "Iteration 58, loss = 0.42367044\n",
      "Iteration 59, loss = 0.42404115\n",
      "Iteration 60, loss = 0.42384286\n",
      "Iteration 61, loss = 0.42376087\n",
      "Iteration 62, loss = 0.42090966\n",
      "Iteration 63, loss = 0.42181870\n",
      "Iteration 64, loss = 0.42147353\n",
      "Iteration 65, loss = 0.42434797\n",
      "Iteration 66, loss = 0.42312299\n",
      "Iteration 67, loss = 0.42305483\n",
      "Iteration 68, loss = 0.42165322\n",
      "Iteration 69, loss = 0.42322825\n",
      "Iteration 70, loss = 0.42253771\n",
      "Iteration 71, loss = 0.42102567\n",
      "Iteration 72, loss = 0.42073531\n",
      "Iteration 73, loss = 0.42564011\n",
      "Iteration 74, loss = 0.42096384\n",
      "Iteration 75, loss = 0.41882025\n",
      "Iteration 76, loss = 0.41844675\n",
      "Iteration 77, loss = 0.42106586\n",
      "Iteration 78, loss = 0.41999946\n",
      "Iteration 79, loss = 0.42320445\n",
      "Iteration 80, loss = 0.42000114\n",
      "Iteration 81, loss = 0.42057994\n",
      "Iteration 82, loss = 0.41992054\n",
      "Iteration 83, loss = 0.42307513\n",
      "Iteration 84, loss = 0.42439633\n",
      "Iteration 85, loss = 0.42338430\n",
      "Iteration 86, loss = 0.42039119\n",
      "Iteration 87, loss = 0.42058608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55183771\n",
      "Iteration 2, loss = 0.48699358\n",
      "Iteration 3, loss = 0.46924557\n",
      "Iteration 4, loss = 0.46470560\n",
      "Iteration 5, loss = 0.45931168\n",
      "Iteration 6, loss = 0.45655348\n",
      "Iteration 7, loss = 0.45506712\n",
      "Iteration 8, loss = 0.45103661\n",
      "Iteration 9, loss = 0.45093924\n",
      "Iteration 10, loss = 0.44840056\n",
      "Iteration 11, loss = 0.44712221\n",
      "Iteration 12, loss = 0.44639165\n",
      "Iteration 13, loss = 0.44458091\n",
      "Iteration 14, loss = 0.44378803\n",
      "Iteration 15, loss = 0.44330541\n",
      "Iteration 16, loss = 0.44191282\n",
      "Iteration 17, loss = 0.44355399\n",
      "Iteration 18, loss = 0.44279333\n",
      "Iteration 19, loss = 0.44124594\n",
      "Iteration 20, loss = 0.43943007\n",
      "Iteration 21, loss = 0.44031323\n",
      "Iteration 22, loss = 0.44136667\n",
      "Iteration 23, loss = 0.44010273\n",
      "Iteration 24, loss = 0.43665158\n",
      "Iteration 25, loss = 0.43683421\n",
      "Iteration 26, loss = 0.43624960\n",
      "Iteration 27, loss = 0.43603692\n",
      "Iteration 28, loss = 0.43851243\n",
      "Iteration 29, loss = 0.43575207\n",
      "Iteration 30, loss = 0.43466859\n",
      "Iteration 31, loss = 0.43452910\n",
      "Iteration 32, loss = 0.43529080\n",
      "Iteration 33, loss = 0.43405887\n",
      "Iteration 34, loss = 0.43551697\n",
      "Iteration 35, loss = 0.43454342\n",
      "Iteration 36, loss = 0.43425840\n",
      "Iteration 37, loss = 0.43376296\n",
      "Iteration 38, loss = 0.43320496\n",
      "Iteration 39, loss = 0.43379837\n",
      "Iteration 40, loss = 0.43323916\n",
      "Iteration 41, loss = 0.43263604\n",
      "Iteration 42, loss = 0.43108421\n",
      "Iteration 43, loss = 0.43129878\n",
      "Iteration 44, loss = 0.43209316\n",
      "Iteration 45, loss = 0.43157465\n",
      "Iteration 46, loss = 0.43021737\n",
      "Iteration 47, loss = 0.43149801\n",
      "Iteration 48, loss = 0.43122220\n",
      "Iteration 49, loss = 0.42979584\n",
      "Iteration 50, loss = 0.42981855\n",
      "Iteration 51, loss = 0.42948034\n",
      "Iteration 52, loss = 0.42893648\n",
      "Iteration 53, loss = 0.43116722\n",
      "Iteration 54, loss = 0.42911497\n",
      "Iteration 55, loss = 0.42790468\n",
      "Iteration 56, loss = 0.42668051\n",
      "Iteration 57, loss = 0.42949060\n",
      "Iteration 58, loss = 0.42885579\n",
      "Iteration 59, loss = 0.42554448\n",
      "Iteration 60, loss = 0.42683954\n",
      "Iteration 61, loss = 0.42618603\n",
      "Iteration 62, loss = 0.42688970\n",
      "Iteration 63, loss = 0.42674966\n",
      "Iteration 64, loss = 0.42956225\n",
      "Iteration 65, loss = 0.42629008\n",
      "Iteration 66, loss = 0.42441786\n",
      "Iteration 67, loss = 0.42745297\n",
      "Iteration 68, loss = 0.42791962\n",
      "Iteration 69, loss = 0.42710083\n",
      "Iteration 70, loss = 0.42444531\n",
      "Iteration 71, loss = 0.42566003\n",
      "Iteration 72, loss = 0.42397440\n",
      "Iteration 73, loss = 0.42453348\n",
      "Iteration 74, loss = 0.42401115\n",
      "Iteration 75, loss = 0.42468895\n",
      "Iteration 76, loss = 0.42307401\n",
      "Iteration 77, loss = 0.42446435\n",
      "Iteration 78, loss = 0.42527144\n",
      "Iteration 79, loss = 0.42256132\n",
      "Iteration 80, loss = 0.42394635\n",
      "Iteration 81, loss = 0.42292760\n",
      "Iteration 82, loss = 0.42379962\n",
      "Iteration 83, loss = 0.42139035\n",
      "Iteration 84, loss = 0.42094256\n",
      "Iteration 85, loss = 0.42056799\n",
      "Iteration 86, loss = 0.42109345\n",
      "Iteration 87, loss = 0.41836856\n",
      "Iteration 88, loss = 0.42062869\n",
      "Iteration 89, loss = 0.41878976\n",
      "Iteration 90, loss = 0.41876212\n",
      "Iteration 91, loss = 0.41923639\n",
      "Iteration 92, loss = 0.41839132\n",
      "Iteration 93, loss = 0.41909229\n",
      "Iteration 94, loss = 0.41974066\n",
      "Iteration 95, loss = 0.42012297\n",
      "Iteration 96, loss = 0.41700080\n",
      "Iteration 97, loss = 0.41621243\n",
      "Iteration 98, loss = 0.41700501\n",
      "Iteration 99, loss = 0.41766488\n",
      "Iteration 100, loss = 0.41652818\n",
      "Iteration 101, loss = 0.41858475\n",
      "Iteration 102, loss = 0.41664258\n",
      "Iteration 103, loss = 0.41493631\n",
      "Iteration 104, loss = 0.41615227\n",
      "Iteration 105, loss = 0.41659637\n",
      "Iteration 106, loss = 0.41584012\n",
      "Iteration 107, loss = 0.41559555\n",
      "Iteration 108, loss = 0.41602929\n",
      "Iteration 109, loss = 0.41532249\n",
      "Iteration 110, loss = 0.41481280\n",
      "Iteration 111, loss = 0.41609754\n",
      "Iteration 112, loss = 0.41461899\n",
      "Iteration 113, loss = 0.41482109\n",
      "Iteration 114, loss = 0.41942591\n",
      "Iteration 115, loss = 0.41575383\n",
      "Iteration 116, loss = 0.41589800\n",
      "Iteration 117, loss = 0.41818974\n",
      "Iteration 118, loss = 0.41328795\n",
      "Iteration 119, loss = 0.41427039\n",
      "Iteration 120, loss = 0.41469271\n",
      "Iteration 121, loss = 0.41456612\n",
      "Iteration 122, loss = 0.41927111\n",
      "Iteration 123, loss = 0.41766414\n",
      "Iteration 124, loss = 0.41824033\n",
      "Iteration 125, loss = 0.41498673\n",
      "Iteration 126, loss = 0.41888912\n",
      "Iteration 127, loss = 0.41608946\n",
      "Iteration 128, loss = 0.41488074\n",
      "Iteration 129, loss = 0.41640654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54636450\n",
      "Iteration 2, loss = 0.49240820\n",
      "Iteration 3, loss = 0.47944171\n",
      "Iteration 4, loss = 0.47610441\n",
      "Iteration 5, loss = 0.47109122\n",
      "Iteration 6, loss = 0.46946794\n",
      "Iteration 7, loss = 0.46787889\n",
      "Iteration 8, loss = 0.46509742\n",
      "Iteration 9, loss = 0.46582314\n",
      "Iteration 10, loss = 0.46058729\n",
      "Iteration 11, loss = 0.45942592\n",
      "Iteration 12, loss = 0.45487423\n",
      "Iteration 13, loss = 0.45505588\n",
      "Iteration 14, loss = 0.45094363\n",
      "Iteration 15, loss = 0.45093871\n",
      "Iteration 16, loss = 0.45071863\n",
      "Iteration 17, loss = 0.44987265\n",
      "Iteration 18, loss = 0.44844976\n",
      "Iteration 19, loss = 0.44468041\n",
      "Iteration 20, loss = 0.44070543\n",
      "Iteration 21, loss = 0.43971824\n",
      "Iteration 22, loss = 0.43720343\n",
      "Iteration 23, loss = 0.43232835\n",
      "Iteration 24, loss = 0.43007344\n",
      "Iteration 25, loss = 0.42924178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 26, loss = 0.42886247\n",
      "Iteration 27, loss = 0.42880405\n",
      "Iteration 28, loss = 0.42672650\n",
      "Iteration 29, loss = 0.42568554\n",
      "Iteration 30, loss = 0.42616947\n",
      "Iteration 31, loss = 0.42577958\n",
      "Iteration 32, loss = 0.42570282\n",
      "Iteration 33, loss = 0.42650647\n",
      "Iteration 34, loss = 0.42418824\n",
      "Iteration 35, loss = 0.42284286\n",
      "Iteration 36, loss = 0.42224182\n",
      "Iteration 37, loss = 0.42417288\n",
      "Iteration 38, loss = 0.42128762\n",
      "Iteration 39, loss = 0.42310938\n",
      "Iteration 40, loss = 0.42163977\n",
      "Iteration 41, loss = 0.41972875\n",
      "Iteration 42, loss = 0.41991483\n",
      "Iteration 43, loss = 0.41797055\n",
      "Iteration 44, loss = 0.42026863\n",
      "Iteration 45, loss = 0.42045257\n",
      "Iteration 46, loss = 0.41741940\n",
      "Iteration 47, loss = 0.41658717\n",
      "Iteration 48, loss = 0.42026533\n",
      "Iteration 49, loss = 0.41918449\n",
      "Iteration 50, loss = 0.41644722\n",
      "Iteration 51, loss = 0.41845880\n",
      "Iteration 52, loss = 0.41949673\n",
      "Iteration 53, loss = 0.42010473\n",
      "Iteration 54, loss = 0.41859886\n",
      "Iteration 55, loss = 0.41925326\n",
      "Iteration 56, loss = 0.41578051\n",
      "Iteration 57, loss = 0.42043860\n",
      "Iteration 58, loss = 0.41776773\n",
      "Iteration 59, loss = 0.41900489\n",
      "Iteration 60, loss = 0.41698317\n",
      "Iteration 61, loss = 0.42093590\n",
      "Iteration 62, loss = 0.41847952\n",
      "Iteration 63, loss = 0.41712031\n",
      "Iteration 64, loss = 0.41590720\n",
      "Iteration 65, loss = 0.41608205\n",
      "Iteration 66, loss = 0.41431891\n",
      "Iteration 67, loss = 0.41507995\n",
      "Iteration 68, loss = 0.41602633\n",
      "Iteration 69, loss = 0.41812354\n",
      "Iteration 70, loss = 0.41520916\n",
      "Iteration 71, loss = 0.41376455\n",
      "Iteration 72, loss = 0.41506445\n",
      "Iteration 73, loss = 0.41669688\n",
      "Iteration 74, loss = 0.41758976\n",
      "Iteration 75, loss = 0.41566168\n",
      "Iteration 76, loss = 0.41537971\n",
      "Iteration 77, loss = 0.41814370\n",
      "Iteration 78, loss = 0.41649381\n",
      "Iteration 79, loss = 0.41716067\n",
      "Iteration 80, loss = 0.41742273\n",
      "Iteration 81, loss = 0.41742841\n",
      "Iteration 82, loss = 0.41468898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52184158\n",
      "Iteration 2, loss = 0.48185285\n",
      "Iteration 3, loss = 0.47656368\n",
      "Iteration 4, loss = 0.46859494\n",
      "Iteration 5, loss = 0.46389008\n",
      "Iteration 6, loss = 0.46203137\n",
      "Iteration 7, loss = 0.45838102\n",
      "Iteration 8, loss = 0.45860299\n",
      "Iteration 9, loss = 0.45536921\n",
      "Iteration 10, loss = 0.45519936\n",
      "Iteration 11, loss = 0.45441319\n",
      "Iteration 12, loss = 0.45430102\n",
      "Iteration 13, loss = 0.45597058\n",
      "Iteration 14, loss = 0.45557502\n",
      "Iteration 15, loss = 0.45602035\n",
      "Iteration 16, loss = 0.45564647\n",
      "Iteration 17, loss = 0.45507778\n",
      "Iteration 18, loss = 0.45196707\n",
      "Iteration 19, loss = 0.45218933\n",
      "Iteration 20, loss = 0.45129467\n",
      "Iteration 21, loss = 0.45226123\n",
      "Iteration 22, loss = 0.45174781\n",
      "Iteration 23, loss = 0.45211864\n",
      "Iteration 24, loss = 0.45443204\n",
      "Iteration 25, loss = 0.45143929\n",
      "Iteration 26, loss = 0.45075721\n",
      "Iteration 27, loss = 0.45220501\n",
      "Iteration 28, loss = 0.45257330\n",
      "Iteration 29, loss = 0.45198670\n",
      "Iteration 30, loss = 0.45287797\n",
      "Iteration 31, loss = 0.45210897\n",
      "Iteration 32, loss = 0.45070752\n",
      "Iteration 33, loss = 0.45118419\n",
      "Iteration 34, loss = 0.45034383\n",
      "Iteration 35, loss = 0.45054771\n",
      "Iteration 36, loss = 0.45076282\n",
      "Iteration 37, loss = 0.45324340\n",
      "Iteration 38, loss = 0.45325163\n",
      "Iteration 39, loss = 0.45032609\n",
      "Iteration 40, loss = 0.45034021\n",
      "Iteration 41, loss = 0.44906428\n",
      "Iteration 42, loss = 0.45380898\n",
      "Iteration 43, loss = 0.44967810\n",
      "Iteration 44, loss = 0.44904350\n",
      "Iteration 45, loss = 0.45209199\n",
      "Iteration 46, loss = 0.44869741\n",
      "Iteration 47, loss = 0.44871750\n",
      "Iteration 48, loss = 0.44900142\n",
      "Iteration 49, loss = 0.44858877\n",
      "Iteration 50, loss = 0.44889146\n",
      "Iteration 51, loss = 0.44773871\n",
      "Iteration 52, loss = 0.44793707\n",
      "Iteration 53, loss = 0.44806337\n",
      "Iteration 54, loss = 0.44900408\n",
      "Iteration 55, loss = 0.44890113\n",
      "Iteration 56, loss = 0.44758800\n",
      "Iteration 57, loss = 0.44819942\n",
      "Iteration 58, loss = 0.44740985\n",
      "Iteration 59, loss = 0.44641203\n",
      "Iteration 60, loss = 0.44701912\n",
      "Iteration 61, loss = 0.44752075\n",
      "Iteration 62, loss = 0.44777955\n",
      "Iteration 63, loss = 0.44655906\n",
      "Iteration 64, loss = 0.44777337\n",
      "Iteration 65, loss = 0.44788454\n",
      "Iteration 66, loss = 0.44736807\n",
      "Iteration 67, loss = 0.44880384\n",
      "Iteration 68, loss = 0.44682159\n",
      "Iteration 69, loss = 0.44633072\n",
      "Iteration 70, loss = 0.44601824\n",
      "Iteration 71, loss = 0.44681089\n",
      "Iteration 72, loss = 0.44706736\n",
      "Iteration 73, loss = 0.44629529\n",
      "Iteration 74, loss = 0.44611715\n",
      "Iteration 75, loss = 0.44561233\n",
      "Iteration 76, loss = 0.44529798\n",
      "Iteration 77, loss = 0.44567760\n",
      "Iteration 78, loss = 0.44599814\n",
      "Iteration 79, loss = 0.44787236\n",
      "Iteration 80, loss = 0.44692897\n",
      "Iteration 81, loss = 0.44556118\n",
      "Iteration 82, loss = 0.44581553\n",
      "Iteration 83, loss = 0.44617040\n",
      "Iteration 84, loss = 0.44657788\n",
      "Iteration 85, loss = 0.44785052\n",
      "Iteration 86, loss = 0.44577700\n",
      "Iteration 87, loss = 0.44559674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52330939\n",
      "Iteration 2, loss = 0.47882672\n",
      "Iteration 3, loss = 0.47539367\n",
      "Iteration 4, loss = 0.47434035\n",
      "Iteration 5, loss = 0.46970818\n",
      "Iteration 6, loss = 0.46532800\n",
      "Iteration 7, loss = 0.46365239\n",
      "Iteration 8, loss = 0.46159658\n",
      "Iteration 9, loss = 0.45749909\n",
      "Iteration 10, loss = 0.45863432\n",
      "Iteration 11, loss = 0.45630626\n",
      "Iteration 12, loss = 0.45715953\n",
      "Iteration 13, loss = 0.45699122\n",
      "Iteration 14, loss = 0.45531993\n",
      "Iteration 15, loss = 0.45580939\n",
      "Iteration 16, loss = 0.45553441\n",
      "Iteration 17, loss = 0.45660374\n",
      "Iteration 18, loss = 0.45383982\n",
      "Iteration 19, loss = 0.45432432\n",
      "Iteration 20, loss = 0.45365967\n",
      "Iteration 21, loss = 0.45471081\n",
      "Iteration 22, loss = 0.45367623\n",
      "Iteration 23, loss = 0.45430716\n",
      "Iteration 24, loss = 0.45354670\n",
      "Iteration 25, loss = 0.45419020\n",
      "Iteration 26, loss = 0.45254851\n",
      "Iteration 27, loss = 0.45242942\n",
      "Iteration 28, loss = 0.45307868\n",
      "Iteration 29, loss = 0.45526625\n",
      "Iteration 30, loss = 0.45412709\n",
      "Iteration 31, loss = 0.45328583\n",
      "Iteration 32, loss = 0.45397923\n",
      "Iteration 33, loss = 0.45707725\n",
      "Iteration 34, loss = 0.45324170\n",
      "Iteration 35, loss = 0.45353965\n",
      "Iteration 36, loss = 0.45571067\n",
      "Iteration 37, loss = 0.45365067\n",
      "Iteration 38, loss = 0.45217977\n",
      "Iteration 39, loss = 0.45226330\n",
      "Iteration 40, loss = 0.45315355\n",
      "Iteration 41, loss = 0.45352818\n",
      "Iteration 42, loss = 0.45214486\n",
      "Iteration 43, loss = 0.45246916\n",
      "Iteration 44, loss = 0.45141720\n",
      "Iteration 45, loss = 0.45348735\n",
      "Iteration 46, loss = 0.45140299\n",
      "Iteration 47, loss = 0.45106630\n",
      "Iteration 48, loss = 0.45126278\n",
      "Iteration 49, loss = 0.45209444\n",
      "Iteration 50, loss = 0.45060752\n",
      "Iteration 51, loss = 0.45060217\n",
      "Iteration 52, loss = 0.45160504\n",
      "Iteration 53, loss = 0.45483115\n",
      "Iteration 54, loss = 0.45054195\n",
      "Iteration 55, loss = 0.45177260\n",
      "Iteration 56, loss = 0.45182065\n",
      "Iteration 57, loss = 0.44969222\n",
      "Iteration 58, loss = 0.44982853\n",
      "Iteration 59, loss = 0.45029769\n",
      "Iteration 60, loss = 0.44899743\n",
      "Iteration 61, loss = 0.45052302\n",
      "Iteration 62, loss = 0.44951709\n",
      "Iteration 63, loss = 0.45134471\n",
      "Iteration 64, loss = 0.45116110\n",
      "Iteration 65, loss = 0.45088653\n",
      "Iteration 66, loss = 0.44944554\n",
      "Iteration 67, loss = 0.44931162\n",
      "Iteration 68, loss = 0.44889258\n",
      "Iteration 69, loss = 0.45219557\n",
      "Iteration 70, loss = 0.44855751\n",
      "Iteration 71, loss = 0.44840325\n",
      "Iteration 72, loss = 0.44837358\n",
      "Iteration 73, loss = 0.45009248\n",
      "Iteration 74, loss = 0.44781232\n",
      "Iteration 75, loss = 0.44780332\n",
      "Iteration 76, loss = 0.44863864\n",
      "Iteration 77, loss = 0.44758673\n",
      "Iteration 78, loss = 0.44767641\n",
      "Iteration 79, loss = 0.44898204\n",
      "Iteration 80, loss = 0.44877196\n",
      "Iteration 81, loss = 0.44823404\n",
      "Iteration 82, loss = 0.44830355\n",
      "Iteration 83, loss = 0.44901828\n",
      "Iteration 84, loss = 0.45105184\n",
      "Iteration 85, loss = 0.44883641\n",
      "Iteration 86, loss = 0.44803783\n",
      "Iteration 87, loss = 0.44828801\n",
      "Iteration 88, loss = 0.44885234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52190677\n",
      "Iteration 2, loss = 0.47993171\n",
      "Iteration 3, loss = 0.47637409\n",
      "Iteration 4, loss = 0.47132123\n",
      "Iteration 5, loss = 0.46569621\n",
      "Iteration 6, loss = 0.46333670\n",
      "Iteration 7, loss = 0.46096212\n",
      "Iteration 8, loss = 0.45883047\n",
      "Iteration 9, loss = 0.45812656\n",
      "Iteration 10, loss = 0.45663300\n",
      "Iteration 11, loss = 0.45607739\n",
      "Iteration 12, loss = 0.45492189\n",
      "Iteration 13, loss = 0.45730806\n",
      "Iteration 14, loss = 0.45381557\n",
      "Iteration 15, loss = 0.45646928\n",
      "Iteration 16, loss = 0.45407826\n",
      "Iteration 17, loss = 0.45654028\n",
      "Iteration 18, loss = 0.45362586\n",
      "Iteration 19, loss = 0.45352704\n",
      "Iteration 20, loss = 0.45389363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, loss = 0.45588187\n",
      "Iteration 22, loss = 0.45272693\n",
      "Iteration 23, loss = 0.45218012\n",
      "Iteration 24, loss = 0.45246272\n",
      "Iteration 25, loss = 0.45283926\n",
      "Iteration 26, loss = 0.45288262\n",
      "Iteration 27, loss = 0.45387399\n",
      "Iteration 28, loss = 0.45282528\n",
      "Iteration 29, loss = 0.45339323\n",
      "Iteration 30, loss = 0.45268825\n",
      "Iteration 31, loss = 0.45306800\n",
      "Iteration 32, loss = 0.45190829\n",
      "Iteration 33, loss = 0.45197494\n",
      "Iteration 34, loss = 0.45198689\n",
      "Iteration 35, loss = 0.45269120\n",
      "Iteration 36, loss = 0.45205206\n",
      "Iteration 37, loss = 0.45265117\n",
      "Iteration 38, loss = 0.45160533\n",
      "Iteration 39, loss = 0.45211914\n",
      "Iteration 40, loss = 0.45179114\n",
      "Iteration 41, loss = 0.45022759\n",
      "Iteration 42, loss = 0.45109648\n",
      "Iteration 43, loss = 0.45026616\n",
      "Iteration 44, loss = 0.45013039\n",
      "Iteration 45, loss = 0.45035477\n",
      "Iteration 46, loss = 0.44948423\n",
      "Iteration 47, loss = 0.44955041\n",
      "Iteration 48, loss = 0.44987875\n",
      "Iteration 49, loss = 0.45020166\n",
      "Iteration 50, loss = 0.44935159\n",
      "Iteration 51, loss = 0.45074626\n",
      "Iteration 52, loss = 0.44972663\n",
      "Iteration 53, loss = 0.45086753\n",
      "Iteration 54, loss = 0.44977209\n",
      "Iteration 55, loss = 0.45047070\n",
      "Iteration 56, loss = 0.45154048\n",
      "Iteration 57, loss = 0.45191895\n",
      "Iteration 58, loss = 0.44950121\n",
      "Iteration 59, loss = 0.45191440\n",
      "Iteration 60, loss = 0.45213206\n",
      "Iteration 61, loss = 0.45070451\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54994549\n",
      "Iteration 2, loss = 0.51103351\n",
      "Iteration 3, loss = 0.49735396\n",
      "Iteration 4, loss = 0.49422826\n",
      "Iteration 5, loss = 0.49008501\n",
      "Iteration 6, loss = 0.48795661\n",
      "Iteration 7, loss = 0.48866751\n",
      "Iteration 8, loss = 0.49079887\n",
      "Iteration 9, loss = 0.48811836\n",
      "Iteration 10, loss = 0.48558952\n",
      "Iteration 11, loss = 0.48288920\n",
      "Iteration 12, loss = 0.47678038\n",
      "Iteration 13, loss = 0.47505729\n",
      "Iteration 14, loss = 0.47320378\n",
      "Iteration 15, loss = 0.47007606\n",
      "Iteration 16, loss = 0.46905837\n",
      "Iteration 17, loss = 0.47098659\n",
      "Iteration 18, loss = 0.46714033\n",
      "Iteration 19, loss = 0.46403755\n",
      "Iteration 20, loss = 0.46328498\n",
      "Iteration 21, loss = 0.46257264\n",
      "Iteration 22, loss = 0.46108553\n",
      "Iteration 23, loss = 0.45604689\n",
      "Iteration 24, loss = 0.45501521\n",
      "Iteration 25, loss = 0.45076376\n",
      "Iteration 26, loss = 0.45091797\n",
      "Iteration 27, loss = 0.44650354\n",
      "Iteration 28, loss = 0.44776760\n",
      "Iteration 29, loss = 0.44324039\n",
      "Iteration 30, loss = 0.44441720\n",
      "Iteration 31, loss = 0.44664361\n",
      "Iteration 32, loss = 0.43951665\n",
      "Iteration 33, loss = 0.43835627\n",
      "Iteration 34, loss = 0.44010036\n",
      "Iteration 35, loss = 0.43903698\n",
      "Iteration 36, loss = 0.43948384\n",
      "Iteration 37, loss = 0.43611101\n",
      "Iteration 38, loss = 0.43814678\n",
      "Iteration 39, loss = 0.43830083\n",
      "Iteration 40, loss = 0.43528969\n",
      "Iteration 41, loss = 0.43387742\n",
      "Iteration 42, loss = 0.43706247\n",
      "Iteration 43, loss = 0.43569758\n",
      "Iteration 44, loss = 0.43320771\n",
      "Iteration 45, loss = 0.43502695\n",
      "Iteration 46, loss = 0.43753633\n",
      "Iteration 47, loss = 0.43325145\n",
      "Iteration 48, loss = 0.43313932\n",
      "Iteration 49, loss = 0.43230538\n",
      "Iteration 50, loss = 0.43293561\n",
      "Iteration 51, loss = 0.43429533\n",
      "Iteration 52, loss = 0.43090333\n",
      "Iteration 53, loss = 0.43348768\n",
      "Iteration 54, loss = 0.43493001\n",
      "Iteration 55, loss = 0.43122328\n",
      "Iteration 56, loss = 0.43081216\n",
      "Iteration 57, loss = 0.42968854\n",
      "Iteration 58, loss = 0.43254329\n",
      "Iteration 59, loss = 0.42811861\n",
      "Iteration 60, loss = 0.42934082\n",
      "Iteration 61, loss = 0.43118842\n",
      "Iteration 62, loss = 0.43016839\n",
      "Iteration 63, loss = 0.42908682\n",
      "Iteration 64, loss = 0.42614232\n",
      "Iteration 65, loss = 0.42436707\n",
      "Iteration 66, loss = 0.42555036\n",
      "Iteration 67, loss = 0.42746209\n",
      "Iteration 68, loss = 0.42724815\n",
      "Iteration 69, loss = 0.42543906\n",
      "Iteration 70, loss = 0.42560439\n",
      "Iteration 71, loss = 0.42717357\n",
      "Iteration 72, loss = 0.42427138\n",
      "Iteration 73, loss = 0.42474620\n",
      "Iteration 74, loss = 0.42946397\n",
      "Iteration 75, loss = 0.42534952\n",
      "Iteration 76, loss = 0.42312972\n",
      "Iteration 77, loss = 0.42360364\n",
      "Iteration 78, loss = 0.42384019\n",
      "Iteration 79, loss = 0.42352749\n",
      "Iteration 80, loss = 0.42266986\n",
      "Iteration 81, loss = 0.42176219\n",
      "Iteration 82, loss = 0.42346842\n",
      "Iteration 83, loss = 0.42178688\n",
      "Iteration 84, loss = 0.42187871\n",
      "Iteration 85, loss = 0.42472557\n",
      "Iteration 86, loss = 0.42378527\n",
      "Iteration 87, loss = 0.42201179\n",
      "Iteration 88, loss = 0.42392314\n",
      "Iteration 89, loss = 0.42202304\n",
      "Iteration 90, loss = 0.42125786\n",
      "Iteration 91, loss = 0.42119331\n",
      "Iteration 92, loss = 0.42135788\n",
      "Iteration 93, loss = 0.42021157\n",
      "Iteration 94, loss = 0.41989336\n",
      "Iteration 95, loss = 0.42272958\n",
      "Iteration 96, loss = 0.42487896\n",
      "Iteration 97, loss = 0.42108769\n",
      "Iteration 98, loss = 0.42726454\n",
      "Iteration 99, loss = 0.42202067\n",
      "Iteration 100, loss = 0.41743507\n",
      "Iteration 101, loss = 0.41927286\n",
      "Iteration 102, loss = 0.42087698\n",
      "Iteration 103, loss = 0.42237914\n",
      "Iteration 104, loss = 0.41831591\n",
      "Iteration 105, loss = 0.41996202\n",
      "Iteration 106, loss = 0.41743905\n",
      "Iteration 107, loss = 0.42197249\n",
      "Iteration 108, loss = 0.42612456\n",
      "Iteration 109, loss = 0.41904648\n",
      "Iteration 110, loss = 0.42082519\n",
      "Iteration 111, loss = 0.41751168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54455377\n",
      "Iteration 2, loss = 0.50505944\n",
      "Iteration 3, loss = 0.49173347\n",
      "Iteration 4, loss = 0.49003226\n",
      "Iteration 5, loss = 0.48634239\n",
      "Iteration 6, loss = 0.48355582\n",
      "Iteration 7, loss = 0.48305899\n",
      "Iteration 8, loss = 0.48158480\n",
      "Iteration 9, loss = 0.48004922\n",
      "Iteration 10, loss = 0.47950033\n",
      "Iteration 11, loss = 0.48023542\n",
      "Iteration 12, loss = 0.47800040\n",
      "Iteration 13, loss = 0.47722586\n",
      "Iteration 14, loss = 0.47165196\n",
      "Iteration 15, loss = 0.46557875\n",
      "Iteration 16, loss = 0.46274611\n",
      "Iteration 17, loss = 0.46182107\n",
      "Iteration 18, loss = 0.46006659\n",
      "Iteration 19, loss = 0.45431713\n",
      "Iteration 20, loss = 0.45416951\n",
      "Iteration 21, loss = 0.45395006\n",
      "Iteration 22, loss = 0.45114784\n",
      "Iteration 23, loss = 0.44789084\n",
      "Iteration 24, loss = 0.44577604\n",
      "Iteration 25, loss = 0.44762806\n",
      "Iteration 26, loss = 0.44509577\n",
      "Iteration 27, loss = 0.44372589\n",
      "Iteration 28, loss = 0.44067657\n",
      "Iteration 29, loss = 0.44014367\n",
      "Iteration 30, loss = 0.43720653\n",
      "Iteration 31, loss = 0.43945877\n",
      "Iteration 32, loss = 0.43977115\n",
      "Iteration 33, loss = 0.43548462\n",
      "Iteration 34, loss = 0.43853807\n",
      "Iteration 35, loss = 0.44077142\n",
      "Iteration 36, loss = 0.43726960\n",
      "Iteration 37, loss = 0.43649397\n",
      "Iteration 38, loss = 0.43166626\n",
      "Iteration 39, loss = 0.43225949\n",
      "Iteration 40, loss = 0.43215062\n",
      "Iteration 41, loss = 0.43131555\n",
      "Iteration 42, loss = 0.43151388\n",
      "Iteration 43, loss = 0.42874877\n",
      "Iteration 44, loss = 0.43068464\n",
      "Iteration 45, loss = 0.43199456\n",
      "Iteration 46, loss = 0.42951651\n",
      "Iteration 47, loss = 0.42969923\n",
      "Iteration 48, loss = 0.42910241\n",
      "Iteration 49, loss = 0.42868162\n",
      "Iteration 50, loss = 0.42668222\n",
      "Iteration 51, loss = 0.42713174\n",
      "Iteration 52, loss = 0.42558987\n",
      "Iteration 53, loss = 0.42812983\n",
      "Iteration 54, loss = 0.42564519\n",
      "Iteration 55, loss = 0.42501642\n",
      "Iteration 56, loss = 0.42735707\n",
      "Iteration 57, loss = 0.42546873\n",
      "Iteration 58, loss = 0.42547011\n",
      "Iteration 59, loss = 0.42567821\n",
      "Iteration 60, loss = 0.42679868\n",
      "Iteration 61, loss = 0.42834338\n",
      "Iteration 62, loss = 0.42467728\n",
      "Iteration 63, loss = 0.42463457\n",
      "Iteration 64, loss = 0.42575935\n",
      "Iteration 65, loss = 0.42483461\n",
      "Iteration 66, loss = 0.42474801\n",
      "Iteration 67, loss = 0.42419007\n",
      "Iteration 68, loss = 0.42843986\n",
      "Iteration 69, loss = 0.42693461\n",
      "Iteration 70, loss = 0.42370463\n",
      "Iteration 71, loss = 0.42498114\n",
      "Iteration 72, loss = 0.42244254\n",
      "Iteration 73, loss = 0.42340387\n",
      "Iteration 74, loss = 0.42491856\n",
      "Iteration 75, loss = 0.42370113\n",
      "Iteration 76, loss = 0.42334999\n",
      "Iteration 77, loss = 0.42623405\n",
      "Iteration 78, loss = 0.42395400\n",
      "Iteration 79, loss = 0.42398739\n",
      "Iteration 80, loss = 0.42392809\n",
      "Iteration 81, loss = 0.42408666\n",
      "Iteration 82, loss = 0.42356982\n",
      "Iteration 83, loss = 0.42292679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55397292\n",
      "Iteration 2, loss = 0.51145154\n",
      "Iteration 3, loss = 0.49652497\n",
      "Iteration 4, loss = 0.48800554\n",
      "Iteration 5, loss = 0.48630249\n",
      "Iteration 6, loss = 0.48725779\n",
      "Iteration 7, loss = 0.48451492\n",
      "Iteration 8, loss = 0.48297206\n",
      "Iteration 9, loss = 0.48444710\n",
      "Iteration 10, loss = 0.48163822\n",
      "Iteration 11, loss = 0.48321523\n",
      "Iteration 12, loss = 0.48073840\n",
      "Iteration 13, loss = 0.47982701\n",
      "Iteration 14, loss = 0.47870244\n",
      "Iteration 15, loss = 0.47831454\n",
      "Iteration 16, loss = 0.47615005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.47328447\n",
      "Iteration 18, loss = 0.46473730\n",
      "Iteration 19, loss = 0.45947257\n",
      "Iteration 20, loss = 0.45556191\n",
      "Iteration 21, loss = 0.45295097\n",
      "Iteration 22, loss = 0.45019825\n",
      "Iteration 23, loss = 0.44660236\n",
      "Iteration 24, loss = 0.44434092\n",
      "Iteration 25, loss = 0.44427119\n",
      "Iteration 26, loss = 0.44158062\n",
      "Iteration 27, loss = 0.44261733\n",
      "Iteration 28, loss = 0.44161275\n",
      "Iteration 29, loss = 0.43828609\n",
      "Iteration 30, loss = 0.43615351\n",
      "Iteration 31, loss = 0.43812507\n",
      "Iteration 32, loss = 0.43812148\n",
      "Iteration 33, loss = 0.43434208\n",
      "Iteration 34, loss = 0.43541175\n",
      "Iteration 35, loss = 0.43554830\n",
      "Iteration 36, loss = 0.43309952\n",
      "Iteration 37, loss = 0.43443191\n",
      "Iteration 38, loss = 0.43359468\n",
      "Iteration 39, loss = 0.43134485\n",
      "Iteration 40, loss = 0.43307779\n",
      "Iteration 41, loss = 0.43132530\n",
      "Iteration 42, loss = 0.43082954\n",
      "Iteration 43, loss = 0.43079900\n",
      "Iteration 44, loss = 0.43020939\n",
      "Iteration 45, loss = 0.43145258\n",
      "Iteration 46, loss = 0.43085633\n",
      "Iteration 47, loss = 0.43169289\n",
      "Iteration 48, loss = 0.43083380\n",
      "Iteration 49, loss = 0.43109624\n",
      "Iteration 50, loss = 0.43027195\n",
      "Iteration 51, loss = 0.42957522\n",
      "Iteration 52, loss = 0.43125955\n",
      "Iteration 53, loss = 0.43462861\n",
      "Iteration 54, loss = 0.43306812\n",
      "Iteration 55, loss = 0.43021654\n",
      "Iteration 56, loss = 0.42854340\n",
      "Iteration 57, loss = 0.42894314\n",
      "Iteration 58, loss = 0.42955078\n",
      "Iteration 59, loss = 0.42717957\n",
      "Iteration 60, loss = 0.42991513\n",
      "Iteration 61, loss = 0.43725305\n",
      "Iteration 62, loss = 0.43166823\n",
      "Iteration 63, loss = 0.42778000\n",
      "Iteration 64, loss = 0.42613841\n",
      "Iteration 65, loss = 0.42643610\n",
      "Iteration 66, loss = 0.42729732\n",
      "Iteration 67, loss = 0.42546089\n",
      "Iteration 68, loss = 0.42852512\n",
      "Iteration 69, loss = 0.43097255\n",
      "Iteration 70, loss = 0.42684420\n",
      "Iteration 71, loss = 0.42458865\n",
      "Iteration 72, loss = 0.42549171\n",
      "Iteration 73, loss = 0.42571996\n",
      "Iteration 74, loss = 0.42743560\n",
      "Iteration 75, loss = 0.42737898\n",
      "Iteration 76, loss = 0.42461344\n",
      "Iteration 77, loss = 0.42749495\n",
      "Iteration 78, loss = 0.42617538\n",
      "Iteration 79, loss = 0.42858515\n",
      "Iteration 80, loss = 0.42543529\n",
      "Iteration 81, loss = 0.42373449\n",
      "Iteration 82, loss = 0.42479551\n",
      "Iteration 83, loss = 0.42485029\n",
      "Iteration 84, loss = 0.42726608\n",
      "Iteration 85, loss = 0.42401905\n",
      "Iteration 86, loss = 0.42702019\n",
      "Iteration 87, loss = 0.42411275\n",
      "Iteration 88, loss = 0.42249139\n",
      "Iteration 89, loss = 0.42429779\n",
      "Iteration 90, loss = 0.42403271\n",
      "Iteration 91, loss = 0.42245814\n",
      "Iteration 92, loss = 0.42431323\n",
      "Iteration 93, loss = 0.42702595\n",
      "Iteration 94, loss = 0.42442205\n",
      "Iteration 95, loss = 0.43052275\n",
      "Iteration 96, loss = 0.42383996\n",
      "Iteration 97, loss = 0.42257767\n",
      "Iteration 98, loss = 0.42485743\n",
      "Iteration 99, loss = 0.42238279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 4 min. and 53.548726081848145 sec.\n",
      "\n",
      "21 total combinations for subset length 5.\n",
      "Iteration 1, loss = 0.59428786\n",
      "Iteration 2, loss = 0.49278605\n",
      "Iteration 3, loss = 0.48375563\n",
      "Iteration 4, loss = 0.48124936\n",
      "Iteration 5, loss = 0.47851177\n",
      "Iteration 6, loss = 0.47555520\n",
      "Iteration 7, loss = 0.47356288\n",
      "Iteration 8, loss = 0.47082185\n",
      "Iteration 9, loss = 0.47037831\n",
      "Iteration 10, loss = 0.46886985\n",
      "Iteration 11, loss = 0.47152805\n",
      "Iteration 12, loss = 0.46920143\n",
      "Iteration 13, loss = 0.46822527\n",
      "Iteration 14, loss = 0.46665221\n",
      "Iteration 15, loss = 0.46462239\n",
      "Iteration 16, loss = 0.46238493\n",
      "Iteration 17, loss = 0.46228752\n",
      "Iteration 18, loss = 0.45922738\n",
      "Iteration 19, loss = 0.45836211\n",
      "Iteration 20, loss = 0.45604514\n",
      "Iteration 21, loss = 0.45456919\n",
      "Iteration 22, loss = 0.45554131\n",
      "Iteration 23, loss = 0.45736485\n",
      "Iteration 24, loss = 0.45435904\n",
      "Iteration 25, loss = 0.45421741\n",
      "Iteration 26, loss = 0.45377370\n",
      "Iteration 27, loss = 0.45168642\n",
      "Iteration 28, loss = 0.45106904\n",
      "Iteration 29, loss = 0.45060826\n",
      "Iteration 30, loss = 0.44936175\n",
      "Iteration 31, loss = 0.44709626\n",
      "Iteration 32, loss = 0.45057893\n",
      "Iteration 33, loss = 0.45074569\n",
      "Iteration 34, loss = 0.44882469\n",
      "Iteration 35, loss = 0.44487389\n",
      "Iteration 36, loss = 0.44643345\n",
      "Iteration 37, loss = 0.44974002\n",
      "Iteration 38, loss = 0.45183203\n",
      "Iteration 39, loss = 0.44421085\n",
      "Iteration 40, loss = 0.44271803\n",
      "Iteration 41, loss = 0.44169030\n",
      "Iteration 42, loss = 0.44289699\n",
      "Iteration 43, loss = 0.44230699\n",
      "Iteration 44, loss = 0.43946181\n",
      "Iteration 45, loss = 0.44343526\n",
      "Iteration 46, loss = 0.43928625\n",
      "Iteration 47, loss = 0.43962739\n",
      "Iteration 48, loss = 0.43944371\n",
      "Iteration 49, loss = 0.44145868\n",
      "Iteration 50, loss = 0.44282028\n",
      "Iteration 51, loss = 0.43921658\n",
      "Iteration 52, loss = 0.44050522\n",
      "Iteration 53, loss = 0.44090341\n",
      "Iteration 54, loss = 0.43762502\n",
      "Iteration 55, loss = 0.43811563\n",
      "Iteration 56, loss = 0.43970725\n",
      "Iteration 57, loss = 0.43901654\n",
      "Iteration 58, loss = 0.43818974\n",
      "Iteration 59, loss = 0.43624985\n",
      "Iteration 60, loss = 0.43709559\n",
      "Iteration 61, loss = 0.43970480\n",
      "Iteration 62, loss = 0.43810804\n",
      "Iteration 63, loss = 0.43899348\n",
      "Iteration 64, loss = 0.43756577\n",
      "Iteration 65, loss = 0.43816667\n",
      "Iteration 66, loss = 0.44091466\n",
      "Iteration 67, loss = 0.43978520\n",
      "Iteration 68, loss = 0.43665581\n",
      "Iteration 69, loss = 0.43801064\n",
      "Iteration 70, loss = 0.43582453\n",
      "Iteration 71, loss = 0.43848960\n",
      "Iteration 72, loss = 0.43667984\n",
      "Iteration 73, loss = 0.43789197\n",
      "Iteration 74, loss = 0.43687520\n",
      "Iteration 75, loss = 0.43604466\n",
      "Iteration 76, loss = 0.43890108\n",
      "Iteration 77, loss = 0.43958050\n",
      "Iteration 78, loss = 0.43976814\n",
      "Iteration 79, loss = 0.44052276\n",
      "Iteration 80, loss = 0.43842719\n",
      "Iteration 81, loss = 0.43629696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59194829\n",
      "Iteration 2, loss = 0.48948343\n",
      "Iteration 3, loss = 0.47891334\n",
      "Iteration 4, loss = 0.47526082\n",
      "Iteration 5, loss = 0.47375973\n",
      "Iteration 6, loss = 0.47132979\n",
      "Iteration 7, loss = 0.47053484\n",
      "Iteration 8, loss = 0.46931446\n",
      "Iteration 9, loss = 0.46679275\n",
      "Iteration 10, loss = 0.46590851\n",
      "Iteration 11, loss = 0.46490654\n",
      "Iteration 12, loss = 0.46521204\n",
      "Iteration 13, loss = 0.46591303\n",
      "Iteration 14, loss = 0.45840089\n",
      "Iteration 15, loss = 0.45499489\n",
      "Iteration 16, loss = 0.45185326\n",
      "Iteration 17, loss = 0.45245313\n",
      "Iteration 18, loss = 0.44830465\n",
      "Iteration 19, loss = 0.44737378\n",
      "Iteration 20, loss = 0.44404564\n",
      "Iteration 21, loss = 0.44244923\n",
      "Iteration 22, loss = 0.44157564\n",
      "Iteration 23, loss = 0.44013339\n",
      "Iteration 24, loss = 0.43467415\n",
      "Iteration 25, loss = 0.43151246\n",
      "Iteration 26, loss = 0.43000598\n",
      "Iteration 27, loss = 0.42846938\n",
      "Iteration 28, loss = 0.42733992\n",
      "Iteration 29, loss = 0.42777954\n",
      "Iteration 30, loss = 0.42799315\n",
      "Iteration 31, loss = 0.42756061\n",
      "Iteration 32, loss = 0.42859310\n",
      "Iteration 33, loss = 0.42580147\n",
      "Iteration 34, loss = 0.42429549\n",
      "Iteration 35, loss = 0.42442541\n",
      "Iteration 36, loss = 0.42347194\n",
      "Iteration 37, loss = 0.42285086\n",
      "Iteration 38, loss = 0.42419277\n",
      "Iteration 39, loss = 0.42427446\n",
      "Iteration 40, loss = 0.42512393\n",
      "Iteration 41, loss = 0.42311071\n",
      "Iteration 42, loss = 0.42337071\n",
      "Iteration 43, loss = 0.42257007\n",
      "Iteration 44, loss = 0.42138436\n",
      "Iteration 45, loss = 0.42259509\n",
      "Iteration 46, loss = 0.42159242\n",
      "Iteration 47, loss = 0.42189768\n",
      "Iteration 48, loss = 0.42142366\n",
      "Iteration 49, loss = 0.42167774\n",
      "Iteration 50, loss = 0.41969522\n",
      "Iteration 51, loss = 0.41799072\n",
      "Iteration 52, loss = 0.42053294\n",
      "Iteration 53, loss = 0.41885855\n",
      "Iteration 54, loss = 0.41894670\n",
      "Iteration 55, loss = 0.42136687\n",
      "Iteration 56, loss = 0.41975542\n",
      "Iteration 57, loss = 0.41904004\n",
      "Iteration 58, loss = 0.42035475\n",
      "Iteration 59, loss = 0.41850852\n",
      "Iteration 60, loss = 0.41980315\n",
      "Iteration 61, loss = 0.41831321\n",
      "Iteration 62, loss = 0.42005076\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59321491\n",
      "Iteration 2, loss = 0.49149433\n",
      "Iteration 3, loss = 0.48245249\n",
      "Iteration 4, loss = 0.47771397\n",
      "Iteration 5, loss = 0.47551829\n",
      "Iteration 6, loss = 0.47261916\n",
      "Iteration 7, loss = 0.47099364\n",
      "Iteration 8, loss = 0.47057426\n",
      "Iteration 9, loss = 0.46973958\n",
      "Iteration 10, loss = 0.46697675\n",
      "Iteration 11, loss = 0.46648240\n",
      "Iteration 12, loss = 0.46639920\n",
      "Iteration 13, loss = 0.46420660\n",
      "Iteration 14, loss = 0.46390825\n",
      "Iteration 15, loss = 0.46173399\n",
      "Iteration 16, loss = 0.45997002\n",
      "Iteration 17, loss = 0.45848649\n",
      "Iteration 18, loss = 0.45708950\n",
      "Iteration 19, loss = 0.45796254\n",
      "Iteration 20, loss = 0.45659698\n",
      "Iteration 21, loss = 0.45366871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.45368472\n",
      "Iteration 23, loss = 0.45327860\n",
      "Iteration 24, loss = 0.45135223\n",
      "Iteration 25, loss = 0.44846574\n",
      "Iteration 26, loss = 0.44577614\n",
      "Iteration 27, loss = 0.44371254\n",
      "Iteration 28, loss = 0.44470433\n",
      "Iteration 29, loss = 0.44888019\n",
      "Iteration 30, loss = 0.44434123\n",
      "Iteration 31, loss = 0.44195498\n",
      "Iteration 32, loss = 0.44302367\n",
      "Iteration 33, loss = 0.44093502\n",
      "Iteration 34, loss = 0.43982011\n",
      "Iteration 35, loss = 0.43868178\n",
      "Iteration 36, loss = 0.43785061\n",
      "Iteration 37, loss = 0.43602410\n",
      "Iteration 38, loss = 0.43622962\n",
      "Iteration 39, loss = 0.43443493\n",
      "Iteration 40, loss = 0.43923634\n",
      "Iteration 41, loss = 0.43548348\n",
      "Iteration 42, loss = 0.43753289\n",
      "Iteration 43, loss = 0.43477066\n",
      "Iteration 44, loss = 0.43348915\n",
      "Iteration 45, loss = 0.43231270\n",
      "Iteration 46, loss = 0.43427586\n",
      "Iteration 47, loss = 0.43775334\n",
      "Iteration 48, loss = 0.43494907\n",
      "Iteration 49, loss = 0.43084751\n",
      "Iteration 50, loss = 0.42864469\n",
      "Iteration 51, loss = 0.43014042\n",
      "Iteration 52, loss = 0.42999841\n",
      "Iteration 53, loss = 0.42868923\n",
      "Iteration 54, loss = 0.42644027\n",
      "Iteration 55, loss = 0.42708887\n",
      "Iteration 56, loss = 0.42784293\n",
      "Iteration 57, loss = 0.43052467\n",
      "Iteration 58, loss = 0.42530267\n",
      "Iteration 59, loss = 0.42405127\n",
      "Iteration 60, loss = 0.42634442\n",
      "Iteration 61, loss = 0.42580296\n",
      "Iteration 62, loss = 0.42345924\n",
      "Iteration 63, loss = 0.42488086\n",
      "Iteration 64, loss = 0.42297667\n",
      "Iteration 65, loss = 0.42461845\n",
      "Iteration 66, loss = 0.42360359\n",
      "Iteration 67, loss = 0.42168711\n",
      "Iteration 68, loss = 0.42188662\n",
      "Iteration 69, loss = 0.42322739\n",
      "Iteration 70, loss = 0.42288442\n",
      "Iteration 71, loss = 0.42321549\n",
      "Iteration 72, loss = 0.42064729\n",
      "Iteration 73, loss = 0.42190861\n",
      "Iteration 74, loss = 0.42282944\n",
      "Iteration 75, loss = 0.41981711\n",
      "Iteration 76, loss = 0.42034841\n",
      "Iteration 77, loss = 0.42343917\n",
      "Iteration 78, loss = 0.42239237\n",
      "Iteration 79, loss = 0.42215414\n",
      "Iteration 80, loss = 0.42083184\n",
      "Iteration 81, loss = 0.42067229\n",
      "Iteration 82, loss = 0.42177041\n",
      "Iteration 83, loss = 0.42043676\n",
      "Iteration 84, loss = 0.42158651\n",
      "Iteration 85, loss = 0.41942314\n",
      "Iteration 86, loss = 0.42070039\n",
      "Iteration 87, loss = 0.42023714\n",
      "Iteration 88, loss = 0.42143234\n",
      "Iteration 89, loss = 0.42021391\n",
      "Iteration 90, loss = 0.42117967\n",
      "Iteration 91, loss = 0.42058141\n",
      "Iteration 92, loss = 0.42084619\n",
      "Iteration 93, loss = 0.42208115\n",
      "Iteration 94, loss = 0.42245518\n",
      "Iteration 95, loss = 0.42050403\n",
      "Iteration 96, loss = 0.41975157\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58260452\n",
      "Iteration 2, loss = 0.49659270\n",
      "Iteration 3, loss = 0.48654399\n",
      "Iteration 4, loss = 0.48322573\n",
      "Iteration 5, loss = 0.48084580\n",
      "Iteration 6, loss = 0.48012058\n",
      "Iteration 7, loss = 0.47892858\n",
      "Iteration 8, loss = 0.47846376\n",
      "Iteration 9, loss = 0.47533956\n",
      "Iteration 10, loss = 0.47405554\n",
      "Iteration 11, loss = 0.47279824\n",
      "Iteration 12, loss = 0.46924944\n",
      "Iteration 13, loss = 0.47011769\n",
      "Iteration 14, loss = 0.46737957\n",
      "Iteration 15, loss = 0.46516896\n",
      "Iteration 16, loss = 0.46534772\n",
      "Iteration 17, loss = 0.46290301\n",
      "Iteration 18, loss = 0.46184005\n",
      "Iteration 19, loss = 0.46144170\n",
      "Iteration 20, loss = 0.46098648\n",
      "Iteration 21, loss = 0.45811449\n",
      "Iteration 22, loss = 0.45827817\n",
      "Iteration 23, loss = 0.45923274\n",
      "Iteration 24, loss = 0.45619777\n",
      "Iteration 25, loss = 0.45568184\n",
      "Iteration 26, loss = 0.45379128\n",
      "Iteration 27, loss = 0.45289345\n",
      "Iteration 28, loss = 0.45103488\n",
      "Iteration 29, loss = 0.44929606\n",
      "Iteration 30, loss = 0.44859439\n",
      "Iteration 31, loss = 0.44688686\n",
      "Iteration 32, loss = 0.44850865\n",
      "Iteration 33, loss = 0.44621159\n",
      "Iteration 34, loss = 0.44513461\n",
      "Iteration 35, loss = 0.44220990\n",
      "Iteration 36, loss = 0.44010640\n",
      "Iteration 37, loss = 0.44117643\n",
      "Iteration 38, loss = 0.44091483\n",
      "Iteration 39, loss = 0.43928570\n",
      "Iteration 40, loss = 0.43669613\n",
      "Iteration 41, loss = 0.43427188\n",
      "Iteration 42, loss = 0.43177354\n",
      "Iteration 43, loss = 0.43229805\n",
      "Iteration 44, loss = 0.43144916\n",
      "Iteration 45, loss = 0.42950685\n",
      "Iteration 46, loss = 0.42997705\n",
      "Iteration 47, loss = 0.42740699\n",
      "Iteration 48, loss = 0.42829345\n",
      "Iteration 49, loss = 0.42687707\n",
      "Iteration 50, loss = 0.43020005\n",
      "Iteration 51, loss = 0.42815638\n",
      "Iteration 52, loss = 0.42557973\n",
      "Iteration 53, loss = 0.42562804\n",
      "Iteration 54, loss = 0.42291782\n",
      "Iteration 55, loss = 0.42287636\n",
      "Iteration 56, loss = 0.42208148\n",
      "Iteration 57, loss = 0.42322822\n",
      "Iteration 58, loss = 0.42464373\n",
      "Iteration 59, loss = 0.42539530\n",
      "Iteration 60, loss = 0.42529640\n",
      "Iteration 61, loss = 0.42527023\n",
      "Iteration 62, loss = 0.42234565\n",
      "Iteration 63, loss = 0.42322933\n",
      "Iteration 64, loss = 0.42373787\n",
      "Iteration 65, loss = 0.42695947\n",
      "Iteration 66, loss = 0.42797999\n",
      "Iteration 67, loss = 0.42283555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58921660\n",
      "Iteration 2, loss = 0.49258010\n",
      "Iteration 3, loss = 0.48011003\n",
      "Iteration 4, loss = 0.47699377\n",
      "Iteration 5, loss = 0.47535687\n",
      "Iteration 6, loss = 0.47397287\n",
      "Iteration 7, loss = 0.47322820\n",
      "Iteration 8, loss = 0.47364726\n",
      "Iteration 9, loss = 0.47183838\n",
      "Iteration 10, loss = 0.47094392\n",
      "Iteration 11, loss = 0.47155813\n",
      "Iteration 12, loss = 0.47084722\n",
      "Iteration 13, loss = 0.47149610\n",
      "Iteration 14, loss = 0.46564949\n",
      "Iteration 15, loss = 0.46394276\n",
      "Iteration 16, loss = 0.46201892\n",
      "Iteration 17, loss = 0.46082918\n",
      "Iteration 18, loss = 0.45717519\n",
      "Iteration 19, loss = 0.45523914\n",
      "Iteration 20, loss = 0.45159566\n",
      "Iteration 21, loss = 0.44811899\n",
      "Iteration 22, loss = 0.44714742\n",
      "Iteration 23, loss = 0.44536839\n",
      "Iteration 24, loss = 0.43779009\n",
      "Iteration 25, loss = 0.43658102\n",
      "Iteration 26, loss = 0.43272814\n",
      "Iteration 27, loss = 0.43079651\n",
      "Iteration 28, loss = 0.42853981\n",
      "Iteration 29, loss = 0.42758020\n",
      "Iteration 30, loss = 0.42717364\n",
      "Iteration 31, loss = 0.42496400\n",
      "Iteration 32, loss = 0.42444929\n",
      "Iteration 33, loss = 0.42245864\n",
      "Iteration 34, loss = 0.42146862\n",
      "Iteration 35, loss = 0.42041747\n",
      "Iteration 36, loss = 0.41854692\n",
      "Iteration 37, loss = 0.41848417\n",
      "Iteration 38, loss = 0.41955056\n",
      "Iteration 39, loss = 0.41837306\n",
      "Iteration 40, loss = 0.41755790\n",
      "Iteration 41, loss = 0.41688355\n",
      "Iteration 42, loss = 0.41621502\n",
      "Iteration 43, loss = 0.41553864\n",
      "Iteration 44, loss = 0.41811717\n",
      "Iteration 45, loss = 0.41839820\n",
      "Iteration 46, loss = 0.41824241\n",
      "Iteration 47, loss = 0.41511009\n",
      "Iteration 48, loss = 0.41539875\n",
      "Iteration 49, loss = 0.41550214\n",
      "Iteration 50, loss = 0.41545417\n",
      "Iteration 51, loss = 0.41550601\n",
      "Iteration 52, loss = 0.41460597\n",
      "Iteration 53, loss = 0.41577363\n",
      "Iteration 54, loss = 0.41503306\n",
      "Iteration 55, loss = 0.41390799\n",
      "Iteration 56, loss = 0.41501563\n",
      "Iteration 57, loss = 0.41390373\n",
      "Iteration 58, loss = 0.41379454\n",
      "Iteration 59, loss = 0.41308975\n",
      "Iteration 60, loss = 0.41592140\n",
      "Iteration 61, loss = 0.41359474\n",
      "Iteration 62, loss = 0.41556081\n",
      "Iteration 63, loss = 0.41485911\n",
      "Iteration 64, loss = 0.41452033\n",
      "Iteration 65, loss = 0.41565360\n",
      "Iteration 66, loss = 0.41641452\n",
      "Iteration 67, loss = 0.41565716\n",
      "Iteration 68, loss = 0.41420837\n",
      "Iteration 69, loss = 0.41456189\n",
      "Iteration 70, loss = 0.41407986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59172792\n",
      "Iteration 2, loss = 0.49572626\n",
      "Iteration 3, loss = 0.48431528\n",
      "Iteration 4, loss = 0.48106392\n",
      "Iteration 5, loss = 0.47831435\n",
      "Iteration 6, loss = 0.47722814\n",
      "Iteration 7, loss = 0.47598468\n",
      "Iteration 8, loss = 0.47674151\n",
      "Iteration 9, loss = 0.47507928\n",
      "Iteration 10, loss = 0.47433680\n",
      "Iteration 11, loss = 0.47332447\n",
      "Iteration 12, loss = 0.47362297\n",
      "Iteration 13, loss = 0.47207058\n",
      "Iteration 14, loss = 0.46988643\n",
      "Iteration 15, loss = 0.46956316\n",
      "Iteration 16, loss = 0.46842587\n",
      "Iteration 17, loss = 0.46510370\n",
      "Iteration 18, loss = 0.46311052\n",
      "Iteration 19, loss = 0.46053040\n",
      "Iteration 20, loss = 0.45664610\n",
      "Iteration 21, loss = 0.45226308\n",
      "Iteration 22, loss = 0.44859126\n",
      "Iteration 23, loss = 0.44590463\n",
      "Iteration 24, loss = 0.44289422\n",
      "Iteration 25, loss = 0.43848875\n",
      "Iteration 26, loss = 0.43762678\n",
      "Iteration 27, loss = 0.43184496\n",
      "Iteration 28, loss = 0.43089835\n",
      "Iteration 29, loss = 0.43015756\n",
      "Iteration 30, loss = 0.42661447\n",
      "Iteration 31, loss = 0.42788235\n",
      "Iteration 32, loss = 0.42634090\n",
      "Iteration 33, loss = 0.42276482\n",
      "Iteration 34, loss = 0.42289669\n",
      "Iteration 35, loss = 0.42327654\n",
      "Iteration 36, loss = 0.42146389\n",
      "Iteration 37, loss = 0.42193019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.42166057\n",
      "Iteration 39, loss = 0.42276291\n",
      "Iteration 40, loss = 0.42261190\n",
      "Iteration 41, loss = 0.42106108\n",
      "Iteration 42, loss = 0.42011361\n",
      "Iteration 43, loss = 0.42117921\n",
      "Iteration 44, loss = 0.42104926\n",
      "Iteration 45, loss = 0.42046049\n",
      "Iteration 46, loss = 0.42128590\n",
      "Iteration 47, loss = 0.42151661\n",
      "Iteration 48, loss = 0.42162249\n",
      "Iteration 49, loss = 0.42056716\n",
      "Iteration 50, loss = 0.41930002\n",
      "Iteration 51, loss = 0.41868887\n",
      "Iteration 52, loss = 0.41957529\n",
      "Iteration 53, loss = 0.41690694\n",
      "Iteration 54, loss = 0.41793144\n",
      "Iteration 55, loss = 0.41910143\n",
      "Iteration 56, loss = 0.41955229\n",
      "Iteration 57, loss = 0.41997544\n",
      "Iteration 58, loss = 0.41802462\n",
      "Iteration 59, loss = 0.41666074\n",
      "Iteration 60, loss = 0.41819111\n",
      "Iteration 61, loss = 0.41896545\n",
      "Iteration 62, loss = 0.41800642\n",
      "Iteration 63, loss = 0.41944653\n",
      "Iteration 64, loss = 0.41890701\n",
      "Iteration 65, loss = 0.41693424\n",
      "Iteration 66, loss = 0.41739450\n",
      "Iteration 67, loss = 0.41858385\n",
      "Iteration 68, loss = 0.41860413\n",
      "Iteration 69, loss = 0.41836792\n",
      "Iteration 70, loss = 0.41608624\n",
      "Iteration 71, loss = 0.41585301\n",
      "Iteration 72, loss = 0.41531792\n",
      "Iteration 73, loss = 0.41709583\n",
      "Iteration 74, loss = 0.41687138\n",
      "Iteration 75, loss = 0.41670541\n",
      "Iteration 76, loss = 0.41764905\n",
      "Iteration 77, loss = 0.41996895\n",
      "Iteration 78, loss = 0.41792563\n",
      "Iteration 79, loss = 0.41622561\n",
      "Iteration 80, loss = 0.41644147\n",
      "Iteration 81, loss = 0.41664638\n",
      "Iteration 82, loss = 0.41557503\n",
      "Iteration 83, loss = 0.41460942\n",
      "Iteration 84, loss = 0.41760057\n",
      "Iteration 85, loss = 0.41602572\n",
      "Iteration 86, loss = 0.41675449\n",
      "Iteration 87, loss = 0.41643148\n",
      "Iteration 88, loss = 0.41557308\n",
      "Iteration 89, loss = 0.41645619\n",
      "Iteration 90, loss = 0.41509138\n",
      "Iteration 91, loss = 0.41741070\n",
      "Iteration 92, loss = 0.41573452\n",
      "Iteration 93, loss = 0.41591383\n",
      "Iteration 94, loss = 0.41726348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57597728\n",
      "Iteration 2, loss = 0.49138546\n",
      "Iteration 3, loss = 0.48038018\n",
      "Iteration 4, loss = 0.47828391\n",
      "Iteration 5, loss = 0.47673050\n",
      "Iteration 6, loss = 0.47639145\n",
      "Iteration 7, loss = 0.47471254\n",
      "Iteration 8, loss = 0.47352162\n",
      "Iteration 9, loss = 0.47048089\n",
      "Iteration 10, loss = 0.47133096\n",
      "Iteration 11, loss = 0.47075681\n",
      "Iteration 12, loss = 0.46863364\n",
      "Iteration 13, loss = 0.46996234\n",
      "Iteration 14, loss = 0.47104810\n",
      "Iteration 15, loss = 0.46907758\n",
      "Iteration 16, loss = 0.46984323\n",
      "Iteration 17, loss = 0.46915427\n",
      "Iteration 18, loss = 0.46760180\n",
      "Iteration 19, loss = 0.46796537\n",
      "Iteration 20, loss = 0.46661485\n",
      "Iteration 21, loss = 0.46535647\n",
      "Iteration 22, loss = 0.46415023\n",
      "Iteration 23, loss = 0.46800321\n",
      "Iteration 24, loss = 0.46209435\n",
      "Iteration 25, loss = 0.46160744\n",
      "Iteration 26, loss = 0.46035566\n",
      "Iteration 27, loss = 0.45974526\n",
      "Iteration 28, loss = 0.45945908\n",
      "Iteration 29, loss = 0.45750437\n",
      "Iteration 30, loss = 0.45641395\n",
      "Iteration 31, loss = 0.45711181\n",
      "Iteration 32, loss = 0.45694869\n",
      "Iteration 33, loss = 0.45673083\n",
      "Iteration 34, loss = 0.45727507\n",
      "Iteration 35, loss = 0.45416895\n",
      "Iteration 36, loss = 0.45382918\n",
      "Iteration 37, loss = 0.45504802\n",
      "Iteration 38, loss = 0.45625361\n",
      "Iteration 39, loss = 0.45298077\n",
      "Iteration 40, loss = 0.45436803\n",
      "Iteration 41, loss = 0.45198867\n",
      "Iteration 42, loss = 0.45323493\n",
      "Iteration 43, loss = 0.45244395\n",
      "Iteration 44, loss = 0.45087960\n",
      "Iteration 45, loss = 0.45018615\n",
      "Iteration 46, loss = 0.44963090\n",
      "Iteration 47, loss = 0.44918490\n",
      "Iteration 48, loss = 0.44787852\n",
      "Iteration 49, loss = 0.44991262\n",
      "Iteration 50, loss = 0.44890878\n",
      "Iteration 51, loss = 0.44705705\n",
      "Iteration 52, loss = 0.44307237\n",
      "Iteration 53, loss = 0.44485966\n",
      "Iteration 54, loss = 0.43763824\n",
      "Iteration 55, loss = 0.43616677\n",
      "Iteration 56, loss = 0.43491604\n",
      "Iteration 57, loss = 0.43185599\n",
      "Iteration 58, loss = 0.43180480\n",
      "Iteration 59, loss = 0.43079431\n",
      "Iteration 60, loss = 0.42845010\n",
      "Iteration 61, loss = 0.42932962\n",
      "Iteration 62, loss = 0.42835692\n",
      "Iteration 63, loss = 0.42769985\n",
      "Iteration 64, loss = 0.42590055\n",
      "Iteration 65, loss = 0.42815574\n",
      "Iteration 66, loss = 0.42570881\n",
      "Iteration 67, loss = 0.42414705\n",
      "Iteration 68, loss = 0.42597088\n",
      "Iteration 69, loss = 0.42573912\n",
      "Iteration 70, loss = 0.42286955\n",
      "Iteration 71, loss = 0.42535434\n",
      "Iteration 72, loss = 0.42305206\n",
      "Iteration 73, loss = 0.42317258\n",
      "Iteration 74, loss = 0.42311785\n",
      "Iteration 75, loss = 0.42224457\n",
      "Iteration 76, loss = 0.42642873\n",
      "Iteration 77, loss = 0.42251504\n",
      "Iteration 78, loss = 0.42445154\n",
      "Iteration 79, loss = 0.42427509\n",
      "Iteration 80, loss = 0.42008767\n",
      "Iteration 81, loss = 0.42079045\n",
      "Iteration 82, loss = 0.42273826\n",
      "Iteration 83, loss = 0.42040647\n",
      "Iteration 84, loss = 0.41913644\n",
      "Iteration 85, loss = 0.41859085\n",
      "Iteration 86, loss = 0.41881805\n",
      "Iteration 87, loss = 0.41879968\n",
      "Iteration 88, loss = 0.41846159\n",
      "Iteration 89, loss = 0.41832914\n",
      "Iteration 90, loss = 0.42000245\n",
      "Iteration 91, loss = 0.41904107\n",
      "Iteration 92, loss = 0.42233308\n",
      "Iteration 93, loss = 0.41800882\n",
      "Iteration 94, loss = 0.42196159\n",
      "Iteration 95, loss = 0.41725028\n",
      "Iteration 96, loss = 0.41992874\n",
      "Iteration 97, loss = 0.41967435\n",
      "Iteration 98, loss = 0.41736211\n",
      "Iteration 99, loss = 0.41832856\n",
      "Iteration 100, loss = 0.41647311\n",
      "Iteration 101, loss = 0.41949189\n",
      "Iteration 102, loss = 0.41943287\n",
      "Iteration 103, loss = 0.41730621\n",
      "Iteration 104, loss = 0.41462898\n",
      "Iteration 105, loss = 0.41665387\n",
      "Iteration 106, loss = 0.41732863\n",
      "Iteration 107, loss = 0.41612263\n",
      "Iteration 108, loss = 0.41692590\n",
      "Iteration 109, loss = 0.41718164\n",
      "Iteration 110, loss = 0.41697908\n",
      "Iteration 111, loss = 0.42021430\n",
      "Iteration 112, loss = 0.41932579\n",
      "Iteration 113, loss = 0.41490047\n",
      "Iteration 114, loss = 0.41566420\n",
      "Iteration 115, loss = 0.41547887\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58120479\n",
      "Iteration 2, loss = 0.48782079\n",
      "Iteration 3, loss = 0.47624392\n",
      "Iteration 4, loss = 0.47361956\n",
      "Iteration 5, loss = 0.47306127\n",
      "Iteration 6, loss = 0.47124393\n",
      "Iteration 7, loss = 0.46965389\n",
      "Iteration 8, loss = 0.47066009\n",
      "Iteration 9, loss = 0.46909971\n",
      "Iteration 10, loss = 0.46810026\n",
      "Iteration 11, loss = 0.46948356\n",
      "Iteration 12, loss = 0.46801256\n",
      "Iteration 13, loss = 0.47009295\n",
      "Iteration 14, loss = 0.46659737\n",
      "Iteration 15, loss = 0.46576134\n",
      "Iteration 16, loss = 0.46435565\n",
      "Iteration 17, loss = 0.46465355\n",
      "Iteration 18, loss = 0.46357159\n",
      "Iteration 19, loss = 0.45945433\n",
      "Iteration 20, loss = 0.45659530\n",
      "Iteration 21, loss = 0.45613526\n",
      "Iteration 22, loss = 0.45620362\n",
      "Iteration 23, loss = 0.45765342\n",
      "Iteration 24, loss = 0.45362104\n",
      "Iteration 25, loss = 0.45367656\n",
      "Iteration 26, loss = 0.45163836\n",
      "Iteration 27, loss = 0.45013255\n",
      "Iteration 28, loss = 0.44813728\n",
      "Iteration 29, loss = 0.44902546\n",
      "Iteration 30, loss = 0.44827103\n",
      "Iteration 31, loss = 0.44604685\n",
      "Iteration 32, loss = 0.44618645\n",
      "Iteration 33, loss = 0.44429554\n",
      "Iteration 34, loss = 0.44250033\n",
      "Iteration 35, loss = 0.44037944\n",
      "Iteration 36, loss = 0.43997239\n",
      "Iteration 37, loss = 0.43757743\n",
      "Iteration 38, loss = 0.43844222\n",
      "Iteration 39, loss = 0.43523705\n",
      "Iteration 40, loss = 0.43168235\n",
      "Iteration 41, loss = 0.42842718\n",
      "Iteration 42, loss = 0.42775292\n",
      "Iteration 43, loss = 0.42455066\n",
      "Iteration 44, loss = 0.42413985\n",
      "Iteration 45, loss = 0.42513352\n",
      "Iteration 46, loss = 0.42369291\n",
      "Iteration 47, loss = 0.42012805\n",
      "Iteration 48, loss = 0.42010768\n",
      "Iteration 49, loss = 0.41855577\n",
      "Iteration 50, loss = 0.41742006\n",
      "Iteration 51, loss = 0.41603141\n",
      "Iteration 52, loss = 0.41745834\n",
      "Iteration 53, loss = 0.41609443\n",
      "Iteration 54, loss = 0.41598081\n",
      "Iteration 55, loss = 0.41571025\n",
      "Iteration 56, loss = 0.41597430\n",
      "Iteration 57, loss = 0.41592396\n",
      "Iteration 58, loss = 0.41497098\n",
      "Iteration 59, loss = 0.41376410\n",
      "Iteration 60, loss = 0.41645041\n",
      "Iteration 61, loss = 0.41364726\n",
      "Iteration 62, loss = 0.41503802\n",
      "Iteration 63, loss = 0.41475721\n",
      "Iteration 64, loss = 0.41520084\n",
      "Iteration 65, loss = 0.41562706\n",
      "Iteration 66, loss = 0.41665012\n",
      "Iteration 67, loss = 0.41376814\n",
      "Iteration 68, loss = 0.41413858\n",
      "Iteration 69, loss = 0.41518147\n",
      "Iteration 70, loss = 0.41449253\n",
      "Iteration 71, loss = 0.41519727\n",
      "Iteration 72, loss = 0.41346863\n",
      "Iteration 73, loss = 0.41519664\n",
      "Iteration 74, loss = 0.41378939\n",
      "Iteration 75, loss = 0.41376563\n",
      "Iteration 76, loss = 0.41443675\n",
      "Iteration 77, loss = 0.41373754\n",
      "Iteration 78, loss = 0.41397276\n",
      "Iteration 79, loss = 0.41523073\n",
      "Iteration 80, loss = 0.41368657\n",
      "Iteration 81, loss = 0.41442200\n",
      "Iteration 82, loss = 0.41393205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, loss = 0.41321015\n",
      "Iteration 84, loss = 0.41288301\n",
      "Iteration 85, loss = 0.41378236\n",
      "Iteration 86, loss = 0.41289191\n",
      "Iteration 87, loss = 0.41622907\n",
      "Iteration 88, loss = 0.41215197\n",
      "Iteration 89, loss = 0.41305361\n",
      "Iteration 90, loss = 0.41357963\n",
      "Iteration 91, loss = 0.41357829\n",
      "Iteration 92, loss = 0.41388373\n",
      "Iteration 93, loss = 0.41294703\n",
      "Iteration 94, loss = 0.41323566\n",
      "Iteration 95, loss = 0.41272779\n",
      "Iteration 96, loss = 0.41398768\n",
      "Iteration 97, loss = 0.41323768\n",
      "Iteration 98, loss = 0.41553775\n",
      "Iteration 99, loss = 0.41217911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58014544\n",
      "Iteration 2, loss = 0.48625333\n",
      "Iteration 3, loss = 0.47795526\n",
      "Iteration 4, loss = 0.47509791\n",
      "Iteration 5, loss = 0.47243687\n",
      "Iteration 6, loss = 0.47107993\n",
      "Iteration 7, loss = 0.46981836\n",
      "Iteration 8, loss = 0.47192926\n",
      "Iteration 9, loss = 0.46859783\n",
      "Iteration 10, loss = 0.46895453\n",
      "Iteration 11, loss = 0.46744752\n",
      "Iteration 12, loss = 0.46855094\n",
      "Iteration 13, loss = 0.46756727\n",
      "Iteration 14, loss = 0.46770560\n",
      "Iteration 15, loss = 0.46570619\n",
      "Iteration 16, loss = 0.46669116\n",
      "Iteration 17, loss = 0.46409058\n",
      "Iteration 18, loss = 0.46310702\n",
      "Iteration 19, loss = 0.46249638\n",
      "Iteration 20, loss = 0.46264909\n",
      "Iteration 21, loss = 0.46191972\n",
      "Iteration 22, loss = 0.46142115\n",
      "Iteration 23, loss = 0.46193948\n",
      "Iteration 24, loss = 0.46030227\n",
      "Iteration 25, loss = 0.45942703\n",
      "Iteration 26, loss = 0.45931384\n",
      "Iteration 27, loss = 0.45781873\n",
      "Iteration 28, loss = 0.45828707\n",
      "Iteration 29, loss = 0.45874666\n",
      "Iteration 30, loss = 0.45733417\n",
      "Iteration 31, loss = 0.45771876\n",
      "Iteration 32, loss = 0.45490875\n",
      "Iteration 33, loss = 0.45227092\n",
      "Iteration 34, loss = 0.45105435\n",
      "Iteration 35, loss = 0.44975163\n",
      "Iteration 36, loss = 0.44502088\n",
      "Iteration 37, loss = 0.44399977\n",
      "Iteration 38, loss = 0.44117105\n",
      "Iteration 39, loss = 0.43983899\n",
      "Iteration 40, loss = 0.44003076\n",
      "Iteration 41, loss = 0.43734280\n",
      "Iteration 42, loss = 0.43571685\n",
      "Iteration 43, loss = 0.43568476\n",
      "Iteration 44, loss = 0.43608286\n",
      "Iteration 45, loss = 0.43391415\n",
      "Iteration 46, loss = 0.43381469\n",
      "Iteration 47, loss = 0.43496715\n",
      "Iteration 48, loss = 0.43820573\n",
      "Iteration 49, loss = 0.43420310\n",
      "Iteration 50, loss = 0.43328664\n",
      "Iteration 51, loss = 0.43126779\n",
      "Iteration 52, loss = 0.43228579\n",
      "Iteration 53, loss = 0.43236294\n",
      "Iteration 54, loss = 0.43285748\n",
      "Iteration 55, loss = 0.43431253\n",
      "Iteration 56, loss = 0.43100568\n",
      "Iteration 57, loss = 0.43368119\n",
      "Iteration 58, loss = 0.43165566\n",
      "Iteration 59, loss = 0.43059339\n",
      "Iteration 60, loss = 0.42954610\n",
      "Iteration 61, loss = 0.43096526\n",
      "Iteration 62, loss = 0.42977369\n",
      "Iteration 63, loss = 0.43241743\n",
      "Iteration 64, loss = 0.43099172\n",
      "Iteration 65, loss = 0.42900592\n",
      "Iteration 66, loss = 0.43039095\n",
      "Iteration 67, loss = 0.43069092\n",
      "Iteration 68, loss = 0.42939066\n",
      "Iteration 69, loss = 0.42733317\n",
      "Iteration 70, loss = 0.42925861\n",
      "Iteration 71, loss = 0.42798397\n",
      "Iteration 72, loss = 0.42834451\n",
      "Iteration 73, loss = 0.42763937\n",
      "Iteration 74, loss = 0.42845298\n",
      "Iteration 75, loss = 0.42827189\n",
      "Iteration 76, loss = 0.42906214\n",
      "Iteration 77, loss = 0.42699951\n",
      "Iteration 78, loss = 0.42810591\n",
      "Iteration 79, loss = 0.42813556\n",
      "Iteration 80, loss = 0.42649273\n",
      "Iteration 81, loss = 0.42726841\n",
      "Iteration 82, loss = 0.42837440\n",
      "Iteration 83, loss = 0.42582436\n",
      "Iteration 84, loss = 0.42639026\n",
      "Iteration 85, loss = 0.42722294\n",
      "Iteration 86, loss = 0.42741034\n",
      "Iteration 87, loss = 0.42898874\n",
      "Iteration 88, loss = 0.42871503\n",
      "Iteration 89, loss = 0.43273309\n",
      "Iteration 90, loss = 0.42960950\n",
      "Iteration 91, loss = 0.43000234\n",
      "Iteration 92, loss = 0.42583001\n",
      "Iteration 93, loss = 0.42791257\n",
      "Iteration 94, loss = 0.42766898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56213554\n",
      "Iteration 2, loss = 0.49573684\n",
      "Iteration 3, loss = 0.49070238\n",
      "Iteration 4, loss = 0.48694488\n",
      "Iteration 5, loss = 0.48457110\n",
      "Iteration 6, loss = 0.48353907\n",
      "Iteration 7, loss = 0.48295685\n",
      "Iteration 8, loss = 0.48146498\n",
      "Iteration 9, loss = 0.48039157\n",
      "Iteration 10, loss = 0.48064978\n",
      "Iteration 11, loss = 0.47947390\n",
      "Iteration 12, loss = 0.47841903\n",
      "Iteration 13, loss = 0.47877474\n",
      "Iteration 14, loss = 0.47775033\n",
      "Iteration 15, loss = 0.47683171\n",
      "Iteration 16, loss = 0.47704588\n",
      "Iteration 17, loss = 0.47625028\n",
      "Iteration 18, loss = 0.47414056\n",
      "Iteration 19, loss = 0.47246384\n",
      "Iteration 20, loss = 0.47034594\n",
      "Iteration 21, loss = 0.46701985\n",
      "Iteration 22, loss = 0.46693436\n",
      "Iteration 23, loss = 0.46834828\n",
      "Iteration 24, loss = 0.46259510\n",
      "Iteration 25, loss = 0.46321363\n",
      "Iteration 26, loss = 0.46132405\n",
      "Iteration 27, loss = 0.45823643\n",
      "Iteration 28, loss = 0.45697332\n",
      "Iteration 29, loss = 0.45504578\n",
      "Iteration 30, loss = 0.45341805\n",
      "Iteration 31, loss = 0.45242906\n",
      "Iteration 32, loss = 0.45200495\n",
      "Iteration 33, loss = 0.45135335\n",
      "Iteration 34, loss = 0.44769684\n",
      "Iteration 35, loss = 0.44430744\n",
      "Iteration 36, loss = 0.44290267\n",
      "Iteration 37, loss = 0.44361979\n",
      "Iteration 38, loss = 0.44513777\n",
      "Iteration 39, loss = 0.44334790\n",
      "Iteration 40, loss = 0.44256912\n",
      "Iteration 41, loss = 0.43809894\n",
      "Iteration 42, loss = 0.44030211\n",
      "Iteration 43, loss = 0.44034000\n",
      "Iteration 44, loss = 0.43986733\n",
      "Iteration 45, loss = 0.43941937\n",
      "Iteration 46, loss = 0.43870364\n",
      "Iteration 47, loss = 0.44144355\n",
      "Iteration 48, loss = 0.44061468\n",
      "Iteration 49, loss = 0.43945849\n",
      "Iteration 50, loss = 0.43708331\n",
      "Iteration 51, loss = 0.43759574\n",
      "Iteration 52, loss = 0.43808842\n",
      "Iteration 53, loss = 0.43606057\n",
      "Iteration 54, loss = 0.43533703\n",
      "Iteration 55, loss = 0.43388354\n",
      "Iteration 56, loss = 0.43367595\n",
      "Iteration 57, loss = 0.43351469\n",
      "Iteration 58, loss = 0.43402923\n",
      "Iteration 59, loss = 0.43424760\n",
      "Iteration 60, loss = 0.43456654\n",
      "Iteration 61, loss = 0.43508806\n",
      "Iteration 62, loss = 0.43357681\n",
      "Iteration 63, loss = 0.43418265\n",
      "Iteration 64, loss = 0.43211473\n",
      "Iteration 65, loss = 0.43577808\n",
      "Iteration 66, loss = 0.43396348\n",
      "Iteration 67, loss = 0.43121339\n",
      "Iteration 68, loss = 0.43383102\n",
      "Iteration 69, loss = 0.43288531\n",
      "Iteration 70, loss = 0.43066559\n",
      "Iteration 71, loss = 0.43110282\n",
      "Iteration 72, loss = 0.43053752\n",
      "Iteration 73, loss = 0.43185669\n",
      "Iteration 74, loss = 0.43089469\n",
      "Iteration 75, loss = 0.42993555\n",
      "Iteration 76, loss = 0.42900870\n",
      "Iteration 77, loss = 0.43039359\n",
      "Iteration 78, loss = 0.42959718\n",
      "Iteration 79, loss = 0.43040667\n",
      "Iteration 80, loss = 0.42922303\n",
      "Iteration 81, loss = 0.43065594\n",
      "Iteration 82, loss = 0.43113190\n",
      "Iteration 83, loss = 0.42978120\n",
      "Iteration 84, loss = 0.43031213\n",
      "Iteration 85, loss = 0.42906160\n",
      "Iteration 86, loss = 0.42986414\n",
      "Iteration 87, loss = 0.42754090\n",
      "Iteration 88, loss = 0.43019450\n",
      "Iteration 89, loss = 0.43019814\n",
      "Iteration 90, loss = 0.42859137\n",
      "Iteration 91, loss = 0.42968525\n",
      "Iteration 92, loss = 0.42808221\n",
      "Iteration 93, loss = 0.42721442\n",
      "Iteration 94, loss = 0.42574356\n",
      "Iteration 95, loss = 0.42542054\n",
      "Iteration 96, loss = 0.42569443\n",
      "Iteration 97, loss = 0.42406868\n",
      "Iteration 98, loss = 0.42380124\n",
      "Iteration 99, loss = 0.42586647\n",
      "Iteration 100, loss = 0.42277579\n",
      "Iteration 101, loss = 0.42239572\n",
      "Iteration 102, loss = 0.42770383\n",
      "Iteration 103, loss = 0.42187260\n",
      "Iteration 104, loss = 0.42264833\n",
      "Iteration 105, loss = 0.41932994\n",
      "Iteration 106, loss = 0.42014608\n",
      "Iteration 107, loss = 0.41858799\n",
      "Iteration 108, loss = 0.41859266\n",
      "Iteration 109, loss = 0.41964989\n",
      "Iteration 110, loss = 0.41974441\n",
      "Iteration 111, loss = 0.42330334\n",
      "Iteration 112, loss = 0.41877206\n",
      "Iteration 113, loss = 0.41802452\n",
      "Iteration 114, loss = 0.41762712\n",
      "Iteration 115, loss = 0.41976300\n",
      "Iteration 116, loss = 0.41932133\n",
      "Iteration 117, loss = 0.41822487\n",
      "Iteration 118, loss = 0.41973191\n",
      "Iteration 119, loss = 0.41609396\n",
      "Iteration 120, loss = 0.41849220\n",
      "Iteration 121, loss = 0.41673263\n",
      "Iteration 122, loss = 0.41615252\n",
      "Iteration 123, loss = 0.41592115\n",
      "Iteration 124, loss = 0.41523733\n",
      "Iteration 125, loss = 0.41647500\n",
      "Iteration 126, loss = 0.41832410\n",
      "Iteration 127, loss = 0.41867075\n",
      "Iteration 128, loss = 0.41730085\n",
      "Iteration 129, loss = 0.41691062\n",
      "Iteration 130, loss = 0.41864229\n",
      "Iteration 131, loss = 0.41697662\n",
      "Iteration 132, loss = 0.41450066\n",
      "Iteration 133, loss = 0.41392968\n",
      "Iteration 134, loss = 0.41230801\n",
      "Iteration 135, loss = 0.41445470\n",
      "Iteration 136, loss = 0.41597545\n",
      "Iteration 137, loss = 0.41602962\n",
      "Iteration 138, loss = 0.41668046\n",
      "Iteration 139, loss = 0.41535550\n",
      "Iteration 140, loss = 0.41700449\n",
      "Iteration 141, loss = 0.41533775\n",
      "Iteration 142, loss = 0.41684658\n",
      "Iteration 143, loss = 0.41461616\n",
      "Iteration 144, loss = 0.41483111\n",
      "Iteration 145, loss = 0.41296511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.56064468\n",
      "Iteration 2, loss = 0.48975015\n",
      "Iteration 3, loss = 0.48529696\n",
      "Iteration 4, loss = 0.48282725\n",
      "Iteration 5, loss = 0.48165316\n",
      "Iteration 6, loss = 0.47963686\n",
      "Iteration 7, loss = 0.47895600\n",
      "Iteration 8, loss = 0.47853938\n",
      "Iteration 9, loss = 0.47746868\n",
      "Iteration 10, loss = 0.47758351\n",
      "Iteration 11, loss = 0.47631890\n",
      "Iteration 12, loss = 0.47614655\n",
      "Iteration 13, loss = 0.47870998\n",
      "Iteration 14, loss = 0.47428337\n",
      "Iteration 15, loss = 0.47236595\n",
      "Iteration 16, loss = 0.47151606\n",
      "Iteration 17, loss = 0.47312312\n",
      "Iteration 18, loss = 0.47117628\n",
      "Iteration 19, loss = 0.47001973\n",
      "Iteration 20, loss = 0.46928619\n",
      "Iteration 21, loss = 0.46820542\n",
      "Iteration 22, loss = 0.46797138\n",
      "Iteration 23, loss = 0.46748239\n",
      "Iteration 24, loss = 0.46298704\n",
      "Iteration 25, loss = 0.46133329\n",
      "Iteration 26, loss = 0.46030435\n",
      "Iteration 27, loss = 0.45847373\n",
      "Iteration 28, loss = 0.45784314\n",
      "Iteration 29, loss = 0.45794296\n",
      "Iteration 30, loss = 0.45824548\n",
      "Iteration 31, loss = 0.45850916\n",
      "Iteration 32, loss = 0.45768958\n",
      "Iteration 33, loss = 0.45965572\n",
      "Iteration 34, loss = 0.45702742\n",
      "Iteration 35, loss = 0.45903064\n",
      "Iteration 36, loss = 0.45610103\n",
      "Iteration 37, loss = 0.45710132\n",
      "Iteration 38, loss = 0.45740304\n",
      "Iteration 39, loss = 0.45670059\n",
      "Iteration 40, loss = 0.45519426\n",
      "Iteration 41, loss = 0.45419167\n",
      "Iteration 42, loss = 0.45359446\n",
      "Iteration 43, loss = 0.45315102\n",
      "Iteration 44, loss = 0.45228093\n",
      "Iteration 45, loss = 0.45346933\n",
      "Iteration 46, loss = 0.45231056\n",
      "Iteration 47, loss = 0.45068917\n",
      "Iteration 48, loss = 0.44928359\n",
      "Iteration 49, loss = 0.44747054\n",
      "Iteration 50, loss = 0.44642084\n",
      "Iteration 51, loss = 0.44450059\n",
      "Iteration 52, loss = 0.44387788\n",
      "Iteration 53, loss = 0.44248295\n",
      "Iteration 54, loss = 0.44105790\n",
      "Iteration 55, loss = 0.43895857\n",
      "Iteration 56, loss = 0.43909441\n",
      "Iteration 57, loss = 0.43739189\n",
      "Iteration 58, loss = 0.43732190\n",
      "Iteration 59, loss = 0.43544210\n",
      "Iteration 60, loss = 0.43842324\n",
      "Iteration 61, loss = 0.43564277\n",
      "Iteration 62, loss = 0.43494401\n",
      "Iteration 63, loss = 0.43742374\n",
      "Iteration 64, loss = 0.43391646\n",
      "Iteration 65, loss = 0.43507619\n",
      "Iteration 66, loss = 0.43726589\n",
      "Iteration 67, loss = 0.43618797\n",
      "Iteration 68, loss = 0.43264524\n",
      "Iteration 69, loss = 0.43316842\n",
      "Iteration 70, loss = 0.43344638\n",
      "Iteration 71, loss = 0.43544319\n",
      "Iteration 72, loss = 0.43335539\n",
      "Iteration 73, loss = 0.43472893\n",
      "Iteration 74, loss = 0.43278626\n",
      "Iteration 75, loss = 0.43265942\n",
      "Iteration 76, loss = 0.43807734\n",
      "Iteration 77, loss = 0.43543595\n",
      "Iteration 78, loss = 0.43236711\n",
      "Iteration 79, loss = 0.43431045\n",
      "Iteration 80, loss = 0.43209330\n",
      "Iteration 81, loss = 0.43167246\n",
      "Iteration 82, loss = 0.43456661\n",
      "Iteration 83, loss = 0.43181234\n",
      "Iteration 84, loss = 0.43115994\n",
      "Iteration 85, loss = 0.43354336\n",
      "Iteration 86, loss = 0.43538112\n",
      "Iteration 87, loss = 0.43304995\n",
      "Iteration 88, loss = 0.43227604\n",
      "Iteration 89, loss = 0.43324484\n",
      "Iteration 90, loss = 0.43196061\n",
      "Iteration 91, loss = 0.43118687\n",
      "Iteration 92, loss = 0.43559772\n",
      "Iteration 93, loss = 0.43167792\n",
      "Iteration 94, loss = 0.43154001\n",
      "Iteration 95, loss = 0.43133502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56187417\n",
      "Iteration 2, loss = 0.49219961\n",
      "Iteration 3, loss = 0.48532392\n",
      "Iteration 4, loss = 0.48175656\n",
      "Iteration 5, loss = 0.48070763\n",
      "Iteration 6, loss = 0.47897734\n",
      "Iteration 7, loss = 0.47738465\n",
      "Iteration 8, loss = 0.47851209\n",
      "Iteration 9, loss = 0.47609811\n",
      "Iteration 10, loss = 0.47660043\n",
      "Iteration 11, loss = 0.47627719\n",
      "Iteration 12, loss = 0.47529935\n",
      "Iteration 13, loss = 0.47313955\n",
      "Iteration 14, loss = 0.47415040\n",
      "Iteration 15, loss = 0.47148974\n",
      "Iteration 16, loss = 0.47222302\n",
      "Iteration 17, loss = 0.47217115\n",
      "Iteration 18, loss = 0.46960252\n",
      "Iteration 19, loss = 0.46867419\n",
      "Iteration 20, loss = 0.46995475\n",
      "Iteration 21, loss = 0.46767123\n",
      "Iteration 22, loss = 0.46702518\n",
      "Iteration 23, loss = 0.46866142\n",
      "Iteration 24, loss = 0.46656788\n",
      "Iteration 25, loss = 0.46569594\n",
      "Iteration 26, loss = 0.46536446\n",
      "Iteration 27, loss = 0.46493843\n",
      "Iteration 28, loss = 0.46572817\n",
      "Iteration 29, loss = 0.46742142\n",
      "Iteration 30, loss = 0.46592590\n",
      "Iteration 31, loss = 0.46419370\n",
      "Iteration 32, loss = 0.46365783\n",
      "Iteration 33, loss = 0.46153091\n",
      "Iteration 34, loss = 0.46060555\n",
      "Iteration 35, loss = 0.46023859\n",
      "Iteration 36, loss = 0.45897106\n",
      "Iteration 37, loss = 0.45828891\n",
      "Iteration 38, loss = 0.45820458\n",
      "Iteration 39, loss = 0.45542677\n",
      "Iteration 40, loss = 0.45366980\n",
      "Iteration 41, loss = 0.45353478\n",
      "Iteration 42, loss = 0.45109055\n",
      "Iteration 43, loss = 0.45052372\n",
      "Iteration 44, loss = 0.44971776\n",
      "Iteration 45, loss = 0.44695309\n",
      "Iteration 46, loss = 0.44598616\n",
      "Iteration 47, loss = 0.44864738\n",
      "Iteration 48, loss = 0.44724645\n",
      "Iteration 49, loss = 0.44448206\n",
      "Iteration 50, loss = 0.44274724\n",
      "Iteration 51, loss = 0.44125185\n",
      "Iteration 52, loss = 0.44120042\n",
      "Iteration 53, loss = 0.43745245\n",
      "Iteration 54, loss = 0.43883286\n",
      "Iteration 55, loss = 0.44072582\n",
      "Iteration 56, loss = 0.43799777\n",
      "Iteration 57, loss = 0.44162612\n",
      "Iteration 58, loss = 0.43848494\n",
      "Iteration 59, loss = 0.43655717\n",
      "Iteration 60, loss = 0.44018743\n",
      "Iteration 61, loss = 0.43713763\n",
      "Iteration 62, loss = 0.43675434\n",
      "Iteration 63, loss = 0.43946194\n",
      "Iteration 64, loss = 0.44322690\n",
      "Iteration 65, loss = 0.43587425\n",
      "Iteration 66, loss = 0.43591710\n",
      "Iteration 67, loss = 0.43725250\n",
      "Iteration 68, loss = 0.43831499\n",
      "Iteration 69, loss = 0.43517151\n",
      "Iteration 70, loss = 0.43933823\n",
      "Iteration 71, loss = 0.43667698\n",
      "Iteration 72, loss = 0.43541313\n",
      "Iteration 73, loss = 0.43459633\n",
      "Iteration 74, loss = 0.43430644\n",
      "Iteration 75, loss = 0.43465952\n",
      "Iteration 76, loss = 0.43442545\n",
      "Iteration 77, loss = 0.43735203\n",
      "Iteration 78, loss = 0.43749565\n",
      "Iteration 79, loss = 0.43824402\n",
      "Iteration 80, loss = 0.43456798\n",
      "Iteration 81, loss = 0.43643525\n",
      "Iteration 82, loss = 0.43638682\n",
      "Iteration 83, loss = 0.43341632\n",
      "Iteration 84, loss = 0.43422021\n",
      "Iteration 85, loss = 0.43531259\n",
      "Iteration 86, loss = 0.43532761\n",
      "Iteration 87, loss = 0.43375404\n",
      "Iteration 88, loss = 0.43534661\n",
      "Iteration 89, loss = 0.43653733\n",
      "Iteration 90, loss = 0.43466921\n",
      "Iteration 91, loss = 0.43390765\n",
      "Iteration 92, loss = 0.43299926\n",
      "Iteration 93, loss = 0.43431374\n",
      "Iteration 94, loss = 0.43257932\n",
      "Iteration 95, loss = 0.43632289\n",
      "Iteration 96, loss = 0.43528317\n",
      "Iteration 97, loss = 0.43694331\n",
      "Iteration 98, loss = 0.43620970\n",
      "Iteration 99, loss = 0.43456792\n",
      "Iteration 100, loss = 0.43776615\n",
      "Iteration 101, loss = 0.43351341\n",
      "Iteration 102, loss = 0.43340261\n",
      "Iteration 103, loss = 0.43606812\n",
      "Iteration 104, loss = 0.43517391\n",
      "Iteration 105, loss = 0.43706601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55632277\n",
      "Iteration 2, loss = 0.49464194\n",
      "Iteration 3, loss = 0.48981151\n",
      "Iteration 4, loss = 0.48817967\n",
      "Iteration 5, loss = 0.48608881\n",
      "Iteration 6, loss = 0.48529340\n",
      "Iteration 7, loss = 0.48452666\n",
      "Iteration 8, loss = 0.48113372\n",
      "Iteration 9, loss = 0.47948609\n",
      "Iteration 10, loss = 0.48080721\n",
      "Iteration 11, loss = 0.47933645\n",
      "Iteration 12, loss = 0.47763186\n",
      "Iteration 13, loss = 0.47813164\n",
      "Iteration 14, loss = 0.47623040\n",
      "Iteration 15, loss = 0.47644893\n",
      "Iteration 16, loss = 0.47549673\n",
      "Iteration 17, loss = 0.47495009\n",
      "Iteration 18, loss = 0.47456508\n",
      "Iteration 19, loss = 0.47252392\n",
      "Iteration 20, loss = 0.47152175\n",
      "Iteration 21, loss = 0.46970642\n",
      "Iteration 22, loss = 0.47079344\n",
      "Iteration 23, loss = 0.47276549\n",
      "Iteration 24, loss = 0.47146509\n",
      "Iteration 25, loss = 0.46954997\n",
      "Iteration 26, loss = 0.46879866\n",
      "Iteration 27, loss = 0.46718820\n",
      "Iteration 28, loss = 0.46646591\n",
      "Iteration 29, loss = 0.46463158\n",
      "Iteration 30, loss = 0.46270648\n",
      "Iteration 31, loss = 0.46250176\n",
      "Iteration 32, loss = 0.46099798\n",
      "Iteration 33, loss = 0.46332919\n",
      "Iteration 34, loss = 0.46042459\n",
      "Iteration 35, loss = 0.45676017\n",
      "Iteration 36, loss = 0.45450424\n",
      "Iteration 37, loss = 0.45477619\n",
      "Iteration 38, loss = 0.45916580\n",
      "Iteration 39, loss = 0.45459232\n",
      "Iteration 40, loss = 0.45437353\n",
      "Iteration 41, loss = 0.45304424\n",
      "Iteration 42, loss = 0.45481893\n",
      "Iteration 43, loss = 0.45388612\n",
      "Iteration 44, loss = 0.45248438\n",
      "Iteration 45, loss = 0.45336873\n",
      "Iteration 46, loss = 0.45340151\n",
      "Iteration 47, loss = 0.45299607\n",
      "Iteration 48, loss = 0.45263834\n",
      "Iteration 49, loss = 0.45575642\n",
      "Iteration 50, loss = 0.45252956\n",
      "Iteration 51, loss = 0.45358460\n",
      "Iteration 52, loss = 0.45301121\n",
      "Iteration 53, loss = 0.45197978\n",
      "Iteration 54, loss = 0.44943234\n",
      "Iteration 55, loss = 0.45025686\n",
      "Iteration 56, loss = 0.45085324\n",
      "Iteration 57, loss = 0.45076797\n",
      "Iteration 58, loss = 0.44934472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 59, loss = 0.44866978\n",
      "Iteration 60, loss = 0.44838123\n",
      "Iteration 61, loss = 0.45142603\n",
      "Iteration 62, loss = 0.44780896\n",
      "Iteration 63, loss = 0.44756755\n",
      "Iteration 64, loss = 0.44697815\n",
      "Iteration 65, loss = 0.45154818\n",
      "Iteration 66, loss = 0.44707072\n",
      "Iteration 67, loss = 0.44631315\n",
      "Iteration 68, loss = 0.44628676\n",
      "Iteration 69, loss = 0.44793686\n",
      "Iteration 70, loss = 0.44612174\n",
      "Iteration 71, loss = 0.44485403\n",
      "Iteration 72, loss = 0.44475506\n",
      "Iteration 73, loss = 0.44576354\n",
      "Iteration 74, loss = 0.44573665\n",
      "Iteration 75, loss = 0.44427216\n",
      "Iteration 76, loss = 0.44464940\n",
      "Iteration 77, loss = 0.44605557\n",
      "Iteration 78, loss = 0.44571450\n",
      "Iteration 79, loss = 0.44471604\n",
      "Iteration 80, loss = 0.44578615\n",
      "Iteration 81, loss = 0.44555169\n",
      "Iteration 82, loss = 0.44519259\n",
      "Iteration 83, loss = 0.44546107\n",
      "Iteration 84, loss = 0.45119702\n",
      "Iteration 85, loss = 0.44443760\n",
      "Iteration 86, loss = 0.44321578\n",
      "Iteration 87, loss = 0.44454620\n",
      "Iteration 88, loss = 0.44449691\n",
      "Iteration 89, loss = 0.44518747\n",
      "Iteration 90, loss = 0.44511676\n",
      "Iteration 91, loss = 0.44666622\n",
      "Iteration 92, loss = 0.44589838\n",
      "Iteration 93, loss = 0.44385892\n",
      "Iteration 94, loss = 0.44550536\n",
      "Iteration 95, loss = 0.44799808\n",
      "Iteration 96, loss = 0.44409155\n",
      "Iteration 97, loss = 0.44829177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55527239\n",
      "Iteration 2, loss = 0.48988458\n",
      "Iteration 3, loss = 0.48664374\n",
      "Iteration 4, loss = 0.48371996\n",
      "Iteration 5, loss = 0.48208553\n",
      "Iteration 6, loss = 0.47980397\n",
      "Iteration 7, loss = 0.47863936\n",
      "Iteration 8, loss = 0.47692226\n",
      "Iteration 9, loss = 0.47577233\n",
      "Iteration 10, loss = 0.47637849\n",
      "Iteration 11, loss = 0.47556852\n",
      "Iteration 12, loss = 0.47419485\n",
      "Iteration 13, loss = 0.47814139\n",
      "Iteration 14, loss = 0.47280981\n",
      "Iteration 15, loss = 0.47065559\n",
      "Iteration 16, loss = 0.46933163\n",
      "Iteration 17, loss = 0.46959248\n",
      "Iteration 18, loss = 0.46884360\n",
      "Iteration 19, loss = 0.46888891\n",
      "Iteration 20, loss = 0.46870545\n",
      "Iteration 21, loss = 0.46857927\n",
      "Iteration 22, loss = 0.46975445\n",
      "Iteration 23, loss = 0.46803711\n",
      "Iteration 24, loss = 0.46288254\n",
      "Iteration 25, loss = 0.46131824\n",
      "Iteration 26, loss = 0.45751405\n",
      "Iteration 27, loss = 0.45577207\n",
      "Iteration 28, loss = 0.45397413\n",
      "Iteration 29, loss = 0.45378279\n",
      "Iteration 30, loss = 0.45415938\n",
      "Iteration 31, loss = 0.45446398\n",
      "Iteration 32, loss = 0.45232190\n",
      "Iteration 33, loss = 0.45300323\n",
      "Iteration 34, loss = 0.45026542\n",
      "Iteration 35, loss = 0.45051293\n",
      "Iteration 36, loss = 0.45020036\n",
      "Iteration 37, loss = 0.44990085\n",
      "Iteration 38, loss = 0.45332895\n",
      "Iteration 39, loss = 0.45214802\n",
      "Iteration 40, loss = 0.44978937\n",
      "Iteration 41, loss = 0.44961959\n",
      "Iteration 42, loss = 0.44866830\n",
      "Iteration 43, loss = 0.44757372\n",
      "Iteration 44, loss = 0.44730239\n",
      "Iteration 45, loss = 0.44689452\n",
      "Iteration 46, loss = 0.44572399\n",
      "Iteration 47, loss = 0.44427303\n",
      "Iteration 48, loss = 0.44648526\n",
      "Iteration 49, loss = 0.44405193\n",
      "Iteration 50, loss = 0.44645535\n",
      "Iteration 51, loss = 0.44356322\n",
      "Iteration 52, loss = 0.44447448\n",
      "Iteration 53, loss = 0.44369744\n",
      "Iteration 54, loss = 0.44133343\n",
      "Iteration 55, loss = 0.44064455\n",
      "Iteration 56, loss = 0.44423191\n",
      "Iteration 57, loss = 0.44435302\n",
      "Iteration 58, loss = 0.44117814\n",
      "Iteration 59, loss = 0.43969367\n",
      "Iteration 60, loss = 0.43925369\n",
      "Iteration 61, loss = 0.43796292\n",
      "Iteration 62, loss = 0.43664767\n",
      "Iteration 63, loss = 0.43850725\n",
      "Iteration 64, loss = 0.43847432\n",
      "Iteration 65, loss = 0.43848325\n",
      "Iteration 66, loss = 0.44143766\n",
      "Iteration 67, loss = 0.43744489\n",
      "Iteration 68, loss = 0.43376339\n",
      "Iteration 69, loss = 0.43362069\n",
      "Iteration 70, loss = 0.43478277\n",
      "Iteration 71, loss = 0.43475122\n",
      "Iteration 72, loss = 0.43397355\n",
      "Iteration 73, loss = 0.43446021\n",
      "Iteration 74, loss = 0.43383757\n",
      "Iteration 75, loss = 0.43358143\n",
      "Iteration 76, loss = 0.43559537\n",
      "Iteration 77, loss = 0.43422046\n",
      "Iteration 78, loss = 0.43318795\n",
      "Iteration 79, loss = 0.43458497\n",
      "Iteration 80, loss = 0.43235783\n",
      "Iteration 81, loss = 0.43413742\n",
      "Iteration 82, loss = 0.43161484\n",
      "Iteration 83, loss = 0.43160148\n",
      "Iteration 84, loss = 0.43315055\n",
      "Iteration 85, loss = 0.43249659\n",
      "Iteration 86, loss = 0.43405155\n",
      "Iteration 87, loss = 0.43145053\n",
      "Iteration 88, loss = 0.43044741\n",
      "Iteration 89, loss = 0.43189968\n",
      "Iteration 90, loss = 0.43176284\n",
      "Iteration 91, loss = 0.43068786\n",
      "Iteration 92, loss = 0.43072846\n",
      "Iteration 93, loss = 0.43022745\n",
      "Iteration 94, loss = 0.42941140\n",
      "Iteration 95, loss = 0.43048500\n",
      "Iteration 96, loss = 0.43079810\n",
      "Iteration 97, loss = 0.43140702\n",
      "Iteration 98, loss = 0.43107540\n",
      "Iteration 99, loss = 0.42964226\n",
      "Iteration 100, loss = 0.43052614\n",
      "Iteration 101, loss = 0.43211490\n",
      "Iteration 102, loss = 0.43050853\n",
      "Iteration 103, loss = 0.42876944\n",
      "Iteration 104, loss = 0.42907397\n",
      "Iteration 105, loss = 0.42873765\n",
      "Iteration 106, loss = 0.42959327\n",
      "Iteration 107, loss = 0.43325137\n",
      "Iteration 108, loss = 0.42929567\n",
      "Iteration 109, loss = 0.43046886\n",
      "Iteration 110, loss = 0.42857540\n",
      "Iteration 111, loss = 0.43038136\n",
      "Iteration 112, loss = 0.42877365\n",
      "Iteration 113, loss = 0.42934249\n",
      "Iteration 114, loss = 0.43017602\n",
      "Iteration 115, loss = 0.43051814\n",
      "Iteration 116, loss = 0.43199468\n",
      "Iteration 117, loss = 0.42944499\n",
      "Iteration 118, loss = 0.42819464\n",
      "Iteration 119, loss = 0.42977901\n",
      "Iteration 120, loss = 0.42936400\n",
      "Iteration 121, loss = 0.43047081\n",
      "Iteration 122, loss = 0.42836697\n",
      "Iteration 123, loss = 0.42945584\n",
      "Iteration 124, loss = 0.42873758\n",
      "Iteration 125, loss = 0.42810026\n",
      "Iteration 126, loss = 0.42735796\n",
      "Iteration 127, loss = 0.42902363\n",
      "Iteration 128, loss = 0.42811970\n",
      "Iteration 129, loss = 0.42761406\n",
      "Iteration 130, loss = 0.42766236\n",
      "Iteration 131, loss = 0.42763631\n",
      "Iteration 132, loss = 0.42764125\n",
      "Iteration 133, loss = 0.42748236\n",
      "Iteration 134, loss = 0.42755638\n",
      "Iteration 135, loss = 0.42747422\n",
      "Iteration 136, loss = 0.42655664\n",
      "Iteration 137, loss = 0.42937512\n",
      "Iteration 138, loss = 0.42977377\n",
      "Iteration 139, loss = 0.43142433\n",
      "Iteration 140, loss = 0.42757613\n",
      "Iteration 141, loss = 0.42669906\n",
      "Iteration 142, loss = 0.42825747\n",
      "Iteration 143, loss = 0.42877836\n",
      "Iteration 144, loss = 0.42790449\n",
      "Iteration 145, loss = 0.42781138\n",
      "Iteration 146, loss = 0.42638101\n",
      "Iteration 147, loss = 0.42664604\n",
      "Iteration 148, loss = 0.42830645\n",
      "Iteration 149, loss = 0.42798112\n",
      "Iteration 150, loss = 0.42838901\n",
      "Iteration 151, loss = 0.42648845\n",
      "Iteration 152, loss = 0.42687546\n",
      "Iteration 153, loss = 0.42575917\n",
      "Iteration 154, loss = 0.42597895\n",
      "Iteration 155, loss = 0.42879350\n",
      "Iteration 156, loss = 0.42707707\n",
      "Iteration 157, loss = 0.42785409\n",
      "Iteration 158, loss = 0.42637879\n",
      "Iteration 159, loss = 0.42601388\n",
      "Iteration 160, loss = 0.42937796\n",
      "Iteration 161, loss = 0.42569958\n",
      "Iteration 162, loss = 0.42554760\n",
      "Iteration 163, loss = 0.42545784\n",
      "Iteration 164, loss = 0.42817276\n",
      "Iteration 165, loss = 0.42679042\n",
      "Iteration 166, loss = 0.42833776\n",
      "Iteration 167, loss = 0.42771993\n",
      "Iteration 168, loss = 0.42729583\n",
      "Iteration 169, loss = 0.42708376\n",
      "Iteration 170, loss = 0.42784742\n",
      "Iteration 171, loss = 0.42569617\n",
      "Iteration 172, loss = 0.42564829\n",
      "Iteration 173, loss = 0.42581856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55501890\n",
      "Iteration 2, loss = 0.49164926\n",
      "Iteration 3, loss = 0.48809496\n",
      "Iteration 4, loss = 0.48687868\n",
      "Iteration 5, loss = 0.48508161\n",
      "Iteration 6, loss = 0.48443137\n",
      "Iteration 7, loss = 0.48272803\n",
      "Iteration 8, loss = 0.48404900\n",
      "Iteration 9, loss = 0.48295736\n",
      "Iteration 10, loss = 0.48265143\n",
      "Iteration 11, loss = 0.48180178\n",
      "Iteration 12, loss = 0.48181790\n",
      "Iteration 13, loss = 0.47965350\n",
      "Iteration 14, loss = 0.48043083\n",
      "Iteration 15, loss = 0.47884600\n",
      "Iteration 16, loss = 0.47975805\n",
      "Iteration 17, loss = 0.47851774\n",
      "Iteration 18, loss = 0.47775783\n",
      "Iteration 19, loss = 0.47510515\n",
      "Iteration 20, loss = 0.47725745\n",
      "Iteration 21, loss = 0.47614002\n",
      "Iteration 22, loss = 0.47512364\n",
      "Iteration 23, loss = 0.47487731\n",
      "Iteration 24, loss = 0.47416999\n",
      "Iteration 25, loss = 0.47421915\n",
      "Iteration 26, loss = 0.47367554\n",
      "Iteration 27, loss = 0.47311040\n",
      "Iteration 28, loss = 0.47220873\n",
      "Iteration 29, loss = 0.47563049\n",
      "Iteration 30, loss = 0.47564661\n",
      "Iteration 31, loss = 0.47313300\n",
      "Iteration 32, loss = 0.47412551\n",
      "Iteration 33, loss = 0.47143277\n",
      "Iteration 34, loss = 0.47339522\n",
      "Iteration 35, loss = 0.46951309\n",
      "Iteration 36, loss = 0.46955211\n",
      "Iteration 37, loss = 0.46916164\n",
      "Iteration 38, loss = 0.46792693\n",
      "Iteration 39, loss = 0.46705004\n",
      "Iteration 40, loss = 0.46855681\n",
      "Iteration 41, loss = 0.46864313\n",
      "Iteration 42, loss = 0.46919859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 43, loss = 0.46756165\n",
      "Iteration 44, loss = 0.46419440\n",
      "Iteration 45, loss = 0.46529073\n",
      "Iteration 46, loss = 0.46442399\n",
      "Iteration 47, loss = 0.46664570\n",
      "Iteration 48, loss = 0.46790995\n",
      "Iteration 49, loss = 0.46441611\n",
      "Iteration 50, loss = 0.46523918\n",
      "Iteration 51, loss = 0.46235693\n",
      "Iteration 52, loss = 0.46212861\n",
      "Iteration 53, loss = 0.46109499\n",
      "Iteration 54, loss = 0.46019470\n",
      "Iteration 55, loss = 0.46009678\n",
      "Iteration 56, loss = 0.45883548\n",
      "Iteration 57, loss = 0.46372842\n",
      "Iteration 58, loss = 0.46220144\n",
      "Iteration 59, loss = 0.45911542\n",
      "Iteration 60, loss = 0.46066892\n",
      "Iteration 61, loss = 0.46116956\n",
      "Iteration 62, loss = 0.46044263\n",
      "Iteration 63, loss = 0.46230875\n",
      "Iteration 64, loss = 0.45965993\n",
      "Iteration 65, loss = 0.45626005\n",
      "Iteration 66, loss = 0.45660104\n",
      "Iteration 67, loss = 0.45657359\n",
      "Iteration 68, loss = 0.45629898\n",
      "Iteration 69, loss = 0.45696016\n",
      "Iteration 70, loss = 0.45763572\n",
      "Iteration 71, loss = 0.45825439\n",
      "Iteration 72, loss = 0.45759843\n",
      "Iteration 73, loss = 0.45635046\n",
      "Iteration 74, loss = 0.45441331\n",
      "Iteration 75, loss = 0.45346708\n",
      "Iteration 76, loss = 0.45311303\n",
      "Iteration 77, loss = 0.45427055\n",
      "Iteration 78, loss = 0.45804632\n",
      "Iteration 79, loss = 0.45622544\n",
      "Iteration 80, loss = 0.45494132\n",
      "Iteration 81, loss = 0.45391208\n",
      "Iteration 82, loss = 0.45581479\n",
      "Iteration 83, loss = 0.45278898\n",
      "Iteration 84, loss = 0.45187397\n",
      "Iteration 85, loss = 0.45047440\n",
      "Iteration 86, loss = 0.45175051\n",
      "Iteration 87, loss = 0.44978864\n",
      "Iteration 88, loss = 0.45502552\n",
      "Iteration 89, loss = 0.45550625\n",
      "Iteration 90, loss = 0.45455021\n",
      "Iteration 91, loss = 0.45336969\n",
      "Iteration 92, loss = 0.45033589\n",
      "Iteration 93, loss = 0.45126366\n",
      "Iteration 94, loss = 0.44926295\n",
      "Iteration 95, loss = 0.44832687\n",
      "Iteration 96, loss = 0.45345391\n",
      "Iteration 97, loss = 0.45014499\n",
      "Iteration 98, loss = 0.44935294\n",
      "Iteration 99, loss = 0.44987060\n",
      "Iteration 100, loss = 0.44902583\n",
      "Iteration 101, loss = 0.44928607\n",
      "Iteration 102, loss = 0.45079097\n",
      "Iteration 103, loss = 0.44970749\n",
      "Iteration 104, loss = 0.45290922\n",
      "Iteration 105, loss = 0.45149547\n",
      "Iteration 106, loss = 0.44749795\n",
      "Iteration 107, loss = 0.45081970\n",
      "Iteration 108, loss = 0.45306855\n",
      "Iteration 109, loss = 0.45225131\n",
      "Iteration 110, loss = 0.44787167\n",
      "Iteration 111, loss = 0.45190888\n",
      "Iteration 112, loss = 0.44799205\n",
      "Iteration 113, loss = 0.44821417\n",
      "Iteration 114, loss = 0.44784937\n",
      "Iteration 115, loss = 0.44951902\n",
      "Iteration 116, loss = 0.44785764\n",
      "Iteration 117, loss = 0.44764323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56204479\n",
      "Iteration 2, loss = 0.49838621\n",
      "Iteration 3, loss = 0.49171843\n",
      "Iteration 4, loss = 0.48873508\n",
      "Iteration 5, loss = 0.48603916\n",
      "Iteration 6, loss = 0.48438528\n",
      "Iteration 7, loss = 0.48327571\n",
      "Iteration 8, loss = 0.48102594\n",
      "Iteration 9, loss = 0.48019003\n",
      "Iteration 10, loss = 0.48114577\n",
      "Iteration 11, loss = 0.47947375\n",
      "Iteration 12, loss = 0.47877736\n",
      "Iteration 13, loss = 0.47954343\n",
      "Iteration 14, loss = 0.47793411\n",
      "Iteration 15, loss = 0.47822348\n",
      "Iteration 16, loss = 0.47878193\n",
      "Iteration 17, loss = 0.47904122\n",
      "Iteration 18, loss = 0.47833359\n",
      "Iteration 19, loss = 0.47629332\n",
      "Iteration 20, loss = 0.47645490\n",
      "Iteration 21, loss = 0.47531141\n",
      "Iteration 22, loss = 0.47623813\n",
      "Iteration 23, loss = 0.47873298\n",
      "Iteration 24, loss = 0.47492083\n",
      "Iteration 25, loss = 0.47086543\n",
      "Iteration 26, loss = 0.46950716\n",
      "Iteration 27, loss = 0.46616386\n",
      "Iteration 28, loss = 0.46487983\n",
      "Iteration 29, loss = 0.46558169\n",
      "Iteration 30, loss = 0.46228412\n",
      "Iteration 31, loss = 0.46086801\n",
      "Iteration 32, loss = 0.46204507\n",
      "Iteration 33, loss = 0.46260849\n",
      "Iteration 34, loss = 0.46318266\n",
      "Iteration 35, loss = 0.46088387\n",
      "Iteration 36, loss = 0.46006235\n",
      "Iteration 37, loss = 0.45895568\n",
      "Iteration 38, loss = 0.46466943\n",
      "Iteration 39, loss = 0.46108042\n",
      "Iteration 40, loss = 0.46049779\n",
      "Iteration 41, loss = 0.45750073\n",
      "Iteration 42, loss = 0.45938095\n",
      "Iteration 43, loss = 0.45885554\n",
      "Iteration 44, loss = 0.45751473\n",
      "Iteration 45, loss = 0.45743006\n",
      "Iteration 46, loss = 0.45812008\n",
      "Iteration 47, loss = 0.45660022\n",
      "Iteration 48, loss = 0.45742187\n",
      "Iteration 49, loss = 0.45916510\n",
      "Iteration 50, loss = 0.45910389\n",
      "Iteration 51, loss = 0.45755932\n",
      "Iteration 52, loss = 0.45859083\n",
      "Iteration 53, loss = 0.45886245\n",
      "Iteration 54, loss = 0.45585175\n",
      "Iteration 55, loss = 0.45617894\n",
      "Iteration 56, loss = 0.45597515\n",
      "Iteration 57, loss = 0.45706218\n",
      "Iteration 58, loss = 0.45671453\n",
      "Iteration 59, loss = 0.45605206\n",
      "Iteration 60, loss = 0.45509906\n",
      "Iteration 61, loss = 0.45989848\n",
      "Iteration 62, loss = 0.45876031\n",
      "Iteration 63, loss = 0.45595176\n",
      "Iteration 64, loss = 0.45532714\n",
      "Iteration 65, loss = 0.46141328\n",
      "Iteration 66, loss = 0.45712632\n",
      "Iteration 67, loss = 0.45521227\n",
      "Iteration 68, loss = 0.45700758\n",
      "Iteration 69, loss = 0.45673115\n",
      "Iteration 70, loss = 0.45551482\n",
      "Iteration 71, loss = 0.45384489\n",
      "Iteration 72, loss = 0.45373313\n",
      "Iteration 73, loss = 0.45528963\n",
      "Iteration 74, loss = 0.45441696\n",
      "Iteration 75, loss = 0.45370863\n",
      "Iteration 76, loss = 0.45619720\n",
      "Iteration 77, loss = 0.45605352\n",
      "Iteration 78, loss = 0.45577189\n",
      "Iteration 79, loss = 0.45441776\n",
      "Iteration 80, loss = 0.45466352\n",
      "Iteration 81, loss = 0.45435694\n",
      "Iteration 82, loss = 0.45493426\n",
      "Iteration 83, loss = 0.45456090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56351624\n",
      "Iteration 2, loss = 0.49536076\n",
      "Iteration 3, loss = 0.48992102\n",
      "Iteration 4, loss = 0.48534964\n",
      "Iteration 5, loss = 0.48275252\n",
      "Iteration 6, loss = 0.48034325\n",
      "Iteration 7, loss = 0.47874394\n",
      "Iteration 8, loss = 0.47720487\n",
      "Iteration 9, loss = 0.47433484\n",
      "Iteration 10, loss = 0.47354623\n",
      "Iteration 11, loss = 0.47231495\n",
      "Iteration 12, loss = 0.47186537\n",
      "Iteration 13, loss = 0.47590338\n",
      "Iteration 14, loss = 0.47026384\n",
      "Iteration 15, loss = 0.46859330\n",
      "Iteration 16, loss = 0.46836376\n",
      "Iteration 17, loss = 0.46928528\n",
      "Iteration 18, loss = 0.46836985\n",
      "Iteration 19, loss = 0.46815616\n",
      "Iteration 20, loss = 0.46823628\n",
      "Iteration 21, loss = 0.46902450\n",
      "Iteration 22, loss = 0.46823114\n",
      "Iteration 23, loss = 0.47058537\n",
      "Iteration 24, loss = 0.46678623\n",
      "Iteration 25, loss = 0.46685723\n",
      "Iteration 26, loss = 0.46638824\n",
      "Iteration 27, loss = 0.46630581\n",
      "Iteration 28, loss = 0.46523149\n",
      "Iteration 29, loss = 0.46561459\n",
      "Iteration 30, loss = 0.46552207\n",
      "Iteration 31, loss = 0.46578507\n",
      "Iteration 32, loss = 0.46596179\n",
      "Iteration 33, loss = 0.46685602\n",
      "Iteration 34, loss = 0.46242236\n",
      "Iteration 35, loss = 0.46525423\n",
      "Iteration 36, loss = 0.46292584\n",
      "Iteration 37, loss = 0.46089033\n",
      "Iteration 38, loss = 0.46286878\n",
      "Iteration 39, loss = 0.46079246\n",
      "Iteration 40, loss = 0.45994675\n",
      "Iteration 41, loss = 0.46001872\n",
      "Iteration 42, loss = 0.45933175\n",
      "Iteration 43, loss = 0.45832698\n",
      "Iteration 44, loss = 0.45898827\n",
      "Iteration 45, loss = 0.45948342\n",
      "Iteration 46, loss = 0.45783167\n",
      "Iteration 47, loss = 0.45930843\n",
      "Iteration 48, loss = 0.45903898\n",
      "Iteration 49, loss = 0.45740902\n",
      "Iteration 50, loss = 0.45781502\n",
      "Iteration 51, loss = 0.45733548\n",
      "Iteration 52, loss = 0.45811866\n",
      "Iteration 53, loss = 0.45614109\n",
      "Iteration 54, loss = 0.45532611\n",
      "Iteration 55, loss = 0.45474618\n",
      "Iteration 56, loss = 0.45666751\n",
      "Iteration 57, loss = 0.45669376\n",
      "Iteration 58, loss = 0.45610709\n",
      "Iteration 59, loss = 0.45565294\n",
      "Iteration 60, loss = 0.45532885\n",
      "Iteration 61, loss = 0.45393745\n",
      "Iteration 62, loss = 0.45351430\n",
      "Iteration 63, loss = 0.45417595\n",
      "Iteration 64, loss = 0.45570065\n",
      "Iteration 65, loss = 0.45784827\n",
      "Iteration 66, loss = 0.46067985\n",
      "Iteration 67, loss = 0.45866211\n",
      "Iteration 68, loss = 0.45384851\n",
      "Iteration 69, loss = 0.45242315\n",
      "Iteration 70, loss = 0.45312042\n",
      "Iteration 71, loss = 0.45357791\n",
      "Iteration 72, loss = 0.45218104\n",
      "Iteration 73, loss = 0.45271309\n",
      "Iteration 74, loss = 0.45274326\n",
      "Iteration 75, loss = 0.45422472\n",
      "Iteration 76, loss = 0.45767544\n",
      "Iteration 77, loss = 0.45443826\n",
      "Iteration 78, loss = 0.45433361\n",
      "Iteration 79, loss = 0.45428072\n",
      "Iteration 80, loss = 0.45342120\n",
      "Iteration 81, loss = 0.45399790\n",
      "Iteration 82, loss = 0.45435004\n",
      "Iteration 83, loss = 0.45173059\n",
      "Iteration 84, loss = 0.45134350\n",
      "Iteration 85, loss = 0.45324122\n",
      "Iteration 86, loss = 0.45376581\n",
      "Iteration 87, loss = 0.45257597\n",
      "Iteration 88, loss = 0.45127091\n",
      "Iteration 89, loss = 0.45369181\n",
      "Iteration 90, loss = 0.45352191\n",
      "Iteration 91, loss = 0.45170234\n",
      "Iteration 92, loss = 0.45465647\n",
      "Iteration 93, loss = 0.45088040\n",
      "Iteration 94, loss = 0.45219851\n",
      "Iteration 95, loss = 0.45143054\n",
      "Iteration 96, loss = 0.45279186\n",
      "Iteration 97, loss = 0.45531365\n",
      "Iteration 98, loss = 0.45229913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 0.45195229\n",
      "Iteration 100, loss = 0.45256315\n",
      "Iteration 101, loss = 0.45152678\n",
      "Iteration 102, loss = 0.45331814\n",
      "Iteration 103, loss = 0.45149343\n",
      "Iteration 104, loss = 0.45158365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56041157\n",
      "Iteration 2, loss = 0.49480825\n",
      "Iteration 3, loss = 0.48789826\n",
      "Iteration 4, loss = 0.48612536\n",
      "Iteration 5, loss = 0.48317119\n",
      "Iteration 6, loss = 0.48231109\n",
      "Iteration 7, loss = 0.48118981\n",
      "Iteration 8, loss = 0.48177119\n",
      "Iteration 9, loss = 0.47989689\n",
      "Iteration 10, loss = 0.48072377\n",
      "Iteration 11, loss = 0.47885284\n",
      "Iteration 12, loss = 0.47860754\n",
      "Iteration 13, loss = 0.47711979\n",
      "Iteration 14, loss = 0.47770855\n",
      "Iteration 15, loss = 0.47584300\n",
      "Iteration 16, loss = 0.47579012\n",
      "Iteration 17, loss = 0.47547174\n",
      "Iteration 18, loss = 0.47412129\n",
      "Iteration 19, loss = 0.47249039\n",
      "Iteration 20, loss = 0.47383162\n",
      "Iteration 21, loss = 0.47132362\n",
      "Iteration 22, loss = 0.47232043\n",
      "Iteration 23, loss = 0.47216579\n",
      "Iteration 24, loss = 0.47077335\n",
      "Iteration 25, loss = 0.46981365\n",
      "Iteration 26, loss = 0.47002515\n",
      "Iteration 27, loss = 0.46974066\n",
      "Iteration 28, loss = 0.47025947\n",
      "Iteration 29, loss = 0.47330634\n",
      "Iteration 30, loss = 0.47142166\n",
      "Iteration 31, loss = 0.47050210\n",
      "Iteration 32, loss = 0.47096057\n",
      "Iteration 33, loss = 0.46925115\n",
      "Iteration 34, loss = 0.46798089\n",
      "Iteration 35, loss = 0.46715064\n",
      "Iteration 36, loss = 0.46830219\n",
      "Iteration 37, loss = 0.46746149\n",
      "Iteration 38, loss = 0.46584812\n",
      "Iteration 39, loss = 0.46525996\n",
      "Iteration 40, loss = 0.46586899\n",
      "Iteration 41, loss = 0.46752209\n",
      "Iteration 42, loss = 0.46681126\n",
      "Iteration 43, loss = 0.46687271\n",
      "Iteration 44, loss = 0.46756596\n",
      "Iteration 45, loss = 0.46772063\n",
      "Iteration 46, loss = 0.46459253\n",
      "Iteration 47, loss = 0.47143102\n",
      "Iteration 48, loss = 0.46745002\n",
      "Iteration 49, loss = 0.46375995\n",
      "Iteration 50, loss = 0.46399092\n",
      "Iteration 51, loss = 0.46181486\n",
      "Iteration 52, loss = 0.46135947\n",
      "Iteration 53, loss = 0.46010007\n",
      "Iteration 54, loss = 0.46239894\n",
      "Iteration 55, loss = 0.46022067\n",
      "Iteration 56, loss = 0.45970273\n",
      "Iteration 57, loss = 0.46056780\n",
      "Iteration 58, loss = 0.45764806\n",
      "Iteration 59, loss = 0.45546240\n",
      "Iteration 60, loss = 0.45876762\n",
      "Iteration 61, loss = 0.45566858\n",
      "Iteration 62, loss = 0.45655987\n",
      "Iteration 63, loss = 0.45795563\n",
      "Iteration 64, loss = 0.45831659\n",
      "Iteration 65, loss = 0.45386542\n",
      "Iteration 66, loss = 0.45699345\n",
      "Iteration 67, loss = 0.45229054\n",
      "Iteration 68, loss = 0.45101733\n",
      "Iteration 69, loss = 0.45085970\n",
      "Iteration 70, loss = 0.45471181\n",
      "Iteration 71, loss = 0.45475794\n",
      "Iteration 72, loss = 0.45118687\n",
      "Iteration 73, loss = 0.45039788\n",
      "Iteration 74, loss = 0.44684175\n",
      "Iteration 75, loss = 0.44943875\n",
      "Iteration 76, loss = 0.45157729\n",
      "Iteration 77, loss = 0.44884853\n",
      "Iteration 78, loss = 0.44931569\n",
      "Iteration 79, loss = 0.45150928\n",
      "Iteration 80, loss = 0.44692008\n",
      "Iteration 81, loss = 0.44590595\n",
      "Iteration 82, loss = 0.44942396\n",
      "Iteration 83, loss = 0.44645655\n",
      "Iteration 84, loss = 0.44739969\n",
      "Iteration 85, loss = 0.44654048\n",
      "Iteration 86, loss = 0.44679038\n",
      "Iteration 87, loss = 0.44768850\n",
      "Iteration 88, loss = 0.44873847\n",
      "Iteration 89, loss = 0.45083569\n",
      "Iteration 90, loss = 0.44796120\n",
      "Iteration 91, loss = 0.45210215\n",
      "Iteration 92, loss = 0.44916045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62771566\n",
      "Iteration 2, loss = 0.53762446\n",
      "Iteration 3, loss = 0.52486987\n",
      "Iteration 4, loss = 0.52087592\n",
      "Iteration 5, loss = 0.51719959\n",
      "Iteration 6, loss = 0.51500977\n",
      "Iteration 7, loss = 0.51246846\n",
      "Iteration 8, loss = 0.51063206\n",
      "Iteration 9, loss = 0.50926567\n",
      "Iteration 10, loss = 0.50735915\n",
      "Iteration 11, loss = 0.50528336\n",
      "Iteration 12, loss = 0.50189731\n",
      "Iteration 13, loss = 0.49919409\n",
      "Iteration 14, loss = 0.49103103\n",
      "Iteration 15, loss = 0.48009797\n",
      "Iteration 16, loss = 0.47949551\n",
      "Iteration 17, loss = 0.47704787\n",
      "Iteration 18, loss = 0.47573777\n",
      "Iteration 19, loss = 0.47738055\n",
      "Iteration 20, loss = 0.47707375\n",
      "Iteration 21, loss = 0.47341483\n",
      "Iteration 22, loss = 0.47565539\n",
      "Iteration 23, loss = 0.47698099\n",
      "Iteration 24, loss = 0.47395189\n",
      "Iteration 25, loss = 0.47451017\n",
      "Iteration 26, loss = 0.47390386\n",
      "Iteration 27, loss = 0.47340445\n",
      "Iteration 28, loss = 0.47400570\n",
      "Iteration 29, loss = 0.47109785\n",
      "Iteration 30, loss = 0.47073210\n",
      "Iteration 31, loss = 0.47033079\n",
      "Iteration 32, loss = 0.46846556\n",
      "Iteration 33, loss = 0.46700709\n",
      "Iteration 34, loss = 0.46582024\n",
      "Iteration 35, loss = 0.46273294\n",
      "Iteration 36, loss = 0.46217799\n",
      "Iteration 37, loss = 0.46207181\n",
      "Iteration 38, loss = 0.46243667\n",
      "Iteration 39, loss = 0.46042681\n",
      "Iteration 40, loss = 0.45999989\n",
      "Iteration 41, loss = 0.45781603\n",
      "Iteration 42, loss = 0.45841298\n",
      "Iteration 43, loss = 0.45696325\n",
      "Iteration 44, loss = 0.45658312\n",
      "Iteration 45, loss = 0.45558105\n",
      "Iteration 46, loss = 0.45562576\n",
      "Iteration 47, loss = 0.45538105\n",
      "Iteration 48, loss = 0.45408328\n",
      "Iteration 49, loss = 0.45597523\n",
      "Iteration 50, loss = 0.45754081\n",
      "Iteration 51, loss = 0.45490364\n",
      "Iteration 52, loss = 0.45390985\n",
      "Iteration 53, loss = 0.45255521\n",
      "Iteration 54, loss = 0.45234301\n",
      "Iteration 55, loss = 0.45274879\n",
      "Iteration 56, loss = 0.45302603\n",
      "Iteration 57, loss = 0.45180931\n",
      "Iteration 58, loss = 0.45268676\n",
      "Iteration 59, loss = 0.45140093\n",
      "Iteration 60, loss = 0.45123824\n",
      "Iteration 61, loss = 0.45424696\n",
      "Iteration 62, loss = 0.45267964\n",
      "Iteration 63, loss = 0.45099108\n",
      "Iteration 64, loss = 0.44998261\n",
      "Iteration 65, loss = 0.45181265\n",
      "Iteration 66, loss = 0.45213666\n",
      "Iteration 67, loss = 0.45069440\n",
      "Iteration 68, loss = 0.44954846\n",
      "Iteration 69, loss = 0.45131655\n",
      "Iteration 70, loss = 0.44934103\n",
      "Iteration 71, loss = 0.44998864\n",
      "Iteration 72, loss = 0.45136286\n",
      "Iteration 73, loss = 0.45025441\n",
      "Iteration 74, loss = 0.45165501\n",
      "Iteration 75, loss = 0.44953208\n",
      "Iteration 76, loss = 0.44849500\n",
      "Iteration 77, loss = 0.44896069\n",
      "Iteration 78, loss = 0.45089237\n",
      "Iteration 79, loss = 0.45272853\n",
      "Iteration 80, loss = 0.45068791\n",
      "Iteration 81, loss = 0.44942354\n",
      "Iteration 82, loss = 0.45298863\n",
      "Iteration 83, loss = 0.44869804\n",
      "Iteration 84, loss = 0.45292743\n",
      "Iteration 85, loss = 0.44861402\n",
      "Iteration 86, loss = 0.45010778\n",
      "Iteration 87, loss = 0.45002932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63306504\n",
      "Iteration 2, loss = 0.54059628\n",
      "Iteration 3, loss = 0.52336051\n",
      "Iteration 4, loss = 0.51855359\n",
      "Iteration 5, loss = 0.51253036\n",
      "Iteration 6, loss = 0.50931776\n",
      "Iteration 7, loss = 0.50688486\n",
      "Iteration 8, loss = 0.50368399\n",
      "Iteration 9, loss = 0.50063635\n",
      "Iteration 10, loss = 0.49891277\n",
      "Iteration 11, loss = 0.49921996\n",
      "Iteration 12, loss = 0.49460672\n",
      "Iteration 13, loss = 0.49780297\n",
      "Iteration 14, loss = 0.49254868\n",
      "Iteration 15, loss = 0.49188552\n",
      "Iteration 16, loss = 0.49247196\n",
      "Iteration 17, loss = 0.49103372\n",
      "Iteration 18, loss = 0.48793641\n",
      "Iteration 19, loss = 0.48733724\n",
      "Iteration 20, loss = 0.48650925\n",
      "Iteration 21, loss = 0.48410170\n",
      "Iteration 22, loss = 0.47847790\n",
      "Iteration 23, loss = 0.47334519\n",
      "Iteration 24, loss = 0.46861790\n",
      "Iteration 25, loss = 0.46777618\n",
      "Iteration 26, loss = 0.46607726\n",
      "Iteration 27, loss = 0.46897202\n",
      "Iteration 28, loss = 0.46619265\n",
      "Iteration 29, loss = 0.46382826\n",
      "Iteration 30, loss = 0.46644848\n",
      "Iteration 31, loss = 0.46597941\n",
      "Iteration 32, loss = 0.46523996\n",
      "Iteration 33, loss = 0.46455513\n",
      "Iteration 34, loss = 0.46401194\n",
      "Iteration 35, loss = 0.46425304\n",
      "Iteration 36, loss = 0.46306150\n",
      "Iteration 37, loss = 0.46337815\n",
      "Iteration 38, loss = 0.46462174\n",
      "Iteration 39, loss = 0.46411808\n",
      "Iteration 40, loss = 0.46355345\n",
      "Iteration 41, loss = 0.46259767\n",
      "Iteration 42, loss = 0.46505946\n",
      "Iteration 43, loss = 0.46260002\n",
      "Iteration 44, loss = 0.46038630\n",
      "Iteration 45, loss = 0.46237964\n",
      "Iteration 46, loss = 0.46317621\n",
      "Iteration 47, loss = 0.46299571\n",
      "Iteration 48, loss = 0.46224653\n",
      "Iteration 49, loss = 0.46233398\n",
      "Iteration 50, loss = 0.46283115\n",
      "Iteration 51, loss = 0.46291927\n",
      "Iteration 52, loss = 0.46431293\n",
      "Iteration 53, loss = 0.46225080\n",
      "Iteration 54, loss = 0.46084210\n",
      "Iteration 55, loss = 0.45965998\n",
      "Iteration 56, loss = 0.46065604\n",
      "Iteration 57, loss = 0.46137801\n",
      "Iteration 58, loss = 0.46182975\n",
      "Iteration 59, loss = 0.45973053\n",
      "Iteration 60, loss = 0.45991587\n",
      "Iteration 61, loss = 0.46029305\n",
      "Iteration 62, loss = 0.46041758\n",
      "Iteration 63, loss = 0.46027522\n",
      "Iteration 64, loss = 0.46021234\n",
      "Iteration 65, loss = 0.45942717\n",
      "Iteration 66, loss = 0.46031412\n",
      "Iteration 67, loss = 0.45970098\n",
      "Iteration 68, loss = 0.45650175\n",
      "Iteration 69, loss = 0.45473330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, loss = 0.45661606\n",
      "Iteration 71, loss = 0.45640333\n",
      "Iteration 72, loss = 0.45269032\n",
      "Iteration 73, loss = 0.45422119\n",
      "Iteration 74, loss = 0.45256063\n",
      "Iteration 75, loss = 0.45358671\n",
      "Iteration 76, loss = 0.45627375\n",
      "Iteration 77, loss = 0.45202810\n",
      "Iteration 78, loss = 0.45315266\n",
      "Iteration 79, loss = 0.45448095\n",
      "Iteration 80, loss = 0.45231764\n",
      "Iteration 81, loss = 0.45146847\n",
      "Iteration 82, loss = 0.45230003\n",
      "Iteration 83, loss = 0.45144463\n",
      "Iteration 84, loss = 0.45073875\n",
      "Iteration 85, loss = 0.44938827\n",
      "Iteration 86, loss = 0.44913915\n",
      "Iteration 87, loss = 0.44798739\n",
      "Iteration 88, loss = 0.44543924\n",
      "Iteration 89, loss = 0.44346760\n",
      "Iteration 90, loss = 0.43921133\n",
      "Iteration 91, loss = 0.43657162\n",
      "Iteration 92, loss = 0.43825975\n",
      "Iteration 93, loss = 0.43415457\n",
      "Iteration 94, loss = 0.43193323\n",
      "Iteration 95, loss = 0.43535089\n",
      "Iteration 96, loss = 0.43057931\n",
      "Iteration 97, loss = 0.42784769\n",
      "Iteration 98, loss = 0.42629689\n",
      "Iteration 99, loss = 0.42524809\n",
      "Iteration 100, loss = 0.42913743\n",
      "Iteration 101, loss = 0.42690135\n",
      "Iteration 102, loss = 0.42392747\n",
      "Iteration 103, loss = 0.42385809\n",
      "Iteration 104, loss = 0.42387459\n",
      "Iteration 105, loss = 0.42424396\n",
      "Iteration 106, loss = 0.42121764\n",
      "Iteration 107, loss = 0.41967149\n",
      "Iteration 108, loss = 0.42177854\n",
      "Iteration 109, loss = 0.42246916\n",
      "Iteration 110, loss = 0.42388089\n",
      "Iteration 111, loss = 0.42154070\n",
      "Iteration 112, loss = 0.41915866\n",
      "Iteration 113, loss = 0.41892903\n",
      "Iteration 114, loss = 0.42024441\n",
      "Iteration 115, loss = 0.42225242\n",
      "Iteration 116, loss = 0.41833020\n",
      "Iteration 117, loss = 0.41750209\n",
      "Iteration 118, loss = 0.41759152\n",
      "Iteration 119, loss = 0.41885028\n",
      "Iteration 120, loss = 0.41805260\n",
      "Iteration 121, loss = 0.42056609\n",
      "Iteration 122, loss = 0.41792801\n",
      "Iteration 123, loss = 0.41729923\n",
      "Iteration 124, loss = 0.41974976\n",
      "Iteration 125, loss = 0.42471514\n",
      "Iteration 126, loss = 0.41775173\n",
      "Iteration 127, loss = 0.42278713\n",
      "Iteration 128, loss = 0.41827192\n",
      "Iteration 129, loss = 0.41761711\n",
      "Iteration 130, loss = 0.41635802\n",
      "Iteration 131, loss = 0.41598195\n",
      "Iteration 132, loss = 0.41620249\n",
      "Iteration 133, loss = 0.41549979\n",
      "Iteration 134, loss = 0.41618238\n",
      "Iteration 135, loss = 0.41968694\n",
      "Iteration 136, loss = 0.41967031\n",
      "Iteration 137, loss = 0.41626676\n",
      "Iteration 138, loss = 0.41750862\n",
      "Iteration 139, loss = 0.42019685\n",
      "Iteration 140, loss = 0.41676196\n",
      "Iteration 141, loss = 0.41609000\n",
      "Iteration 142, loss = 0.41593791\n",
      "Iteration 143, loss = 0.41739500\n",
      "Iteration 144, loss = 0.41481013\n",
      "Iteration 145, loss = 0.41384042\n",
      "Iteration 146, loss = 0.41410097\n",
      "Iteration 147, loss = 0.41536165\n",
      "Iteration 148, loss = 0.41414369\n",
      "Iteration 149, loss = 0.41668721\n",
      "Iteration 150, loss = 0.41370829\n",
      "Iteration 151, loss = 0.41242671\n",
      "Iteration 152, loss = 0.41281008\n",
      "Iteration 153, loss = 0.41440292\n",
      "Iteration 154, loss = 0.41255768\n",
      "Iteration 155, loss = 0.41601700\n",
      "Iteration 156, loss = 0.41334692\n",
      "Iteration 157, loss = 0.41379065\n",
      "Iteration 158, loss = 0.41252673\n",
      "Iteration 159, loss = 0.41263331\n",
      "Iteration 160, loss = 0.41436923\n",
      "Iteration 161, loss = 0.41355246\n",
      "Iteration 162, loss = 0.41283895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63131142\n",
      "Iteration 2, loss = 0.54199886\n",
      "Iteration 3, loss = 0.52571014\n",
      "Iteration 4, loss = 0.52082098\n",
      "Iteration 5, loss = 0.51637980\n",
      "Iteration 6, loss = 0.51321643\n",
      "Iteration 7, loss = 0.50834302\n",
      "Iteration 8, loss = 0.50511440\n",
      "Iteration 9, loss = 0.50253127\n",
      "Iteration 10, loss = 0.49923566\n",
      "Iteration 11, loss = 0.49678265\n",
      "Iteration 12, loss = 0.49498288\n",
      "Iteration 13, loss = 0.49627310\n",
      "Iteration 14, loss = 0.49533568\n",
      "Iteration 15, loss = 0.49240850\n",
      "Iteration 16, loss = 0.48263025\n",
      "Iteration 17, loss = 0.47595515\n",
      "Iteration 18, loss = 0.47521970\n",
      "Iteration 19, loss = 0.47283020\n",
      "Iteration 20, loss = 0.47417010\n",
      "Iteration 21, loss = 0.47413782\n",
      "Iteration 22, loss = 0.47063118\n",
      "Iteration 23, loss = 0.47248978\n",
      "Iteration 24, loss = 0.47062350\n",
      "Iteration 25, loss = 0.47002648\n",
      "Iteration 26, loss = 0.46995753\n",
      "Iteration 27, loss = 0.47113518\n",
      "Iteration 28, loss = 0.47411977\n",
      "Iteration 29, loss = 0.47172482\n",
      "Iteration 30, loss = 0.47089149\n",
      "Iteration 31, loss = 0.46836921\n",
      "Iteration 32, loss = 0.47045836\n",
      "Iteration 33, loss = 0.46982142\n",
      "Iteration 34, loss = 0.46886120\n",
      "Iteration 35, loss = 0.46963564\n",
      "Iteration 36, loss = 0.46901359\n",
      "Iteration 37, loss = 0.47130068\n",
      "Iteration 38, loss = 0.46831447\n",
      "Iteration 39, loss = 0.46748514\n",
      "Iteration 40, loss = 0.47052420\n",
      "Iteration 41, loss = 0.46812391\n",
      "Iteration 42, loss = 0.46823148\n",
      "Iteration 43, loss = 0.46732638\n",
      "Iteration 44, loss = 0.46809438\n",
      "Iteration 45, loss = 0.46760573\n",
      "Iteration 46, loss = 0.46983688\n",
      "Iteration 47, loss = 0.46698006\n",
      "Iteration 48, loss = 0.46748248\n",
      "Iteration 49, loss = 0.46548245\n",
      "Iteration 50, loss = 0.46469698\n",
      "Iteration 51, loss = 0.46336377\n",
      "Iteration 52, loss = 0.46496121\n",
      "Iteration 53, loss = 0.46340435\n",
      "Iteration 54, loss = 0.46367504\n",
      "Iteration 55, loss = 0.46502228\n",
      "Iteration 56, loss = 0.46333080\n",
      "Iteration 57, loss = 0.46370534\n",
      "Iteration 58, loss = 0.46496239\n",
      "Iteration 59, loss = 0.46582808\n",
      "Iteration 60, loss = 0.46403795\n",
      "Iteration 61, loss = 0.46384563\n",
      "Iteration 62, loss = 0.46402730\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61762367\n",
      "Iteration 2, loss = 0.53921042\n",
      "Iteration 3, loss = 0.52381786\n",
      "Iteration 4, loss = 0.51699338\n",
      "Iteration 5, loss = 0.51066827\n",
      "Iteration 6, loss = 0.50766822\n",
      "Iteration 7, loss = 0.50441074\n",
      "Iteration 8, loss = 0.50158062\n",
      "Iteration 9, loss = 0.49788395\n",
      "Iteration 10, loss = 0.49468938\n",
      "Iteration 11, loss = 0.48989565\n",
      "Iteration 12, loss = 0.48501141\n",
      "Iteration 13, loss = 0.48069019\n",
      "Iteration 14, loss = 0.47348642\n",
      "Iteration 15, loss = 0.46542945\n",
      "Iteration 16, loss = 0.45749347\n",
      "Iteration 17, loss = 0.45091145\n",
      "Iteration 18, loss = 0.44645366\n",
      "Iteration 19, loss = 0.44340589\n",
      "Iteration 20, loss = 0.44458223\n",
      "Iteration 21, loss = 0.44228111\n",
      "Iteration 22, loss = 0.43860489\n",
      "Iteration 23, loss = 0.43870781\n",
      "Iteration 24, loss = 0.43724123\n",
      "Iteration 25, loss = 0.44143693\n",
      "Iteration 26, loss = 0.43984178\n",
      "Iteration 27, loss = 0.44002039\n",
      "Iteration 28, loss = 0.43974789\n",
      "Iteration 29, loss = 0.43907364\n",
      "Iteration 30, loss = 0.44037375\n",
      "Iteration 31, loss = 0.44121531\n",
      "Iteration 32, loss = 0.43799916\n",
      "Iteration 33, loss = 0.43765455\n",
      "Iteration 34, loss = 0.43744924\n",
      "Iteration 35, loss = 0.43641398\n",
      "Iteration 36, loss = 0.43733986\n",
      "Iteration 37, loss = 0.43829674\n",
      "Iteration 38, loss = 0.43820612\n",
      "Iteration 39, loss = 0.43770494\n",
      "Iteration 40, loss = 0.43707250\n",
      "Iteration 41, loss = 0.43699071\n",
      "Iteration 42, loss = 0.43755124\n",
      "Iteration 43, loss = 0.43637070\n",
      "Iteration 44, loss = 0.43594499\n",
      "Iteration 45, loss = 0.43635454\n",
      "Iteration 46, loss = 0.43748411\n",
      "Iteration 47, loss = 0.43653610\n",
      "Iteration 48, loss = 0.43567047\n",
      "Iteration 49, loss = 0.44134373\n",
      "Iteration 50, loss = 0.44144070\n",
      "Iteration 51, loss = 0.44024085\n",
      "Iteration 52, loss = 0.43643560\n",
      "Iteration 53, loss = 0.43609429\n",
      "Iteration 54, loss = 0.43566458\n",
      "Iteration 55, loss = 0.43592407\n",
      "Iteration 56, loss = 0.43721203\n",
      "Iteration 57, loss = 0.43659801\n",
      "Iteration 58, loss = 0.43563833\n",
      "Iteration 59, loss = 0.43455263\n",
      "Iteration 60, loss = 0.43641395\n",
      "Iteration 61, loss = 0.43650195\n",
      "Iteration 62, loss = 0.43639941\n",
      "Iteration 63, loss = 0.43630326\n",
      "Iteration 64, loss = 0.43427458\n",
      "Iteration 65, loss = 0.43836942\n",
      "Iteration 66, loss = 0.43532408\n",
      "Iteration 67, loss = 0.43669094\n",
      "Iteration 68, loss = 0.43652376\n",
      "Iteration 69, loss = 0.43600501\n",
      "Iteration 70, loss = 0.43501178\n",
      "Iteration 71, loss = 0.43517124\n",
      "Iteration 72, loss = 0.43721925\n",
      "Iteration 73, loss = 0.43736349\n",
      "Iteration 74, loss = 0.43890074\n",
      "Iteration 75, loss = 0.43510323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62499320\n",
      "Iteration 2, loss = 0.54144759\n",
      "Iteration 3, loss = 0.52245153\n",
      "Iteration 4, loss = 0.51480231\n",
      "Iteration 5, loss = 0.50748407\n",
      "Iteration 6, loss = 0.50222609\n",
      "Iteration 7, loss = 0.49776546\n",
      "Iteration 8, loss = 0.49274030\n",
      "Iteration 9, loss = 0.48908588\n",
      "Iteration 10, loss = 0.48754435\n",
      "Iteration 11, loss = 0.48486981\n",
      "Iteration 12, loss = 0.47651212\n",
      "Iteration 13, loss = 0.47130062\n",
      "Iteration 14, loss = 0.46752845\n",
      "Iteration 15, loss = 0.46772002\n",
      "Iteration 16, loss = 0.46456286\n",
      "Iteration 17, loss = 0.46438085\n",
      "Iteration 18, loss = 0.46342704\n",
      "Iteration 19, loss = 0.46198844\n",
      "Iteration 20, loss = 0.46147268\n",
      "Iteration 21, loss = 0.46041683\n",
      "Iteration 22, loss = 0.45998808\n",
      "Iteration 23, loss = 0.45747677\n",
      "Iteration 24, loss = 0.45704688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, loss = 0.45543067\n",
      "Iteration 26, loss = 0.45626166\n",
      "Iteration 27, loss = 0.45491994\n",
      "Iteration 28, loss = 0.45504627\n",
      "Iteration 29, loss = 0.45422512\n",
      "Iteration 30, loss = 0.45180418\n",
      "Iteration 31, loss = 0.45179565\n",
      "Iteration 32, loss = 0.45171331\n",
      "Iteration 33, loss = 0.45175756\n",
      "Iteration 34, loss = 0.44914291\n",
      "Iteration 35, loss = 0.44835585\n",
      "Iteration 36, loss = 0.44971679\n",
      "Iteration 37, loss = 0.44578834\n",
      "Iteration 38, loss = 0.44649863\n",
      "Iteration 39, loss = 0.44354072\n",
      "Iteration 40, loss = 0.44104342\n",
      "Iteration 41, loss = 0.44130948\n",
      "Iteration 42, loss = 0.44419761\n",
      "Iteration 43, loss = 0.44066296\n",
      "Iteration 44, loss = 0.43906252\n",
      "Iteration 45, loss = 0.44102941\n",
      "Iteration 46, loss = 0.44070292\n",
      "Iteration 47, loss = 0.44682957\n",
      "Iteration 48, loss = 0.43874420\n",
      "Iteration 49, loss = 0.43869572\n",
      "Iteration 50, loss = 0.43954912\n",
      "Iteration 51, loss = 0.43614689\n",
      "Iteration 52, loss = 0.44086981\n",
      "Iteration 53, loss = 0.44110644\n",
      "Iteration 54, loss = 0.43869234\n",
      "Iteration 55, loss = 0.43700828\n",
      "Iteration 56, loss = 0.43686052\n",
      "Iteration 57, loss = 0.43508031\n",
      "Iteration 58, loss = 0.43475912\n",
      "Iteration 59, loss = 0.43653456\n",
      "Iteration 60, loss = 0.43501081\n",
      "Iteration 61, loss = 0.43470686\n",
      "Iteration 62, loss = 0.43586270\n",
      "Iteration 63, loss = 0.43645037\n",
      "Iteration 64, loss = 0.43414812\n",
      "Iteration 65, loss = 0.43615004\n",
      "Iteration 66, loss = 0.43601888\n",
      "Iteration 67, loss = 0.43453539\n",
      "Iteration 68, loss = 0.43404641\n",
      "Iteration 69, loss = 0.43274254\n",
      "Iteration 70, loss = 0.43352491\n",
      "Iteration 71, loss = 0.43528140\n",
      "Iteration 72, loss = 0.43299407\n",
      "Iteration 73, loss = 0.43564226\n",
      "Iteration 74, loss = 0.43346459\n",
      "Iteration 75, loss = 0.43544528\n",
      "Iteration 76, loss = 0.43427245\n",
      "Iteration 77, loss = 0.43384608\n",
      "Iteration 78, loss = 0.43469526\n",
      "Iteration 79, loss = 0.43864587\n",
      "Iteration 80, loss = 0.43272325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61899157\n",
      "Iteration 2, loss = 0.54125533\n",
      "Iteration 3, loss = 0.52411197\n",
      "Iteration 4, loss = 0.51648407\n",
      "Iteration 5, loss = 0.50927773\n",
      "Iteration 6, loss = 0.50591036\n",
      "Iteration 7, loss = 0.49985365\n",
      "Iteration 8, loss = 0.49380286\n",
      "Iteration 9, loss = 0.49225281\n",
      "Iteration 10, loss = 0.48542088\n",
      "Iteration 11, loss = 0.48160841\n",
      "Iteration 12, loss = 0.47798092\n",
      "Iteration 13, loss = 0.47129242\n",
      "Iteration 14, loss = 0.46678346\n",
      "Iteration 15, loss = 0.46140343\n",
      "Iteration 16, loss = 0.46078707\n",
      "Iteration 17, loss = 0.45674571\n",
      "Iteration 18, loss = 0.45113148\n",
      "Iteration 19, loss = 0.45031605\n",
      "Iteration 20, loss = 0.44725649\n",
      "Iteration 21, loss = 0.44648863\n",
      "Iteration 22, loss = 0.44298219\n",
      "Iteration 23, loss = 0.44226057\n",
      "Iteration 24, loss = 0.43958870\n",
      "Iteration 25, loss = 0.43836603\n",
      "Iteration 26, loss = 0.44068717\n",
      "Iteration 27, loss = 0.43723885\n",
      "Iteration 28, loss = 0.43638857\n",
      "Iteration 29, loss = 0.43684889\n",
      "Iteration 30, loss = 0.43578434\n",
      "Iteration 31, loss = 0.43671625\n",
      "Iteration 32, loss = 0.43407185\n",
      "Iteration 33, loss = 0.43416448\n",
      "Iteration 34, loss = 0.43520167\n",
      "Iteration 35, loss = 0.43399710\n",
      "Iteration 36, loss = 0.43215377\n",
      "Iteration 37, loss = 0.43441847\n",
      "Iteration 38, loss = 0.43380936\n",
      "Iteration 39, loss = 0.43288539\n",
      "Iteration 40, loss = 0.43372681\n",
      "Iteration 41, loss = 0.43407079\n",
      "Iteration 42, loss = 0.43181726\n",
      "Iteration 43, loss = 0.43145541\n",
      "Iteration 44, loss = 0.43099485\n",
      "Iteration 45, loss = 0.43170910\n",
      "Iteration 46, loss = 0.43265954\n",
      "Iteration 47, loss = 0.43157766\n",
      "Iteration 48, loss = 0.43396141\n",
      "Iteration 49, loss = 0.43218595\n",
      "Iteration 50, loss = 0.43160741\n",
      "Iteration 51, loss = 0.43127166\n",
      "Iteration 52, loss = 0.43124343\n",
      "Iteration 53, loss = 0.43082353\n",
      "Iteration 54, loss = 0.42890157\n",
      "Iteration 55, loss = 0.42915781\n",
      "Iteration 56, loss = 0.43069440\n",
      "Iteration 57, loss = 0.43595158\n",
      "Iteration 58, loss = 0.43046100\n",
      "Iteration 59, loss = 0.43199031\n",
      "Iteration 60, loss = 0.42954112\n",
      "Iteration 61, loss = 0.43016978\n",
      "Iteration 62, loss = 0.43195230\n",
      "Iteration 63, loss = 0.43342637\n",
      "Iteration 64, loss = 0.43104673\n",
      "Iteration 65, loss = 0.42964417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62921550\n",
      "Iteration 2, loss = 0.55458033\n",
      "Iteration 3, loss = 0.53718767\n",
      "Iteration 4, loss = 0.52699779\n",
      "Iteration 5, loss = 0.51733812\n",
      "Iteration 6, loss = 0.51230074\n",
      "Iteration 7, loss = 0.50829308\n",
      "Iteration 8, loss = 0.50563931\n",
      "Iteration 9, loss = 0.50018357\n",
      "Iteration 10, loss = 0.50137950\n",
      "Iteration 11, loss = 0.50033436\n",
      "Iteration 12, loss = 0.49856132\n",
      "Iteration 13, loss = 0.49810566\n",
      "Iteration 14, loss = 0.49796455\n",
      "Iteration 15, loss = 0.49698615\n",
      "Iteration 16, loss = 0.49765570\n",
      "Iteration 17, loss = 0.49714989\n",
      "Iteration 18, loss = 0.49584316\n",
      "Iteration 19, loss = 0.49521855\n",
      "Iteration 20, loss = 0.49629153\n",
      "Iteration 21, loss = 0.49503841\n",
      "Iteration 22, loss = 0.49500130\n",
      "Iteration 23, loss = 0.49979981\n",
      "Iteration 24, loss = 0.49589882\n",
      "Iteration 25, loss = 0.49613938\n",
      "Iteration 26, loss = 0.49565965\n",
      "Iteration 27, loss = 0.49545116\n",
      "Iteration 28, loss = 0.49452214\n",
      "Iteration 29, loss = 0.49560257\n",
      "Iteration 30, loss = 0.49423207\n",
      "Iteration 31, loss = 0.49352332\n",
      "Iteration 32, loss = 0.49613131\n",
      "Iteration 33, loss = 0.49504193\n",
      "Iteration 34, loss = 0.49466685\n",
      "Iteration 35, loss = 0.49331471\n",
      "Iteration 36, loss = 0.49326505\n",
      "Iteration 37, loss = 0.49538629\n",
      "Iteration 38, loss = 0.49692557\n",
      "Iteration 39, loss = 0.49597237\n",
      "Iteration 40, loss = 0.49760177\n",
      "Iteration 41, loss = 0.49401933\n",
      "Iteration 42, loss = 0.49639375\n",
      "Iteration 43, loss = 0.49333595\n",
      "Iteration 44, loss = 0.49316205\n",
      "Iteration 45, loss = 0.49335818\n",
      "Iteration 46, loss = 0.49300366\n",
      "Iteration 47, loss = 0.49378113\n",
      "Iteration 48, loss = 0.49316598\n",
      "Iteration 49, loss = 0.49456310\n",
      "Iteration 50, loss = 0.49483022\n",
      "Iteration 51, loss = 0.49248379\n",
      "Iteration 52, loss = 0.49247777\n",
      "Iteration 53, loss = 0.49426509\n",
      "Iteration 54, loss = 0.49178167\n",
      "Iteration 55, loss = 0.49137433\n",
      "Iteration 56, loss = 0.49321591\n",
      "Iteration 57, loss = 0.49176745\n",
      "Iteration 58, loss = 0.49164734\n",
      "Iteration 59, loss = 0.49133374\n",
      "Iteration 60, loss = 0.49165593\n",
      "Iteration 61, loss = 0.49167577\n",
      "Iteration 62, loss = 0.49067648\n",
      "Iteration 63, loss = 0.49111194\n",
      "Iteration 64, loss = 0.49039559\n",
      "Iteration 65, loss = 0.49312753\n",
      "Iteration 66, loss = 0.49060215\n",
      "Iteration 67, loss = 0.49043969\n",
      "Iteration 68, loss = 0.48973939\n",
      "Iteration 69, loss = 0.48924189\n",
      "Iteration 70, loss = 0.48952190\n",
      "Iteration 71, loss = 0.48684771\n",
      "Iteration 72, loss = 0.48707087\n",
      "Iteration 73, loss = 0.48532044\n",
      "Iteration 74, loss = 0.48567626\n",
      "Iteration 75, loss = 0.48431459\n",
      "Iteration 76, loss = 0.48603805\n",
      "Iteration 77, loss = 0.48320931\n",
      "Iteration 78, loss = 0.48282394\n",
      "Iteration 79, loss = 0.48265801\n",
      "Iteration 80, loss = 0.48440505\n",
      "Iteration 81, loss = 0.48238143\n",
      "Iteration 82, loss = 0.48196563\n",
      "Iteration 83, loss = 0.48145947\n",
      "Iteration 84, loss = 0.48566659\n",
      "Iteration 85, loss = 0.48363078\n",
      "Iteration 86, loss = 0.48027262\n",
      "Iteration 87, loss = 0.48093426\n",
      "Iteration 88, loss = 0.47999291\n",
      "Iteration 89, loss = 0.48301171\n",
      "Iteration 90, loss = 0.48003381\n",
      "Iteration 91, loss = 0.47852153\n",
      "Iteration 92, loss = 0.48260955\n",
      "Iteration 93, loss = 0.47609248\n",
      "Iteration 94, loss = 0.47872106\n",
      "Iteration 95, loss = 0.47938275\n",
      "Iteration 96, loss = 0.47763762\n",
      "Iteration 97, loss = 0.47674304\n",
      "Iteration 98, loss = 0.47507273\n",
      "Iteration 99, loss = 0.47970492\n",
      "Iteration 100, loss = 0.47471226\n",
      "Iteration 101, loss = 0.47599466\n",
      "Iteration 102, loss = 0.47943711\n",
      "Iteration 103, loss = 0.47453513\n",
      "Iteration 104, loss = 0.47334907\n",
      "Iteration 105, loss = 0.47327864\n",
      "Iteration 106, loss = 0.47495146\n",
      "Iteration 107, loss = 0.47381856\n",
      "Iteration 108, loss = 0.47409046\n",
      "Iteration 109, loss = 0.47393031\n",
      "Iteration 110, loss = 0.47320495\n",
      "Iteration 111, loss = 0.47610493\n",
      "Iteration 112, loss = 0.47449284\n",
      "Iteration 113, loss = 0.47244993\n",
      "Iteration 114, loss = 0.47168100\n",
      "Iteration 115, loss = 0.47179151\n",
      "Iteration 116, loss = 0.47354806\n",
      "Iteration 117, loss = 0.47203090\n",
      "Iteration 118, loss = 0.47456196\n",
      "Iteration 119, loss = 0.47535731\n",
      "Iteration 120, loss = 0.47491015\n",
      "Iteration 121, loss = 0.47446875\n",
      "Iteration 122, loss = 0.47466177\n",
      "Iteration 123, loss = 0.47326834\n",
      "Iteration 124, loss = 0.47118989\n",
      "Iteration 125, loss = 0.48005946\n",
      "Iteration 126, loss = 0.47522117\n",
      "Iteration 127, loss = 0.47422483\n",
      "Iteration 128, loss = 0.47278454\n",
      "Iteration 129, loss = 0.47138503\n",
      "Iteration 130, loss = 0.47562461\n",
      "Iteration 131, loss = 0.47212362\n",
      "Iteration 132, loss = 0.47098871\n",
      "Iteration 133, loss = 0.47123007\n",
      "Iteration 134, loss = 0.47047069\n",
      "Iteration 135, loss = 0.47185000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 136, loss = 0.47208809\n",
      "Iteration 137, loss = 0.47203598\n",
      "Iteration 138, loss = 0.47256332\n",
      "Iteration 139, loss = 0.46978487\n",
      "Iteration 140, loss = 0.46994491\n",
      "Iteration 141, loss = 0.47081911\n",
      "Iteration 142, loss = 0.47324210\n",
      "Iteration 143, loss = 0.47120145\n",
      "Iteration 144, loss = 0.47223491\n",
      "Iteration 145, loss = 0.47261386\n",
      "Iteration 146, loss = 0.47253318\n",
      "Iteration 147, loss = 0.47331255\n",
      "Iteration 148, loss = 0.47358973\n",
      "Iteration 149, loss = 0.47200394\n",
      "Iteration 150, loss = 0.47021807\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63834428\n",
      "Iteration 2, loss = 0.55919623\n",
      "Iteration 3, loss = 0.53789971\n",
      "Iteration 4, loss = 0.52555356\n",
      "Iteration 5, loss = 0.51448261\n",
      "Iteration 6, loss = 0.50741108\n",
      "Iteration 7, loss = 0.50448659\n",
      "Iteration 8, loss = 0.50030293\n",
      "Iteration 9, loss = 0.49728402\n",
      "Iteration 10, loss = 0.49600289\n",
      "Iteration 11, loss = 0.49763372\n",
      "Iteration 12, loss = 0.49410202\n",
      "Iteration 13, loss = 0.49371646\n",
      "Iteration 14, loss = 0.49050785\n",
      "Iteration 15, loss = 0.49069466\n",
      "Iteration 16, loss = 0.48956710\n",
      "Iteration 17, loss = 0.48910497\n",
      "Iteration 18, loss = 0.48819677\n",
      "Iteration 19, loss = 0.48852492\n",
      "Iteration 20, loss = 0.48793778\n",
      "Iteration 21, loss = 0.48727762\n",
      "Iteration 22, loss = 0.48864922\n",
      "Iteration 23, loss = 0.48857638\n",
      "Iteration 24, loss = 0.48619261\n",
      "Iteration 25, loss = 0.48532607\n",
      "Iteration 26, loss = 0.48713835\n",
      "Iteration 27, loss = 0.48608682\n",
      "Iteration 28, loss = 0.48516059\n",
      "Iteration 29, loss = 0.48421079\n",
      "Iteration 30, loss = 0.48554337\n",
      "Iteration 31, loss = 0.48526523\n",
      "Iteration 32, loss = 0.48523640\n",
      "Iteration 33, loss = 0.48499703\n",
      "Iteration 34, loss = 0.48344438\n",
      "Iteration 35, loss = 0.48432797\n",
      "Iteration 36, loss = 0.48340800\n",
      "Iteration 37, loss = 0.48314045\n",
      "Iteration 38, loss = 0.48417623\n",
      "Iteration 39, loss = 0.48484362\n",
      "Iteration 40, loss = 0.48340161\n",
      "Iteration 41, loss = 0.48385232\n",
      "Iteration 42, loss = 0.48492815\n",
      "Iteration 43, loss = 0.48304066\n",
      "Iteration 44, loss = 0.48332125\n",
      "Iteration 45, loss = 0.48379777\n",
      "Iteration 46, loss = 0.48459032\n",
      "Iteration 47, loss = 0.48407300\n",
      "Iteration 48, loss = 0.48288341\n",
      "Iteration 49, loss = 0.48331450\n",
      "Iteration 50, loss = 0.48225368\n",
      "Iteration 51, loss = 0.48277837\n",
      "Iteration 52, loss = 0.48444851\n",
      "Iteration 53, loss = 0.48294649\n",
      "Iteration 54, loss = 0.48322473\n",
      "Iteration 55, loss = 0.48229494\n",
      "Iteration 56, loss = 0.48176234\n",
      "Iteration 57, loss = 0.48368815\n",
      "Iteration 58, loss = 0.48260056\n",
      "Iteration 59, loss = 0.48161724\n",
      "Iteration 60, loss = 0.48250487\n",
      "Iteration 61, loss = 0.48340942\n",
      "Iteration 62, loss = 0.48147851\n",
      "Iteration 63, loss = 0.48295587\n",
      "Iteration 64, loss = 0.48404955\n",
      "Iteration 65, loss = 0.48338896\n",
      "Iteration 66, loss = 0.48753257\n",
      "Iteration 67, loss = 0.48384754\n",
      "Iteration 68, loss = 0.48205574\n",
      "Iteration 69, loss = 0.48260370\n",
      "Iteration 70, loss = 0.48242203\n",
      "Iteration 71, loss = 0.48363001\n",
      "Iteration 72, loss = 0.48274635\n",
      "Iteration 73, loss = 0.48060534\n",
      "Iteration 74, loss = 0.48212142\n",
      "Iteration 75, loss = 0.48379202\n",
      "Iteration 76, loss = 0.48286886\n",
      "Iteration 77, loss = 0.48245996\n",
      "Iteration 78, loss = 0.48289016\n",
      "Iteration 79, loss = 0.48361259\n",
      "Iteration 80, loss = 0.48283404\n",
      "Iteration 81, loss = 0.48100718\n",
      "Iteration 82, loss = 0.48100872\n",
      "Iteration 83, loss = 0.48086662\n",
      "Iteration 84, loss = 0.47982654\n",
      "Iteration 85, loss = 0.48253715\n",
      "Iteration 86, loss = 0.48258502\n",
      "Iteration 87, loss = 0.48213067\n",
      "Iteration 88, loss = 0.48014336\n",
      "Iteration 89, loss = 0.48251341\n",
      "Iteration 90, loss = 0.48188488\n",
      "Iteration 91, loss = 0.48062181\n",
      "Iteration 92, loss = 0.48049557\n",
      "Iteration 93, loss = 0.48029762\n",
      "Iteration 94, loss = 0.48106478\n",
      "Iteration 95, loss = 0.48003241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63086128\n",
      "Iteration 2, loss = 0.55679317\n",
      "Iteration 3, loss = 0.53897563\n",
      "Iteration 4, loss = 0.52748567\n",
      "Iteration 5, loss = 0.51500552\n",
      "Iteration 6, loss = 0.50823740\n",
      "Iteration 7, loss = 0.50271803\n",
      "Iteration 8, loss = 0.50038842\n",
      "Iteration 9, loss = 0.49741363\n",
      "Iteration 10, loss = 0.49574492\n",
      "Iteration 11, loss = 0.49445546\n",
      "Iteration 12, loss = 0.49375063\n",
      "Iteration 13, loss = 0.49309260\n",
      "Iteration 14, loss = 0.49508321\n",
      "Iteration 15, loss = 0.49437124\n",
      "Iteration 16, loss = 0.49334522\n",
      "Iteration 17, loss = 0.49281736\n",
      "Iteration 18, loss = 0.49301685\n",
      "Iteration 19, loss = 0.49190346\n",
      "Iteration 20, loss = 0.49226466\n",
      "Iteration 21, loss = 0.49288230\n",
      "Iteration 22, loss = 0.49108684\n",
      "Iteration 23, loss = 0.49164024\n",
      "Iteration 24, loss = 0.49190525\n",
      "Iteration 25, loss = 0.49191155\n",
      "Iteration 26, loss = 0.49241167\n",
      "Iteration 27, loss = 0.49051545\n",
      "Iteration 28, loss = 0.49070341\n",
      "Iteration 29, loss = 0.49398730\n",
      "Iteration 30, loss = 0.49184287\n",
      "Iteration 31, loss = 0.49196114\n",
      "Iteration 32, loss = 0.49152194\n",
      "Iteration 33, loss = 0.49017242\n",
      "Iteration 34, loss = 0.49116554\n",
      "Iteration 35, loss = 0.48964915\n",
      "Iteration 36, loss = 0.49001804\n",
      "Iteration 37, loss = 0.49063232\n",
      "Iteration 38, loss = 0.48974397\n",
      "Iteration 39, loss = 0.49061834\n",
      "Iteration 40, loss = 0.48995404\n",
      "Iteration 41, loss = 0.49273139\n",
      "Iteration 42, loss = 0.48979875\n",
      "Iteration 43, loss = 0.48911720\n",
      "Iteration 44, loss = 0.49029074\n",
      "Iteration 45, loss = 0.48864739\n",
      "Iteration 46, loss = 0.48912322\n",
      "Iteration 47, loss = 0.49158962\n",
      "Iteration 48, loss = 0.49446842\n",
      "Iteration 49, loss = 0.49084506\n",
      "Iteration 50, loss = 0.48867448\n",
      "Iteration 51, loss = 0.48882258\n",
      "Iteration 52, loss = 0.48917736\n",
      "Iteration 53, loss = 0.48812853\n",
      "Iteration 54, loss = 0.48813086\n",
      "Iteration 55, loss = 0.48724689\n",
      "Iteration 56, loss = 0.48824204\n",
      "Iteration 57, loss = 0.48920640\n",
      "Iteration 58, loss = 0.48584087\n",
      "Iteration 59, loss = 0.48438016\n",
      "Iteration 60, loss = 0.48473841\n",
      "Iteration 61, loss = 0.48402391\n",
      "Iteration 62, loss = 0.48484342\n",
      "Iteration 63, loss = 0.48630775\n",
      "Iteration 64, loss = 0.48674439\n",
      "Iteration 65, loss = 0.48504182\n",
      "Iteration 66, loss = 0.48339316\n",
      "Iteration 67, loss = 0.48221933\n",
      "Iteration 68, loss = 0.48518282\n",
      "Iteration 69, loss = 0.47995204\n",
      "Iteration 70, loss = 0.48125755\n",
      "Iteration 71, loss = 0.48125732\n",
      "Iteration 72, loss = 0.47912719\n",
      "Iteration 73, loss = 0.47912329\n",
      "Iteration 74, loss = 0.47836386\n",
      "Iteration 75, loss = 0.47844553\n",
      "Iteration 76, loss = 0.47892183\n",
      "Iteration 77, loss = 0.47788869\n",
      "Iteration 78, loss = 0.47830739\n",
      "Iteration 79, loss = 0.48112200\n",
      "Iteration 80, loss = 0.47671875\n",
      "Iteration 81, loss = 0.47796692\n",
      "Iteration 82, loss = 0.47808113\n",
      "Iteration 83, loss = 0.47899610\n",
      "Iteration 84, loss = 0.47536200\n",
      "Iteration 85, loss = 0.47507664\n",
      "Iteration 86, loss = 0.47540771\n",
      "Iteration 87, loss = 0.47609449\n",
      "Iteration 88, loss = 0.47808184\n",
      "Iteration 89, loss = 0.47858340\n",
      "Iteration 90, loss = 0.47728372\n",
      "Iteration 91, loss = 0.47524238\n",
      "Iteration 92, loss = 0.47414249\n",
      "Iteration 93, loss = 0.47748768\n",
      "Iteration 94, loss = 0.47508247\n",
      "Iteration 95, loss = 0.47190651\n",
      "Iteration 96, loss = 0.47530202\n",
      "Iteration 97, loss = 0.47551427\n",
      "Iteration 98, loss = 0.47650692\n",
      "Iteration 99, loss = 0.47524919\n",
      "Iteration 100, loss = 0.47296702\n",
      "Iteration 101, loss = 0.47285379\n",
      "Iteration 102, loss = 0.47466840\n",
      "Iteration 103, loss = 0.47773287\n",
      "Iteration 104, loss = 0.47842997\n",
      "Iteration 105, loss = 0.47442500\n",
      "Iteration 106, loss = 0.47285534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59105292\n",
      "Iteration 2, loss = 0.53295547\n",
      "Iteration 3, loss = 0.52659682\n",
      "Iteration 4, loss = 0.52236855\n",
      "Iteration 5, loss = 0.51448662\n",
      "Iteration 6, loss = 0.50878549\n",
      "Iteration 7, loss = 0.50479607\n",
      "Iteration 8, loss = 0.50034236\n",
      "Iteration 9, loss = 0.49509283\n",
      "Iteration 10, loss = 0.49323392\n",
      "Iteration 11, loss = 0.48506604\n",
      "Iteration 12, loss = 0.47716025\n",
      "Iteration 13, loss = 0.47237569\n",
      "Iteration 14, loss = 0.46953238\n",
      "Iteration 15, loss = 0.46715022\n",
      "Iteration 16, loss = 0.46555998\n",
      "Iteration 17, loss = 0.46430757\n",
      "Iteration 18, loss = 0.46487991\n",
      "Iteration 19, loss = 0.46197659\n",
      "Iteration 20, loss = 0.45993959\n",
      "Iteration 21, loss = 0.46256702\n",
      "Iteration 22, loss = 0.45859594\n",
      "Iteration 23, loss = 0.46139634\n",
      "Iteration 24, loss = 0.45948919\n",
      "Iteration 25, loss = 0.46339164\n",
      "Iteration 26, loss = 0.46359865\n",
      "Iteration 27, loss = 0.45909954\n",
      "Iteration 28, loss = 0.45939949\n",
      "Iteration 29, loss = 0.45961418\n",
      "Iteration 30, loss = 0.45879424\n",
      "Iteration 31, loss = 0.45762279\n",
      "Iteration 32, loss = 0.45733197\n",
      "Iteration 33, loss = 0.45783903\n",
      "Iteration 34, loss = 0.45899074\n",
      "Iteration 35, loss = 0.45712247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 36, loss = 0.45311848\n",
      "Iteration 37, loss = 0.45384275\n",
      "Iteration 38, loss = 0.45574365\n",
      "Iteration 39, loss = 0.45339989\n",
      "Iteration 40, loss = 0.45274138\n",
      "Iteration 41, loss = 0.45160028\n",
      "Iteration 42, loss = 0.45130709\n",
      "Iteration 43, loss = 0.45073155\n",
      "Iteration 44, loss = 0.45054878\n",
      "Iteration 45, loss = 0.44845578\n",
      "Iteration 46, loss = 0.44887286\n",
      "Iteration 47, loss = 0.44809354\n",
      "Iteration 48, loss = 0.44678257\n",
      "Iteration 49, loss = 0.45116980\n",
      "Iteration 50, loss = 0.45107901\n",
      "Iteration 51, loss = 0.44894059\n",
      "Iteration 52, loss = 0.44739806\n",
      "Iteration 53, loss = 0.44476415\n",
      "Iteration 54, loss = 0.44414851\n",
      "Iteration 55, loss = 0.44409713\n",
      "Iteration 56, loss = 0.44421300\n",
      "Iteration 57, loss = 0.44340290\n",
      "Iteration 58, loss = 0.44196569\n",
      "Iteration 59, loss = 0.44248282\n",
      "Iteration 60, loss = 0.44214782\n",
      "Iteration 61, loss = 0.44559288\n",
      "Iteration 62, loss = 0.44395412\n",
      "Iteration 63, loss = 0.44258469\n",
      "Iteration 64, loss = 0.44215604\n",
      "Iteration 65, loss = 0.44566910\n",
      "Iteration 66, loss = 0.44275244\n",
      "Iteration 67, loss = 0.44231413\n",
      "Iteration 68, loss = 0.44108333\n",
      "Iteration 69, loss = 0.44404170\n",
      "Iteration 70, loss = 0.44402584\n",
      "Iteration 71, loss = 0.43855804\n",
      "Iteration 72, loss = 0.44013220\n",
      "Iteration 73, loss = 0.44072231\n",
      "Iteration 74, loss = 0.44054796\n",
      "Iteration 75, loss = 0.43694624\n",
      "Iteration 76, loss = 0.43830401\n",
      "Iteration 77, loss = 0.43786535\n",
      "Iteration 78, loss = 0.43869806\n",
      "Iteration 79, loss = 0.43731262\n",
      "Iteration 80, loss = 0.43828176\n",
      "Iteration 81, loss = 0.43686947\n",
      "Iteration 82, loss = 0.44019077\n",
      "Iteration 83, loss = 0.43823214\n",
      "Iteration 84, loss = 0.44233337\n",
      "Iteration 85, loss = 0.43781822\n",
      "Iteration 86, loss = 0.43598362\n",
      "Iteration 87, loss = 0.43892351\n",
      "Iteration 88, loss = 0.43581261\n",
      "Iteration 89, loss = 0.43798513\n",
      "Iteration 90, loss = 0.43683050\n",
      "Iteration 91, loss = 0.43688441\n",
      "Iteration 92, loss = 0.43782707\n",
      "Iteration 93, loss = 0.43366702\n",
      "Iteration 94, loss = 0.43687997\n",
      "Iteration 95, loss = 0.43670328\n",
      "Iteration 96, loss = 0.43551944\n",
      "Iteration 97, loss = 0.44136138\n",
      "Iteration 98, loss = 0.43737898\n",
      "Iteration 99, loss = 0.43887619\n",
      "Iteration 100, loss = 0.43620134\n",
      "Iteration 101, loss = 0.43423851\n",
      "Iteration 102, loss = 0.43592329\n",
      "Iteration 103, loss = 0.43489061\n",
      "Iteration 104, loss = 0.43347383\n",
      "Iteration 105, loss = 0.43307629\n",
      "Iteration 106, loss = 0.43632747\n",
      "Iteration 107, loss = 0.43503299\n",
      "Iteration 108, loss = 0.43477853\n",
      "Iteration 109, loss = 0.43747074\n",
      "Iteration 110, loss = 0.43564681\n",
      "Iteration 111, loss = 0.43421739\n",
      "Iteration 112, loss = 0.43513988\n",
      "Iteration 113, loss = 0.43764740\n",
      "Iteration 114, loss = 0.43411409\n",
      "Iteration 115, loss = 0.43384472\n",
      "Iteration 116, loss = 0.43509712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58901163\n",
      "Iteration 2, loss = 0.52976537\n",
      "Iteration 3, loss = 0.52485808\n",
      "Iteration 4, loss = 0.51890766\n",
      "Iteration 5, loss = 0.51344938\n",
      "Iteration 6, loss = 0.50753516\n",
      "Iteration 7, loss = 0.50275142\n",
      "Iteration 8, loss = 0.49670729\n",
      "Iteration 9, loss = 0.49029861\n",
      "Iteration 10, loss = 0.48873312\n",
      "Iteration 11, loss = 0.48800168\n",
      "Iteration 12, loss = 0.47754305\n",
      "Iteration 13, loss = 0.47184581\n",
      "Iteration 14, loss = 0.46675458\n",
      "Iteration 15, loss = 0.46321221\n",
      "Iteration 16, loss = 0.46247041\n",
      "Iteration 17, loss = 0.46345756\n",
      "Iteration 18, loss = 0.46130070\n",
      "Iteration 19, loss = 0.46257260\n",
      "Iteration 20, loss = 0.46024497\n",
      "Iteration 21, loss = 0.45964629\n",
      "Iteration 22, loss = 0.46178766\n",
      "Iteration 23, loss = 0.45932227\n",
      "Iteration 24, loss = 0.45501685\n",
      "Iteration 25, loss = 0.45351583\n",
      "Iteration 26, loss = 0.45465783\n",
      "Iteration 27, loss = 0.45348344\n",
      "Iteration 28, loss = 0.45084348\n",
      "Iteration 29, loss = 0.45372970\n",
      "Iteration 30, loss = 0.45356464\n",
      "Iteration 31, loss = 0.45469312\n",
      "Iteration 32, loss = 0.45095853\n",
      "Iteration 33, loss = 0.45100826\n",
      "Iteration 34, loss = 0.44871308\n",
      "Iteration 35, loss = 0.44907985\n",
      "Iteration 36, loss = 0.44849169\n",
      "Iteration 37, loss = 0.44914324\n",
      "Iteration 38, loss = 0.45176327\n",
      "Iteration 39, loss = 0.45038632\n",
      "Iteration 40, loss = 0.44769555\n",
      "Iteration 41, loss = 0.44856777\n",
      "Iteration 42, loss = 0.44670523\n",
      "Iteration 43, loss = 0.44792474\n",
      "Iteration 44, loss = 0.44632589\n",
      "Iteration 45, loss = 0.44655589\n",
      "Iteration 46, loss = 0.44593992\n",
      "Iteration 47, loss = 0.44418510\n",
      "Iteration 48, loss = 0.44573753\n",
      "Iteration 49, loss = 0.44624303\n",
      "Iteration 50, loss = 0.44903255\n",
      "Iteration 51, loss = 0.44710989\n",
      "Iteration 52, loss = 0.44778255\n",
      "Iteration 53, loss = 0.44732951\n",
      "Iteration 54, loss = 0.44608465\n",
      "Iteration 55, loss = 0.44524049\n",
      "Iteration 56, loss = 0.44876202\n",
      "Iteration 57, loss = 0.44845356\n",
      "Iteration 58, loss = 0.44659773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58950854\n",
      "Iteration 2, loss = 0.53168513\n",
      "Iteration 3, loss = 0.52671843\n",
      "Iteration 4, loss = 0.52196567\n",
      "Iteration 5, loss = 0.51627563\n",
      "Iteration 6, loss = 0.51123168\n",
      "Iteration 7, loss = 0.50500509\n",
      "Iteration 8, loss = 0.50271865\n",
      "Iteration 9, loss = 0.49946744\n",
      "Iteration 10, loss = 0.49548239\n",
      "Iteration 11, loss = 0.49357351\n",
      "Iteration 12, loss = 0.49038280\n",
      "Iteration 13, loss = 0.48697426\n",
      "Iteration 14, loss = 0.48633818\n",
      "Iteration 15, loss = 0.48216808\n",
      "Iteration 16, loss = 0.48082051\n",
      "Iteration 17, loss = 0.48154418\n",
      "Iteration 18, loss = 0.47833452\n",
      "Iteration 19, loss = 0.47415454\n",
      "Iteration 20, loss = 0.47290186\n",
      "Iteration 21, loss = 0.47029891\n",
      "Iteration 22, loss = 0.46868887\n",
      "Iteration 23, loss = 0.46783411\n",
      "Iteration 24, loss = 0.46579984\n",
      "Iteration 25, loss = 0.46571980\n",
      "Iteration 26, loss = 0.46596195\n",
      "Iteration 27, loss = 0.46533370\n",
      "Iteration 28, loss = 0.46444357\n",
      "Iteration 29, loss = 0.46489643\n",
      "Iteration 30, loss = 0.46277306\n",
      "Iteration 31, loss = 0.46140364\n",
      "Iteration 32, loss = 0.46099448\n",
      "Iteration 33, loss = 0.46056470\n",
      "Iteration 34, loss = 0.46366360\n",
      "Iteration 35, loss = 0.46067811\n",
      "Iteration 36, loss = 0.46131327\n",
      "Iteration 37, loss = 0.45897539\n",
      "Iteration 38, loss = 0.45836494\n",
      "Iteration 39, loss = 0.45737800\n",
      "Iteration 40, loss = 0.45697911\n",
      "Iteration 41, loss = 0.45676069\n",
      "Iteration 42, loss = 0.45553849\n",
      "Iteration 43, loss = 0.45371254\n",
      "Iteration 44, loss = 0.45584722\n",
      "Iteration 45, loss = 0.45386022\n",
      "Iteration 46, loss = 0.45292546\n",
      "Iteration 47, loss = 0.45518845\n",
      "Iteration 48, loss = 0.45482627\n",
      "Iteration 49, loss = 0.45135462\n",
      "Iteration 50, loss = 0.45328399\n",
      "Iteration 51, loss = 0.45362465\n",
      "Iteration 52, loss = 0.45097913\n",
      "Iteration 53, loss = 0.45034501\n",
      "Iteration 54, loss = 0.45066680\n",
      "Iteration 55, loss = 0.44782992\n",
      "Iteration 56, loss = 0.44890018\n",
      "Iteration 57, loss = 0.44935622\n",
      "Iteration 58, loss = 0.45172529\n",
      "Iteration 59, loss = 0.44743412\n",
      "Iteration 60, loss = 0.44876431\n",
      "Iteration 61, loss = 0.44922785\n",
      "Iteration 62, loss = 0.45057028\n",
      "Iteration 63, loss = 0.45131741\n",
      "Iteration 64, loss = 0.44874884\n",
      "Iteration 65, loss = 0.44655566\n",
      "Iteration 66, loss = 0.44820127\n",
      "Iteration 67, loss = 0.44891481\n",
      "Iteration 68, loss = 0.44482774\n",
      "Iteration 69, loss = 0.44650176\n",
      "Iteration 70, loss = 0.44751101\n",
      "Iteration 71, loss = 0.44736894\n",
      "Iteration 72, loss = 0.44746132\n",
      "Iteration 73, loss = 0.44614175\n",
      "Iteration 74, loss = 0.44480478\n",
      "Iteration 75, loss = 0.44689844\n",
      "Iteration 76, loss = 0.44639725\n",
      "Iteration 77, loss = 0.44727444\n",
      "Iteration 78, loss = 0.45122142\n",
      "Iteration 79, loss = 0.44928690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61552660\n",
      "Iteration 2, loss = 0.48998895\n",
      "Iteration 3, loss = 0.48069201\n",
      "Iteration 4, loss = 0.47682063\n",
      "Iteration 5, loss = 0.47349719\n",
      "Iteration 6, loss = 0.47361519\n",
      "Iteration 7, loss = 0.47045174\n",
      "Iteration 8, loss = 0.46799290\n",
      "Iteration 9, loss = 0.46702255\n",
      "Iteration 10, loss = 0.47089109\n",
      "Iteration 11, loss = 0.46489847\n",
      "Iteration 12, loss = 0.46027988\n",
      "Iteration 13, loss = 0.46091105\n",
      "Iteration 14, loss = 0.45724836\n",
      "Iteration 15, loss = 0.45661699\n",
      "Iteration 16, loss = 0.45522182\n",
      "Iteration 17, loss = 0.45341147\n",
      "Iteration 18, loss = 0.45707972\n",
      "Iteration 19, loss = 0.45417920\n",
      "Iteration 20, loss = 0.45173882\n",
      "Iteration 21, loss = 0.45091982\n",
      "Iteration 22, loss = 0.45222947\n",
      "Iteration 23, loss = 0.45115192\n",
      "Iteration 24, loss = 0.44752734\n",
      "Iteration 25, loss = 0.44802588\n",
      "Iteration 26, loss = 0.44807000\n",
      "Iteration 27, loss = 0.45051770\n",
      "Iteration 28, loss = 0.44535466\n",
      "Iteration 29, loss = 0.44468936\n",
      "Iteration 30, loss = 0.44351450\n",
      "Iteration 31, loss = 0.44463931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.44452301\n",
      "Iteration 33, loss = 0.44610468\n",
      "Iteration 34, loss = 0.44297013\n",
      "Iteration 35, loss = 0.44373524\n",
      "Iteration 36, loss = 0.44022666\n",
      "Iteration 37, loss = 0.44084671\n",
      "Iteration 38, loss = 0.44458433\n",
      "Iteration 39, loss = 0.43956151\n",
      "Iteration 40, loss = 0.43976085\n",
      "Iteration 41, loss = 0.43836358\n",
      "Iteration 42, loss = 0.43867267\n",
      "Iteration 43, loss = 0.43648866\n",
      "Iteration 44, loss = 0.43534119\n",
      "Iteration 45, loss = 0.43747100\n",
      "Iteration 46, loss = 0.43649836\n",
      "Iteration 47, loss = 0.43554192\n",
      "Iteration 48, loss = 0.43621686\n",
      "Iteration 49, loss = 0.43779354\n",
      "Iteration 50, loss = 0.44265358\n",
      "Iteration 51, loss = 0.43410173\n",
      "Iteration 52, loss = 0.43349776\n",
      "Iteration 53, loss = 0.43365865\n",
      "Iteration 54, loss = 0.43088798\n",
      "Iteration 55, loss = 0.43209390\n",
      "Iteration 56, loss = 0.43022246\n",
      "Iteration 57, loss = 0.43198611\n",
      "Iteration 58, loss = 0.43106745\n",
      "Iteration 59, loss = 0.43075394\n",
      "Iteration 60, loss = 0.43248898\n",
      "Iteration 61, loss = 0.43194003\n",
      "Iteration 62, loss = 0.42981964\n",
      "Iteration 63, loss = 0.43005102\n",
      "Iteration 64, loss = 0.42924165\n",
      "Iteration 65, loss = 0.43191127\n",
      "Iteration 66, loss = 0.43185063\n",
      "Iteration 67, loss = 0.42682098\n",
      "Iteration 68, loss = 0.42865398\n",
      "Iteration 69, loss = 0.42996643\n",
      "Iteration 70, loss = 0.42798689\n",
      "Iteration 71, loss = 0.42940050\n",
      "Iteration 72, loss = 0.42939976\n",
      "Iteration 73, loss = 0.42955903\n",
      "Iteration 74, loss = 0.42825533\n",
      "Iteration 75, loss = 0.42712612\n",
      "Iteration 76, loss = 0.42897102\n",
      "Iteration 77, loss = 0.42654374\n",
      "Iteration 78, loss = 0.43040209\n",
      "Iteration 79, loss = 0.42880543\n",
      "Iteration 80, loss = 0.42758481\n",
      "Iteration 81, loss = 0.42646496\n",
      "Iteration 82, loss = 0.42739023\n",
      "Iteration 83, loss = 0.42779667\n",
      "Iteration 84, loss = 0.42862307\n",
      "Iteration 85, loss = 0.42561670\n",
      "Iteration 86, loss = 0.42520864\n",
      "Iteration 87, loss = 0.42564483\n",
      "Iteration 88, loss = 0.42514280\n",
      "Iteration 89, loss = 0.42626955\n",
      "Iteration 90, loss = 0.42721914\n",
      "Iteration 91, loss = 0.42694652\n",
      "Iteration 92, loss = 0.42584895\n",
      "Iteration 93, loss = 0.42343936\n",
      "Iteration 94, loss = 0.42682214\n",
      "Iteration 95, loss = 0.42429006\n",
      "Iteration 96, loss = 0.42644035\n",
      "Iteration 97, loss = 0.42477240\n",
      "Iteration 98, loss = 0.42550140\n",
      "Iteration 99, loss = 0.42313727\n",
      "Iteration 100, loss = 0.42308636\n",
      "Iteration 101, loss = 0.42565602\n",
      "Iteration 102, loss = 0.42305004\n",
      "Iteration 103, loss = 0.42359190\n",
      "Iteration 104, loss = 0.42395092\n",
      "Iteration 105, loss = 0.42315514\n",
      "Iteration 106, loss = 0.42284450\n",
      "Iteration 107, loss = 0.42263876\n",
      "Iteration 108, loss = 0.42351409\n",
      "Iteration 109, loss = 0.42282943\n",
      "Iteration 110, loss = 0.42519074\n",
      "Iteration 111, loss = 0.42408610\n",
      "Iteration 112, loss = 0.42384127\n",
      "Iteration 113, loss = 0.42329928\n",
      "Iteration 114, loss = 0.42215218\n",
      "Iteration 115, loss = 0.42281020\n",
      "Iteration 116, loss = 0.42258503\n",
      "Iteration 117, loss = 0.42339877\n",
      "Iteration 118, loss = 0.42483068\n",
      "Iteration 119, loss = 0.42568613\n",
      "Iteration 120, loss = 0.42330217\n",
      "Iteration 121, loss = 0.42497823\n",
      "Iteration 122, loss = 0.42135919\n",
      "Iteration 123, loss = 0.42367510\n",
      "Iteration 124, loss = 0.42738129\n",
      "Iteration 125, loss = 0.42378175\n",
      "Iteration 126, loss = 0.42343628\n",
      "Iteration 127, loss = 0.42219325\n",
      "Iteration 128, loss = 0.42349416\n",
      "Iteration 129, loss = 0.42262732\n",
      "Iteration 130, loss = 0.42517165\n",
      "Iteration 131, loss = 0.42393016\n",
      "Iteration 132, loss = 0.42218010\n",
      "Iteration 133, loss = 0.42392398\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61213474\n",
      "Iteration 2, loss = 0.48545678\n",
      "Iteration 3, loss = 0.47564120\n",
      "Iteration 4, loss = 0.47210129\n",
      "Iteration 5, loss = 0.46812154\n",
      "Iteration 6, loss = 0.46472219\n",
      "Iteration 7, loss = 0.46244542\n",
      "Iteration 8, loss = 0.46014160\n",
      "Iteration 9, loss = 0.45967554\n",
      "Iteration 10, loss = 0.45829317\n",
      "Iteration 11, loss = 0.45686454\n",
      "Iteration 12, loss = 0.45601733\n",
      "Iteration 13, loss = 0.45623589\n",
      "Iteration 14, loss = 0.45455930\n",
      "Iteration 15, loss = 0.45216594\n",
      "Iteration 16, loss = 0.45134323\n",
      "Iteration 17, loss = 0.45132911\n",
      "Iteration 18, loss = 0.45204110\n",
      "Iteration 19, loss = 0.45246670\n",
      "Iteration 20, loss = 0.44989601\n",
      "Iteration 21, loss = 0.45051872\n",
      "Iteration 22, loss = 0.45116402\n",
      "Iteration 23, loss = 0.45025518\n",
      "Iteration 24, loss = 0.44832778\n",
      "Iteration 25, loss = 0.44782325\n",
      "Iteration 26, loss = 0.44669089\n",
      "Iteration 27, loss = 0.45350942\n",
      "Iteration 28, loss = 0.45264734\n",
      "Iteration 29, loss = 0.44890367\n",
      "Iteration 30, loss = 0.44968509\n",
      "Iteration 31, loss = 0.44535875\n",
      "Iteration 32, loss = 0.44829991\n",
      "Iteration 33, loss = 0.44726131\n",
      "Iteration 34, loss = 0.44511968\n",
      "Iteration 35, loss = 0.44528767\n",
      "Iteration 36, loss = 0.44436651\n",
      "Iteration 37, loss = 0.44502982\n",
      "Iteration 38, loss = 0.44601844\n",
      "Iteration 39, loss = 0.44507965\n",
      "Iteration 40, loss = 0.44697393\n",
      "Iteration 41, loss = 0.44598357\n",
      "Iteration 42, loss = 0.44641445\n",
      "Iteration 43, loss = 0.44356747\n",
      "Iteration 44, loss = 0.44259114\n",
      "Iteration 45, loss = 0.44404185\n",
      "Iteration 46, loss = 0.44405858\n",
      "Iteration 47, loss = 0.44143353\n",
      "Iteration 48, loss = 0.44202033\n",
      "Iteration 49, loss = 0.44277979\n",
      "Iteration 50, loss = 0.44329229\n",
      "Iteration 51, loss = 0.44390377\n",
      "Iteration 52, loss = 0.44347479\n",
      "Iteration 53, loss = 0.43962537\n",
      "Iteration 54, loss = 0.44035749\n",
      "Iteration 55, loss = 0.44032796\n",
      "Iteration 56, loss = 0.44246663\n",
      "Iteration 57, loss = 0.44087387\n",
      "Iteration 58, loss = 0.44005300\n",
      "Iteration 59, loss = 0.43943667\n",
      "Iteration 60, loss = 0.44002976\n",
      "Iteration 61, loss = 0.44229185\n",
      "Iteration 62, loss = 0.43918537\n",
      "Iteration 63, loss = 0.43789226\n",
      "Iteration 64, loss = 0.44269177\n",
      "Iteration 65, loss = 0.44111247\n",
      "Iteration 66, loss = 0.43839329\n",
      "Iteration 67, loss = 0.43794615\n",
      "Iteration 68, loss = 0.43912989\n",
      "Iteration 69, loss = 0.43811393\n",
      "Iteration 70, loss = 0.43633932\n",
      "Iteration 71, loss = 0.43795910\n",
      "Iteration 72, loss = 0.43703513\n",
      "Iteration 73, loss = 0.43773080\n",
      "Iteration 74, loss = 0.43774591\n",
      "Iteration 75, loss = 0.43833297\n",
      "Iteration 76, loss = 0.43818012\n",
      "Iteration 77, loss = 0.43595312\n",
      "Iteration 78, loss = 0.43665154\n",
      "Iteration 79, loss = 0.43686067\n",
      "Iteration 80, loss = 0.43533413\n",
      "Iteration 81, loss = 0.43520546\n",
      "Iteration 82, loss = 0.43538459\n",
      "Iteration 83, loss = 0.43504149\n",
      "Iteration 84, loss = 0.43365760\n",
      "Iteration 85, loss = 0.43559920\n",
      "Iteration 86, loss = 0.43755227\n",
      "Iteration 87, loss = 0.43282925\n",
      "Iteration 88, loss = 0.43530457\n",
      "Iteration 89, loss = 0.43310185\n",
      "Iteration 90, loss = 0.43442075\n",
      "Iteration 91, loss = 0.43455519\n",
      "Iteration 92, loss = 0.44152429\n",
      "Iteration 93, loss = 0.43400481\n",
      "Iteration 94, loss = 0.43107666\n",
      "Iteration 95, loss = 0.43185475\n",
      "Iteration 96, loss = 0.43193303\n",
      "Iteration 97, loss = 0.43192696\n",
      "Iteration 98, loss = 0.43237537\n",
      "Iteration 99, loss = 0.43125473\n",
      "Iteration 100, loss = 0.43237112\n",
      "Iteration 101, loss = 0.43297408\n",
      "Iteration 102, loss = 0.43126923\n",
      "Iteration 103, loss = 0.43008369\n",
      "Iteration 104, loss = 0.43077167\n",
      "Iteration 105, loss = 0.43074127\n",
      "Iteration 106, loss = 0.42900066\n",
      "Iteration 107, loss = 0.42983302\n",
      "Iteration 108, loss = 0.43148938\n",
      "Iteration 109, loss = 0.43105317\n",
      "Iteration 110, loss = 0.43099635\n",
      "Iteration 111, loss = 0.42977285\n",
      "Iteration 112, loss = 0.43057556\n",
      "Iteration 113, loss = 0.42861595\n",
      "Iteration 114, loss = 0.43081203\n",
      "Iteration 115, loss = 0.42915952\n",
      "Iteration 116, loss = 0.42888297\n",
      "Iteration 117, loss = 0.42663665\n",
      "Iteration 118, loss = 0.42747181\n",
      "Iteration 119, loss = 0.42994862\n",
      "Iteration 120, loss = 0.42991966\n",
      "Iteration 121, loss = 0.42963988\n",
      "Iteration 122, loss = 0.42974616\n",
      "Iteration 123, loss = 0.43033328\n",
      "Iteration 124, loss = 0.42797370\n",
      "Iteration 125, loss = 0.42713635\n",
      "Iteration 126, loss = 0.42438200\n",
      "Iteration 127, loss = 0.42510509\n",
      "Iteration 128, loss = 0.42363922\n",
      "Iteration 129, loss = 0.42379233\n",
      "Iteration 130, loss = 0.42403948\n",
      "Iteration 131, loss = 0.42637060\n",
      "Iteration 132, loss = 0.42297052\n",
      "Iteration 133, loss = 0.42293535\n",
      "Iteration 134, loss = 0.42364196\n",
      "Iteration 135, loss = 0.42425534\n",
      "Iteration 136, loss = 0.42193539\n",
      "Iteration 137, loss = 0.42193589\n",
      "Iteration 138, loss = 0.42440918\n",
      "Iteration 139, loss = 0.42448244\n",
      "Iteration 140, loss = 0.42390722\n",
      "Iteration 141, loss = 0.42309543\n",
      "Iteration 142, loss = 0.42232732\n",
      "Iteration 143, loss = 0.42112396\n",
      "Iteration 144, loss = 0.41986069\n",
      "Iteration 145, loss = 0.42181685\n",
      "Iteration 146, loss = 0.42177830\n",
      "Iteration 147, loss = 0.42028911\n",
      "Iteration 148, loss = 0.42208517\n",
      "Iteration 149, loss = 0.42118876\n",
      "Iteration 150, loss = 0.41992018\n",
      "Iteration 151, loss = 0.42235927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 152, loss = 0.41989960\n",
      "Iteration 153, loss = 0.42031756\n",
      "Iteration 154, loss = 0.41979898\n",
      "Iteration 155, loss = 0.41840722\n",
      "Iteration 156, loss = 0.41831673\n",
      "Iteration 157, loss = 0.42039202\n",
      "Iteration 158, loss = 0.41827124\n",
      "Iteration 159, loss = 0.41803682\n",
      "Iteration 160, loss = 0.41893103\n",
      "Iteration 161, loss = 0.41820721\n",
      "Iteration 162, loss = 0.41722760\n",
      "Iteration 163, loss = 0.41782785\n",
      "Iteration 164, loss = 0.41633023\n",
      "Iteration 165, loss = 0.41703465\n",
      "Iteration 166, loss = 0.41810858\n",
      "Iteration 167, loss = 0.41682032\n",
      "Iteration 168, loss = 0.41543758\n",
      "Iteration 169, loss = 0.41906707\n",
      "Iteration 170, loss = 0.41572013\n",
      "Iteration 171, loss = 0.41632163\n",
      "Iteration 172, loss = 0.41940515\n",
      "Iteration 173, loss = 0.41920887\n",
      "Iteration 174, loss = 0.41421662\n",
      "Iteration 175, loss = 0.41666374\n",
      "Iteration 176, loss = 0.41730112\n",
      "Iteration 177, loss = 0.41730884\n",
      "Iteration 178, loss = 0.41705104\n",
      "Iteration 179, loss = 0.41936967\n",
      "Iteration 180, loss = 0.41674484\n",
      "Iteration 181, loss = 0.41622246\n",
      "Iteration 182, loss = 0.41453878\n",
      "Iteration 183, loss = 0.41507759\n",
      "Iteration 184, loss = 0.41476029\n",
      "Iteration 185, loss = 0.41928179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61254825\n",
      "Iteration 2, loss = 0.48814496\n",
      "Iteration 3, loss = 0.47409157\n",
      "Iteration 4, loss = 0.47079681\n",
      "Iteration 5, loss = 0.46853859\n",
      "Iteration 6, loss = 0.46666747\n",
      "Iteration 7, loss = 0.46472025\n",
      "Iteration 8, loss = 0.46548678\n",
      "Iteration 9, loss = 0.46397251\n",
      "Iteration 10, loss = 0.46206914\n",
      "Iteration 11, loss = 0.46171132\n",
      "Iteration 12, loss = 0.45908671\n",
      "Iteration 13, loss = 0.45787857\n",
      "Iteration 14, loss = 0.45973642\n",
      "Iteration 15, loss = 0.45988816\n",
      "Iteration 16, loss = 0.45681816\n",
      "Iteration 17, loss = 0.45824856\n",
      "Iteration 18, loss = 0.45810129\n",
      "Iteration 19, loss = 0.45212582\n",
      "Iteration 20, loss = 0.45187987\n",
      "Iteration 21, loss = 0.45008281\n",
      "Iteration 22, loss = 0.44973927\n",
      "Iteration 23, loss = 0.45024180\n",
      "Iteration 24, loss = 0.45161523\n",
      "Iteration 25, loss = 0.44865544\n",
      "Iteration 26, loss = 0.44733552\n",
      "Iteration 27, loss = 0.44706868\n",
      "Iteration 28, loss = 0.44691464\n",
      "Iteration 29, loss = 0.45196222\n",
      "Iteration 30, loss = 0.45086444\n",
      "Iteration 31, loss = 0.45053122\n",
      "Iteration 32, loss = 0.44672855\n",
      "Iteration 33, loss = 0.44438350\n",
      "Iteration 34, loss = 0.44576793\n",
      "Iteration 35, loss = 0.44591566\n",
      "Iteration 36, loss = 0.44315276\n",
      "Iteration 37, loss = 0.44239246\n",
      "Iteration 38, loss = 0.44321474\n",
      "Iteration 39, loss = 0.44269388\n",
      "Iteration 40, loss = 0.44385726\n",
      "Iteration 41, loss = 0.44025925\n",
      "Iteration 42, loss = 0.44105530\n",
      "Iteration 43, loss = 0.43904955\n",
      "Iteration 44, loss = 0.43958710\n",
      "Iteration 45, loss = 0.43906800\n",
      "Iteration 46, loss = 0.44271133\n",
      "Iteration 47, loss = 0.44291958\n",
      "Iteration 48, loss = 0.44196710\n",
      "Iteration 49, loss = 0.44029739\n",
      "Iteration 50, loss = 0.43829224\n",
      "Iteration 51, loss = 0.43719075\n",
      "Iteration 52, loss = 0.43868072\n",
      "Iteration 53, loss = 0.43756659\n",
      "Iteration 54, loss = 0.43748528\n",
      "Iteration 55, loss = 0.43726418\n",
      "Iteration 56, loss = 0.43631588\n",
      "Iteration 57, loss = 0.43814193\n",
      "Iteration 58, loss = 0.43638481\n",
      "Iteration 59, loss = 0.43566252\n",
      "Iteration 60, loss = 0.43794835\n",
      "Iteration 61, loss = 0.43661591\n",
      "Iteration 62, loss = 0.43885557\n",
      "Iteration 63, loss = 0.43944778\n",
      "Iteration 64, loss = 0.43883734\n",
      "Iteration 65, loss = 0.43573817\n",
      "Iteration 66, loss = 0.43680117\n",
      "Iteration 67, loss = 0.43512463\n",
      "Iteration 68, loss = 0.43562763\n",
      "Iteration 69, loss = 0.43451438\n",
      "Iteration 70, loss = 0.43537758\n",
      "Iteration 71, loss = 0.43691440\n",
      "Iteration 72, loss = 0.43364410\n",
      "Iteration 73, loss = 0.43508246\n",
      "Iteration 74, loss = 0.43295062\n",
      "Iteration 75, loss = 0.43272173\n",
      "Iteration 76, loss = 0.43204792\n",
      "Iteration 77, loss = 0.43362086\n",
      "Iteration 78, loss = 0.43394035\n",
      "Iteration 79, loss = 0.43391316\n",
      "Iteration 80, loss = 0.43153764\n",
      "Iteration 81, loss = 0.43751658\n",
      "Iteration 82, loss = 0.43157882\n",
      "Iteration 83, loss = 0.43017873\n",
      "Iteration 84, loss = 0.43220534\n",
      "Iteration 85, loss = 0.43131408\n",
      "Iteration 86, loss = 0.43127118\n",
      "Iteration 87, loss = 0.43037718\n",
      "Iteration 88, loss = 0.43262758\n",
      "Iteration 89, loss = 0.43089250\n",
      "Iteration 90, loss = 0.43629565\n",
      "Iteration 91, loss = 0.43188880\n",
      "Iteration 92, loss = 0.43156973\n",
      "Iteration 93, loss = 0.43261919\n",
      "Iteration 94, loss = 0.42957987\n",
      "Iteration 95, loss = 0.42921172\n",
      "Iteration 96, loss = 0.43056264\n",
      "Iteration 97, loss = 0.43027649\n",
      "Iteration 98, loss = 0.43070463\n",
      "Iteration 99, loss = 0.42877487\n",
      "Iteration 100, loss = 0.43024386\n",
      "Iteration 101, loss = 0.42948815\n",
      "Iteration 102, loss = 0.43167159\n",
      "Iteration 103, loss = 0.43477265\n",
      "Iteration 104, loss = 0.43663308\n",
      "Iteration 105, loss = 0.43525635\n",
      "Iteration 106, loss = 0.43250337\n",
      "Iteration 107, loss = 0.43246692\n",
      "Iteration 108, loss = 0.43335134\n",
      "Iteration 109, loss = 0.43247472\n",
      "Iteration 110, loss = 0.43160599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60011131\n",
      "Iteration 2, loss = 0.48784115\n",
      "Iteration 3, loss = 0.48022280\n",
      "Iteration 4, loss = 0.47669068\n",
      "Iteration 5, loss = 0.47482587\n",
      "Iteration 6, loss = 0.47674139\n",
      "Iteration 7, loss = 0.47384342\n",
      "Iteration 8, loss = 0.47260269\n",
      "Iteration 9, loss = 0.47309299\n",
      "Iteration 10, loss = 0.47427761\n",
      "Iteration 11, loss = 0.47029239\n",
      "Iteration 12, loss = 0.46773630\n",
      "Iteration 13, loss = 0.47097484\n",
      "Iteration 14, loss = 0.46782951\n",
      "Iteration 15, loss = 0.46577395\n",
      "Iteration 16, loss = 0.46422746\n",
      "Iteration 17, loss = 0.46450689\n",
      "Iteration 18, loss = 0.45984856\n",
      "Iteration 19, loss = 0.45996964\n",
      "Iteration 20, loss = 0.45955984\n",
      "Iteration 21, loss = 0.45568473\n",
      "Iteration 22, loss = 0.45716501\n",
      "Iteration 23, loss = 0.45464318\n",
      "Iteration 24, loss = 0.45384650\n",
      "Iteration 25, loss = 0.45461196\n",
      "Iteration 26, loss = 0.45357187\n",
      "Iteration 27, loss = 0.45441450\n",
      "Iteration 28, loss = 0.45222730\n",
      "Iteration 29, loss = 0.45039779\n",
      "Iteration 30, loss = 0.45013672\n",
      "Iteration 31, loss = 0.45555439\n",
      "Iteration 32, loss = 0.45117836\n",
      "Iteration 33, loss = 0.45064544\n",
      "Iteration 34, loss = 0.44891894\n",
      "Iteration 35, loss = 0.45091944\n",
      "Iteration 36, loss = 0.44771192\n",
      "Iteration 37, loss = 0.45164575\n",
      "Iteration 38, loss = 0.45506958\n",
      "Iteration 39, loss = 0.45319704\n",
      "Iteration 40, loss = 0.44734004\n",
      "Iteration 41, loss = 0.44662261\n",
      "Iteration 42, loss = 0.44706076\n",
      "Iteration 43, loss = 0.44774575\n",
      "Iteration 44, loss = 0.44774047\n",
      "Iteration 45, loss = 0.44668793\n",
      "Iteration 46, loss = 0.44707509\n",
      "Iteration 47, loss = 0.44551310\n",
      "Iteration 48, loss = 0.45148835\n",
      "Iteration 49, loss = 0.44781641\n",
      "Iteration 50, loss = 0.44953655\n",
      "Iteration 51, loss = 0.44557325\n",
      "Iteration 52, loss = 0.44564257\n",
      "Iteration 53, loss = 0.44541219\n",
      "Iteration 54, loss = 0.44402493\n",
      "Iteration 55, loss = 0.44281345\n",
      "Iteration 56, loss = 0.44262092\n",
      "Iteration 57, loss = 0.44466071\n",
      "Iteration 58, loss = 0.44313708\n",
      "Iteration 59, loss = 0.44461677\n",
      "Iteration 60, loss = 0.44504740\n",
      "Iteration 61, loss = 0.44560532\n",
      "Iteration 62, loss = 0.44653854\n",
      "Iteration 63, loss = 0.44610455\n",
      "Iteration 64, loss = 0.44426593\n",
      "Iteration 65, loss = 0.44556650\n",
      "Iteration 66, loss = 0.44599906\n",
      "Iteration 67, loss = 0.44211649\n",
      "Iteration 68, loss = 0.44361827\n",
      "Iteration 69, loss = 0.44412680\n",
      "Iteration 70, loss = 0.44297381\n",
      "Iteration 71, loss = 0.44493916\n",
      "Iteration 72, loss = 0.44300842\n",
      "Iteration 73, loss = 0.44691125\n",
      "Iteration 74, loss = 0.44552148\n",
      "Iteration 75, loss = 0.44303543\n",
      "Iteration 76, loss = 0.44423460\n",
      "Iteration 77, loss = 0.44262811\n",
      "Iteration 78, loss = 0.44459050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59761237\n",
      "Iteration 2, loss = 0.48424895\n",
      "Iteration 3, loss = 0.47697635\n",
      "Iteration 4, loss = 0.47407869\n",
      "Iteration 5, loss = 0.47199785\n",
      "Iteration 6, loss = 0.47025576\n",
      "Iteration 7, loss = 0.46968145\n",
      "Iteration 8, loss = 0.47005409\n",
      "Iteration 9, loss = 0.46879335\n",
      "Iteration 10, loss = 0.46970818\n",
      "Iteration 11, loss = 0.47010791\n",
      "Iteration 12, loss = 0.46951875\n",
      "Iteration 13, loss = 0.47004374\n",
      "Iteration 14, loss = 0.46824914\n",
      "Iteration 15, loss = 0.46823942\n",
      "Iteration 16, loss = 0.46762128\n",
      "Iteration 17, loss = 0.46785778\n",
      "Iteration 18, loss = 0.46682833\n",
      "Iteration 19, loss = 0.46763500\n",
      "Iteration 20, loss = 0.46625473\n",
      "Iteration 21, loss = 0.46693082\n",
      "Iteration 22, loss = 0.46676639\n",
      "Iteration 23, loss = 0.46937642\n",
      "Iteration 24, loss = 0.46565506\n",
      "Iteration 25, loss = 0.46522458\n",
      "Iteration 26, loss = 0.46561243\n",
      "Iteration 27, loss = 0.46561685\n",
      "Iteration 28, loss = 0.46546971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29, loss = 0.46528747\n",
      "Iteration 30, loss = 0.46595800\n",
      "Iteration 31, loss = 0.46558811\n",
      "Iteration 32, loss = 0.46659325\n",
      "Iteration 33, loss = 0.46546374\n",
      "Iteration 34, loss = 0.46469427\n",
      "Iteration 35, loss = 0.46508685\n",
      "Iteration 36, loss = 0.46441738\n",
      "Iteration 37, loss = 0.46481969\n",
      "Iteration 38, loss = 0.46514980\n",
      "Iteration 39, loss = 0.46524846\n",
      "Iteration 40, loss = 0.46385269\n",
      "Iteration 41, loss = 0.46353847\n",
      "Iteration 42, loss = 0.46388055\n",
      "Iteration 43, loss = 0.46262055\n",
      "Iteration 44, loss = 0.46330256\n",
      "Iteration 45, loss = 0.46456473\n",
      "Iteration 46, loss = 0.46408754\n",
      "Iteration 47, loss = 0.46313170\n",
      "Iteration 48, loss = 0.46324028\n",
      "Iteration 49, loss = 0.46348457\n",
      "Iteration 50, loss = 0.46197208\n",
      "Iteration 51, loss = 0.46299475\n",
      "Iteration 52, loss = 0.46226032\n",
      "Iteration 53, loss = 0.46037531\n",
      "Iteration 54, loss = 0.45874920\n",
      "Iteration 55, loss = 0.45830243\n",
      "Iteration 56, loss = 0.45687617\n",
      "Iteration 57, loss = 0.45557117\n",
      "Iteration 58, loss = 0.45596491\n",
      "Iteration 59, loss = 0.45538531\n",
      "Iteration 60, loss = 0.45424664\n",
      "Iteration 61, loss = 0.45316224\n",
      "Iteration 62, loss = 0.45311971\n",
      "Iteration 63, loss = 0.45178024\n",
      "Iteration 64, loss = 0.45229327\n",
      "Iteration 65, loss = 0.45030612\n",
      "Iteration 66, loss = 0.45134639\n",
      "Iteration 67, loss = 0.45001967\n",
      "Iteration 68, loss = 0.44905161\n",
      "Iteration 69, loss = 0.45132035\n",
      "Iteration 70, loss = 0.45033389\n",
      "Iteration 71, loss = 0.44994101\n",
      "Iteration 72, loss = 0.44972314\n",
      "Iteration 73, loss = 0.44851838\n",
      "Iteration 74, loss = 0.44797261\n",
      "Iteration 75, loss = 0.44804597\n",
      "Iteration 76, loss = 0.44865240\n",
      "Iteration 77, loss = 0.44921851\n",
      "Iteration 78, loss = 0.45198994\n",
      "Iteration 79, loss = 0.44857103\n",
      "Iteration 80, loss = 0.44959104\n",
      "Iteration 81, loss = 0.44801170\n",
      "Iteration 82, loss = 0.44811208\n",
      "Iteration 83, loss = 0.44789122\n",
      "Iteration 84, loss = 0.44430095\n",
      "Iteration 85, loss = 0.44510209\n",
      "Iteration 86, loss = 0.44735785\n",
      "Iteration 87, loss = 0.44478051\n",
      "Iteration 88, loss = 0.44368087\n",
      "Iteration 89, loss = 0.44505130\n",
      "Iteration 90, loss = 0.44755396\n",
      "Iteration 91, loss = 0.44401697\n",
      "Iteration 92, loss = 0.44549698\n",
      "Iteration 93, loss = 0.44227812\n",
      "Iteration 94, loss = 0.44236595\n",
      "Iteration 95, loss = 0.44350809\n",
      "Iteration 96, loss = 0.44214510\n",
      "Iteration 97, loss = 0.44330123\n",
      "Iteration 98, loss = 0.44195115\n",
      "Iteration 99, loss = 0.44044905\n",
      "Iteration 100, loss = 0.44341135\n",
      "Iteration 101, loss = 0.44142047\n",
      "Iteration 102, loss = 0.44244922\n",
      "Iteration 103, loss = 0.44119341\n",
      "Iteration 104, loss = 0.44209809\n",
      "Iteration 105, loss = 0.44122891\n",
      "Iteration 106, loss = 0.43954831\n",
      "Iteration 107, loss = 0.44086728\n",
      "Iteration 108, loss = 0.44305669\n",
      "Iteration 109, loss = 0.44372901\n",
      "Iteration 110, loss = 0.44293334\n",
      "Iteration 111, loss = 0.44392323\n",
      "Iteration 112, loss = 0.44168827\n",
      "Iteration 113, loss = 0.43965583\n",
      "Iteration 114, loss = 0.43913308\n",
      "Iteration 115, loss = 0.43878834\n",
      "Iteration 116, loss = 0.43987967\n",
      "Iteration 117, loss = 0.43782908\n",
      "Iteration 118, loss = 0.43808845\n",
      "Iteration 119, loss = 0.43731804\n",
      "Iteration 120, loss = 0.43611136\n",
      "Iteration 121, loss = 0.43836009\n",
      "Iteration 122, loss = 0.43572344\n",
      "Iteration 123, loss = 0.43556568\n",
      "Iteration 124, loss = 0.44350966\n",
      "Iteration 125, loss = 0.43719956\n",
      "Iteration 126, loss = 0.43569294\n",
      "Iteration 127, loss = 0.43581311\n",
      "Iteration 128, loss = 0.43456534\n",
      "Iteration 129, loss = 0.43748047\n",
      "Iteration 130, loss = 0.43751038\n",
      "Iteration 131, loss = 0.43975385\n",
      "Iteration 132, loss = 0.43683216\n",
      "Iteration 133, loss = 0.43523563\n",
      "Iteration 134, loss = 0.43559999\n",
      "Iteration 135, loss = 0.43362098\n",
      "Iteration 136, loss = 0.43444028\n",
      "Iteration 137, loss = 0.43489515\n",
      "Iteration 138, loss = 0.43428220\n",
      "Iteration 139, loss = 0.43247269\n",
      "Iteration 140, loss = 0.43114566\n",
      "Iteration 141, loss = 0.42991291\n",
      "Iteration 142, loss = 0.43012744\n",
      "Iteration 143, loss = 0.42867201\n",
      "Iteration 144, loss = 0.42953956\n",
      "Iteration 145, loss = 0.42971334\n",
      "Iteration 146, loss = 0.42955395\n",
      "Iteration 147, loss = 0.43257655\n",
      "Iteration 148, loss = 0.43130936\n",
      "Iteration 149, loss = 0.42771451\n",
      "Iteration 150, loss = 0.42773657\n",
      "Iteration 151, loss = 0.42702512\n",
      "Iteration 152, loss = 0.42694389\n",
      "Iteration 153, loss = 0.42739434\n",
      "Iteration 154, loss = 0.42739445\n",
      "Iteration 155, loss = 0.42982794\n",
      "Iteration 156, loss = 0.42898555\n",
      "Iteration 157, loss = 0.42801981\n",
      "Iteration 158, loss = 0.42709439\n",
      "Iteration 159, loss = 0.42646611\n",
      "Iteration 160, loss = 0.43038566\n",
      "Iteration 161, loss = 0.42529915\n",
      "Iteration 162, loss = 0.42642534\n",
      "Iteration 163, loss = 0.42556886\n",
      "Iteration 164, loss = 0.42927172\n",
      "Iteration 165, loss = 0.42639645\n",
      "Iteration 166, loss = 0.42836706\n",
      "Iteration 167, loss = 0.42778456\n",
      "Iteration 168, loss = 0.42685475\n",
      "Iteration 169, loss = 0.42814641\n",
      "Iteration 170, loss = 0.42591825\n",
      "Iteration 171, loss = 0.42606499\n",
      "Iteration 172, loss = 0.42638298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59768830\n",
      "Iteration 2, loss = 0.48760685\n",
      "Iteration 3, loss = 0.47587494\n",
      "Iteration 4, loss = 0.47252621\n",
      "Iteration 5, loss = 0.47017099\n",
      "Iteration 6, loss = 0.46877256\n",
      "Iteration 7, loss = 0.46781748\n",
      "Iteration 8, loss = 0.46876154\n",
      "Iteration 9, loss = 0.46724734\n",
      "Iteration 10, loss = 0.46621742\n",
      "Iteration 11, loss = 0.46630844\n",
      "Iteration 12, loss = 0.46491199\n",
      "Iteration 13, loss = 0.46485752\n",
      "Iteration 14, loss = 0.46542885\n",
      "Iteration 15, loss = 0.46184297\n",
      "Iteration 16, loss = 0.46178804\n",
      "Iteration 17, loss = 0.46134768\n",
      "Iteration 18, loss = 0.46113540\n",
      "Iteration 19, loss = 0.45978153\n",
      "Iteration 20, loss = 0.45737043\n",
      "Iteration 21, loss = 0.45551795\n",
      "Iteration 22, loss = 0.45647092\n",
      "Iteration 23, loss = 0.45662961\n",
      "Iteration 24, loss = 0.45446182\n",
      "Iteration 25, loss = 0.45502491\n",
      "Iteration 26, loss = 0.45448963\n",
      "Iteration 27, loss = 0.45326828\n",
      "Iteration 28, loss = 0.45489849\n",
      "Iteration 29, loss = 0.46040597\n",
      "Iteration 30, loss = 0.45855084\n",
      "Iteration 31, loss = 0.45459594\n",
      "Iteration 32, loss = 0.45368494\n",
      "Iteration 33, loss = 0.45315006\n",
      "Iteration 34, loss = 0.45331560\n",
      "Iteration 35, loss = 0.45140195\n",
      "Iteration 36, loss = 0.45049579\n",
      "Iteration 37, loss = 0.45041773\n",
      "Iteration 38, loss = 0.44991723\n",
      "Iteration 39, loss = 0.45067254\n",
      "Iteration 40, loss = 0.45239795\n",
      "Iteration 41, loss = 0.45032052\n",
      "Iteration 42, loss = 0.45167362\n",
      "Iteration 43, loss = 0.44939507\n",
      "Iteration 44, loss = 0.44948140\n",
      "Iteration 45, loss = 0.45010087\n",
      "Iteration 46, loss = 0.45212232\n",
      "Iteration 47, loss = 0.45277507\n",
      "Iteration 48, loss = 0.44923931\n",
      "Iteration 49, loss = 0.44979076\n",
      "Iteration 50, loss = 0.44885169\n",
      "Iteration 51, loss = 0.44930199\n",
      "Iteration 52, loss = 0.44808018\n",
      "Iteration 53, loss = 0.44692002\n",
      "Iteration 54, loss = 0.44822847\n",
      "Iteration 55, loss = 0.44709751\n",
      "Iteration 56, loss = 0.44597288\n",
      "Iteration 57, loss = 0.44879804\n",
      "Iteration 58, loss = 0.44693246\n",
      "Iteration 59, loss = 0.44642114\n",
      "Iteration 60, loss = 0.44871131\n",
      "Iteration 61, loss = 0.44652508\n",
      "Iteration 62, loss = 0.44761271\n",
      "Iteration 63, loss = 0.45005610\n",
      "Iteration 64, loss = 0.44895177\n",
      "Iteration 65, loss = 0.44759266\n",
      "Iteration 66, loss = 0.45123353\n",
      "Iteration 67, loss = 0.44761322\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60684569\n",
      "Iteration 2, loss = 0.49016859\n",
      "Iteration 3, loss = 0.48138721\n",
      "Iteration 4, loss = 0.47867503\n",
      "Iteration 5, loss = 0.47773630\n",
      "Iteration 6, loss = 0.47652587\n",
      "Iteration 7, loss = 0.47575689\n",
      "Iteration 8, loss = 0.47628905\n",
      "Iteration 9, loss = 0.47539446\n",
      "Iteration 10, loss = 0.47580003\n",
      "Iteration 11, loss = 0.47497327\n",
      "Iteration 12, loss = 0.47372119\n",
      "Iteration 13, loss = 0.47428726\n",
      "Iteration 14, loss = 0.47297469\n",
      "Iteration 15, loss = 0.47264714\n",
      "Iteration 16, loss = 0.47335007\n",
      "Iteration 17, loss = 0.47436875\n",
      "Iteration 18, loss = 0.47220130\n",
      "Iteration 19, loss = 0.47371226\n",
      "Iteration 20, loss = 0.47214955\n",
      "Iteration 21, loss = 0.47060090\n",
      "Iteration 22, loss = 0.47043148\n",
      "Iteration 23, loss = 0.47196514\n",
      "Iteration 24, loss = 0.47193917\n",
      "Iteration 25, loss = 0.47139491\n",
      "Iteration 26, loss = 0.46948469\n",
      "Iteration 27, loss = 0.47129797\n",
      "Iteration 28, loss = 0.46985567\n",
      "Iteration 29, loss = 0.46773928\n",
      "Iteration 30, loss = 0.46037974\n",
      "Iteration 31, loss = 0.46249210\n",
      "Iteration 32, loss = 0.45685479\n",
      "Iteration 33, loss = 0.45622348\n",
      "Iteration 34, loss = 0.45411150\n",
      "Iteration 35, loss = 0.45343670\n",
      "Iteration 36, loss = 0.45149982\n",
      "Iteration 37, loss = 0.45279974\n",
      "Iteration 38, loss = 0.45573209\n",
      "Iteration 39, loss = 0.44819109\n",
      "Iteration 40, loss = 0.44793032\n",
      "Iteration 41, loss = 0.44732566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.44611499\n",
      "Iteration 43, loss = 0.44727275\n",
      "Iteration 44, loss = 0.44621730\n",
      "Iteration 45, loss = 0.44573013\n",
      "Iteration 46, loss = 0.44797733\n",
      "Iteration 47, loss = 0.44687383\n",
      "Iteration 48, loss = 0.44763548\n",
      "Iteration 49, loss = 0.44879955\n",
      "Iteration 50, loss = 0.44776872\n",
      "Iteration 51, loss = 0.44059011\n",
      "Iteration 52, loss = 0.43328673\n",
      "Iteration 53, loss = 0.43372227\n",
      "Iteration 54, loss = 0.43236004\n",
      "Iteration 55, loss = 0.43318483\n",
      "Iteration 56, loss = 0.43126993\n",
      "Iteration 57, loss = 0.43264104\n",
      "Iteration 58, loss = 0.43349728\n",
      "Iteration 59, loss = 0.42874426\n",
      "Iteration 60, loss = 0.43035861\n",
      "Iteration 61, loss = 0.43088323\n",
      "Iteration 62, loss = 0.43476597\n",
      "Iteration 63, loss = 0.43699487\n",
      "Iteration 64, loss = 0.42867174\n",
      "Iteration 65, loss = 0.42986663\n",
      "Iteration 66, loss = 0.43753426\n",
      "Iteration 67, loss = 0.42809655\n",
      "Iteration 68, loss = 0.42780318\n",
      "Iteration 69, loss = 0.43170313\n",
      "Iteration 70, loss = 0.42893114\n",
      "Iteration 71, loss = 0.43221078\n",
      "Iteration 72, loss = 0.42741561\n",
      "Iteration 73, loss = 0.42962500\n",
      "Iteration 74, loss = 0.42767839\n",
      "Iteration 75, loss = 0.42844923\n",
      "Iteration 76, loss = 0.42751179\n",
      "Iteration 77, loss = 0.42592466\n",
      "Iteration 78, loss = 0.42963153\n",
      "Iteration 79, loss = 0.42900562\n",
      "Iteration 80, loss = 0.42735340\n",
      "Iteration 81, loss = 0.42750763\n",
      "Iteration 82, loss = 0.42781234\n",
      "Iteration 83, loss = 0.42689156\n",
      "Iteration 84, loss = 0.43064559\n",
      "Iteration 85, loss = 0.42576870\n",
      "Iteration 86, loss = 0.42365542\n",
      "Iteration 87, loss = 0.42496771\n",
      "Iteration 88, loss = 0.42520101\n",
      "Iteration 89, loss = 0.42577024\n",
      "Iteration 90, loss = 0.42556178\n",
      "Iteration 91, loss = 0.42648412\n",
      "Iteration 92, loss = 0.42917561\n",
      "Iteration 93, loss = 0.42511371\n",
      "Iteration 94, loss = 0.42668920\n",
      "Iteration 95, loss = 0.42547776\n",
      "Iteration 96, loss = 0.42640125\n",
      "Iteration 97, loss = 0.42683646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60534775\n",
      "Iteration 2, loss = 0.48592143\n",
      "Iteration 3, loss = 0.47786982\n",
      "Iteration 4, loss = 0.47449083\n",
      "Iteration 5, loss = 0.47294030\n",
      "Iteration 6, loss = 0.47199842\n",
      "Iteration 7, loss = 0.47097020\n",
      "Iteration 8, loss = 0.47092468\n",
      "Iteration 9, loss = 0.47015921\n",
      "Iteration 10, loss = 0.47035816\n",
      "Iteration 11, loss = 0.46950936\n",
      "Iteration 12, loss = 0.46946985\n",
      "Iteration 13, loss = 0.46928174\n",
      "Iteration 14, loss = 0.46773782\n",
      "Iteration 15, loss = 0.46761935\n",
      "Iteration 16, loss = 0.46629937\n",
      "Iteration 17, loss = 0.46759279\n",
      "Iteration 18, loss = 0.46525638\n",
      "Iteration 19, loss = 0.46638369\n",
      "Iteration 20, loss = 0.46391937\n",
      "Iteration 21, loss = 0.46240480\n",
      "Iteration 22, loss = 0.46057078\n",
      "Iteration 23, loss = 0.46144811\n",
      "Iteration 24, loss = 0.45407411\n",
      "Iteration 25, loss = 0.45134430\n",
      "Iteration 26, loss = 0.45088317\n",
      "Iteration 27, loss = 0.44919112\n",
      "Iteration 28, loss = 0.44654927\n",
      "Iteration 29, loss = 0.44644289\n",
      "Iteration 30, loss = 0.44600357\n",
      "Iteration 31, loss = 0.44467953\n",
      "Iteration 32, loss = 0.44444185\n",
      "Iteration 33, loss = 0.44267260\n",
      "Iteration 34, loss = 0.43950676\n",
      "Iteration 35, loss = 0.44205679\n",
      "Iteration 36, loss = 0.43807906\n",
      "Iteration 37, loss = 0.43574984\n",
      "Iteration 38, loss = 0.44268955\n",
      "Iteration 39, loss = 0.43801593\n",
      "Iteration 40, loss = 0.43301754\n",
      "Iteration 41, loss = 0.43331216\n",
      "Iteration 42, loss = 0.43480077\n",
      "Iteration 43, loss = 0.43113061\n",
      "Iteration 44, loss = 0.42979820\n",
      "Iteration 45, loss = 0.43294644\n",
      "Iteration 46, loss = 0.43495672\n",
      "Iteration 47, loss = 0.43071603\n",
      "Iteration 48, loss = 0.42951369\n",
      "Iteration 49, loss = 0.43029647\n",
      "Iteration 50, loss = 0.42705137\n",
      "Iteration 51, loss = 0.42731132\n",
      "Iteration 52, loss = 0.42677695\n",
      "Iteration 53, loss = 0.42582190\n",
      "Iteration 54, loss = 0.42388081\n",
      "Iteration 55, loss = 0.42557109\n",
      "Iteration 56, loss = 0.42782738\n",
      "Iteration 57, loss = 0.42785834\n",
      "Iteration 58, loss = 0.42506925\n",
      "Iteration 59, loss = 0.42298524\n",
      "Iteration 60, loss = 0.42394090\n",
      "Iteration 61, loss = 0.42294904\n",
      "Iteration 62, loss = 0.42486346\n",
      "Iteration 63, loss = 0.42625692\n",
      "Iteration 64, loss = 0.42227827\n",
      "Iteration 65, loss = 0.42208878\n",
      "Iteration 66, loss = 0.42178068\n",
      "Iteration 67, loss = 0.42257713\n",
      "Iteration 68, loss = 0.42248116\n",
      "Iteration 69, loss = 0.42167800\n",
      "Iteration 70, loss = 0.42323829\n",
      "Iteration 71, loss = 0.42398380\n",
      "Iteration 72, loss = 0.41966981\n",
      "Iteration 73, loss = 0.42159014\n",
      "Iteration 74, loss = 0.41980301\n",
      "Iteration 75, loss = 0.42036250\n",
      "Iteration 76, loss = 0.42103079\n",
      "Iteration 77, loss = 0.41993305\n",
      "Iteration 78, loss = 0.42277711\n",
      "Iteration 79, loss = 0.42110244\n",
      "Iteration 80, loss = 0.41880195\n",
      "Iteration 81, loss = 0.41986770\n",
      "Iteration 82, loss = 0.42028275\n",
      "Iteration 83, loss = 0.42066656\n",
      "Iteration 84, loss = 0.41872143\n",
      "Iteration 85, loss = 0.41774214\n",
      "Iteration 86, loss = 0.42058772\n",
      "Iteration 87, loss = 0.41773360\n",
      "Iteration 88, loss = 0.41908960\n",
      "Iteration 89, loss = 0.41842057\n",
      "Iteration 90, loss = 0.42086434\n",
      "Iteration 91, loss = 0.42245770\n",
      "Iteration 92, loss = 0.42363414\n",
      "Iteration 93, loss = 0.41992272\n",
      "Iteration 94, loss = 0.41832951\n",
      "Iteration 95, loss = 0.41937334\n",
      "Iteration 96, loss = 0.41847549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60201131\n",
      "Iteration 2, loss = 0.48635729\n",
      "Iteration 3, loss = 0.47391145\n",
      "Iteration 4, loss = 0.47211700\n",
      "Iteration 5, loss = 0.47039268\n",
      "Iteration 6, loss = 0.46847480\n",
      "Iteration 7, loss = 0.46681378\n",
      "Iteration 8, loss = 0.46811059\n",
      "Iteration 9, loss = 0.46507962\n",
      "Iteration 10, loss = 0.46404892\n",
      "Iteration 11, loss = 0.46415242\n",
      "Iteration 12, loss = 0.46266907\n",
      "Iteration 13, loss = 0.46197460\n",
      "Iteration 14, loss = 0.46277948\n",
      "Iteration 15, loss = 0.46163799\n",
      "Iteration 16, loss = 0.46163944\n",
      "Iteration 17, loss = 0.46168941\n",
      "Iteration 18, loss = 0.46270512\n",
      "Iteration 19, loss = 0.46037060\n",
      "Iteration 20, loss = 0.45959124\n",
      "Iteration 21, loss = 0.45877831\n",
      "Iteration 22, loss = 0.45932585\n",
      "Iteration 23, loss = 0.45990143\n",
      "Iteration 24, loss = 0.45959611\n",
      "Iteration 25, loss = 0.45863323\n",
      "Iteration 26, loss = 0.45788968\n",
      "Iteration 27, loss = 0.45759050\n",
      "Iteration 28, loss = 0.45729809\n",
      "Iteration 29, loss = 0.45926566\n",
      "Iteration 30, loss = 0.46002841\n",
      "Iteration 31, loss = 0.45963902\n",
      "Iteration 32, loss = 0.45670890\n",
      "Iteration 33, loss = 0.45584089\n",
      "Iteration 34, loss = 0.45728835\n",
      "Iteration 35, loss = 0.45582089\n",
      "Iteration 36, loss = 0.45233833\n",
      "Iteration 37, loss = 0.45282606\n",
      "Iteration 38, loss = 0.45351265\n",
      "Iteration 39, loss = 0.45206065\n",
      "Iteration 40, loss = 0.45435755\n",
      "Iteration 41, loss = 0.45202139\n",
      "Iteration 42, loss = 0.45196176\n",
      "Iteration 43, loss = 0.44992768\n",
      "Iteration 44, loss = 0.44997709\n",
      "Iteration 45, loss = 0.44871393\n",
      "Iteration 46, loss = 0.45190173\n",
      "Iteration 47, loss = 0.44827121\n",
      "Iteration 48, loss = 0.44829805\n",
      "Iteration 49, loss = 0.44924103\n",
      "Iteration 50, loss = 0.44725483\n",
      "Iteration 51, loss = 0.44766239\n",
      "Iteration 52, loss = 0.44721793\n",
      "Iteration 53, loss = 0.44687864\n",
      "Iteration 54, loss = 0.44727012\n",
      "Iteration 55, loss = 0.44844988\n",
      "Iteration 56, loss = 0.44868197\n",
      "Iteration 57, loss = 0.44820737\n",
      "Iteration 58, loss = 0.44716491\n",
      "Iteration 59, loss = 0.44613244\n",
      "Iteration 60, loss = 0.44812350\n",
      "Iteration 61, loss = 0.44763895\n",
      "Iteration 62, loss = 0.44715664\n",
      "Iteration 63, loss = 0.44947264\n",
      "Iteration 64, loss = 0.45047863\n",
      "Iteration 65, loss = 0.44784689\n",
      "Iteration 66, loss = 0.45032859\n",
      "Iteration 67, loss = 0.44673764\n",
      "Iteration 68, loss = 0.44618467\n",
      "Iteration 69, loss = 0.44419608\n",
      "Iteration 70, loss = 0.44523193\n",
      "Iteration 71, loss = 0.44647631\n",
      "Iteration 72, loss = 0.44356324\n",
      "Iteration 73, loss = 0.44530191\n",
      "Iteration 74, loss = 0.44450323\n",
      "Iteration 75, loss = 0.44350742\n",
      "Iteration 76, loss = 0.44334437\n",
      "Iteration 77, loss = 0.44321617\n",
      "Iteration 78, loss = 0.44228067\n",
      "Iteration 79, loss = 0.44464341\n",
      "Iteration 80, loss = 0.44096727\n",
      "Iteration 81, loss = 0.44754695\n",
      "Iteration 82, loss = 0.44202655\n",
      "Iteration 83, loss = 0.44107433\n",
      "Iteration 84, loss = 0.44254321\n",
      "Iteration 85, loss = 0.44274648\n",
      "Iteration 86, loss = 0.44152535\n",
      "Iteration 87, loss = 0.44184215\n",
      "Iteration 88, loss = 0.44645098\n",
      "Iteration 89, loss = 0.44257276\n",
      "Iteration 90, loss = 0.44332996\n",
      "Iteration 91, loss = 0.44341090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56116463\n",
      "Iteration 2, loss = 0.49635133\n",
      "Iteration 3, loss = 0.49013095\n",
      "Iteration 4, loss = 0.48603261\n",
      "Iteration 5, loss = 0.48136266\n",
      "Iteration 6, loss = 0.47931739\n",
      "Iteration 7, loss = 0.47578528\n",
      "Iteration 8, loss = 0.47455620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.47122642\n",
      "Iteration 10, loss = 0.47154717\n",
      "Iteration 11, loss = 0.46840695\n",
      "Iteration 12, loss = 0.46678404\n",
      "Iteration 13, loss = 0.46714215\n",
      "Iteration 14, loss = 0.46564707\n",
      "Iteration 15, loss = 0.46260605\n",
      "Iteration 16, loss = 0.46353204\n",
      "Iteration 17, loss = 0.46238465\n",
      "Iteration 18, loss = 0.46132812\n",
      "Iteration 19, loss = 0.46144889\n",
      "Iteration 20, loss = 0.46020620\n",
      "Iteration 21, loss = 0.45991276\n",
      "Iteration 22, loss = 0.46046246\n",
      "Iteration 23, loss = 0.46048738\n",
      "Iteration 24, loss = 0.46181661\n",
      "Iteration 25, loss = 0.45913189\n",
      "Iteration 26, loss = 0.45938711\n",
      "Iteration 27, loss = 0.45780734\n",
      "Iteration 28, loss = 0.45860667\n",
      "Iteration 29, loss = 0.45726694\n",
      "Iteration 30, loss = 0.45726935\n",
      "Iteration 31, loss = 0.45809807\n",
      "Iteration 32, loss = 0.45738908\n",
      "Iteration 33, loss = 0.45728108\n",
      "Iteration 34, loss = 0.45538008\n",
      "Iteration 35, loss = 0.45605021\n",
      "Iteration 36, loss = 0.45555359\n",
      "Iteration 37, loss = 0.45618991\n",
      "Iteration 38, loss = 0.45740878\n",
      "Iteration 39, loss = 0.45508326\n",
      "Iteration 40, loss = 0.45449224\n",
      "Iteration 41, loss = 0.45495492\n",
      "Iteration 42, loss = 0.45454692\n",
      "Iteration 43, loss = 0.45465608\n",
      "Iteration 44, loss = 0.45262826\n",
      "Iteration 45, loss = 0.45173561\n",
      "Iteration 46, loss = 0.45277581\n",
      "Iteration 47, loss = 0.45150427\n",
      "Iteration 48, loss = 0.45124699\n",
      "Iteration 49, loss = 0.45407115\n",
      "Iteration 50, loss = 0.45323470\n",
      "Iteration 51, loss = 0.45221182\n",
      "Iteration 52, loss = 0.45350603\n",
      "Iteration 53, loss = 0.45472381\n",
      "Iteration 54, loss = 0.45183695\n",
      "Iteration 55, loss = 0.45106133\n",
      "Iteration 56, loss = 0.45208295\n",
      "Iteration 57, loss = 0.45123358\n",
      "Iteration 58, loss = 0.45163567\n",
      "Iteration 59, loss = 0.45059893\n",
      "Iteration 60, loss = 0.45120106\n",
      "Iteration 61, loss = 0.45091680\n",
      "Iteration 62, loss = 0.45160326\n",
      "Iteration 63, loss = 0.45063526\n",
      "Iteration 64, loss = 0.45060544\n",
      "Iteration 65, loss = 0.45274960\n",
      "Iteration 66, loss = 0.45225604\n",
      "Iteration 67, loss = 0.45064195\n",
      "Iteration 68, loss = 0.45084498\n",
      "Iteration 69, loss = 0.45220725\n",
      "Iteration 70, loss = 0.45152874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55724392\n",
      "Iteration 2, loss = 0.49254501\n",
      "Iteration 3, loss = 0.48849003\n",
      "Iteration 4, loss = 0.48614091\n",
      "Iteration 5, loss = 0.48235596\n",
      "Iteration 6, loss = 0.47940947\n",
      "Iteration 7, loss = 0.47692075\n",
      "Iteration 8, loss = 0.47663374\n",
      "Iteration 9, loss = 0.47409963\n",
      "Iteration 10, loss = 0.47267748\n",
      "Iteration 11, loss = 0.47139377\n",
      "Iteration 12, loss = 0.46971758\n",
      "Iteration 13, loss = 0.47003481\n",
      "Iteration 14, loss = 0.46693867\n",
      "Iteration 15, loss = 0.46517656\n",
      "Iteration 16, loss = 0.46371862\n",
      "Iteration 17, loss = 0.46397479\n",
      "Iteration 18, loss = 0.46023451\n",
      "Iteration 19, loss = 0.45838891\n",
      "Iteration 20, loss = 0.45778788\n",
      "Iteration 21, loss = 0.45624553\n",
      "Iteration 22, loss = 0.45681793\n",
      "Iteration 23, loss = 0.45599679\n",
      "Iteration 24, loss = 0.45529041\n",
      "Iteration 25, loss = 0.45462302\n",
      "Iteration 26, loss = 0.45437445\n",
      "Iteration 27, loss = 0.45437678\n",
      "Iteration 28, loss = 0.45395340\n",
      "Iteration 29, loss = 0.45407823\n",
      "Iteration 30, loss = 0.45464395\n",
      "Iteration 31, loss = 0.45357376\n",
      "Iteration 32, loss = 0.45405938\n",
      "Iteration 33, loss = 0.45422396\n",
      "Iteration 34, loss = 0.45277875\n",
      "Iteration 35, loss = 0.45176522\n",
      "Iteration 36, loss = 0.45243212\n",
      "Iteration 37, loss = 0.45209169\n",
      "Iteration 38, loss = 0.45315062\n",
      "Iteration 39, loss = 0.45114337\n",
      "Iteration 40, loss = 0.45029878\n",
      "Iteration 41, loss = 0.45036973\n",
      "Iteration 42, loss = 0.44987055\n",
      "Iteration 43, loss = 0.44922054\n",
      "Iteration 44, loss = 0.44921412\n",
      "Iteration 45, loss = 0.44991778\n",
      "Iteration 46, loss = 0.44944653\n",
      "Iteration 47, loss = 0.44972111\n",
      "Iteration 48, loss = 0.44924400\n",
      "Iteration 49, loss = 0.44919133\n",
      "Iteration 50, loss = 0.44919176\n",
      "Iteration 51, loss = 0.44919578\n",
      "Iteration 52, loss = 0.44970469\n",
      "Iteration 53, loss = 0.44857666\n",
      "Iteration 54, loss = 0.44868314\n",
      "Iteration 55, loss = 0.44902466\n",
      "Iteration 56, loss = 0.44886290\n",
      "Iteration 57, loss = 0.44857198\n",
      "Iteration 58, loss = 0.44965234\n",
      "Iteration 59, loss = 0.44870153\n",
      "Iteration 60, loss = 0.45000546\n",
      "Iteration 61, loss = 0.44837984\n",
      "Iteration 62, loss = 0.44820379\n",
      "Iteration 63, loss = 0.44869644\n",
      "Iteration 64, loss = 0.44831908\n",
      "Iteration 65, loss = 0.44949141\n",
      "Iteration 66, loss = 0.44828672\n",
      "Iteration 67, loss = 0.44791466\n",
      "Iteration 68, loss = 0.44815484\n",
      "Iteration 69, loss = 0.44855452\n",
      "Iteration 70, loss = 0.44832889\n",
      "Iteration 71, loss = 0.44859672\n",
      "Iteration 72, loss = 0.44826010\n",
      "Iteration 73, loss = 0.44909806\n",
      "Iteration 74, loss = 0.44787235\n",
      "Iteration 75, loss = 0.44885647\n",
      "Iteration 76, loss = 0.44864748\n",
      "Iteration 77, loss = 0.44843558\n",
      "Iteration 78, loss = 0.44913996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55391882\n",
      "Iteration 2, loss = 0.49302069\n",
      "Iteration 3, loss = 0.48721907\n",
      "Iteration 4, loss = 0.48544233\n",
      "Iteration 5, loss = 0.48365533\n",
      "Iteration 6, loss = 0.48211608\n",
      "Iteration 7, loss = 0.48074720\n",
      "Iteration 8, loss = 0.48073487\n",
      "Iteration 9, loss = 0.47956045\n",
      "Iteration 10, loss = 0.47885624\n",
      "Iteration 11, loss = 0.47837236\n",
      "Iteration 12, loss = 0.47823689\n",
      "Iteration 13, loss = 0.47605763\n",
      "Iteration 14, loss = 0.47461177\n",
      "Iteration 15, loss = 0.47298198\n",
      "Iteration 16, loss = 0.47097704\n",
      "Iteration 17, loss = 0.46707981\n",
      "Iteration 18, loss = 0.46450313\n",
      "Iteration 19, loss = 0.46291708\n",
      "Iteration 20, loss = 0.46192451\n",
      "Iteration 21, loss = 0.46005535\n",
      "Iteration 22, loss = 0.45780896\n",
      "Iteration 23, loss = 0.45777227\n",
      "Iteration 24, loss = 0.45873231\n",
      "Iteration 25, loss = 0.45668082\n",
      "Iteration 26, loss = 0.45604653\n",
      "Iteration 27, loss = 0.45572710\n",
      "Iteration 28, loss = 0.45467497\n",
      "Iteration 29, loss = 0.45468659\n",
      "Iteration 30, loss = 0.45479222\n",
      "Iteration 31, loss = 0.45596080\n",
      "Iteration 32, loss = 0.45430215\n",
      "Iteration 33, loss = 0.45286511\n",
      "Iteration 34, loss = 0.45300453\n",
      "Iteration 35, loss = 0.45413944\n",
      "Iteration 36, loss = 0.45228957\n",
      "Iteration 37, loss = 0.45329621\n",
      "Iteration 38, loss = 0.45172803\n",
      "Iteration 39, loss = 0.45152199\n",
      "Iteration 40, loss = 0.45189723\n",
      "Iteration 41, loss = 0.45161089\n",
      "Iteration 42, loss = 0.44972448\n",
      "Iteration 43, loss = 0.45191207\n",
      "Iteration 44, loss = 0.45038444\n",
      "Iteration 45, loss = 0.45219882\n",
      "Iteration 46, loss = 0.45076772\n",
      "Iteration 47, loss = 0.45046590\n",
      "Iteration 48, loss = 0.44907223\n",
      "Iteration 49, loss = 0.45046184\n",
      "Iteration 50, loss = 0.45024748\n",
      "Iteration 51, loss = 0.44942604\n",
      "Iteration 52, loss = 0.45138741\n",
      "Iteration 53, loss = 0.44940370\n",
      "Iteration 54, loss = 0.44858485\n",
      "Iteration 55, loss = 0.44948341\n",
      "Iteration 56, loss = 0.45030664\n",
      "Iteration 57, loss = 0.45114104\n",
      "Iteration 58, loss = 0.44927251\n",
      "Iteration 59, loss = 0.44818369\n",
      "Iteration 60, loss = 0.45023076\n",
      "Iteration 61, loss = 0.45002733\n",
      "Iteration 62, loss = 0.44868707\n",
      "Iteration 63, loss = 0.45163468\n",
      "Iteration 64, loss = 0.45058490\n",
      "Iteration 65, loss = 0.44898281\n",
      "Iteration 66, loss = 0.44942156\n",
      "Iteration 67, loss = 0.44893042\n",
      "Iteration 68, loss = 0.44878171\n",
      "Iteration 69, loss = 0.44810377\n",
      "Iteration 70, loss = 0.44788809\n",
      "Iteration 71, loss = 0.44768624\n",
      "Iteration 72, loss = 0.44768332\n",
      "Iteration 73, loss = 0.44867113\n",
      "Iteration 74, loss = 0.44775833\n",
      "Iteration 75, loss = 0.44811863\n",
      "Iteration 76, loss = 0.44789017\n",
      "Iteration 77, loss = 0.44875079\n",
      "Iteration 78, loss = 0.44922011\n",
      "Iteration 79, loss = 0.44769637\n",
      "Iteration 80, loss = 0.44876951\n",
      "Iteration 81, loss = 0.44718882\n",
      "Iteration 82, loss = 0.44739997\n",
      "Iteration 83, loss = 0.44786453\n",
      "Iteration 84, loss = 0.44677471\n",
      "Iteration 85, loss = 0.44640814\n",
      "Iteration 86, loss = 0.44668773\n",
      "Iteration 87, loss = 0.44642927\n",
      "Iteration 88, loss = 0.44693962\n",
      "Iteration 89, loss = 0.44724066\n",
      "Iteration 90, loss = 0.44676632\n",
      "Iteration 91, loss = 0.44828940\n",
      "Iteration 92, loss = 0.44656683\n",
      "Iteration 93, loss = 0.44560564\n",
      "Iteration 94, loss = 0.44721717\n",
      "Iteration 95, loss = 0.44584153\n",
      "Iteration 96, loss = 0.44743248\n",
      "Iteration 97, loss = 0.44860420\n",
      "Iteration 98, loss = 0.44585358\n",
      "Iteration 99, loss = 0.44573032\n",
      "Iteration 100, loss = 0.44574446\n",
      "Iteration 101, loss = 0.44624261\n",
      "Iteration 102, loss = 0.44689682\n",
      "Iteration 103, loss = 0.44983948\n",
      "Iteration 104, loss = 0.44649327\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59959006\n",
      "Iteration 2, loss = 0.52069266\n",
      "Iteration 3, loss = 0.51455014\n",
      "Iteration 4, loss = 0.51054992\n",
      "Iteration 5, loss = 0.50740026\n",
      "Iteration 6, loss = 0.50534359\n",
      "Iteration 7, loss = 0.50048144\n",
      "Iteration 8, loss = 0.49839398\n",
      "Iteration 9, loss = 0.49309556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.48928705\n",
      "Iteration 11, loss = 0.48479487\n",
      "Iteration 12, loss = 0.48218947\n",
      "Iteration 13, loss = 0.48252391\n",
      "Iteration 14, loss = 0.48047197\n",
      "Iteration 15, loss = 0.47654878\n",
      "Iteration 16, loss = 0.47526755\n",
      "Iteration 17, loss = 0.47454651\n",
      "Iteration 18, loss = 0.47353585\n",
      "Iteration 19, loss = 0.47464638\n",
      "Iteration 20, loss = 0.47294337\n",
      "Iteration 21, loss = 0.47239268\n",
      "Iteration 22, loss = 0.47261611\n",
      "Iteration 23, loss = 0.47515615\n",
      "Iteration 24, loss = 0.47211816\n",
      "Iteration 25, loss = 0.47358204\n",
      "Iteration 26, loss = 0.47358987\n",
      "Iteration 27, loss = 0.47049035\n",
      "Iteration 28, loss = 0.47101299\n",
      "Iteration 29, loss = 0.47018825\n",
      "Iteration 30, loss = 0.46984287\n",
      "Iteration 31, loss = 0.47049681\n",
      "Iteration 32, loss = 0.47023441\n",
      "Iteration 33, loss = 0.47018207\n",
      "Iteration 34, loss = 0.46957409\n",
      "Iteration 35, loss = 0.46914897\n",
      "Iteration 36, loss = 0.46906010\n",
      "Iteration 37, loss = 0.47059676\n",
      "Iteration 38, loss = 0.47176152\n",
      "Iteration 39, loss = 0.46981804\n",
      "Iteration 40, loss = 0.46914051\n",
      "Iteration 41, loss = 0.46912112\n",
      "Iteration 42, loss = 0.46835959\n",
      "Iteration 43, loss = 0.46861054\n",
      "Iteration 44, loss = 0.47023548\n",
      "Iteration 45, loss = 0.46834419\n",
      "Iteration 46, loss = 0.46862823\n",
      "Iteration 47, loss = 0.46962801\n",
      "Iteration 48, loss = 0.47008173\n",
      "Iteration 49, loss = 0.47204202\n",
      "Iteration 50, loss = 0.47145492\n",
      "Iteration 51, loss = 0.46776913\n",
      "Iteration 52, loss = 0.46827347\n",
      "Iteration 53, loss = 0.46798085\n",
      "Iteration 54, loss = 0.46804077\n",
      "Iteration 55, loss = 0.46847797\n",
      "Iteration 56, loss = 0.46869325\n",
      "Iteration 57, loss = 0.46988970\n",
      "Iteration 58, loss = 0.46722560\n",
      "Iteration 59, loss = 0.46749714\n",
      "Iteration 60, loss = 0.46845146\n",
      "Iteration 61, loss = 0.46769485\n",
      "Iteration 62, loss = 0.46705588\n",
      "Iteration 63, loss = 0.46727276\n",
      "Iteration 64, loss = 0.46691919\n",
      "Iteration 65, loss = 0.47012695\n",
      "Iteration 66, loss = 0.46761160\n",
      "Iteration 67, loss = 0.46555563\n",
      "Iteration 68, loss = 0.46686684\n",
      "Iteration 69, loss = 0.46946096\n",
      "Iteration 70, loss = 0.46626355\n",
      "Iteration 71, loss = 0.46541710\n",
      "Iteration 72, loss = 0.46686579\n",
      "Iteration 73, loss = 0.46809892\n",
      "Iteration 74, loss = 0.46818198\n",
      "Iteration 75, loss = 0.46663799\n",
      "Iteration 76, loss = 0.46666697\n",
      "Iteration 77, loss = 0.46558925\n",
      "Iteration 78, loss = 0.46576138\n",
      "Iteration 79, loss = 0.46563052\n",
      "Iteration 80, loss = 0.46567042\n",
      "Iteration 81, loss = 0.46565296\n",
      "Iteration 82, loss = 0.46582858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59841474\n",
      "Iteration 2, loss = 0.51796606\n",
      "Iteration 3, loss = 0.51042033\n",
      "Iteration 4, loss = 0.50557166\n",
      "Iteration 5, loss = 0.50190682\n",
      "Iteration 6, loss = 0.49682060\n",
      "Iteration 7, loss = 0.49326171\n",
      "Iteration 8, loss = 0.49166436\n",
      "Iteration 9, loss = 0.48865254\n",
      "Iteration 10, loss = 0.48546867\n",
      "Iteration 11, loss = 0.48388798\n",
      "Iteration 12, loss = 0.47934039\n",
      "Iteration 13, loss = 0.47759731\n",
      "Iteration 14, loss = 0.47218502\n",
      "Iteration 15, loss = 0.47010903\n",
      "Iteration 16, loss = 0.46814595\n",
      "Iteration 17, loss = 0.46825498\n",
      "Iteration 18, loss = 0.46640045\n",
      "Iteration 19, loss = 0.46518306\n",
      "Iteration 20, loss = 0.46548581\n",
      "Iteration 21, loss = 0.46294470\n",
      "Iteration 22, loss = 0.46195624\n",
      "Iteration 23, loss = 0.46121188\n",
      "Iteration 24, loss = 0.45717061\n",
      "Iteration 25, loss = 0.45343943\n",
      "Iteration 26, loss = 0.45632041\n",
      "Iteration 27, loss = 0.45172762\n",
      "Iteration 28, loss = 0.44962091\n",
      "Iteration 29, loss = 0.45427333\n",
      "Iteration 30, loss = 0.45014295\n",
      "Iteration 31, loss = 0.44891581\n",
      "Iteration 32, loss = 0.44989351\n",
      "Iteration 33, loss = 0.44730810\n",
      "Iteration 34, loss = 0.44525216\n",
      "Iteration 35, loss = 0.44216617\n",
      "Iteration 36, loss = 0.44061650\n",
      "Iteration 37, loss = 0.43957002\n",
      "Iteration 38, loss = 0.44100458\n",
      "Iteration 39, loss = 0.44079123\n",
      "Iteration 40, loss = 0.43904801\n",
      "Iteration 41, loss = 0.43847717\n",
      "Iteration 42, loss = 0.43876227\n",
      "Iteration 43, loss = 0.43537146\n",
      "Iteration 44, loss = 0.43626134\n",
      "Iteration 45, loss = 0.43377378\n",
      "Iteration 46, loss = 0.43746599\n",
      "Iteration 47, loss = 0.43316062\n",
      "Iteration 48, loss = 0.43416020\n",
      "Iteration 49, loss = 0.43442607\n",
      "Iteration 50, loss = 0.43328562\n",
      "Iteration 51, loss = 0.43353892\n",
      "Iteration 52, loss = 0.43348599\n",
      "Iteration 53, loss = 0.43406816\n",
      "Iteration 54, loss = 0.43220896\n",
      "Iteration 55, loss = 0.43211346\n",
      "Iteration 56, loss = 0.43129034\n",
      "Iteration 57, loss = 0.43134952\n",
      "Iteration 58, loss = 0.43172717\n",
      "Iteration 59, loss = 0.43146656\n",
      "Iteration 60, loss = 0.43037958\n",
      "Iteration 61, loss = 0.43133030\n",
      "Iteration 62, loss = 0.43105954\n",
      "Iteration 63, loss = 0.42997735\n",
      "Iteration 64, loss = 0.42996533\n",
      "Iteration 65, loss = 0.43425830\n",
      "Iteration 66, loss = 0.43180353\n",
      "Iteration 67, loss = 0.42986679\n",
      "Iteration 68, loss = 0.42985129\n",
      "Iteration 69, loss = 0.42938854\n",
      "Iteration 70, loss = 0.42905871\n",
      "Iteration 71, loss = 0.42863620\n",
      "Iteration 72, loss = 0.42758491\n",
      "Iteration 73, loss = 0.43057092\n",
      "Iteration 74, loss = 0.42991536\n",
      "Iteration 75, loss = 0.42798588\n",
      "Iteration 76, loss = 0.42891330\n",
      "Iteration 77, loss = 0.42946762\n",
      "Iteration 78, loss = 0.42836192\n",
      "Iteration 79, loss = 0.42931211\n",
      "Iteration 80, loss = 0.42661474\n",
      "Iteration 81, loss = 0.42681713\n",
      "Iteration 82, loss = 0.42784341\n",
      "Iteration 83, loss = 0.42612668\n",
      "Iteration 84, loss = 0.42649243\n",
      "Iteration 85, loss = 0.42719368\n",
      "Iteration 86, loss = 0.42647737\n",
      "Iteration 87, loss = 0.42996879\n",
      "Iteration 88, loss = 0.42769117\n",
      "Iteration 89, loss = 0.42805447\n",
      "Iteration 90, loss = 0.42719964\n",
      "Iteration 91, loss = 0.42528874\n",
      "Iteration 92, loss = 0.42874519\n",
      "Iteration 93, loss = 0.42755520\n",
      "Iteration 94, loss = 0.42585897\n",
      "Iteration 95, loss = 0.42628815\n",
      "Iteration 96, loss = 0.42541835\n",
      "Iteration 97, loss = 0.42622480\n",
      "Iteration 98, loss = 0.42700964\n",
      "Iteration 99, loss = 0.42491782\n",
      "Iteration 100, loss = 0.42638045\n",
      "Iteration 101, loss = 0.42765781\n",
      "Iteration 102, loss = 0.42448152\n",
      "Iteration 103, loss = 0.42516624\n",
      "Iteration 104, loss = 0.42576449\n",
      "Iteration 105, loss = 0.42669971\n",
      "Iteration 106, loss = 0.42597858\n",
      "Iteration 107, loss = 0.42694721\n",
      "Iteration 108, loss = 0.42498502\n",
      "Iteration 109, loss = 0.42569302\n",
      "Iteration 110, loss = 0.42547483\n",
      "Iteration 111, loss = 0.43127757\n",
      "Iteration 112, loss = 0.42908700\n",
      "Iteration 113, loss = 0.42423835\n",
      "Iteration 114, loss = 0.42438692\n",
      "Iteration 115, loss = 0.42546906\n",
      "Iteration 116, loss = 0.42635711\n",
      "Iteration 117, loss = 0.42384095\n",
      "Iteration 118, loss = 0.42511147\n",
      "Iteration 119, loss = 0.42385510\n",
      "Iteration 120, loss = 0.42503673\n",
      "Iteration 121, loss = 0.42731138\n",
      "Iteration 122, loss = 0.42475582\n",
      "Iteration 123, loss = 0.42590364\n",
      "Iteration 124, loss = 0.42414005\n",
      "Iteration 125, loss = 0.42345794\n",
      "Iteration 126, loss = 0.42458266\n",
      "Iteration 127, loss = 0.42654017\n",
      "Iteration 128, loss = 0.42399053\n",
      "Iteration 129, loss = 0.42350606\n",
      "Iteration 130, loss = 0.42362961\n",
      "Iteration 131, loss = 0.42425096\n",
      "Iteration 132, loss = 0.42315698\n",
      "Iteration 133, loss = 0.42474372\n",
      "Iteration 134, loss = 0.42366675\n",
      "Iteration 135, loss = 0.42363676\n",
      "Iteration 136, loss = 0.42365454\n",
      "Iteration 137, loss = 0.42570154\n",
      "Iteration 138, loss = 0.42545405\n",
      "Iteration 139, loss = 0.42472875\n",
      "Iteration 140, loss = 0.42577560\n",
      "Iteration 141, loss = 0.42331194\n",
      "Iteration 142, loss = 0.42416901\n",
      "Iteration 143, loss = 0.42535966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60162387\n",
      "Iteration 2, loss = 0.52035850\n",
      "Iteration 3, loss = 0.51097903\n",
      "Iteration 4, loss = 0.50575604\n",
      "Iteration 5, loss = 0.50220786\n",
      "Iteration 6, loss = 0.49753748\n",
      "Iteration 7, loss = 0.49456121\n",
      "Iteration 8, loss = 0.49371532\n",
      "Iteration 9, loss = 0.48983570\n",
      "Iteration 10, loss = 0.48664571\n",
      "Iteration 11, loss = 0.48426026\n",
      "Iteration 12, loss = 0.48286580\n",
      "Iteration 13, loss = 0.48038278\n",
      "Iteration 14, loss = 0.47662668\n",
      "Iteration 15, loss = 0.47407245\n",
      "Iteration 16, loss = 0.47284308\n",
      "Iteration 17, loss = 0.46882408\n",
      "Iteration 18, loss = 0.46550021\n",
      "Iteration 19, loss = 0.46322337\n",
      "Iteration 20, loss = 0.46376165\n",
      "Iteration 21, loss = 0.45930414\n",
      "Iteration 22, loss = 0.45797366\n",
      "Iteration 23, loss = 0.45660182\n",
      "Iteration 24, loss = 0.45414324\n",
      "Iteration 25, loss = 0.45129015\n",
      "Iteration 26, loss = 0.44998742\n",
      "Iteration 27, loss = 0.44935885\n",
      "Iteration 28, loss = 0.44760936\n",
      "Iteration 29, loss = 0.44808629\n",
      "Iteration 30, loss = 0.44560041\n",
      "Iteration 31, loss = 0.44620343\n",
      "Iteration 32, loss = 0.44336201\n",
      "Iteration 33, loss = 0.44403522\n",
      "Iteration 34, loss = 0.44421320\n",
      "Iteration 35, loss = 0.44238701\n",
      "Iteration 36, loss = 0.44127681\n",
      "Iteration 37, loss = 0.44046546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 38, loss = 0.44177511\n",
      "Iteration 39, loss = 0.43961306\n",
      "Iteration 40, loss = 0.44132901\n",
      "Iteration 41, loss = 0.43943587\n",
      "Iteration 42, loss = 0.43913234\n",
      "Iteration 43, loss = 0.43765958\n",
      "Iteration 44, loss = 0.43880963\n",
      "Iteration 45, loss = 0.43887947\n",
      "Iteration 46, loss = 0.43805589\n",
      "Iteration 47, loss = 0.43643795\n",
      "Iteration 48, loss = 0.43800557\n",
      "Iteration 49, loss = 0.43752121\n",
      "Iteration 50, loss = 0.43663882\n",
      "Iteration 51, loss = 0.43625658\n",
      "Iteration 52, loss = 0.43565144\n",
      "Iteration 53, loss = 0.43801594\n",
      "Iteration 54, loss = 0.43601767\n",
      "Iteration 55, loss = 0.43547868\n",
      "Iteration 56, loss = 0.43479289\n",
      "Iteration 57, loss = 0.43742246\n",
      "Iteration 58, loss = 0.43474004\n",
      "Iteration 59, loss = 0.43695171\n",
      "Iteration 60, loss = 0.43428613\n",
      "Iteration 61, loss = 0.43584358\n",
      "Iteration 62, loss = 0.43709142\n",
      "Iteration 63, loss = 0.43651569\n",
      "Iteration 64, loss = 0.43428614\n",
      "Iteration 65, loss = 0.43512744\n",
      "Iteration 66, loss = 0.43808618\n",
      "Iteration 67, loss = 0.43514191\n",
      "Iteration 68, loss = 0.43424195\n",
      "Iteration 69, loss = 0.43350006\n",
      "Iteration 70, loss = 0.43406188\n",
      "Iteration 71, loss = 0.43315576\n",
      "Iteration 72, loss = 0.43229949\n",
      "Iteration 73, loss = 0.43339022\n",
      "Iteration 74, loss = 0.43240288\n",
      "Iteration 75, loss = 0.43531931\n",
      "Iteration 76, loss = 0.43282335\n",
      "Iteration 77, loss = 0.43222941\n",
      "Iteration 78, loss = 0.43264907\n",
      "Iteration 79, loss = 0.43269873\n",
      "Iteration 80, loss = 0.43052049\n",
      "Iteration 81, loss = 0.43379203\n",
      "Iteration 82, loss = 0.43088428\n",
      "Iteration 83, loss = 0.43132005\n",
      "Iteration 84, loss = 0.43081061\n",
      "Iteration 85, loss = 0.43180458\n",
      "Iteration 86, loss = 0.43205992\n",
      "Iteration 87, loss = 0.43217431\n",
      "Iteration 88, loss = 0.43290115\n",
      "Iteration 89, loss = 0.43278842\n",
      "Iteration 90, loss = 0.43117252\n",
      "Iteration 91, loss = 0.43150221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58715183\n",
      "Iteration 2, loss = 0.48875257\n",
      "Iteration 3, loss = 0.47993667\n",
      "Iteration 4, loss = 0.47545547\n",
      "Iteration 5, loss = 0.47297138\n",
      "Iteration 6, loss = 0.47189098\n",
      "Iteration 7, loss = 0.47078355\n",
      "Iteration 8, loss = 0.47057433\n",
      "Iteration 9, loss = 0.46927720\n",
      "Iteration 10, loss = 0.46816029\n",
      "Iteration 11, loss = 0.46832681\n",
      "Iteration 12, loss = 0.46645283\n",
      "Iteration 13, loss = 0.46636661\n",
      "Iteration 14, loss = 0.46775284\n",
      "Iteration 15, loss = 0.46503645\n",
      "Iteration 16, loss = 0.46458214\n",
      "Iteration 17, loss = 0.46458336\n",
      "Iteration 18, loss = 0.46232876\n",
      "Iteration 19, loss = 0.46509774\n",
      "Iteration 20, loss = 0.46297662\n",
      "Iteration 21, loss = 0.46132549\n",
      "Iteration 22, loss = 0.46031724\n",
      "Iteration 23, loss = 0.46171379\n",
      "Iteration 24, loss = 0.45993346\n",
      "Iteration 25, loss = 0.45916087\n",
      "Iteration 26, loss = 0.45866629\n",
      "Iteration 27, loss = 0.45932378\n",
      "Iteration 28, loss = 0.45801570\n",
      "Iteration 29, loss = 0.45758820\n",
      "Iteration 30, loss = 0.45634550\n",
      "Iteration 31, loss = 0.45762335\n",
      "Iteration 32, loss = 0.45839731\n",
      "Iteration 33, loss = 0.45687634\n",
      "Iteration 34, loss = 0.45667482\n",
      "Iteration 35, loss = 0.45430098\n",
      "Iteration 36, loss = 0.45349862\n",
      "Iteration 37, loss = 0.45468457\n",
      "Iteration 38, loss = 0.45669876\n",
      "Iteration 39, loss = 0.45234793\n",
      "Iteration 40, loss = 0.45300524\n",
      "Iteration 41, loss = 0.45204830\n",
      "Iteration 42, loss = 0.45262786\n",
      "Iteration 43, loss = 0.45041282\n",
      "Iteration 44, loss = 0.44934512\n",
      "Iteration 45, loss = 0.45011343\n",
      "Iteration 46, loss = 0.44978947\n",
      "Iteration 47, loss = 0.44882600\n",
      "Iteration 48, loss = 0.44842080\n",
      "Iteration 49, loss = 0.44962266\n",
      "Iteration 50, loss = 0.45057247\n",
      "Iteration 51, loss = 0.44714922\n",
      "Iteration 52, loss = 0.44699403\n",
      "Iteration 53, loss = 0.44779115\n",
      "Iteration 54, loss = 0.44555693\n",
      "Iteration 55, loss = 0.44643597\n",
      "Iteration 56, loss = 0.44544256\n",
      "Iteration 57, loss = 0.44479501\n",
      "Iteration 58, loss = 0.44330742\n",
      "Iteration 59, loss = 0.44482059\n",
      "Iteration 60, loss = 0.44391127\n",
      "Iteration 61, loss = 0.44361318\n",
      "Iteration 62, loss = 0.44167852\n",
      "Iteration 63, loss = 0.44099154\n",
      "Iteration 64, loss = 0.44193991\n",
      "Iteration 65, loss = 0.44186712\n",
      "Iteration 66, loss = 0.44133559\n",
      "Iteration 67, loss = 0.43794191\n",
      "Iteration 68, loss = 0.43905586\n",
      "Iteration 69, loss = 0.44041362\n",
      "Iteration 70, loss = 0.43697787\n",
      "Iteration 71, loss = 0.43846511\n",
      "Iteration 72, loss = 0.43806549\n",
      "Iteration 73, loss = 0.43672485\n",
      "Iteration 74, loss = 0.43648025\n",
      "Iteration 75, loss = 0.43734162\n",
      "Iteration 76, loss = 0.43731522\n",
      "Iteration 77, loss = 0.43840644\n",
      "Iteration 78, loss = 0.43756960\n",
      "Iteration 79, loss = 0.43621872\n",
      "Iteration 80, loss = 0.43331044\n",
      "Iteration 81, loss = 0.43458164\n",
      "Iteration 82, loss = 0.43449666\n",
      "Iteration 83, loss = 0.43484282\n",
      "Iteration 84, loss = 0.43811302\n",
      "Iteration 85, loss = 0.43151081\n",
      "Iteration 86, loss = 0.43225954\n",
      "Iteration 87, loss = 0.43458669\n",
      "Iteration 88, loss = 0.43396433\n",
      "Iteration 89, loss = 0.43378255\n",
      "Iteration 90, loss = 0.43356380\n",
      "Iteration 91, loss = 0.43305072\n",
      "Iteration 92, loss = 0.43315811\n",
      "Iteration 93, loss = 0.43359669\n",
      "Iteration 94, loss = 0.43162606\n",
      "Iteration 95, loss = 0.43144666\n",
      "Iteration 96, loss = 0.43438533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58525528\n",
      "Iteration 2, loss = 0.48307064\n",
      "Iteration 3, loss = 0.47587504\n",
      "Iteration 4, loss = 0.47032503\n",
      "Iteration 5, loss = 0.46791131\n",
      "Iteration 6, loss = 0.46662643\n",
      "Iteration 7, loss = 0.46584946\n",
      "Iteration 8, loss = 0.46429937\n",
      "Iteration 9, loss = 0.46426493\n",
      "Iteration 10, loss = 0.46424982\n",
      "Iteration 11, loss = 0.46446838\n",
      "Iteration 12, loss = 0.46592674\n",
      "Iteration 13, loss = 0.46673232\n",
      "Iteration 14, loss = 0.46266885\n",
      "Iteration 15, loss = 0.46115536\n",
      "Iteration 16, loss = 0.46066911\n",
      "Iteration 17, loss = 0.46235298\n",
      "Iteration 18, loss = 0.46048862\n",
      "Iteration 19, loss = 0.46105602\n",
      "Iteration 20, loss = 0.45977996\n",
      "Iteration 21, loss = 0.45917744\n",
      "Iteration 22, loss = 0.46018836\n",
      "Iteration 23, loss = 0.46083987\n",
      "Iteration 24, loss = 0.45861746\n",
      "Iteration 25, loss = 0.45857081\n",
      "Iteration 26, loss = 0.45966061\n",
      "Iteration 27, loss = 0.45960727\n",
      "Iteration 28, loss = 0.45730557\n",
      "Iteration 29, loss = 0.45788999\n",
      "Iteration 30, loss = 0.45763067\n",
      "Iteration 31, loss = 0.45745743\n",
      "Iteration 32, loss = 0.45784441\n",
      "Iteration 33, loss = 0.45670554\n",
      "Iteration 34, loss = 0.45426526\n",
      "Iteration 35, loss = 0.45464344\n",
      "Iteration 36, loss = 0.45416519\n",
      "Iteration 37, loss = 0.45293019\n",
      "Iteration 38, loss = 0.45453509\n",
      "Iteration 39, loss = 0.45311749\n",
      "Iteration 40, loss = 0.45417175\n",
      "Iteration 41, loss = 0.45241976\n",
      "Iteration 42, loss = 0.45507367\n",
      "Iteration 43, loss = 0.45321845\n",
      "Iteration 44, loss = 0.45422951\n",
      "Iteration 45, loss = 0.45459233\n",
      "Iteration 46, loss = 0.45213953\n",
      "Iteration 47, loss = 0.45180942\n",
      "Iteration 48, loss = 0.45256020\n",
      "Iteration 49, loss = 0.45294476\n",
      "Iteration 50, loss = 0.45106506\n",
      "Iteration 51, loss = 0.45270755\n",
      "Iteration 52, loss = 0.45258141\n",
      "Iteration 53, loss = 0.45156751\n",
      "Iteration 54, loss = 0.45114266\n",
      "Iteration 55, loss = 0.45025579\n",
      "Iteration 56, loss = 0.45112910\n",
      "Iteration 57, loss = 0.45071101\n",
      "Iteration 58, loss = 0.45045875\n",
      "Iteration 59, loss = 0.45089238\n",
      "Iteration 60, loss = 0.45063534\n",
      "Iteration 61, loss = 0.45154467\n",
      "Iteration 62, loss = 0.44973355\n",
      "Iteration 63, loss = 0.45205592\n",
      "Iteration 64, loss = 0.45073859\n",
      "Iteration 65, loss = 0.45138663\n",
      "Iteration 66, loss = 0.45145118\n",
      "Iteration 67, loss = 0.45133134\n",
      "Iteration 68, loss = 0.45155418\n",
      "Iteration 69, loss = 0.45274985\n",
      "Iteration 70, loss = 0.45135813\n",
      "Iteration 71, loss = 0.45061231\n",
      "Iteration 72, loss = 0.44850405\n",
      "Iteration 73, loss = 0.44915942\n",
      "Iteration 74, loss = 0.44857569\n",
      "Iteration 75, loss = 0.44892904\n",
      "Iteration 76, loss = 0.45000740\n",
      "Iteration 77, loss = 0.44953898\n",
      "Iteration 78, loss = 0.45239531\n",
      "Iteration 79, loss = 0.45420782\n",
      "Iteration 80, loss = 0.45031020\n",
      "Iteration 81, loss = 0.45147170\n",
      "Iteration 82, loss = 0.45253468\n",
      "Iteration 83, loss = 0.45055129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58205290\n",
      "Iteration 2, loss = 0.48815346\n",
      "Iteration 3, loss = 0.47904093\n",
      "Iteration 4, loss = 0.47416188\n",
      "Iteration 5, loss = 0.47197064\n",
      "Iteration 6, loss = 0.47079993\n",
      "Iteration 7, loss = 0.46948159\n",
      "Iteration 8, loss = 0.46916431\n",
      "Iteration 9, loss = 0.46829550\n",
      "Iteration 10, loss = 0.46610455\n",
      "Iteration 11, loss = 0.46681235\n",
      "Iteration 12, loss = 0.46719912\n",
      "Iteration 13, loss = 0.46525698\n",
      "Iteration 14, loss = 0.46460156\n",
      "Iteration 15, loss = 0.46339600\n",
      "Iteration 16, loss = 0.46391098\n",
      "Iteration 17, loss = 0.46333856\n",
      "Iteration 18, loss = 0.46280861\n",
      "Iteration 19, loss = 0.46098567\n",
      "Iteration 20, loss = 0.46124185\n",
      "Iteration 21, loss = 0.45990114\n",
      "Iteration 22, loss = 0.45926631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 23, loss = 0.46086322\n",
      "Iteration 24, loss = 0.45852621\n",
      "Iteration 25, loss = 0.45863685\n",
      "Iteration 26, loss = 0.45789335\n",
      "Iteration 27, loss = 0.45751082\n",
      "Iteration 28, loss = 0.45676319\n",
      "Iteration 29, loss = 0.45671561\n",
      "Iteration 30, loss = 0.45673929\n",
      "Iteration 31, loss = 0.45563808\n",
      "Iteration 32, loss = 0.45461106\n",
      "Iteration 33, loss = 0.45284873\n",
      "Iteration 34, loss = 0.45308068\n",
      "Iteration 35, loss = 0.45217320\n",
      "Iteration 36, loss = 0.45154944\n",
      "Iteration 37, loss = 0.45153098\n",
      "Iteration 38, loss = 0.45006047\n",
      "Iteration 39, loss = 0.45084201\n",
      "Iteration 40, loss = 0.45348688\n",
      "Iteration 41, loss = 0.45125917\n",
      "Iteration 42, loss = 0.45133267\n",
      "Iteration 43, loss = 0.44940576\n",
      "Iteration 44, loss = 0.44943599\n",
      "Iteration 45, loss = 0.44879615\n",
      "Iteration 46, loss = 0.44951932\n",
      "Iteration 47, loss = 0.45328852\n",
      "Iteration 48, loss = 0.45092818\n",
      "Iteration 49, loss = 0.44879840\n",
      "Iteration 50, loss = 0.44856925\n",
      "Iteration 51, loss = 0.44701666\n",
      "Iteration 52, loss = 0.44886429\n",
      "Iteration 53, loss = 0.44681929\n",
      "Iteration 54, loss = 0.44724445\n",
      "Iteration 55, loss = 0.44703158\n",
      "Iteration 56, loss = 0.44733186\n",
      "Iteration 57, loss = 0.44838034\n",
      "Iteration 58, loss = 0.44615500\n",
      "Iteration 59, loss = 0.44459827\n",
      "Iteration 60, loss = 0.44757589\n",
      "Iteration 61, loss = 0.44595650\n",
      "Iteration 62, loss = 0.44579097\n",
      "Iteration 63, loss = 0.44890376\n",
      "Iteration 64, loss = 0.44913468\n",
      "Iteration 65, loss = 0.44501087\n",
      "Iteration 66, loss = 0.44446588\n",
      "Iteration 67, loss = 0.44217041\n",
      "Iteration 68, loss = 0.44289664\n",
      "Iteration 69, loss = 0.44135435\n",
      "Iteration 70, loss = 0.44394396\n",
      "Iteration 71, loss = 0.44213497\n",
      "Iteration 72, loss = 0.43899766\n",
      "Iteration 73, loss = 0.43767511\n",
      "Iteration 74, loss = 0.43785776\n",
      "Iteration 75, loss = 0.43894277\n",
      "Iteration 76, loss = 0.43730341\n",
      "Iteration 77, loss = 0.43492576\n",
      "Iteration 78, loss = 0.43751374\n",
      "Iteration 79, loss = 0.43622498\n",
      "Iteration 80, loss = 0.43488667\n",
      "Iteration 81, loss = 0.43876998\n",
      "Iteration 82, loss = 0.43577909\n",
      "Iteration 83, loss = 0.43208573\n",
      "Iteration 84, loss = 0.43304098\n",
      "Iteration 85, loss = 0.43346712\n",
      "Iteration 86, loss = 0.43383314\n",
      "Iteration 87, loss = 0.43404108\n",
      "Iteration 88, loss = 0.43362207\n",
      "Iteration 89, loss = 0.43440604\n",
      "Iteration 90, loss = 0.43518828\n",
      "Iteration 91, loss = 0.43386258\n",
      "Iteration 92, loss = 0.43300254\n",
      "Iteration 93, loss = 0.43551508\n",
      "Iteration 94, loss = 0.43353593\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57193954\n",
      "Iteration 2, loss = 0.48626458\n",
      "Iteration 3, loss = 0.47984200\n",
      "Iteration 4, loss = 0.47535366\n",
      "Iteration 5, loss = 0.47371518\n",
      "Iteration 6, loss = 0.47355849\n",
      "Iteration 7, loss = 0.47239508\n",
      "Iteration 8, loss = 0.47160509\n",
      "Iteration 9, loss = 0.47061930\n",
      "Iteration 10, loss = 0.47015530\n",
      "Iteration 11, loss = 0.46996360\n",
      "Iteration 12, loss = 0.46927106\n",
      "Iteration 13, loss = 0.46922146\n",
      "Iteration 14, loss = 0.46896122\n",
      "Iteration 15, loss = 0.46779881\n",
      "Iteration 16, loss = 0.46695430\n",
      "Iteration 17, loss = 0.46758907\n",
      "Iteration 18, loss = 0.46655602\n",
      "Iteration 19, loss = 0.46825812\n",
      "Iteration 20, loss = 0.46743873\n",
      "Iteration 21, loss = 0.46429370\n",
      "Iteration 22, loss = 0.46543697\n",
      "Iteration 23, loss = 0.46767596\n",
      "Iteration 24, loss = 0.46537843\n",
      "Iteration 25, loss = 0.46449671\n",
      "Iteration 26, loss = 0.46327467\n",
      "Iteration 27, loss = 0.46362661\n",
      "Iteration 28, loss = 0.46449445\n",
      "Iteration 29, loss = 0.46310505\n",
      "Iteration 30, loss = 0.46259688\n",
      "Iteration 31, loss = 0.46296451\n",
      "Iteration 32, loss = 0.46243488\n",
      "Iteration 33, loss = 0.46467940\n",
      "Iteration 34, loss = 0.46279366\n",
      "Iteration 35, loss = 0.46131214\n",
      "Iteration 36, loss = 0.46079664\n",
      "Iteration 37, loss = 0.46345911\n",
      "Iteration 38, loss = 0.46359408\n",
      "Iteration 39, loss = 0.46279054\n",
      "Iteration 40, loss = 0.46211742\n",
      "Iteration 41, loss = 0.46170696\n",
      "Iteration 42, loss = 0.46183019\n",
      "Iteration 43, loss = 0.45999131\n",
      "Iteration 44, loss = 0.46003938\n",
      "Iteration 45, loss = 0.45904164\n",
      "Iteration 46, loss = 0.45878292\n",
      "Iteration 47, loss = 0.45880893\n",
      "Iteration 48, loss = 0.45905750\n",
      "Iteration 49, loss = 0.45956937\n",
      "Iteration 50, loss = 0.46214849\n",
      "Iteration 51, loss = 0.45889139\n",
      "Iteration 52, loss = 0.45770255\n",
      "Iteration 53, loss = 0.45711610\n",
      "Iteration 54, loss = 0.45876078\n",
      "Iteration 55, loss = 0.45655333\n",
      "Iteration 56, loss = 0.45795643\n",
      "Iteration 57, loss = 0.45926060\n",
      "Iteration 58, loss = 0.45657840\n",
      "Iteration 59, loss = 0.45541783\n",
      "Iteration 60, loss = 0.45635602\n",
      "Iteration 61, loss = 0.45633890\n",
      "Iteration 62, loss = 0.45648058\n",
      "Iteration 63, loss = 0.45485389\n",
      "Iteration 64, loss = 0.45442870\n",
      "Iteration 65, loss = 0.45500846\n",
      "Iteration 66, loss = 0.45627062\n",
      "Iteration 67, loss = 0.45246388\n",
      "Iteration 68, loss = 0.45441017\n",
      "Iteration 69, loss = 0.45503018\n",
      "Iteration 70, loss = 0.45430536\n",
      "Iteration 71, loss = 0.45658940\n",
      "Iteration 72, loss = 0.45289611\n",
      "Iteration 73, loss = 0.45578409\n",
      "Iteration 74, loss = 0.45379666\n",
      "Iteration 75, loss = 0.45141160\n",
      "Iteration 76, loss = 0.45282202\n",
      "Iteration 77, loss = 0.45258323\n",
      "Iteration 78, loss = 0.45152967\n",
      "Iteration 79, loss = 0.45287743\n",
      "Iteration 80, loss = 0.45294937\n",
      "Iteration 81, loss = 0.45290958\n",
      "Iteration 82, loss = 0.45353124\n",
      "Iteration 83, loss = 0.45201277\n",
      "Iteration 84, loss = 0.45039498\n",
      "Iteration 85, loss = 0.45099499\n",
      "Iteration 86, loss = 0.45288754\n",
      "Iteration 87, loss = 0.45075695\n",
      "Iteration 88, loss = 0.45095772\n",
      "Iteration 89, loss = 0.45250517\n",
      "Iteration 90, loss = 0.45099468\n",
      "Iteration 91, loss = 0.45062723\n",
      "Iteration 92, loss = 0.44953093\n",
      "Iteration 93, loss = 0.45003921\n",
      "Iteration 94, loss = 0.45126497\n",
      "Iteration 95, loss = 0.45036990\n",
      "Iteration 96, loss = 0.45053226\n",
      "Iteration 97, loss = 0.45232319\n",
      "Iteration 98, loss = 0.44957351\n",
      "Iteration 99, loss = 0.45009485\n",
      "Iteration 100, loss = 0.45053534\n",
      "Iteration 101, loss = 0.44856221\n",
      "Iteration 102, loss = 0.44955319\n",
      "Iteration 103, loss = 0.45019134\n",
      "Iteration 104, loss = 0.44842748\n",
      "Iteration 105, loss = 0.44959748\n",
      "Iteration 106, loss = 0.44961149\n",
      "Iteration 107, loss = 0.44883555\n",
      "Iteration 108, loss = 0.44900418\n",
      "Iteration 109, loss = 0.44931756\n",
      "Iteration 110, loss = 0.45193447\n",
      "Iteration 111, loss = 0.45142370\n",
      "Iteration 112, loss = 0.44894787\n",
      "Iteration 113, loss = 0.44706070\n",
      "Iteration 114, loss = 0.44902512\n",
      "Iteration 115, loss = 0.44779691\n",
      "Iteration 116, loss = 0.44856764\n",
      "Iteration 117, loss = 0.44730745\n",
      "Iteration 118, loss = 0.44930009\n",
      "Iteration 119, loss = 0.44852446\n",
      "Iteration 120, loss = 0.44799819\n",
      "Iteration 121, loss = 0.44894791\n",
      "Iteration 122, loss = 0.44679891\n",
      "Iteration 123, loss = 0.44728828\n",
      "Iteration 124, loss = 0.44817937\n",
      "Iteration 125, loss = 0.44796868\n",
      "Iteration 126, loss = 0.44876914\n",
      "Iteration 127, loss = 0.44885231\n",
      "Iteration 128, loss = 0.45113654\n",
      "Iteration 129, loss = 0.44927335\n",
      "Iteration 130, loss = 0.44792289\n",
      "Iteration 131, loss = 0.44749866\n",
      "Iteration 132, loss = 0.44721285\n",
      "Iteration 133, loss = 0.44595775\n",
      "Iteration 134, loss = 0.44807833\n",
      "Iteration 135, loss = 0.44695056\n",
      "Iteration 136, loss = 0.44606065\n",
      "Iteration 137, loss = 0.44685535\n",
      "Iteration 138, loss = 0.44897542\n",
      "Iteration 139, loss = 0.44668726\n",
      "Iteration 140, loss = 0.44623752\n",
      "Iteration 141, loss = 0.44608083\n",
      "Iteration 142, loss = 0.44624611\n",
      "Iteration 143, loss = 0.44749760\n",
      "Iteration 144, loss = 0.44671455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57058391\n",
      "Iteration 2, loss = 0.48069248\n",
      "Iteration 3, loss = 0.47334493\n",
      "Iteration 4, loss = 0.46942353\n",
      "Iteration 5, loss = 0.46767553\n",
      "Iteration 6, loss = 0.46629459\n",
      "Iteration 7, loss = 0.46569033\n",
      "Iteration 8, loss = 0.46573266\n",
      "Iteration 9, loss = 0.46492794\n",
      "Iteration 10, loss = 0.46472106\n",
      "Iteration 11, loss = 0.46436942\n",
      "Iteration 12, loss = 0.46534105\n",
      "Iteration 13, loss = 0.46508173\n",
      "Iteration 14, loss = 0.46177272\n",
      "Iteration 15, loss = 0.46038026\n",
      "Iteration 16, loss = 0.45947704\n",
      "Iteration 17, loss = 0.46065503\n",
      "Iteration 18, loss = 0.45838889\n",
      "Iteration 19, loss = 0.45823033\n",
      "Iteration 20, loss = 0.45632973\n",
      "Iteration 21, loss = 0.45636642\n",
      "Iteration 22, loss = 0.45750367\n",
      "Iteration 23, loss = 0.45752015\n",
      "Iteration 24, loss = 0.45478088\n",
      "Iteration 25, loss = 0.45358468\n",
      "Iteration 26, loss = 0.45529520\n",
      "Iteration 27, loss = 0.45442736\n",
      "Iteration 28, loss = 0.45333319\n",
      "Iteration 29, loss = 0.45286940\n",
      "Iteration 30, loss = 0.45286662\n",
      "Iteration 31, loss = 0.45243516\n",
      "Iteration 32, loss = 0.45325904\n",
      "Iteration 33, loss = 0.45091586\n",
      "Iteration 34, loss = 0.44762032\n",
      "Iteration 35, loss = 0.44795983\n",
      "Iteration 36, loss = 0.44823600\n",
      "Iteration 37, loss = 0.44604217\n",
      "Iteration 38, loss = 0.44893282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 39, loss = 0.44819349\n",
      "Iteration 40, loss = 0.44774493\n",
      "Iteration 41, loss = 0.44790808\n",
      "Iteration 42, loss = 0.45089865\n",
      "Iteration 43, loss = 0.44549950\n",
      "Iteration 44, loss = 0.44522684\n",
      "Iteration 45, loss = 0.44562272\n",
      "Iteration 46, loss = 0.44762293\n",
      "Iteration 47, loss = 0.44673707\n",
      "Iteration 48, loss = 0.44566226\n",
      "Iteration 49, loss = 0.44454019\n",
      "Iteration 50, loss = 0.44461913\n",
      "Iteration 51, loss = 0.44443718\n",
      "Iteration 52, loss = 0.44437576\n",
      "Iteration 53, loss = 0.44217602\n",
      "Iteration 54, loss = 0.44277055\n",
      "Iteration 55, loss = 0.44195218\n",
      "Iteration 56, loss = 0.44302180\n",
      "Iteration 57, loss = 0.44217401\n",
      "Iteration 58, loss = 0.44256698\n",
      "Iteration 59, loss = 0.44274272\n",
      "Iteration 60, loss = 0.44256844\n",
      "Iteration 61, loss = 0.44114361\n",
      "Iteration 62, loss = 0.44077480\n",
      "Iteration 63, loss = 0.44294796\n",
      "Iteration 64, loss = 0.44140288\n",
      "Iteration 65, loss = 0.44086232\n",
      "Iteration 66, loss = 0.44071017\n",
      "Iteration 67, loss = 0.44094105\n",
      "Iteration 68, loss = 0.43902508\n",
      "Iteration 69, loss = 0.43824037\n",
      "Iteration 70, loss = 0.43878634\n",
      "Iteration 71, loss = 0.43756833\n",
      "Iteration 72, loss = 0.43883505\n",
      "Iteration 73, loss = 0.43852632\n",
      "Iteration 74, loss = 0.43757341\n",
      "Iteration 75, loss = 0.43523083\n",
      "Iteration 76, loss = 0.43456747\n",
      "Iteration 77, loss = 0.43421547\n",
      "Iteration 78, loss = 0.43411701\n",
      "Iteration 79, loss = 0.43591598\n",
      "Iteration 80, loss = 0.43241947\n",
      "Iteration 81, loss = 0.43450448\n",
      "Iteration 82, loss = 0.43469589\n",
      "Iteration 83, loss = 0.43284523\n",
      "Iteration 84, loss = 0.43247363\n",
      "Iteration 85, loss = 0.43478547\n",
      "Iteration 86, loss = 0.43647762\n",
      "Iteration 87, loss = 0.43363733\n",
      "Iteration 88, loss = 0.43221010\n",
      "Iteration 89, loss = 0.43263819\n",
      "Iteration 90, loss = 0.43132603\n",
      "Iteration 91, loss = 0.43287145\n",
      "Iteration 92, loss = 0.43283038\n",
      "Iteration 93, loss = 0.43108790\n",
      "Iteration 94, loss = 0.43104780\n",
      "Iteration 95, loss = 0.43107622\n",
      "Iteration 96, loss = 0.43229158\n",
      "Iteration 97, loss = 0.43179865\n",
      "Iteration 98, loss = 0.43150925\n",
      "Iteration 99, loss = 0.43271415\n",
      "Iteration 100, loss = 0.43371466\n",
      "Iteration 101, loss = 0.43168237\n",
      "Iteration 102, loss = 0.43396372\n",
      "Iteration 103, loss = 0.43404665\n",
      "Iteration 104, loss = 0.43178395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56585329\n",
      "Iteration 2, loss = 0.48477844\n",
      "Iteration 3, loss = 0.47769959\n",
      "Iteration 4, loss = 0.47424081\n",
      "Iteration 5, loss = 0.47335524\n",
      "Iteration 6, loss = 0.47149944\n",
      "Iteration 7, loss = 0.47148518\n",
      "Iteration 8, loss = 0.47299821\n",
      "Iteration 9, loss = 0.47102981\n",
      "Iteration 10, loss = 0.47072867\n",
      "Iteration 11, loss = 0.47090618\n",
      "Iteration 12, loss = 0.47137132\n",
      "Iteration 13, loss = 0.46964941\n",
      "Iteration 14, loss = 0.46877094\n",
      "Iteration 15, loss = 0.46867507\n",
      "Iteration 16, loss = 0.46806414\n",
      "Iteration 17, loss = 0.46670100\n",
      "Iteration 18, loss = 0.46500105\n",
      "Iteration 19, loss = 0.46300021\n",
      "Iteration 20, loss = 0.46291157\n",
      "Iteration 21, loss = 0.46131789\n",
      "Iteration 22, loss = 0.46081799\n",
      "Iteration 23, loss = 0.46083831\n",
      "Iteration 24, loss = 0.45921810\n",
      "Iteration 25, loss = 0.45923117\n",
      "Iteration 26, loss = 0.45937099\n",
      "Iteration 27, loss = 0.45724514\n",
      "Iteration 28, loss = 0.45724131\n",
      "Iteration 29, loss = 0.45837432\n",
      "Iteration 30, loss = 0.45749739\n",
      "Iteration 31, loss = 0.45815510\n",
      "Iteration 32, loss = 0.45603830\n",
      "Iteration 33, loss = 0.45485974\n",
      "Iteration 34, loss = 0.45614597\n",
      "Iteration 35, loss = 0.45426789\n",
      "Iteration 36, loss = 0.45410606\n",
      "Iteration 37, loss = 0.45497777\n",
      "Iteration 38, loss = 0.45495145\n",
      "Iteration 39, loss = 0.45400177\n",
      "Iteration 40, loss = 0.45614644\n",
      "Iteration 41, loss = 0.45406851\n",
      "Iteration 42, loss = 0.45485612\n",
      "Iteration 43, loss = 0.45324752\n",
      "Iteration 44, loss = 0.45399233\n",
      "Iteration 45, loss = 0.45343231\n",
      "Iteration 46, loss = 0.45359092\n",
      "Iteration 47, loss = 0.45626416\n",
      "Iteration 48, loss = 0.45474480\n",
      "Iteration 49, loss = 0.45223294\n",
      "Iteration 50, loss = 0.45248314\n",
      "Iteration 51, loss = 0.45298501\n",
      "Iteration 52, loss = 0.45412713\n",
      "Iteration 53, loss = 0.45117408\n",
      "Iteration 54, loss = 0.45245936\n",
      "Iteration 55, loss = 0.45262076\n",
      "Iteration 56, loss = 0.45220038\n",
      "Iteration 57, loss = 0.45255438\n",
      "Iteration 58, loss = 0.45244709\n",
      "Iteration 59, loss = 0.45064457\n",
      "Iteration 60, loss = 0.45327147\n",
      "Iteration 61, loss = 0.45227921\n",
      "Iteration 62, loss = 0.45158358\n",
      "Iteration 63, loss = 0.45522004\n",
      "Iteration 64, loss = 0.45592336\n",
      "Iteration 65, loss = 0.45217889\n",
      "Iteration 66, loss = 0.45301963\n",
      "Iteration 67, loss = 0.45137321\n",
      "Iteration 68, loss = 0.45164777\n",
      "Iteration 69, loss = 0.45290610\n",
      "Iteration 70, loss = 0.45446055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57858400\n",
      "Iteration 2, loss = 0.48805763\n",
      "Iteration 3, loss = 0.48199078\n",
      "Iteration 4, loss = 0.47789147\n",
      "Iteration 5, loss = 0.47656232\n",
      "Iteration 6, loss = 0.47672060\n",
      "Iteration 7, loss = 0.47517570\n",
      "Iteration 8, loss = 0.47573704\n",
      "Iteration 9, loss = 0.47403315\n",
      "Iteration 10, loss = 0.47447393\n",
      "Iteration 11, loss = 0.47536053\n",
      "Iteration 12, loss = 0.47344601\n",
      "Iteration 13, loss = 0.47277002\n",
      "Iteration 14, loss = 0.47340530\n",
      "Iteration 15, loss = 0.47237109\n",
      "Iteration 16, loss = 0.47260799\n",
      "Iteration 17, loss = 0.47260692\n",
      "Iteration 18, loss = 0.47038240\n",
      "Iteration 19, loss = 0.47168258\n",
      "Iteration 20, loss = 0.47067498\n",
      "Iteration 21, loss = 0.46807910\n",
      "Iteration 22, loss = 0.46903964\n",
      "Iteration 23, loss = 0.47034708\n",
      "Iteration 24, loss = 0.46806528\n",
      "Iteration 25, loss = 0.46780265\n",
      "Iteration 26, loss = 0.46592344\n",
      "Iteration 27, loss = 0.46565991\n",
      "Iteration 28, loss = 0.46617470\n",
      "Iteration 29, loss = 0.46545404\n",
      "Iteration 30, loss = 0.46462792\n",
      "Iteration 31, loss = 0.46479234\n",
      "Iteration 32, loss = 0.46488710\n",
      "Iteration 33, loss = 0.46387085\n",
      "Iteration 34, loss = 0.46313185\n",
      "Iteration 35, loss = 0.46163945\n",
      "Iteration 36, loss = 0.46109709\n",
      "Iteration 37, loss = 0.46247734\n",
      "Iteration 38, loss = 0.46343216\n",
      "Iteration 39, loss = 0.46099959\n",
      "Iteration 40, loss = 0.46091471\n",
      "Iteration 41, loss = 0.45940494\n",
      "Iteration 42, loss = 0.45965506\n",
      "Iteration 43, loss = 0.45883092\n",
      "Iteration 44, loss = 0.45859273\n",
      "Iteration 45, loss = 0.45923091\n",
      "Iteration 46, loss = 0.45893935\n",
      "Iteration 47, loss = 0.45681638\n",
      "Iteration 48, loss = 0.45589384\n",
      "Iteration 49, loss = 0.45814976\n",
      "Iteration 50, loss = 0.45707468\n",
      "Iteration 51, loss = 0.45506859\n",
      "Iteration 52, loss = 0.45488983\n",
      "Iteration 53, loss = 0.45589236\n",
      "Iteration 54, loss = 0.45342258\n",
      "Iteration 55, loss = 0.45408639\n",
      "Iteration 56, loss = 0.45492424\n",
      "Iteration 57, loss = 0.45401129\n",
      "Iteration 58, loss = 0.45500356\n",
      "Iteration 59, loss = 0.45390853\n",
      "Iteration 60, loss = 0.45314955\n",
      "Iteration 61, loss = 0.45398024\n",
      "Iteration 62, loss = 0.45315319\n",
      "Iteration 63, loss = 0.45405717\n",
      "Iteration 64, loss = 0.45355682\n",
      "Iteration 65, loss = 0.45502469\n",
      "Iteration 66, loss = 0.45436726\n",
      "Iteration 67, loss = 0.45288713\n",
      "Iteration 68, loss = 0.45340191\n",
      "Iteration 69, loss = 0.45560012\n",
      "Iteration 70, loss = 0.45310883\n",
      "Iteration 71, loss = 0.45430302\n",
      "Iteration 72, loss = 0.45381927\n",
      "Iteration 73, loss = 0.45317843\n",
      "Iteration 74, loss = 0.45238084\n",
      "Iteration 75, loss = 0.45259106\n",
      "Iteration 76, loss = 0.45300611\n",
      "Iteration 77, loss = 0.45349803\n",
      "Iteration 78, loss = 0.45352067\n",
      "Iteration 79, loss = 0.45256005\n",
      "Iteration 80, loss = 0.45297229\n",
      "Iteration 81, loss = 0.45305899\n",
      "Iteration 82, loss = 0.45348541\n",
      "Iteration 83, loss = 0.45157281\n",
      "Iteration 84, loss = 0.45294528\n",
      "Iteration 85, loss = 0.45209675\n",
      "Iteration 86, loss = 0.45213500\n",
      "Iteration 87, loss = 0.45124858\n",
      "Iteration 88, loss = 0.45248426\n",
      "Iteration 89, loss = 0.45214431\n",
      "Iteration 90, loss = 0.45202287\n",
      "Iteration 91, loss = 0.45210822\n",
      "Iteration 92, loss = 0.45121300\n",
      "Iteration 93, loss = 0.45354370\n",
      "Iteration 94, loss = 0.45342323\n",
      "Iteration 95, loss = 0.45272366\n",
      "Iteration 96, loss = 0.45230063\n",
      "Iteration 97, loss = 0.45219788\n",
      "Iteration 98, loss = 0.45059642\n",
      "Iteration 99, loss = 0.45233099\n",
      "Iteration 100, loss = 0.45135399\n",
      "Iteration 101, loss = 0.45136975\n",
      "Iteration 102, loss = 0.45284051\n",
      "Iteration 103, loss = 0.45124307\n",
      "Iteration 104, loss = 0.45108138\n",
      "Iteration 105, loss = 0.45181657\n",
      "Iteration 106, loss = 0.45065224\n",
      "Iteration 107, loss = 0.45017753\n",
      "Iteration 108, loss = 0.45171084\n",
      "Iteration 109, loss = 0.45149763\n",
      "Iteration 110, loss = 0.45274271\n",
      "Iteration 111, loss = 0.45346426\n",
      "Iteration 112, loss = 0.45225097\n",
      "Iteration 113, loss = 0.45038255\n",
      "Iteration 114, loss = 0.45068840\n",
      "Iteration 115, loss = 0.45098581\n",
      "Iteration 116, loss = 0.45064253\n",
      "Iteration 117, loss = 0.45060015\n",
      "Iteration 118, loss = 0.45142711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57804414\n",
      "Iteration 2, loss = 0.48501444\n",
      "Iteration 3, loss = 0.47584494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 0.47185288\n",
      "Iteration 5, loss = 0.47067733\n",
      "Iteration 6, loss = 0.46942192\n",
      "Iteration 7, loss = 0.46910106\n",
      "Iteration 8, loss = 0.46903552\n",
      "Iteration 9, loss = 0.46730251\n",
      "Iteration 10, loss = 0.46769273\n",
      "Iteration 11, loss = 0.46768307\n",
      "Iteration 12, loss = 0.46837146\n",
      "Iteration 13, loss = 0.46899169\n",
      "Iteration 14, loss = 0.46561766\n",
      "Iteration 15, loss = 0.46527480\n",
      "Iteration 16, loss = 0.46448127\n",
      "Iteration 17, loss = 0.46537665\n",
      "Iteration 18, loss = 0.46323728\n",
      "Iteration 19, loss = 0.46431887\n",
      "Iteration 20, loss = 0.46267276\n",
      "Iteration 21, loss = 0.46238304\n",
      "Iteration 22, loss = 0.46373271\n",
      "Iteration 23, loss = 0.46625860\n",
      "Iteration 24, loss = 0.46115527\n",
      "Iteration 25, loss = 0.45970769\n",
      "Iteration 26, loss = 0.45916486\n",
      "Iteration 27, loss = 0.45713968\n",
      "Iteration 28, loss = 0.45581274\n",
      "Iteration 29, loss = 0.45616805\n",
      "Iteration 30, loss = 0.45495674\n",
      "Iteration 31, loss = 0.45476362\n",
      "Iteration 32, loss = 0.45772396\n",
      "Iteration 33, loss = 0.45562225\n",
      "Iteration 34, loss = 0.45297133\n",
      "Iteration 35, loss = 0.45282706\n",
      "Iteration 36, loss = 0.45281953\n",
      "Iteration 37, loss = 0.45196856\n",
      "Iteration 38, loss = 0.45418777\n",
      "Iteration 39, loss = 0.45167906\n",
      "Iteration 40, loss = 0.45144190\n",
      "Iteration 41, loss = 0.45156922\n",
      "Iteration 42, loss = 0.45210364\n",
      "Iteration 43, loss = 0.45129816\n",
      "Iteration 44, loss = 0.45136750\n",
      "Iteration 45, loss = 0.45105114\n",
      "Iteration 46, loss = 0.45012962\n",
      "Iteration 47, loss = 0.44944461\n",
      "Iteration 48, loss = 0.44935116\n",
      "Iteration 49, loss = 0.44920134\n",
      "Iteration 50, loss = 0.44699455\n",
      "Iteration 51, loss = 0.44817446\n",
      "Iteration 52, loss = 0.44675362\n",
      "Iteration 53, loss = 0.44709917\n",
      "Iteration 54, loss = 0.44524752\n",
      "Iteration 55, loss = 0.44548935\n",
      "Iteration 56, loss = 0.44575311\n",
      "Iteration 57, loss = 0.44567386\n",
      "Iteration 58, loss = 0.44623758\n",
      "Iteration 59, loss = 0.44639482\n",
      "Iteration 60, loss = 0.44508784\n",
      "Iteration 61, loss = 0.44580654\n",
      "Iteration 62, loss = 0.44506825\n",
      "Iteration 63, loss = 0.44502395\n",
      "Iteration 64, loss = 0.44410163\n",
      "Iteration 65, loss = 0.44482696\n",
      "Iteration 66, loss = 0.44828057\n",
      "Iteration 67, loss = 0.44559596\n",
      "Iteration 68, loss = 0.44473129\n",
      "Iteration 69, loss = 0.44404240\n",
      "Iteration 70, loss = 0.44567104\n",
      "Iteration 71, loss = 0.44411436\n",
      "Iteration 72, loss = 0.44403620\n",
      "Iteration 73, loss = 0.44413200\n",
      "Iteration 74, loss = 0.44402234\n",
      "Iteration 75, loss = 0.44469761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57053686\n",
      "Iteration 2, loss = 0.48793013\n",
      "Iteration 3, loss = 0.47870242\n",
      "Iteration 4, loss = 0.47508604\n",
      "Iteration 5, loss = 0.47320764\n",
      "Iteration 6, loss = 0.47102907\n",
      "Iteration 7, loss = 0.47043438\n",
      "Iteration 8, loss = 0.47141309\n",
      "Iteration 9, loss = 0.47004882\n",
      "Iteration 10, loss = 0.46856573\n",
      "Iteration 11, loss = 0.46941986\n",
      "Iteration 12, loss = 0.47003313\n",
      "Iteration 13, loss = 0.46822245\n",
      "Iteration 14, loss = 0.46784969\n",
      "Iteration 15, loss = 0.46741989\n",
      "Iteration 16, loss = 0.46711066\n",
      "Iteration 17, loss = 0.46671660\n",
      "Iteration 18, loss = 0.46696880\n",
      "Iteration 19, loss = 0.46672239\n",
      "Iteration 20, loss = 0.46634297\n",
      "Iteration 21, loss = 0.46495877\n",
      "Iteration 22, loss = 0.46499680\n",
      "Iteration 23, loss = 0.46407973\n",
      "Iteration 24, loss = 0.46341782\n",
      "Iteration 25, loss = 0.46299212\n",
      "Iteration 26, loss = 0.46557779\n",
      "Iteration 27, loss = 0.46213526\n",
      "Iteration 28, loss = 0.46276677\n",
      "Iteration 29, loss = 0.46349770\n",
      "Iteration 30, loss = 0.46209773\n",
      "Iteration 31, loss = 0.46257901\n",
      "Iteration 32, loss = 0.46277437\n",
      "Iteration 33, loss = 0.46189212\n",
      "Iteration 34, loss = 0.46066629\n",
      "Iteration 35, loss = 0.46044724\n",
      "Iteration 36, loss = 0.46016194\n",
      "Iteration 37, loss = 0.46114641\n",
      "Iteration 38, loss = 0.46018589\n",
      "Iteration 39, loss = 0.46095112\n",
      "Iteration 40, loss = 0.46202346\n",
      "Iteration 41, loss = 0.46053907\n",
      "Iteration 42, loss = 0.46025775\n",
      "Iteration 43, loss = 0.45873417\n",
      "Iteration 44, loss = 0.45928410\n",
      "Iteration 45, loss = 0.45897043\n",
      "Iteration 46, loss = 0.45927814\n",
      "Iteration 47, loss = 0.46018291\n",
      "Iteration 48, loss = 0.45839441\n",
      "Iteration 49, loss = 0.45726118\n",
      "Iteration 50, loss = 0.45780118\n",
      "Iteration 51, loss = 0.45664998\n",
      "Iteration 52, loss = 0.45790673\n",
      "Iteration 53, loss = 0.45589146\n",
      "Iteration 54, loss = 0.45578062\n",
      "Iteration 55, loss = 0.45577470\n",
      "Iteration 56, loss = 0.45430830\n",
      "Iteration 57, loss = 0.45414404\n",
      "Iteration 58, loss = 0.45316632\n",
      "Iteration 59, loss = 0.45309212\n",
      "Iteration 60, loss = 0.45570905\n",
      "Iteration 61, loss = 0.45366348\n",
      "Iteration 62, loss = 0.45264021\n",
      "Iteration 63, loss = 0.45633446\n",
      "Iteration 64, loss = 0.45490522\n",
      "Iteration 65, loss = 0.45465526\n",
      "Iteration 66, loss = 0.45323541\n",
      "Iteration 67, loss = 0.45194686\n",
      "Iteration 68, loss = 0.45268695\n",
      "Iteration 69, loss = 0.45293468\n",
      "Iteration 70, loss = 0.45306942\n",
      "Iteration 71, loss = 0.45176052\n",
      "Iteration 72, loss = 0.45093282\n",
      "Iteration 73, loss = 0.45157860\n",
      "Iteration 74, loss = 0.45188325\n",
      "Iteration 75, loss = 0.45219917\n",
      "Iteration 76, loss = 0.45268398\n",
      "Iteration 77, loss = 0.45250398\n",
      "Iteration 78, loss = 0.45338661\n",
      "Iteration 79, loss = 0.45280259\n",
      "Iteration 80, loss = 0.45252394\n",
      "Iteration 81, loss = 0.45255814\n",
      "Iteration 82, loss = 0.45150362\n",
      "Iteration 83, loss = 0.45088078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55626037\n",
      "Iteration 2, loss = 0.49667720\n",
      "Iteration 3, loss = 0.49084068\n",
      "Iteration 4, loss = 0.48698933\n",
      "Iteration 5, loss = 0.48408473\n",
      "Iteration 6, loss = 0.48217980\n",
      "Iteration 7, loss = 0.48156613\n",
      "Iteration 8, loss = 0.47923128\n",
      "Iteration 9, loss = 0.47768592\n",
      "Iteration 10, loss = 0.47565632\n",
      "Iteration 11, loss = 0.47397747\n",
      "Iteration 12, loss = 0.47174565\n",
      "Iteration 13, loss = 0.47095872\n",
      "Iteration 14, loss = 0.46944113\n",
      "Iteration 15, loss = 0.46578159\n",
      "Iteration 16, loss = 0.46373152\n",
      "Iteration 17, loss = 0.46269186\n",
      "Iteration 18, loss = 0.45901027\n",
      "Iteration 19, loss = 0.45898470\n",
      "Iteration 20, loss = 0.45705050\n",
      "Iteration 21, loss = 0.45624309\n",
      "Iteration 22, loss = 0.45652508\n",
      "Iteration 23, loss = 0.45850365\n",
      "Iteration 24, loss = 0.45434199\n",
      "Iteration 25, loss = 0.45379037\n",
      "Iteration 26, loss = 0.45369067\n",
      "Iteration 27, loss = 0.45265140\n",
      "Iteration 28, loss = 0.45381589\n",
      "Iteration 29, loss = 0.45190386\n",
      "Iteration 30, loss = 0.45038152\n",
      "Iteration 31, loss = 0.45100478\n",
      "Iteration 32, loss = 0.45227081\n",
      "Iteration 33, loss = 0.45124960\n",
      "Iteration 34, loss = 0.45054484\n",
      "Iteration 35, loss = 0.44973931\n",
      "Iteration 36, loss = 0.44863609\n",
      "Iteration 37, loss = 0.45101766\n",
      "Iteration 38, loss = 0.45075604\n",
      "Iteration 39, loss = 0.45027283\n",
      "Iteration 40, loss = 0.44986118\n",
      "Iteration 41, loss = 0.44723297\n",
      "Iteration 42, loss = 0.44737482\n",
      "Iteration 43, loss = 0.44622462\n",
      "Iteration 44, loss = 0.44597110\n",
      "Iteration 45, loss = 0.44519616\n",
      "Iteration 46, loss = 0.44609982\n",
      "Iteration 47, loss = 0.44419328\n",
      "Iteration 48, loss = 0.44637346\n",
      "Iteration 49, loss = 0.44569328\n",
      "Iteration 50, loss = 0.44646156\n",
      "Iteration 51, loss = 0.44739725\n",
      "Iteration 52, loss = 0.44725861\n",
      "Iteration 53, loss = 0.44784918\n",
      "Iteration 54, loss = 0.44396560\n",
      "Iteration 55, loss = 0.44443710\n",
      "Iteration 56, loss = 0.44204547\n",
      "Iteration 57, loss = 0.44157985\n",
      "Iteration 58, loss = 0.44273024\n",
      "Iteration 59, loss = 0.44383156\n",
      "Iteration 60, loss = 0.44271460\n",
      "Iteration 61, loss = 0.44127232\n",
      "Iteration 62, loss = 0.44052997\n",
      "Iteration 63, loss = 0.43963001\n",
      "Iteration 64, loss = 0.43881526\n",
      "Iteration 65, loss = 0.43903155\n",
      "Iteration 66, loss = 0.43965412\n",
      "Iteration 67, loss = 0.43788823\n",
      "Iteration 68, loss = 0.43726016\n",
      "Iteration 69, loss = 0.43926218\n",
      "Iteration 70, loss = 0.43792539\n",
      "Iteration 71, loss = 0.43698784\n",
      "Iteration 72, loss = 0.43717405\n",
      "Iteration 73, loss = 0.43752008\n",
      "Iteration 74, loss = 0.43806990\n",
      "Iteration 75, loss = 0.43595165\n",
      "Iteration 76, loss = 0.43837126\n",
      "Iteration 77, loss = 0.43964057\n",
      "Iteration 78, loss = 0.43677165\n",
      "Iteration 79, loss = 0.43688782\n",
      "Iteration 80, loss = 0.43786763\n",
      "Iteration 81, loss = 0.43713486\n",
      "Iteration 82, loss = 0.43780009\n",
      "Iteration 83, loss = 0.43811452\n",
      "Iteration 84, loss = 0.43626472\n",
      "Iteration 85, loss = 0.43586468\n",
      "Iteration 86, loss = 0.43689056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55299625\n",
      "Iteration 2, loss = 0.49270167\n",
      "Iteration 3, loss = 0.48594608\n",
      "Iteration 4, loss = 0.48187653\n",
      "Iteration 5, loss = 0.47974570\n",
      "Iteration 6, loss = 0.47751853\n",
      "Iteration 7, loss = 0.47620500\n",
      "Iteration 8, loss = 0.47524640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.47302699\n",
      "Iteration 10, loss = 0.47231777\n",
      "Iteration 11, loss = 0.47038930\n",
      "Iteration 12, loss = 0.46901006\n",
      "Iteration 13, loss = 0.46929753\n",
      "Iteration 14, loss = 0.46426997\n",
      "Iteration 15, loss = 0.46134576\n",
      "Iteration 16, loss = 0.45991407\n",
      "Iteration 17, loss = 0.46045788\n",
      "Iteration 18, loss = 0.45762946\n",
      "Iteration 19, loss = 0.45674087\n",
      "Iteration 20, loss = 0.45517940\n",
      "Iteration 21, loss = 0.45414431\n",
      "Iteration 22, loss = 0.45473281\n",
      "Iteration 23, loss = 0.45511617\n",
      "Iteration 24, loss = 0.45197252\n",
      "Iteration 25, loss = 0.45235098\n",
      "Iteration 26, loss = 0.45148247\n",
      "Iteration 27, loss = 0.45218275\n",
      "Iteration 28, loss = 0.44959736\n",
      "Iteration 29, loss = 0.45026334\n",
      "Iteration 30, loss = 0.45059291\n",
      "Iteration 31, loss = 0.45086346\n",
      "Iteration 32, loss = 0.45052499\n",
      "Iteration 33, loss = 0.44944033\n",
      "Iteration 34, loss = 0.44793266\n",
      "Iteration 35, loss = 0.44783895\n",
      "Iteration 36, loss = 0.44694128\n",
      "Iteration 37, loss = 0.44526463\n",
      "Iteration 38, loss = 0.44669811\n",
      "Iteration 39, loss = 0.44386748\n",
      "Iteration 40, loss = 0.44298962\n",
      "Iteration 41, loss = 0.44129623\n",
      "Iteration 42, loss = 0.44087670\n",
      "Iteration 43, loss = 0.43973276\n",
      "Iteration 44, loss = 0.44092558\n",
      "Iteration 45, loss = 0.43997499\n",
      "Iteration 46, loss = 0.43805012\n",
      "Iteration 47, loss = 0.43705924\n",
      "Iteration 48, loss = 0.43885894\n",
      "Iteration 49, loss = 0.43804542\n",
      "Iteration 50, loss = 0.43873996\n",
      "Iteration 51, loss = 0.43726942\n",
      "Iteration 52, loss = 0.43796097\n",
      "Iteration 53, loss = 0.43741649\n",
      "Iteration 54, loss = 0.43723795\n",
      "Iteration 55, loss = 0.43699627\n",
      "Iteration 56, loss = 0.43657353\n",
      "Iteration 57, loss = 0.43620247\n",
      "Iteration 58, loss = 0.43653972\n",
      "Iteration 59, loss = 0.43612571\n",
      "Iteration 60, loss = 0.43778947\n",
      "Iteration 61, loss = 0.43612591\n",
      "Iteration 62, loss = 0.43598918\n",
      "Iteration 63, loss = 0.43631100\n",
      "Iteration 64, loss = 0.43714474\n",
      "Iteration 65, loss = 0.43595003\n",
      "Iteration 66, loss = 0.43774618\n",
      "Iteration 67, loss = 0.43673267\n",
      "Iteration 68, loss = 0.43500584\n",
      "Iteration 69, loss = 0.43511072\n",
      "Iteration 70, loss = 0.43481428\n",
      "Iteration 71, loss = 0.43513100\n",
      "Iteration 72, loss = 0.43670659\n",
      "Iteration 73, loss = 0.43933992\n",
      "Iteration 74, loss = 0.43664770\n",
      "Iteration 75, loss = 0.43561269\n",
      "Iteration 76, loss = 0.43534122\n",
      "Iteration 77, loss = 0.43642425\n",
      "Iteration 78, loss = 0.43528460\n",
      "Iteration 79, loss = 0.43600960\n",
      "Iteration 80, loss = 0.43542108\n",
      "Iteration 81, loss = 0.43706437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55116708\n",
      "Iteration 2, loss = 0.49689897\n",
      "Iteration 3, loss = 0.48981040\n",
      "Iteration 4, loss = 0.48543988\n",
      "Iteration 5, loss = 0.48365406\n",
      "Iteration 6, loss = 0.48199121\n",
      "Iteration 7, loss = 0.47987044\n",
      "Iteration 8, loss = 0.48065896\n",
      "Iteration 9, loss = 0.48055850\n",
      "Iteration 10, loss = 0.47849085\n",
      "Iteration 11, loss = 0.47740146\n",
      "Iteration 12, loss = 0.47798833\n",
      "Iteration 13, loss = 0.47659517\n",
      "Iteration 14, loss = 0.47648174\n",
      "Iteration 15, loss = 0.47546625\n",
      "Iteration 16, loss = 0.47611662\n",
      "Iteration 17, loss = 0.47420572\n",
      "Iteration 18, loss = 0.47313142\n",
      "Iteration 19, loss = 0.47156652\n",
      "Iteration 20, loss = 0.47277224\n",
      "Iteration 21, loss = 0.47137195\n",
      "Iteration 22, loss = 0.47029750\n",
      "Iteration 23, loss = 0.46920667\n",
      "Iteration 24, loss = 0.46887871\n",
      "Iteration 25, loss = 0.46800967\n",
      "Iteration 26, loss = 0.46835803\n",
      "Iteration 27, loss = 0.46805029\n",
      "Iteration 28, loss = 0.46747014\n",
      "Iteration 29, loss = 0.46908416\n",
      "Iteration 30, loss = 0.46689284\n",
      "Iteration 31, loss = 0.46768837\n",
      "Iteration 32, loss = 0.46825330\n",
      "Iteration 33, loss = 0.46634608\n",
      "Iteration 34, loss = 0.46767262\n",
      "Iteration 35, loss = 0.46657980\n",
      "Iteration 36, loss = 0.46554251\n",
      "Iteration 37, loss = 0.46582670\n",
      "Iteration 38, loss = 0.46618251\n",
      "Iteration 39, loss = 0.46425181\n",
      "Iteration 40, loss = 0.46567853\n",
      "Iteration 41, loss = 0.46467466\n",
      "Iteration 42, loss = 0.46458568\n",
      "Iteration 43, loss = 0.46385118\n",
      "Iteration 44, loss = 0.46484102\n",
      "Iteration 45, loss = 0.46349702\n",
      "Iteration 46, loss = 0.46445475\n",
      "Iteration 47, loss = 0.46612465\n",
      "Iteration 48, loss = 0.46368476\n",
      "Iteration 49, loss = 0.46204289\n",
      "Iteration 50, loss = 0.46198522\n",
      "Iteration 51, loss = 0.46064325\n",
      "Iteration 52, loss = 0.46129256\n",
      "Iteration 53, loss = 0.45894026\n",
      "Iteration 54, loss = 0.45833293\n",
      "Iteration 55, loss = 0.45876304\n",
      "Iteration 56, loss = 0.45911215\n",
      "Iteration 57, loss = 0.45817617\n",
      "Iteration 58, loss = 0.45782347\n",
      "Iteration 59, loss = 0.45711112\n",
      "Iteration 60, loss = 0.45862870\n",
      "Iteration 61, loss = 0.45666403\n",
      "Iteration 62, loss = 0.45838471\n",
      "Iteration 63, loss = 0.45967956\n",
      "Iteration 64, loss = 0.45672228\n",
      "Iteration 65, loss = 0.45592765\n",
      "Iteration 66, loss = 0.45519340\n",
      "Iteration 67, loss = 0.45473312\n",
      "Iteration 68, loss = 0.45506901\n",
      "Iteration 69, loss = 0.45463185\n",
      "Iteration 70, loss = 0.45573311\n",
      "Iteration 71, loss = 0.45505506\n",
      "Iteration 72, loss = 0.45372859\n",
      "Iteration 73, loss = 0.45395626\n",
      "Iteration 74, loss = 0.45390353\n",
      "Iteration 75, loss = 0.45500925\n",
      "Iteration 76, loss = 0.45517468\n",
      "Iteration 77, loss = 0.45509070\n",
      "Iteration 78, loss = 0.45520386\n",
      "Iteration 79, loss = 0.45480619\n",
      "Iteration 80, loss = 0.45262188\n",
      "Iteration 81, loss = 0.45545154\n",
      "Iteration 82, loss = 0.45428740\n",
      "Iteration 83, loss = 0.45095773\n",
      "Iteration 84, loss = 0.45269980\n",
      "Iteration 85, loss = 0.45168864\n",
      "Iteration 86, loss = 0.45258436\n",
      "Iteration 87, loss = 0.45129326\n",
      "Iteration 88, loss = 0.45236998\n",
      "Iteration 89, loss = 0.45486869\n",
      "Iteration 90, loss = 0.45234293\n",
      "Iteration 91, loss = 0.45226776\n",
      "Iteration 92, loss = 0.45382235\n",
      "Iteration 93, loss = 0.45264517\n",
      "Iteration 94, loss = 0.45088448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61033556\n",
      "Iteration 2, loss = 0.52401302\n",
      "Iteration 3, loss = 0.51708046\n",
      "Iteration 4, loss = 0.51405994\n",
      "Iteration 5, loss = 0.51016960\n",
      "Iteration 6, loss = 0.50597685\n",
      "Iteration 7, loss = 0.50179432\n",
      "Iteration 8, loss = 0.49645565\n",
      "Iteration 9, loss = 0.49032504\n",
      "Iteration 10, loss = 0.48621224\n",
      "Iteration 11, loss = 0.48200272\n",
      "Iteration 12, loss = 0.47809909\n",
      "Iteration 13, loss = 0.47825757\n",
      "Iteration 14, loss = 0.47571442\n",
      "Iteration 15, loss = 0.47171394\n",
      "Iteration 16, loss = 0.46928138\n",
      "Iteration 17, loss = 0.46742390\n",
      "Iteration 18, loss = 0.46469361\n",
      "Iteration 19, loss = 0.46456224\n",
      "Iteration 20, loss = 0.46372212\n",
      "Iteration 21, loss = 0.46019076\n",
      "Iteration 22, loss = 0.45841750\n",
      "Iteration 23, loss = 0.45767421\n",
      "Iteration 24, loss = 0.45518797\n",
      "Iteration 25, loss = 0.45417415\n",
      "Iteration 26, loss = 0.45147952\n",
      "Iteration 27, loss = 0.45200569\n",
      "Iteration 28, loss = 0.45038684\n",
      "Iteration 29, loss = 0.44925207\n",
      "Iteration 30, loss = 0.44760311\n",
      "Iteration 31, loss = 0.44986166\n",
      "Iteration 32, loss = 0.44915232\n",
      "Iteration 33, loss = 0.44860219\n",
      "Iteration 34, loss = 0.44652626\n",
      "Iteration 35, loss = 0.44557478\n",
      "Iteration 36, loss = 0.44320424\n",
      "Iteration 37, loss = 0.44783820\n",
      "Iteration 38, loss = 0.44615289\n",
      "Iteration 39, loss = 0.44329028\n",
      "Iteration 40, loss = 0.44112242\n",
      "Iteration 41, loss = 0.43962459\n",
      "Iteration 42, loss = 0.44269846\n",
      "Iteration 43, loss = 0.43832153\n",
      "Iteration 44, loss = 0.43952050\n",
      "Iteration 45, loss = 0.43795245\n",
      "Iteration 46, loss = 0.43731403\n",
      "Iteration 47, loss = 0.43872790\n",
      "Iteration 48, loss = 0.44133162\n",
      "Iteration 49, loss = 0.44213973\n",
      "Iteration 50, loss = 0.44367483\n",
      "Iteration 51, loss = 0.44005637\n",
      "Iteration 52, loss = 0.43804757\n",
      "Iteration 53, loss = 0.43889154\n",
      "Iteration 54, loss = 0.43598159\n",
      "Iteration 55, loss = 0.43564751\n",
      "Iteration 56, loss = 0.43565867\n",
      "Iteration 57, loss = 0.43645524\n",
      "Iteration 58, loss = 0.43576269\n",
      "Iteration 59, loss = 0.43410618\n",
      "Iteration 60, loss = 0.43446538\n",
      "Iteration 61, loss = 0.43565432\n",
      "Iteration 62, loss = 0.43630895\n",
      "Iteration 63, loss = 0.43482884\n",
      "Iteration 64, loss = 0.43420981\n",
      "Iteration 65, loss = 0.43635215\n",
      "Iteration 66, loss = 0.43466534\n",
      "Iteration 67, loss = 0.43419581\n",
      "Iteration 68, loss = 0.43471714\n",
      "Iteration 69, loss = 0.43557065\n",
      "Iteration 70, loss = 0.43454428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61022731\n",
      "Iteration 2, loss = 0.52003187\n",
      "Iteration 3, loss = 0.51353333\n",
      "Iteration 4, loss = 0.51031509\n",
      "Iteration 5, loss = 0.50613670\n",
      "Iteration 6, loss = 0.50197758\n",
      "Iteration 7, loss = 0.49901884\n",
      "Iteration 8, loss = 0.49470113\n",
      "Iteration 9, loss = 0.48964190\n",
      "Iteration 10, loss = 0.48694175\n",
      "Iteration 11, loss = 0.48551613\n",
      "Iteration 12, loss = 0.47764289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, loss = 0.47906578\n",
      "Iteration 14, loss = 0.47157993\n",
      "Iteration 15, loss = 0.46982025\n",
      "Iteration 16, loss = 0.46696008\n",
      "Iteration 17, loss = 0.46557304\n",
      "Iteration 18, loss = 0.46253333\n",
      "Iteration 19, loss = 0.46319042\n",
      "Iteration 20, loss = 0.45975016\n",
      "Iteration 21, loss = 0.45711705\n",
      "Iteration 22, loss = 0.45676496\n",
      "Iteration 23, loss = 0.45534959\n",
      "Iteration 24, loss = 0.45243630\n",
      "Iteration 25, loss = 0.45062017\n",
      "Iteration 26, loss = 0.45259279\n",
      "Iteration 27, loss = 0.45079059\n",
      "Iteration 28, loss = 0.44773430\n",
      "Iteration 29, loss = 0.44925052\n",
      "Iteration 30, loss = 0.45001388\n",
      "Iteration 31, loss = 0.44833401\n",
      "Iteration 32, loss = 0.44671483\n",
      "Iteration 33, loss = 0.44282146\n",
      "Iteration 34, loss = 0.44124236\n",
      "Iteration 35, loss = 0.44202083\n",
      "Iteration 36, loss = 0.44049720\n",
      "Iteration 37, loss = 0.44061161\n",
      "Iteration 38, loss = 0.44204881\n",
      "Iteration 39, loss = 0.44161715\n",
      "Iteration 40, loss = 0.43804478\n",
      "Iteration 41, loss = 0.43876485\n",
      "Iteration 42, loss = 0.44128181\n",
      "Iteration 43, loss = 0.43927369\n",
      "Iteration 44, loss = 0.43682280\n",
      "Iteration 45, loss = 0.43628097\n",
      "Iteration 46, loss = 0.43736786\n",
      "Iteration 47, loss = 0.43787931\n",
      "Iteration 48, loss = 0.43729032\n",
      "Iteration 49, loss = 0.43525053\n",
      "Iteration 50, loss = 0.43839838\n",
      "Iteration 51, loss = 0.43586107\n",
      "Iteration 52, loss = 0.43477425\n",
      "Iteration 53, loss = 0.43694884\n",
      "Iteration 54, loss = 0.43688252\n",
      "Iteration 55, loss = 0.43256669\n",
      "Iteration 56, loss = 0.43642619\n",
      "Iteration 57, loss = 0.43353087\n",
      "Iteration 58, loss = 0.43384265\n",
      "Iteration 59, loss = 0.43321350\n",
      "Iteration 60, loss = 0.43225880\n",
      "Iteration 61, loss = 0.43132993\n",
      "Iteration 62, loss = 0.43244931\n",
      "Iteration 63, loss = 0.43347707\n",
      "Iteration 64, loss = 0.43237943\n",
      "Iteration 65, loss = 0.42974131\n",
      "Iteration 66, loss = 0.43219220\n",
      "Iteration 67, loss = 0.43074392\n",
      "Iteration 68, loss = 0.42943659\n",
      "Iteration 69, loss = 0.43046279\n",
      "Iteration 70, loss = 0.43215174\n",
      "Iteration 71, loss = 0.43066512\n",
      "Iteration 72, loss = 0.42935175\n",
      "Iteration 73, loss = 0.42877859\n",
      "Iteration 74, loss = 0.42886639\n",
      "Iteration 75, loss = 0.42792811\n",
      "Iteration 76, loss = 0.42948819\n",
      "Iteration 77, loss = 0.42685794\n",
      "Iteration 78, loss = 0.42720040\n",
      "Iteration 79, loss = 0.42840704\n",
      "Iteration 80, loss = 0.42951938\n",
      "Iteration 81, loss = 0.42638086\n",
      "Iteration 82, loss = 0.42870801\n",
      "Iteration 83, loss = 0.42681878\n",
      "Iteration 84, loss = 0.42523573\n",
      "Iteration 85, loss = 0.42776410\n",
      "Iteration 86, loss = 0.42689532\n",
      "Iteration 87, loss = 0.42753772\n",
      "Iteration 88, loss = 0.42803392\n",
      "Iteration 89, loss = 0.42835451\n",
      "Iteration 90, loss = 0.42742106\n",
      "Iteration 91, loss = 0.42788385\n",
      "Iteration 92, loss = 0.42956251\n",
      "Iteration 93, loss = 0.42919393\n",
      "Iteration 94, loss = 0.42725393\n",
      "Iteration 95, loss = 0.42781838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60330444\n",
      "Iteration 2, loss = 0.52308931\n",
      "Iteration 3, loss = 0.51489981\n",
      "Iteration 4, loss = 0.50978878\n",
      "Iteration 5, loss = 0.50548135\n",
      "Iteration 6, loss = 0.49971385\n",
      "Iteration 7, loss = 0.49560632\n",
      "Iteration 8, loss = 0.49256187\n",
      "Iteration 9, loss = 0.48620033\n",
      "Iteration 10, loss = 0.48251277\n",
      "Iteration 11, loss = 0.47931792\n",
      "Iteration 12, loss = 0.47862726\n",
      "Iteration 13, loss = 0.47543384\n",
      "Iteration 14, loss = 0.47551332\n",
      "Iteration 15, loss = 0.47090757\n",
      "Iteration 16, loss = 0.47117642\n",
      "Iteration 17, loss = 0.46769152\n",
      "Iteration 18, loss = 0.46567098\n",
      "Iteration 19, loss = 0.46390495\n",
      "Iteration 20, loss = 0.46250331\n",
      "Iteration 21, loss = 0.46092511\n",
      "Iteration 22, loss = 0.45927061\n",
      "Iteration 23, loss = 0.45813275\n",
      "Iteration 24, loss = 0.45879172\n",
      "Iteration 25, loss = 0.45666921\n",
      "Iteration 26, loss = 0.45915790\n",
      "Iteration 27, loss = 0.45466658\n",
      "Iteration 28, loss = 0.45389614\n",
      "Iteration 29, loss = 0.45306467\n",
      "Iteration 30, loss = 0.45324127\n",
      "Iteration 31, loss = 0.45489196\n",
      "Iteration 32, loss = 0.45188957\n",
      "Iteration 33, loss = 0.45168025\n",
      "Iteration 34, loss = 0.45396845\n",
      "Iteration 35, loss = 0.44960826\n",
      "Iteration 36, loss = 0.44898942\n",
      "Iteration 37, loss = 0.44887376\n",
      "Iteration 38, loss = 0.44793911\n",
      "Iteration 39, loss = 0.44780796\n",
      "Iteration 40, loss = 0.44912480\n",
      "Iteration 41, loss = 0.44961462\n",
      "Iteration 42, loss = 0.44948028\n",
      "Iteration 43, loss = 0.44630149\n",
      "Iteration 44, loss = 0.44670399\n",
      "Iteration 45, loss = 0.44341499\n",
      "Iteration 46, loss = 0.44579510\n",
      "Iteration 47, loss = 0.44625981\n",
      "Iteration 48, loss = 0.44393570\n",
      "Iteration 49, loss = 0.44520029\n",
      "Iteration 50, loss = 0.44490837\n",
      "Iteration 51, loss = 0.44152044\n",
      "Iteration 52, loss = 0.44150775\n",
      "Iteration 53, loss = 0.44233645\n",
      "Iteration 54, loss = 0.44157383\n",
      "Iteration 55, loss = 0.44171217\n",
      "Iteration 56, loss = 0.44164185\n",
      "Iteration 57, loss = 0.44112550\n",
      "Iteration 58, loss = 0.43926393\n",
      "Iteration 59, loss = 0.43788431\n",
      "Iteration 60, loss = 0.44294888\n",
      "Iteration 61, loss = 0.44188385\n",
      "Iteration 62, loss = 0.44520136\n",
      "Iteration 63, loss = 0.44542286\n",
      "Iteration 64, loss = 0.44156295\n",
      "Iteration 65, loss = 0.43901931\n",
      "Iteration 66, loss = 0.43994093\n",
      "Iteration 67, loss = 0.43713532\n",
      "Iteration 68, loss = 0.44070217\n",
      "Iteration 69, loss = 0.43870219\n",
      "Iteration 70, loss = 0.44150465\n",
      "Iteration 71, loss = 0.43910927\n",
      "Iteration 72, loss = 0.43975587\n",
      "Iteration 73, loss = 0.43853300\n",
      "Iteration 74, loss = 0.43877802\n",
      "Iteration 75, loss = 0.43760817\n",
      "Iteration 76, loss = 0.43778166\n",
      "Iteration 77, loss = 0.43697543\n",
      "Iteration 78, loss = 0.43635818\n",
      "Iteration 79, loss = 0.43739840\n",
      "Iteration 80, loss = 0.43717251\n",
      "Iteration 81, loss = 0.43659872\n",
      "Iteration 82, loss = 0.43768556\n",
      "Iteration 83, loss = 0.43519968\n",
      "Iteration 84, loss = 0.43518287\n",
      "Iteration 85, loss = 0.43522571\n",
      "Iteration 86, loss = 0.43420652\n",
      "Iteration 87, loss = 0.43647674\n",
      "Iteration 88, loss = 0.43469182\n",
      "Iteration 89, loss = 0.44053893\n",
      "Iteration 90, loss = 0.43714185\n",
      "Iteration 91, loss = 0.43541949\n",
      "Iteration 92, loss = 0.43544341\n",
      "Iteration 93, loss = 0.43453444\n",
      "Iteration 94, loss = 0.43725030\n",
      "Iteration 95, loss = 0.43525042\n",
      "Iteration 96, loss = 0.43351587\n",
      "Iteration 97, loss = 0.43276375\n",
      "Iteration 98, loss = 0.43319212\n",
      "Iteration 99, loss = 0.43599389\n",
      "Iteration 100, loss = 0.43305039\n",
      "Iteration 101, loss = 0.43363345\n",
      "Iteration 102, loss = 0.43482133\n",
      "Iteration 103, loss = 0.43153460\n",
      "Iteration 104, loss = 0.43162045\n",
      "Iteration 105, loss = 0.43148212\n",
      "Iteration 106, loss = 0.43376792\n",
      "Iteration 107, loss = 0.43339130\n",
      "Iteration 108, loss = 0.43276148\n",
      "Iteration 109, loss = 0.43662275\n",
      "Iteration 110, loss = 0.43233528\n",
      "Iteration 111, loss = 0.43058491\n",
      "Iteration 112, loss = 0.42923467\n",
      "Iteration 113, loss = 0.42977672\n",
      "Iteration 114, loss = 0.42936186\n",
      "Iteration 115, loss = 0.43072168\n",
      "Iteration 116, loss = 0.42994792\n",
      "Iteration 117, loss = 0.42878789\n",
      "Iteration 118, loss = 0.42898632\n",
      "Iteration 119, loss = 0.43409549\n",
      "Iteration 120, loss = 0.43056046\n",
      "Iteration 121, loss = 0.42736479\n",
      "Iteration 122, loss = 0.43018757\n",
      "Iteration 123, loss = 0.42841290\n",
      "Iteration 124, loss = 0.42869554\n",
      "Iteration 125, loss = 0.42665332\n",
      "Iteration 126, loss = 0.42630304\n",
      "Iteration 127, loss = 0.42888142\n",
      "Iteration 128, loss = 0.43038837\n",
      "Iteration 129, loss = 0.43066256\n",
      "Iteration 130, loss = 0.43014790\n",
      "Iteration 131, loss = 0.42963172\n",
      "Iteration 132, loss = 0.43001769\n",
      "Iteration 133, loss = 0.43062609\n",
      "Iteration 134, loss = 0.43073548\n",
      "Iteration 135, loss = 0.42662587\n",
      "Iteration 136, loss = 0.42845036\n",
      "Iteration 137, loss = 0.42650098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56576905\n",
      "Iteration 2, loss = 0.49716937\n",
      "Iteration 3, loss = 0.48890374\n",
      "Iteration 4, loss = 0.48421206\n",
      "Iteration 5, loss = 0.48122063\n",
      "Iteration 6, loss = 0.48004636\n",
      "Iteration 7, loss = 0.47764868\n",
      "Iteration 8, loss = 0.47622087\n",
      "Iteration 9, loss = 0.47461234\n",
      "Iteration 10, loss = 0.47539565\n",
      "Iteration 11, loss = 0.47206985\n",
      "Iteration 12, loss = 0.46897999\n",
      "Iteration 13, loss = 0.46951957\n",
      "Iteration 14, loss = 0.46698360\n",
      "Iteration 15, loss = 0.46403360\n",
      "Iteration 16, loss = 0.46237040\n",
      "Iteration 17, loss = 0.46124379\n",
      "Iteration 18, loss = 0.45932472\n",
      "Iteration 19, loss = 0.45835227\n",
      "Iteration 20, loss = 0.45834752\n",
      "Iteration 21, loss = 0.45580102\n",
      "Iteration 22, loss = 0.45559527\n",
      "Iteration 23, loss = 0.45781147\n",
      "Iteration 24, loss = 0.45624443\n",
      "Iteration 25, loss = 0.45437258\n",
      "Iteration 26, loss = 0.45137972\n",
      "Iteration 27, loss = 0.45015968\n",
      "Iteration 28, loss = 0.44913230\n",
      "Iteration 29, loss = 0.44653852\n",
      "Iteration 30, loss = 0.44774090\n",
      "Iteration 31, loss = 0.44667848\n",
      "Iteration 32, loss = 0.44516361\n",
      "Iteration 33, loss = 0.44287938\n",
      "Iteration 34, loss = 0.44020791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 35, loss = 0.43907110\n",
      "Iteration 36, loss = 0.43647124\n",
      "Iteration 37, loss = 0.43620605\n",
      "Iteration 38, loss = 0.43715026\n",
      "Iteration 39, loss = 0.43616922\n",
      "Iteration 40, loss = 0.43364647\n",
      "Iteration 41, loss = 0.43184216\n",
      "Iteration 42, loss = 0.43164855\n",
      "Iteration 43, loss = 0.42978544\n",
      "Iteration 44, loss = 0.43047991\n",
      "Iteration 45, loss = 0.42852914\n",
      "Iteration 46, loss = 0.42872063\n",
      "Iteration 47, loss = 0.42695208\n",
      "Iteration 48, loss = 0.43006358\n",
      "Iteration 49, loss = 0.43194531\n",
      "Iteration 50, loss = 0.43150381\n",
      "Iteration 51, loss = 0.42785396\n",
      "Iteration 52, loss = 0.42794224\n",
      "Iteration 53, loss = 0.42736003\n",
      "Iteration 54, loss = 0.42429109\n",
      "Iteration 55, loss = 0.42654111\n",
      "Iteration 56, loss = 0.42721175\n",
      "Iteration 57, loss = 0.42995097\n",
      "Iteration 58, loss = 0.42529661\n",
      "Iteration 59, loss = 0.42463179\n",
      "Iteration 60, loss = 0.42390945\n",
      "Iteration 61, loss = 0.42360260\n",
      "Iteration 62, loss = 0.42444254\n",
      "Iteration 63, loss = 0.42717992\n",
      "Iteration 64, loss = 0.42489872\n",
      "Iteration 65, loss = 0.42419933\n",
      "Iteration 66, loss = 0.42633199\n",
      "Iteration 67, loss = 0.42387648\n",
      "Iteration 68, loss = 0.42550186\n",
      "Iteration 69, loss = 0.42580589\n",
      "Iteration 70, loss = 0.42457406\n",
      "Iteration 71, loss = 0.42841659\n",
      "Iteration 72, loss = 0.42930313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56845592\n",
      "Iteration 2, loss = 0.49300245\n",
      "Iteration 3, loss = 0.48351419\n",
      "Iteration 4, loss = 0.47672074\n",
      "Iteration 5, loss = 0.47299690\n",
      "Iteration 6, loss = 0.47029198\n",
      "Iteration 7, loss = 0.46891625\n",
      "Iteration 8, loss = 0.46869674\n",
      "Iteration 9, loss = 0.46542763\n",
      "Iteration 10, loss = 0.46602369\n",
      "Iteration 11, loss = 0.46531634\n",
      "Iteration 12, loss = 0.46385264\n",
      "Iteration 13, loss = 0.46488776\n",
      "Iteration 14, loss = 0.46014359\n",
      "Iteration 15, loss = 0.46058000\n",
      "Iteration 16, loss = 0.45680943\n",
      "Iteration 17, loss = 0.45776947\n",
      "Iteration 18, loss = 0.45610646\n",
      "Iteration 19, loss = 0.45808444\n",
      "Iteration 20, loss = 0.45483451\n",
      "Iteration 21, loss = 0.45469207\n",
      "Iteration 22, loss = 0.45426228\n",
      "Iteration 23, loss = 0.45463843\n",
      "Iteration 24, loss = 0.45229658\n",
      "Iteration 25, loss = 0.45181518\n",
      "Iteration 26, loss = 0.45188695\n",
      "Iteration 27, loss = 0.45012242\n",
      "Iteration 28, loss = 0.44876257\n",
      "Iteration 29, loss = 0.44956704\n",
      "Iteration 30, loss = 0.44916811\n",
      "Iteration 31, loss = 0.44705594\n",
      "Iteration 32, loss = 0.44871867\n",
      "Iteration 33, loss = 0.44514635\n",
      "Iteration 34, loss = 0.44403885\n",
      "Iteration 35, loss = 0.44324880\n",
      "Iteration 36, loss = 0.44181447\n",
      "Iteration 37, loss = 0.44097114\n",
      "Iteration 38, loss = 0.44166524\n",
      "Iteration 39, loss = 0.44128254\n",
      "Iteration 40, loss = 0.43969590\n",
      "Iteration 41, loss = 0.43825653\n",
      "Iteration 42, loss = 0.43982991\n",
      "Iteration 43, loss = 0.43682293\n",
      "Iteration 44, loss = 0.43624028\n",
      "Iteration 45, loss = 0.43570069\n",
      "Iteration 46, loss = 0.43529450\n",
      "Iteration 47, loss = 0.43207493\n",
      "Iteration 48, loss = 0.43203736\n",
      "Iteration 49, loss = 0.43111818\n",
      "Iteration 50, loss = 0.42994592\n",
      "Iteration 51, loss = 0.43083240\n",
      "Iteration 52, loss = 0.42967738\n",
      "Iteration 53, loss = 0.42807476\n",
      "Iteration 54, loss = 0.42935250\n",
      "Iteration 55, loss = 0.43074401\n",
      "Iteration 56, loss = 0.42746059\n",
      "Iteration 57, loss = 0.42910421\n",
      "Iteration 58, loss = 0.42641641\n",
      "Iteration 59, loss = 0.42589865\n",
      "Iteration 60, loss = 0.42663346\n",
      "Iteration 61, loss = 0.42705680\n",
      "Iteration 62, loss = 0.42642370\n",
      "Iteration 63, loss = 0.42711486\n",
      "Iteration 64, loss = 0.42635970\n",
      "Iteration 65, loss = 0.42702334\n",
      "Iteration 66, loss = 0.42804824\n",
      "Iteration 67, loss = 0.42845581\n",
      "Iteration 68, loss = 0.42488844\n",
      "Iteration 69, loss = 0.42585913\n",
      "Iteration 70, loss = 0.42754034\n",
      "Iteration 71, loss = 0.42753494\n",
      "Iteration 72, loss = 0.42808577\n",
      "Iteration 73, loss = 0.42776737\n",
      "Iteration 74, loss = 0.42930614\n",
      "Iteration 75, loss = 0.42643422\n",
      "Iteration 76, loss = 0.43038596\n",
      "Iteration 77, loss = 0.42937627\n",
      "Iteration 78, loss = 0.42799961\n",
      "Iteration 79, loss = 0.42777511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56494074\n",
      "Iteration 2, loss = 0.49660826\n",
      "Iteration 3, loss = 0.48585900\n",
      "Iteration 4, loss = 0.47979446\n",
      "Iteration 5, loss = 0.47444059\n",
      "Iteration 6, loss = 0.47054912\n",
      "Iteration 7, loss = 0.46944910\n",
      "Iteration 8, loss = 0.47057310\n",
      "Iteration 9, loss = 0.46506781\n",
      "Iteration 10, loss = 0.46423911\n",
      "Iteration 11, loss = 0.46328617\n",
      "Iteration 12, loss = 0.46200165\n",
      "Iteration 13, loss = 0.46068034\n",
      "Iteration 14, loss = 0.45938543\n",
      "Iteration 15, loss = 0.45736796\n",
      "Iteration 16, loss = 0.45695343\n",
      "Iteration 17, loss = 0.45390068\n",
      "Iteration 18, loss = 0.45283987\n",
      "Iteration 19, loss = 0.45154992\n",
      "Iteration 20, loss = 0.44963632\n",
      "Iteration 21, loss = 0.44752070\n",
      "Iteration 22, loss = 0.44533699\n",
      "Iteration 23, loss = 0.44533187\n",
      "Iteration 24, loss = 0.44415781\n",
      "Iteration 25, loss = 0.44141190\n",
      "Iteration 26, loss = 0.44071509\n",
      "Iteration 27, loss = 0.44108025\n",
      "Iteration 28, loss = 0.43848871\n",
      "Iteration 29, loss = 0.44229560\n",
      "Iteration 30, loss = 0.43888776\n",
      "Iteration 31, loss = 0.43856336\n",
      "Iteration 32, loss = 0.43889406\n",
      "Iteration 33, loss = 0.43641027\n",
      "Iteration 34, loss = 0.43616218\n",
      "Iteration 35, loss = 0.43297276\n",
      "Iteration 36, loss = 0.43191727\n",
      "Iteration 37, loss = 0.43167648\n",
      "Iteration 38, loss = 0.43170725\n",
      "Iteration 39, loss = 0.43036041\n",
      "Iteration 40, loss = 0.43035543\n",
      "Iteration 41, loss = 0.43096081\n",
      "Iteration 42, loss = 0.42929214\n",
      "Iteration 43, loss = 0.42764968\n",
      "Iteration 44, loss = 0.42881167\n",
      "Iteration 45, loss = 0.42648652\n",
      "Iteration 46, loss = 0.43081653\n",
      "Iteration 47, loss = 0.42899542\n",
      "Iteration 48, loss = 0.42831592\n",
      "Iteration 49, loss = 0.42725014\n",
      "Iteration 50, loss = 0.42885568\n",
      "Iteration 51, loss = 0.42688057\n",
      "Iteration 52, loss = 0.42773119\n",
      "Iteration 53, loss = 0.42609197\n",
      "Iteration 54, loss = 0.42518527\n",
      "Iteration 55, loss = 0.42626103\n",
      "Iteration 56, loss = 0.42544063\n",
      "Iteration 57, loss = 0.42799481\n",
      "Iteration 58, loss = 0.42607140\n",
      "Iteration 59, loss = 0.42542985\n",
      "Iteration 60, loss = 0.42677193\n",
      "Iteration 61, loss = 0.42943991\n",
      "Iteration 62, loss = 0.42707455\n",
      "Iteration 63, loss = 0.43045454\n",
      "Iteration 64, loss = 0.43001931\n",
      "Iteration 65, loss = 0.42504226\n",
      "Iteration 66, loss = 0.42818612\n",
      "Iteration 67, loss = 0.42726494\n",
      "Iteration 68, loss = 0.42683832\n",
      "Iteration 69, loss = 0.42508038\n",
      "Iteration 70, loss = 0.42401171\n",
      "Iteration 71, loss = 0.42457283\n",
      "Iteration 72, loss = 0.42380628\n",
      "Iteration 73, loss = 0.42359177\n",
      "Iteration 74, loss = 0.42409927\n",
      "Iteration 75, loss = 0.42534799\n",
      "Iteration 76, loss = 0.42381937\n",
      "Iteration 77, loss = 0.42409228\n",
      "Iteration 78, loss = 0.42502230\n",
      "Iteration 79, loss = 0.42468446\n",
      "Iteration 80, loss = 0.42137206\n",
      "Iteration 81, loss = 0.42215143\n",
      "Iteration 82, loss = 0.42378293\n",
      "Iteration 83, loss = 0.42335941\n",
      "Iteration 84, loss = 0.42191169\n",
      "Iteration 85, loss = 0.42175710\n",
      "Iteration 86, loss = 0.42445737\n",
      "Iteration 87, loss = 0.42522151\n",
      "Iteration 88, loss = 0.42374968\n",
      "Iteration 89, loss = 0.42397563\n",
      "Iteration 90, loss = 0.42539889\n",
      "Iteration 91, loss = 0.42369081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 3 min. and 1.9300258159637451 sec.\n",
      "\n",
      "7 total combinations for subset length 6.\n",
      "Iteration 1, loss = 0.57377840\n",
      "Iteration 2, loss = 0.50839639\n",
      "Iteration 3, loss = 0.49721693\n",
      "Iteration 4, loss = 0.49060687\n",
      "Iteration 5, loss = 0.48859896\n",
      "Iteration 6, loss = 0.48093577\n",
      "Iteration 7, loss = 0.47938930\n",
      "Iteration 8, loss = 0.47761786\n",
      "Iteration 9, loss = 0.47567216\n",
      "Iteration 10, loss = 0.46993163\n",
      "Iteration 11, loss = 0.46994371\n",
      "Iteration 12, loss = 0.46681875\n",
      "Iteration 13, loss = 0.46416285\n",
      "Iteration 14, loss = 0.46236532\n",
      "Iteration 15, loss = 0.46157276\n",
      "Iteration 16, loss = 0.46048842\n",
      "Iteration 17, loss = 0.45685223\n",
      "Iteration 18, loss = 0.45850853\n",
      "Iteration 19, loss = 0.45651663\n",
      "Iteration 20, loss = 0.45607661\n",
      "Iteration 21, loss = 0.45521926\n",
      "Iteration 22, loss = 0.45407269\n",
      "Iteration 23, loss = 0.45547599\n",
      "Iteration 24, loss = 0.45109598\n",
      "Iteration 25, loss = 0.45208777\n",
      "Iteration 26, loss = 0.44893924\n",
      "Iteration 27, loss = 0.44365396\n",
      "Iteration 28, loss = 0.44567616\n",
      "Iteration 29, loss = 0.44159605\n",
      "Iteration 30, loss = 0.44184294\n",
      "Iteration 31, loss = 0.43952719\n",
      "Iteration 32, loss = 0.43838323\n",
      "Iteration 33, loss = 0.43886225\n",
      "Iteration 34, loss = 0.43431052\n",
      "Iteration 35, loss = 0.43533820\n",
      "Iteration 36, loss = 0.43586648\n",
      "Iteration 37, loss = 0.43255826\n",
      "Iteration 38, loss = 0.43515279\n",
      "Iteration 39, loss = 0.43366231\n",
      "Iteration 40, loss = 0.43279170\n",
      "Iteration 41, loss = 0.43295328\n",
      "Iteration 42, loss = 0.43255087\n",
      "Iteration 43, loss = 0.43368076\n",
      "Iteration 44, loss = 0.43232997\n",
      "Iteration 45, loss = 0.43088723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.43222846\n",
      "Iteration 47, loss = 0.43517685\n",
      "Iteration 48, loss = 0.43144213\n",
      "Iteration 49, loss = 0.43384870\n",
      "Iteration 50, loss = 0.43185958\n",
      "Iteration 51, loss = 0.43068697\n",
      "Iteration 52, loss = 0.43048629\n",
      "Iteration 53, loss = 0.43239991\n",
      "Iteration 54, loss = 0.43281868\n",
      "Iteration 55, loss = 0.43270412\n",
      "Iteration 56, loss = 0.43245670\n",
      "Iteration 57, loss = 0.42962579\n",
      "Iteration 58, loss = 0.42943252\n",
      "Iteration 59, loss = 0.43261199\n",
      "Iteration 60, loss = 0.43088816\n",
      "Iteration 61, loss = 0.42832344\n",
      "Iteration 62, loss = 0.43051312\n",
      "Iteration 63, loss = 0.42886921\n",
      "Iteration 64, loss = 0.43341480\n",
      "Iteration 65, loss = 0.42918462\n",
      "Iteration 66, loss = 0.42957724\n",
      "Iteration 67, loss = 0.42766534\n",
      "Iteration 68, loss = 0.42662921\n",
      "Iteration 69, loss = 0.43270564\n",
      "Iteration 70, loss = 0.43139654\n",
      "Iteration 71, loss = 0.42765833\n",
      "Iteration 72, loss = 0.42591702\n",
      "Iteration 73, loss = 0.42833809\n",
      "Iteration 74, loss = 0.42483463\n",
      "Iteration 75, loss = 0.42839280\n",
      "Iteration 76, loss = 0.42636967\n",
      "Iteration 77, loss = 0.42740921\n",
      "Iteration 78, loss = 0.42666460\n",
      "Iteration 79, loss = 0.42604622\n",
      "Iteration 80, loss = 0.42332958\n",
      "Iteration 81, loss = 0.42760872\n",
      "Iteration 82, loss = 0.42700645\n",
      "Iteration 83, loss = 0.42711053\n",
      "Iteration 84, loss = 0.42941024\n",
      "Iteration 85, loss = 0.42659858\n",
      "Iteration 86, loss = 0.42597748\n",
      "Iteration 87, loss = 0.42599581\n",
      "Iteration 88, loss = 0.42441393\n",
      "Iteration 89, loss = 0.42544436\n",
      "Iteration 90, loss = 0.43061444\n",
      "Iteration 91, loss = 0.42499664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58026526\n",
      "Iteration 2, loss = 0.50415601\n",
      "Iteration 3, loss = 0.49006009\n",
      "Iteration 4, loss = 0.48546783\n",
      "Iteration 5, loss = 0.48069605\n",
      "Iteration 6, loss = 0.47308989\n",
      "Iteration 7, loss = 0.47048617\n",
      "Iteration 8, loss = 0.46744593\n",
      "Iteration 9, loss = 0.46391273\n",
      "Iteration 10, loss = 0.46228481\n",
      "Iteration 11, loss = 0.46032853\n",
      "Iteration 12, loss = 0.45927740\n",
      "Iteration 13, loss = 0.46039524\n",
      "Iteration 14, loss = 0.45665732\n",
      "Iteration 15, loss = 0.45616281\n",
      "Iteration 16, loss = 0.45688330\n",
      "Iteration 17, loss = 0.45332567\n",
      "Iteration 18, loss = 0.45360937\n",
      "Iteration 19, loss = 0.45165416\n",
      "Iteration 20, loss = 0.45131446\n",
      "Iteration 21, loss = 0.45132226\n",
      "Iteration 22, loss = 0.45023662\n",
      "Iteration 23, loss = 0.44815957\n",
      "Iteration 24, loss = 0.44682487\n",
      "Iteration 25, loss = 0.44928277\n",
      "Iteration 26, loss = 0.44497107\n",
      "Iteration 27, loss = 0.44731503\n",
      "Iteration 28, loss = 0.44597513\n",
      "Iteration 29, loss = 0.44254728\n",
      "Iteration 30, loss = 0.44297614\n",
      "Iteration 31, loss = 0.43824096\n",
      "Iteration 32, loss = 0.43906469\n",
      "Iteration 33, loss = 0.43744373\n",
      "Iteration 34, loss = 0.43634144\n",
      "Iteration 35, loss = 0.43642991\n",
      "Iteration 36, loss = 0.43497649\n",
      "Iteration 37, loss = 0.43351697\n",
      "Iteration 38, loss = 0.43313489\n",
      "Iteration 39, loss = 0.43191342\n",
      "Iteration 40, loss = 0.43133789\n",
      "Iteration 41, loss = 0.42895074\n",
      "Iteration 42, loss = 0.42730501\n",
      "Iteration 43, loss = 0.42855417\n",
      "Iteration 44, loss = 0.42716232\n",
      "Iteration 45, loss = 0.42803006\n",
      "Iteration 46, loss = 0.42699035\n",
      "Iteration 47, loss = 0.42807355\n",
      "Iteration 48, loss = 0.42557665\n",
      "Iteration 49, loss = 0.42648841\n",
      "Iteration 50, loss = 0.42779616\n",
      "Iteration 51, loss = 0.42764832\n",
      "Iteration 52, loss = 0.42549277\n",
      "Iteration 53, loss = 0.42623623\n",
      "Iteration 54, loss = 0.42480373\n",
      "Iteration 55, loss = 0.42445554\n",
      "Iteration 56, loss = 0.42514924\n",
      "Iteration 57, loss = 0.42542138\n",
      "Iteration 58, loss = 0.42658357\n",
      "Iteration 59, loss = 0.42282745\n",
      "Iteration 60, loss = 0.42387059\n",
      "Iteration 61, loss = 0.42451713\n",
      "Iteration 62, loss = 0.42420769\n",
      "Iteration 63, loss = 0.42499117\n",
      "Iteration 64, loss = 0.42275430\n",
      "Iteration 65, loss = 0.42215489\n",
      "Iteration 66, loss = 0.42562373\n",
      "Iteration 67, loss = 0.42440181\n",
      "Iteration 68, loss = 0.42242970\n",
      "Iteration 69, loss = 0.42416304\n",
      "Iteration 70, loss = 0.42527530\n",
      "Iteration 71, loss = 0.42618465\n",
      "Iteration 72, loss = 0.42438808\n",
      "Iteration 73, loss = 0.42456266\n",
      "Iteration 74, loss = 0.42343339\n",
      "Iteration 75, loss = 0.42316886\n",
      "Iteration 76, loss = 0.42402462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57513247\n",
      "Iteration 2, loss = 0.50246932\n",
      "Iteration 3, loss = 0.48938988\n",
      "Iteration 4, loss = 0.48293212\n",
      "Iteration 5, loss = 0.47864060\n",
      "Iteration 6, loss = 0.47472594\n",
      "Iteration 7, loss = 0.47347230\n",
      "Iteration 8, loss = 0.47079265\n",
      "Iteration 9, loss = 0.47010023\n",
      "Iteration 10, loss = 0.46665925\n",
      "Iteration 11, loss = 0.46553605\n",
      "Iteration 12, loss = 0.46601070\n",
      "Iteration 13, loss = 0.46606368\n",
      "Iteration 14, loss = 0.46328721\n",
      "Iteration 15, loss = 0.46131402\n",
      "Iteration 16, loss = 0.45983631\n",
      "Iteration 17, loss = 0.45971524\n",
      "Iteration 18, loss = 0.46039658\n",
      "Iteration 19, loss = 0.45889404\n",
      "Iteration 20, loss = 0.45684384\n",
      "Iteration 21, loss = 0.45156989\n",
      "Iteration 22, loss = 0.45127227\n",
      "Iteration 23, loss = 0.44678162\n",
      "Iteration 24, loss = 0.44422495\n",
      "Iteration 25, loss = 0.44450852\n",
      "Iteration 26, loss = 0.44238390\n",
      "Iteration 27, loss = 0.44372822\n",
      "Iteration 28, loss = 0.44091800\n",
      "Iteration 29, loss = 0.43849635\n",
      "Iteration 30, loss = 0.43759638\n",
      "Iteration 31, loss = 0.43970100\n",
      "Iteration 32, loss = 0.43712044\n",
      "Iteration 33, loss = 0.44127672\n",
      "Iteration 34, loss = 0.43843041\n",
      "Iteration 35, loss = 0.44048628\n",
      "Iteration 36, loss = 0.44093722\n",
      "Iteration 37, loss = 0.43837682\n",
      "Iteration 38, loss = 0.43542495\n",
      "Iteration 39, loss = 0.43737744\n",
      "Iteration 40, loss = 0.43605281\n",
      "Iteration 41, loss = 0.43644962\n",
      "Iteration 42, loss = 0.43838431\n",
      "Iteration 43, loss = 0.43855737\n",
      "Iteration 44, loss = 0.43666322\n",
      "Iteration 45, loss = 0.43452952\n",
      "Iteration 46, loss = 0.43675023\n",
      "Iteration 47, loss = 0.43537548\n",
      "Iteration 48, loss = 0.43366304\n",
      "Iteration 49, loss = 0.43575848\n",
      "Iteration 50, loss = 0.43282860\n",
      "Iteration 51, loss = 0.43280464\n",
      "Iteration 52, loss = 0.43093398\n",
      "Iteration 53, loss = 0.43348091\n",
      "Iteration 54, loss = 0.43345105\n",
      "Iteration 55, loss = 0.43088432\n",
      "Iteration 56, loss = 0.43062312\n",
      "Iteration 57, loss = 0.43095066\n",
      "Iteration 58, loss = 0.43180555\n",
      "Iteration 59, loss = 0.43468098\n",
      "Iteration 60, loss = 0.43308495\n",
      "Iteration 61, loss = 0.43091995\n",
      "Iteration 62, loss = 0.43439919\n",
      "Iteration 63, loss = 0.43297383\n",
      "Iteration 64, loss = 0.43139873\n",
      "Iteration 65, loss = 0.43283251\n",
      "Iteration 66, loss = 0.43202120\n",
      "Iteration 67, loss = 0.43285244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58153212\n",
      "Iteration 2, loss = 0.50509132\n",
      "Iteration 3, loss = 0.49132220\n",
      "Iteration 4, loss = 0.48574040\n",
      "Iteration 5, loss = 0.48403165\n",
      "Iteration 6, loss = 0.47686388\n",
      "Iteration 7, loss = 0.47434099\n",
      "Iteration 8, loss = 0.47321181\n",
      "Iteration 9, loss = 0.47245582\n",
      "Iteration 10, loss = 0.47043902\n",
      "Iteration 11, loss = 0.46860668\n",
      "Iteration 12, loss = 0.46516875\n",
      "Iteration 13, loss = 0.45926675\n",
      "Iteration 14, loss = 0.45568535\n",
      "Iteration 15, loss = 0.45379502\n",
      "Iteration 16, loss = 0.45232118\n",
      "Iteration 17, loss = 0.44761389\n",
      "Iteration 18, loss = 0.44464679\n",
      "Iteration 19, loss = 0.44396211\n",
      "Iteration 20, loss = 0.44331480\n",
      "Iteration 21, loss = 0.44581736\n",
      "Iteration 22, loss = 0.44238321\n",
      "Iteration 23, loss = 0.43883351\n",
      "Iteration 24, loss = 0.43524349\n",
      "Iteration 25, loss = 0.43775546\n",
      "Iteration 26, loss = 0.43281251\n",
      "Iteration 27, loss = 0.43120793\n",
      "Iteration 28, loss = 0.43378157\n",
      "Iteration 29, loss = 0.42991111\n",
      "Iteration 30, loss = 0.43180181\n",
      "Iteration 31, loss = 0.43206168\n",
      "Iteration 32, loss = 0.43677869\n",
      "Iteration 33, loss = 0.43056504\n",
      "Iteration 34, loss = 0.42868616\n",
      "Iteration 35, loss = 0.42760621\n",
      "Iteration 36, loss = 0.42468261\n",
      "Iteration 37, loss = 0.42583579\n",
      "Iteration 38, loss = 0.42706764\n",
      "Iteration 39, loss = 0.42524004\n",
      "Iteration 40, loss = 0.42530440\n",
      "Iteration 41, loss = 0.42636472\n",
      "Iteration 42, loss = 0.42880072\n",
      "Iteration 43, loss = 0.42421033\n",
      "Iteration 44, loss = 0.42397113\n",
      "Iteration 45, loss = 0.42376743\n",
      "Iteration 46, loss = 0.42429787\n",
      "Iteration 47, loss = 0.42200446\n",
      "Iteration 48, loss = 0.42662295\n",
      "Iteration 49, loss = 0.42516120\n",
      "Iteration 50, loss = 0.42237051\n",
      "Iteration 51, loss = 0.42103315\n",
      "Iteration 52, loss = 0.42078384\n",
      "Iteration 53, loss = 0.42467340\n",
      "Iteration 54, loss = 0.42221236\n",
      "Iteration 55, loss = 0.41993328\n",
      "Iteration 56, loss = 0.42224737\n",
      "Iteration 57, loss = 0.42045227\n",
      "Iteration 58, loss = 0.41777130\n",
      "Iteration 59, loss = 0.42175059\n",
      "Iteration 60, loss = 0.41860797\n",
      "Iteration 61, loss = 0.41952358\n",
      "Iteration 62, loss = 0.41870954\n",
      "Iteration 63, loss = 0.41767124\n",
      "Iteration 64, loss = 0.42096746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 65, loss = 0.41850764\n",
      "Iteration 66, loss = 0.42027375\n",
      "Iteration 67, loss = 0.41810271\n",
      "Iteration 68, loss = 0.41701807\n",
      "Iteration 69, loss = 0.42144287\n",
      "Iteration 70, loss = 0.41653109\n",
      "Iteration 71, loss = 0.41971799\n",
      "Iteration 72, loss = 0.42063526\n",
      "Iteration 73, loss = 0.41815122\n",
      "Iteration 74, loss = 0.41973864\n",
      "Iteration 75, loss = 0.42057627\n",
      "Iteration 76, loss = 0.41625656\n",
      "Iteration 77, loss = 0.41897454\n",
      "Iteration 78, loss = 0.41941413\n",
      "Iteration 79, loss = 0.41724605\n",
      "Iteration 80, loss = 0.41563962\n",
      "Iteration 81, loss = 0.41732773\n",
      "Iteration 82, loss = 0.42047464\n",
      "Iteration 83, loss = 0.41623885\n",
      "Iteration 84, loss = 0.41712349\n",
      "Iteration 85, loss = 0.41585190\n",
      "Iteration 86, loss = 0.41753775\n",
      "Iteration 87, loss = 0.41744064\n",
      "Iteration 88, loss = 0.41569627\n",
      "Iteration 89, loss = 0.41877493\n",
      "Iteration 90, loss = 0.41922107\n",
      "Iteration 91, loss = 0.41496519\n",
      "Iteration 92, loss = 0.41517015\n",
      "Iteration 93, loss = 0.41609383\n",
      "Iteration 94, loss = 0.41591347\n",
      "Iteration 95, loss = 0.41778539\n",
      "Iteration 96, loss = 0.41659866\n",
      "Iteration 97, loss = 0.41936927\n",
      "Iteration 98, loss = 0.41743248\n",
      "Iteration 99, loss = 0.41647824\n",
      "Iteration 100, loss = 0.41384019\n",
      "Iteration 101, loss = 0.41524970\n",
      "Iteration 102, loss = 0.41492007\n",
      "Iteration 103, loss = 0.41814182\n",
      "Iteration 104, loss = 0.41832703\n",
      "Iteration 105, loss = 0.42033535\n",
      "Iteration 106, loss = 0.41731465\n",
      "Iteration 107, loss = 0.41657857\n",
      "Iteration 108, loss = 0.41772449\n",
      "Iteration 109, loss = 0.41953221\n",
      "Iteration 110, loss = 0.41549855\n",
      "Iteration 111, loss = 0.41563932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58671776\n",
      "Iteration 2, loss = 0.50369497\n",
      "Iteration 3, loss = 0.48613660\n",
      "Iteration 4, loss = 0.48066568\n",
      "Iteration 5, loss = 0.47751503\n",
      "Iteration 6, loss = 0.47228251\n",
      "Iteration 7, loss = 0.47220442\n",
      "Iteration 8, loss = 0.47210401\n",
      "Iteration 9, loss = 0.46656012\n",
      "Iteration 10, loss = 0.46184145\n",
      "Iteration 11, loss = 0.45692239\n",
      "Iteration 12, loss = 0.45154728\n",
      "Iteration 13, loss = 0.45030630\n",
      "Iteration 14, loss = 0.44666099\n",
      "Iteration 15, loss = 0.44562870\n",
      "Iteration 16, loss = 0.44602411\n",
      "Iteration 17, loss = 0.44232550\n",
      "Iteration 18, loss = 0.44309159\n",
      "Iteration 19, loss = 0.44188853\n",
      "Iteration 20, loss = 0.44413286\n",
      "Iteration 21, loss = 0.44184812\n",
      "Iteration 22, loss = 0.44423179\n",
      "Iteration 23, loss = 0.44056299\n",
      "Iteration 24, loss = 0.44058381\n",
      "Iteration 25, loss = 0.44079896\n",
      "Iteration 26, loss = 0.43716387\n",
      "Iteration 27, loss = 0.44048880\n",
      "Iteration 28, loss = 0.44117699\n",
      "Iteration 29, loss = 0.44268443\n",
      "Iteration 30, loss = 0.43833191\n",
      "Iteration 31, loss = 0.43350773\n",
      "Iteration 32, loss = 0.43492327\n",
      "Iteration 33, loss = 0.43903426\n",
      "Iteration 34, loss = 0.43537974\n",
      "Iteration 35, loss = 0.43322007\n",
      "Iteration 36, loss = 0.43602603\n",
      "Iteration 37, loss = 0.43417212\n",
      "Iteration 38, loss = 0.43341186\n",
      "Iteration 39, loss = 0.43193297\n",
      "Iteration 40, loss = 0.43128264\n",
      "Iteration 41, loss = 0.43040616\n",
      "Iteration 42, loss = 0.43052851\n",
      "Iteration 43, loss = 0.43067187\n",
      "Iteration 44, loss = 0.42923210\n",
      "Iteration 45, loss = 0.43068000\n",
      "Iteration 46, loss = 0.43064163\n",
      "Iteration 47, loss = 0.42926874\n",
      "Iteration 48, loss = 0.42944642\n",
      "Iteration 49, loss = 0.43065297\n",
      "Iteration 50, loss = 0.43032306\n",
      "Iteration 51, loss = 0.42925259\n",
      "Iteration 52, loss = 0.42681802\n",
      "Iteration 53, loss = 0.42823109\n",
      "Iteration 54, loss = 0.42776975\n",
      "Iteration 55, loss = 0.42839721\n",
      "Iteration 56, loss = 0.42639382\n",
      "Iteration 57, loss = 0.42816602\n",
      "Iteration 58, loss = 0.43038811\n",
      "Iteration 59, loss = 0.42706087\n",
      "Iteration 60, loss = 0.42815538\n",
      "Iteration 61, loss = 0.42689784\n",
      "Iteration 62, loss = 0.42718491\n",
      "Iteration 63, loss = 0.42922448\n",
      "Iteration 64, loss = 0.42617405\n",
      "Iteration 65, loss = 0.42615260\n",
      "Iteration 66, loss = 0.42875069\n",
      "Iteration 67, loss = 0.42631056\n",
      "Iteration 68, loss = 0.42445957\n",
      "Iteration 69, loss = 0.42698223\n",
      "Iteration 70, loss = 0.43123561\n",
      "Iteration 71, loss = 0.42703831\n",
      "Iteration 72, loss = 0.42621296\n",
      "Iteration 73, loss = 0.42618875\n",
      "Iteration 74, loss = 0.42577345\n",
      "Iteration 75, loss = 0.42660965\n",
      "Iteration 76, loss = 0.42851530\n",
      "Iteration 77, loss = 0.42660595\n",
      "Iteration 78, loss = 0.42786751\n",
      "Iteration 79, loss = 0.42565746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57891696\n",
      "Iteration 2, loss = 0.50020692\n",
      "Iteration 3, loss = 0.48447053\n",
      "Iteration 4, loss = 0.47850995\n",
      "Iteration 5, loss = 0.47548191\n",
      "Iteration 6, loss = 0.47213803\n",
      "Iteration 7, loss = 0.47039589\n",
      "Iteration 8, loss = 0.46665918\n",
      "Iteration 9, loss = 0.46324252\n",
      "Iteration 10, loss = 0.45880340\n",
      "Iteration 11, loss = 0.45680189\n",
      "Iteration 12, loss = 0.45447940\n",
      "Iteration 13, loss = 0.45470745\n",
      "Iteration 14, loss = 0.45241316\n",
      "Iteration 15, loss = 0.45357341\n",
      "Iteration 16, loss = 0.44848858\n",
      "Iteration 17, loss = 0.44832115\n",
      "Iteration 18, loss = 0.44732123\n",
      "Iteration 19, loss = 0.44529353\n",
      "Iteration 20, loss = 0.44534587\n",
      "Iteration 21, loss = 0.44206902\n",
      "Iteration 22, loss = 0.44224254\n",
      "Iteration 23, loss = 0.43689844\n",
      "Iteration 24, loss = 0.43654047\n",
      "Iteration 25, loss = 0.43429728\n",
      "Iteration 26, loss = 0.43254580\n",
      "Iteration 27, loss = 0.43047563\n",
      "Iteration 28, loss = 0.43078672\n",
      "Iteration 29, loss = 0.42725644\n",
      "Iteration 30, loss = 0.42783701\n",
      "Iteration 31, loss = 0.42805348\n",
      "Iteration 32, loss = 0.42679004\n",
      "Iteration 33, loss = 0.42908510\n",
      "Iteration 34, loss = 0.43082053\n",
      "Iteration 35, loss = 0.42985814\n",
      "Iteration 36, loss = 0.43033509\n",
      "Iteration 37, loss = 0.42952990\n",
      "Iteration 38, loss = 0.42752470\n",
      "Iteration 39, loss = 0.42399931\n",
      "Iteration 40, loss = 0.42504573\n",
      "Iteration 41, loss = 0.42458637\n",
      "Iteration 42, loss = 0.42841111\n",
      "Iteration 43, loss = 0.42759681\n",
      "Iteration 44, loss = 0.42426284\n",
      "Iteration 45, loss = 0.42514243\n",
      "Iteration 46, loss = 0.42420048\n",
      "Iteration 47, loss = 0.42452286\n",
      "Iteration 48, loss = 0.42556695\n",
      "Iteration 49, loss = 0.42443449\n",
      "Iteration 50, loss = 0.42324679\n",
      "Iteration 51, loss = 0.42275167\n",
      "Iteration 52, loss = 0.42301319\n",
      "Iteration 53, loss = 0.42311151\n",
      "Iteration 54, loss = 0.42218057\n",
      "Iteration 55, loss = 0.42216593\n",
      "Iteration 56, loss = 0.42363000\n",
      "Iteration 57, loss = 0.42599605\n",
      "Iteration 58, loss = 0.42346657\n",
      "Iteration 59, loss = 0.42455662\n",
      "Iteration 60, loss = 0.42199173\n",
      "Iteration 61, loss = 0.42360834\n",
      "Iteration 62, loss = 0.42349851\n",
      "Iteration 63, loss = 0.42271364\n",
      "Iteration 64, loss = 0.42312121\n",
      "Iteration 65, loss = 0.42376112\n",
      "Iteration 66, loss = 0.42402888\n",
      "Iteration 67, loss = 0.42598141\n",
      "Iteration 68, loss = 0.42506560\n",
      "Iteration 69, loss = 0.42195355\n",
      "Iteration 70, loss = 0.42300044\n",
      "Iteration 71, loss = 0.42215290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57114388\n",
      "Iteration 2, loss = 0.50712880\n",
      "Iteration 3, loss = 0.49111128\n",
      "Iteration 4, loss = 0.48225224\n",
      "Iteration 5, loss = 0.48192635\n",
      "Iteration 6, loss = 0.47385249\n",
      "Iteration 7, loss = 0.47348354\n",
      "Iteration 8, loss = 0.47157984\n",
      "Iteration 9, loss = 0.47103623\n",
      "Iteration 10, loss = 0.46639629\n",
      "Iteration 11, loss = 0.46690235\n",
      "Iteration 12, loss = 0.46639040\n",
      "Iteration 13, loss = 0.46413469\n",
      "Iteration 14, loss = 0.46166982\n",
      "Iteration 15, loss = 0.46180431\n",
      "Iteration 16, loss = 0.46038506\n",
      "Iteration 17, loss = 0.45802302\n",
      "Iteration 18, loss = 0.45811468\n",
      "Iteration 19, loss = 0.45576728\n",
      "Iteration 20, loss = 0.45602834\n",
      "Iteration 21, loss = 0.45451533\n",
      "Iteration 22, loss = 0.45246402\n",
      "Iteration 23, loss = 0.45199238\n",
      "Iteration 24, loss = 0.44854902\n",
      "Iteration 25, loss = 0.44681290\n",
      "Iteration 26, loss = 0.44663294\n",
      "Iteration 27, loss = 0.44313465\n",
      "Iteration 28, loss = 0.44230575\n",
      "Iteration 29, loss = 0.43996896\n",
      "Iteration 30, loss = 0.44126758\n",
      "Iteration 31, loss = 0.43906068\n",
      "Iteration 32, loss = 0.43980650\n",
      "Iteration 33, loss = 0.43771488\n",
      "Iteration 34, loss = 0.43694494\n",
      "Iteration 35, loss = 0.43886057\n",
      "Iteration 36, loss = 0.43900394\n",
      "Iteration 37, loss = 0.43469084\n",
      "Iteration 38, loss = 0.43697788\n",
      "Iteration 39, loss = 0.43400971\n",
      "Iteration 40, loss = 0.43433849\n",
      "Iteration 41, loss = 0.43380950\n",
      "Iteration 42, loss = 0.43548429\n",
      "Iteration 43, loss = 0.43446577\n",
      "Iteration 44, loss = 0.43415938\n",
      "Iteration 45, loss = 0.43545538\n",
      "Iteration 46, loss = 0.43484435\n",
      "Iteration 47, loss = 0.43126691\n",
      "Iteration 48, loss = 0.42885031\n",
      "Iteration 49, loss = 0.43100908\n",
      "Iteration 50, loss = 0.42951463\n",
      "Iteration 51, loss = 0.43031164\n",
      "Iteration 52, loss = 0.42873661\n",
      "Iteration 53, loss = 0.43251480\n",
      "Iteration 54, loss = 0.42575133\n",
      "Iteration 55, loss = 0.42603439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 56, loss = 0.42852454\n",
      "Iteration 57, loss = 0.42617774\n",
      "Iteration 58, loss = 0.42348855\n",
      "Iteration 59, loss = 0.42722494\n",
      "Iteration 60, loss = 0.42392721\n",
      "Iteration 61, loss = 0.42350277\n",
      "Iteration 62, loss = 0.42418210\n",
      "Iteration 63, loss = 0.42424369\n",
      "Iteration 64, loss = 0.42477591\n",
      "Iteration 65, loss = 0.42424929\n",
      "Iteration 66, loss = 0.42246869\n",
      "Iteration 67, loss = 0.42236272\n",
      "Iteration 68, loss = 0.42380349\n",
      "Iteration 69, loss = 0.42589556\n",
      "Iteration 70, loss = 0.42686021\n",
      "Iteration 71, loss = 0.42402702\n",
      "Iteration 72, loss = 0.42185938\n",
      "Iteration 73, loss = 0.42389534\n",
      "Iteration 74, loss = 0.42376154\n",
      "Iteration 75, loss = 0.42512347\n",
      "Iteration 76, loss = 0.42430731\n",
      "Iteration 77, loss = 0.42280327\n",
      "Iteration 78, loss = 0.42260136\n",
      "Iteration 79, loss = 0.42319965\n",
      "Iteration 80, loss = 0.42208349\n",
      "Iteration 81, loss = 0.42225807\n",
      "Iteration 82, loss = 0.42375152\n",
      "Iteration 83, loss = 0.42272586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57843336\n",
      "Iteration 2, loss = 0.50499363\n",
      "Iteration 3, loss = 0.48548886\n",
      "Iteration 4, loss = 0.48055551\n",
      "Iteration 5, loss = 0.47634605\n",
      "Iteration 6, loss = 0.47162190\n",
      "Iteration 7, loss = 0.47137131\n",
      "Iteration 8, loss = 0.47283602\n",
      "Iteration 9, loss = 0.46968296\n",
      "Iteration 10, loss = 0.46607386\n",
      "Iteration 11, loss = 0.46606730\n",
      "Iteration 12, loss = 0.46387990\n",
      "Iteration 13, loss = 0.46391176\n",
      "Iteration 14, loss = 0.46042058\n",
      "Iteration 15, loss = 0.45872057\n",
      "Iteration 16, loss = 0.45898438\n",
      "Iteration 17, loss = 0.45559226\n",
      "Iteration 18, loss = 0.45536605\n",
      "Iteration 19, loss = 0.45321881\n",
      "Iteration 20, loss = 0.45301252\n",
      "Iteration 21, loss = 0.45198603\n",
      "Iteration 22, loss = 0.45002792\n",
      "Iteration 23, loss = 0.44497293\n",
      "Iteration 24, loss = 0.44487713\n",
      "Iteration 25, loss = 0.44792518\n",
      "Iteration 26, loss = 0.44128372\n",
      "Iteration 27, loss = 0.44190489\n",
      "Iteration 28, loss = 0.44469300\n",
      "Iteration 29, loss = 0.43870762\n",
      "Iteration 30, loss = 0.43862703\n",
      "Iteration 31, loss = 0.43378537\n",
      "Iteration 32, loss = 0.43868715\n",
      "Iteration 33, loss = 0.44248430\n",
      "Iteration 34, loss = 0.43909011\n",
      "Iteration 35, loss = 0.43469686\n",
      "Iteration 36, loss = 0.43966268\n",
      "Iteration 37, loss = 0.43057878\n",
      "Iteration 38, loss = 0.42717017\n",
      "Iteration 39, loss = 0.42605827\n",
      "Iteration 40, loss = 0.42411044\n",
      "Iteration 41, loss = 0.42381421\n",
      "Iteration 42, loss = 0.42290882\n",
      "Iteration 43, loss = 0.42311326\n",
      "Iteration 44, loss = 0.42179863\n",
      "Iteration 45, loss = 0.42122395\n",
      "Iteration 46, loss = 0.41939336\n",
      "Iteration 47, loss = 0.41908861\n",
      "Iteration 48, loss = 0.41776325\n",
      "Iteration 49, loss = 0.41927470\n",
      "Iteration 50, loss = 0.41735354\n",
      "Iteration 51, loss = 0.41670068\n",
      "Iteration 52, loss = 0.41742506\n",
      "Iteration 53, loss = 0.41762023\n",
      "Iteration 54, loss = 0.41364814\n",
      "Iteration 55, loss = 0.41601546\n",
      "Iteration 56, loss = 0.41459850\n",
      "Iteration 57, loss = 0.41952709\n",
      "Iteration 58, loss = 0.41503616\n",
      "Iteration 59, loss = 0.41344386\n",
      "Iteration 60, loss = 0.41265052\n",
      "Iteration 61, loss = 0.41195027\n",
      "Iteration 62, loss = 0.41399352\n",
      "Iteration 63, loss = 0.41403722\n",
      "Iteration 64, loss = 0.41127667\n",
      "Iteration 65, loss = 0.41223613\n",
      "Iteration 66, loss = 0.41243736\n",
      "Iteration 67, loss = 0.41022160\n",
      "Iteration 68, loss = 0.41047092\n",
      "Iteration 69, loss = 0.41140973\n",
      "Iteration 70, loss = 0.41248367\n",
      "Iteration 71, loss = 0.41054544\n",
      "Iteration 72, loss = 0.41057558\n",
      "Iteration 73, loss = 0.41038403\n",
      "Iteration 74, loss = 0.41107556\n",
      "Iteration 75, loss = 0.41183446\n",
      "Iteration 76, loss = 0.41079900\n",
      "Iteration 77, loss = 0.40999576\n",
      "Iteration 78, loss = 0.40934132\n",
      "Iteration 79, loss = 0.41084756\n",
      "Iteration 80, loss = 0.41081208\n",
      "Iteration 81, loss = 0.41071180\n",
      "Iteration 82, loss = 0.40971224\n",
      "Iteration 83, loss = 0.41356784\n",
      "Iteration 84, loss = 0.40953212\n",
      "Iteration 85, loss = 0.41002782\n",
      "Iteration 86, loss = 0.40873218\n",
      "Iteration 87, loss = 0.40915708\n",
      "Iteration 88, loss = 0.40930471\n",
      "Iteration 89, loss = 0.40986442\n",
      "Iteration 90, loss = 0.40852554\n",
      "Iteration 91, loss = 0.40860372\n",
      "Iteration 92, loss = 0.40737883\n",
      "Iteration 93, loss = 0.40717521\n",
      "Iteration 94, loss = 0.40786388\n",
      "Iteration 95, loss = 0.40775964\n",
      "Iteration 96, loss = 0.40909910\n",
      "Iteration 97, loss = 0.40794649\n",
      "Iteration 98, loss = 0.40789859\n",
      "Iteration 99, loss = 0.40728190\n",
      "Iteration 100, loss = 0.41087124\n",
      "Iteration 101, loss = 0.40935963\n",
      "Iteration 102, loss = 0.40925428\n",
      "Iteration 103, loss = 0.40935402\n",
      "Iteration 104, loss = 0.40759220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57346104\n",
      "Iteration 2, loss = 0.50190479\n",
      "Iteration 3, loss = 0.48538864\n",
      "Iteration 4, loss = 0.47834480\n",
      "Iteration 5, loss = 0.47598262\n",
      "Iteration 6, loss = 0.47257523\n",
      "Iteration 7, loss = 0.47238373\n",
      "Iteration 8, loss = 0.47072107\n",
      "Iteration 9, loss = 0.46875156\n",
      "Iteration 10, loss = 0.46824276\n",
      "Iteration 11, loss = 0.46712537\n",
      "Iteration 12, loss = 0.46663585\n",
      "Iteration 13, loss = 0.46698217\n",
      "Iteration 14, loss = 0.46503449\n",
      "Iteration 15, loss = 0.46542004\n",
      "Iteration 16, loss = 0.46367386\n",
      "Iteration 17, loss = 0.46448065\n",
      "Iteration 18, loss = 0.46400778\n",
      "Iteration 19, loss = 0.46357388\n",
      "Iteration 20, loss = 0.45984016\n",
      "Iteration 21, loss = 0.45901846\n",
      "Iteration 22, loss = 0.45824538\n",
      "Iteration 23, loss = 0.45622903\n",
      "Iteration 24, loss = 0.45650312\n",
      "Iteration 25, loss = 0.45601818\n",
      "Iteration 26, loss = 0.45478832\n",
      "Iteration 27, loss = 0.45422274\n",
      "Iteration 28, loss = 0.45418605\n",
      "Iteration 29, loss = 0.45024078\n",
      "Iteration 30, loss = 0.44986815\n",
      "Iteration 31, loss = 0.44731682\n",
      "Iteration 32, loss = 0.44633396\n",
      "Iteration 33, loss = 0.44560038\n",
      "Iteration 34, loss = 0.44565096\n",
      "Iteration 35, loss = 0.44105445\n",
      "Iteration 36, loss = 0.44236999\n",
      "Iteration 37, loss = 0.43845635\n",
      "Iteration 38, loss = 0.43887134\n",
      "Iteration 39, loss = 0.43599827\n",
      "Iteration 40, loss = 0.43507541\n",
      "Iteration 41, loss = 0.43379365\n",
      "Iteration 42, loss = 0.43849936\n",
      "Iteration 43, loss = 0.43522838\n",
      "Iteration 44, loss = 0.43495089\n",
      "Iteration 45, loss = 0.43083176\n",
      "Iteration 46, loss = 0.43104762\n",
      "Iteration 47, loss = 0.42993847\n",
      "Iteration 48, loss = 0.42973886\n",
      "Iteration 49, loss = 0.43045864\n",
      "Iteration 50, loss = 0.42781523\n",
      "Iteration 51, loss = 0.42754112\n",
      "Iteration 52, loss = 0.42747292\n",
      "Iteration 53, loss = 0.42730212\n",
      "Iteration 54, loss = 0.42987015\n",
      "Iteration 55, loss = 0.42657485\n",
      "Iteration 56, loss = 0.42527175\n",
      "Iteration 57, loss = 0.42456163\n",
      "Iteration 58, loss = 0.42297611\n",
      "Iteration 59, loss = 0.42599201\n",
      "Iteration 60, loss = 0.42375123\n",
      "Iteration 61, loss = 0.42143192\n",
      "Iteration 62, loss = 0.42203541\n",
      "Iteration 63, loss = 0.42088848\n",
      "Iteration 64, loss = 0.42121524\n",
      "Iteration 65, loss = 0.42113027\n",
      "Iteration 66, loss = 0.42453188\n",
      "Iteration 67, loss = 0.42292447\n",
      "Iteration 68, loss = 0.42051652\n",
      "Iteration 69, loss = 0.41927883\n",
      "Iteration 70, loss = 0.41958032\n",
      "Iteration 71, loss = 0.42004648\n",
      "Iteration 72, loss = 0.41985655\n",
      "Iteration 73, loss = 0.41821023\n",
      "Iteration 74, loss = 0.41914954\n",
      "Iteration 75, loss = 0.42200252\n",
      "Iteration 76, loss = 0.41906809\n",
      "Iteration 77, loss = 0.42060675\n",
      "Iteration 78, loss = 0.42034246\n",
      "Iteration 79, loss = 0.42055757\n",
      "Iteration 80, loss = 0.41954270\n",
      "Iteration 81, loss = 0.42029166\n",
      "Iteration 82, loss = 0.41797070\n",
      "Iteration 83, loss = 0.41959204\n",
      "Iteration 84, loss = 0.42021327\n",
      "Iteration 85, loss = 0.42066802\n",
      "Iteration 86, loss = 0.41932488\n",
      "Iteration 87, loss = 0.42026187\n",
      "Iteration 88, loss = 0.41876488\n",
      "Iteration 89, loss = 0.42027042\n",
      "Iteration 90, loss = 0.41812944\n",
      "Iteration 91, loss = 0.42056066\n",
      "Iteration 92, loss = 0.41941529\n",
      "Iteration 93, loss = 0.42053832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56240736\n",
      "Iteration 2, loss = 0.50366215\n",
      "Iteration 3, loss = 0.49434575\n",
      "Iteration 4, loss = 0.49074956\n",
      "Iteration 5, loss = 0.48765931\n",
      "Iteration 6, loss = 0.48453934\n",
      "Iteration 7, loss = 0.48451082\n",
      "Iteration 8, loss = 0.48486536\n",
      "Iteration 9, loss = 0.48035448\n",
      "Iteration 10, loss = 0.47843967\n",
      "Iteration 11, loss = 0.47730562\n",
      "Iteration 12, loss = 0.47576574\n",
      "Iteration 13, loss = 0.47254146\n",
      "Iteration 14, loss = 0.47037914\n",
      "Iteration 15, loss = 0.46759370\n",
      "Iteration 16, loss = 0.46559696\n",
      "Iteration 17, loss = 0.45956916\n",
      "Iteration 18, loss = 0.45669416\n",
      "Iteration 19, loss = 0.45429672\n",
      "Iteration 20, loss = 0.45101981\n",
      "Iteration 21, loss = 0.44817305\n",
      "Iteration 22, loss = 0.44665306\n",
      "Iteration 23, loss = 0.44730671\n",
      "Iteration 24, loss = 0.44561187\n",
      "Iteration 25, loss = 0.44657072\n",
      "Iteration 26, loss = 0.44559732\n",
      "Iteration 27, loss = 0.44335052\n",
      "Iteration 28, loss = 0.44423026\n",
      "Iteration 29, loss = 0.44279030\n",
      "Iteration 30, loss = 0.44418333\n",
      "Iteration 31, loss = 0.44329101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.44692830\n",
      "Iteration 33, loss = 0.44450677\n",
      "Iteration 34, loss = 0.44331026\n",
      "Iteration 35, loss = 0.44235232\n",
      "Iteration 36, loss = 0.44387489\n",
      "Iteration 37, loss = 0.44024506\n",
      "Iteration 38, loss = 0.44474535\n",
      "Iteration 39, loss = 0.44173824\n",
      "Iteration 40, loss = 0.44143068\n",
      "Iteration 41, loss = 0.44060306\n",
      "Iteration 42, loss = 0.44069495\n",
      "Iteration 43, loss = 0.44094161\n",
      "Iteration 44, loss = 0.44065754\n",
      "Iteration 45, loss = 0.44132514\n",
      "Iteration 46, loss = 0.44038263\n",
      "Iteration 47, loss = 0.44116325\n",
      "Iteration 48, loss = 0.43973101\n",
      "Iteration 49, loss = 0.44130155\n",
      "Iteration 50, loss = 0.43836910\n",
      "Iteration 51, loss = 0.43839982\n",
      "Iteration 52, loss = 0.43649860\n",
      "Iteration 53, loss = 0.43785154\n",
      "Iteration 54, loss = 0.43645656\n",
      "Iteration 55, loss = 0.43662378\n",
      "Iteration 56, loss = 0.43770296\n",
      "Iteration 57, loss = 0.43690264\n",
      "Iteration 58, loss = 0.43409170\n",
      "Iteration 59, loss = 0.43485967\n",
      "Iteration 60, loss = 0.43592683\n",
      "Iteration 61, loss = 0.43552886\n",
      "Iteration 62, loss = 0.43586674\n",
      "Iteration 63, loss = 0.43437547\n",
      "Iteration 64, loss = 0.43537042\n",
      "Iteration 65, loss = 0.43501705\n",
      "Iteration 66, loss = 0.43505476\n",
      "Iteration 67, loss = 0.43247936\n",
      "Iteration 68, loss = 0.43158761\n",
      "Iteration 69, loss = 0.43577809\n",
      "Iteration 70, loss = 0.43440822\n",
      "Iteration 71, loss = 0.43107428\n",
      "Iteration 72, loss = 0.43096117\n",
      "Iteration 73, loss = 0.43142662\n",
      "Iteration 74, loss = 0.43146557\n",
      "Iteration 75, loss = 0.43188569\n",
      "Iteration 76, loss = 0.43069170\n",
      "Iteration 77, loss = 0.43056481\n",
      "Iteration 78, loss = 0.43105836\n",
      "Iteration 79, loss = 0.43016760\n",
      "Iteration 80, loss = 0.42914369\n",
      "Iteration 81, loss = 0.43045490\n",
      "Iteration 82, loss = 0.43202987\n",
      "Iteration 83, loss = 0.43132034\n",
      "Iteration 84, loss = 0.43033343\n",
      "Iteration 85, loss = 0.42809937\n",
      "Iteration 86, loss = 0.42834564\n",
      "Iteration 87, loss = 0.42932836\n",
      "Iteration 88, loss = 0.42696109\n",
      "Iteration 89, loss = 0.42775639\n",
      "Iteration 90, loss = 0.42815131\n",
      "Iteration 91, loss = 0.42759110\n",
      "Iteration 92, loss = 0.42812722\n",
      "Iteration 93, loss = 0.43050677\n",
      "Iteration 94, loss = 0.42879370\n",
      "Iteration 95, loss = 0.42737229\n",
      "Iteration 96, loss = 0.42881595\n",
      "Iteration 97, loss = 0.42788321\n",
      "Iteration 98, loss = 0.42803199\n",
      "Iteration 99, loss = 0.42731488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56508470\n",
      "Iteration 2, loss = 0.50121369\n",
      "Iteration 3, loss = 0.49061226\n",
      "Iteration 4, loss = 0.48791368\n",
      "Iteration 5, loss = 0.48486789\n",
      "Iteration 6, loss = 0.48010202\n",
      "Iteration 7, loss = 0.47836821\n",
      "Iteration 8, loss = 0.47951099\n",
      "Iteration 9, loss = 0.47373392\n",
      "Iteration 10, loss = 0.47113653\n",
      "Iteration 11, loss = 0.46938723\n",
      "Iteration 12, loss = 0.46645486\n",
      "Iteration 13, loss = 0.46505232\n",
      "Iteration 14, loss = 0.46088003\n",
      "Iteration 15, loss = 0.45947560\n",
      "Iteration 16, loss = 0.45953436\n",
      "Iteration 17, loss = 0.45538255\n",
      "Iteration 18, loss = 0.45458013\n",
      "Iteration 19, loss = 0.45216040\n",
      "Iteration 20, loss = 0.45055607\n",
      "Iteration 21, loss = 0.44548313\n",
      "Iteration 22, loss = 0.44482650\n",
      "Iteration 23, loss = 0.44449306\n",
      "Iteration 24, loss = 0.43961750\n",
      "Iteration 25, loss = 0.44267072\n",
      "Iteration 26, loss = 0.44010272\n",
      "Iteration 27, loss = 0.44041982\n",
      "Iteration 28, loss = 0.44054318\n",
      "Iteration 29, loss = 0.43889588\n",
      "Iteration 30, loss = 0.43878690\n",
      "Iteration 31, loss = 0.43568384\n",
      "Iteration 32, loss = 0.43496217\n",
      "Iteration 33, loss = 0.43738017\n",
      "Iteration 34, loss = 0.43959394\n",
      "Iteration 35, loss = 0.43647633\n",
      "Iteration 36, loss = 0.43673910\n",
      "Iteration 37, loss = 0.43623158\n",
      "Iteration 38, loss = 0.43730960\n",
      "Iteration 39, loss = 0.43603000\n",
      "Iteration 40, loss = 0.43436188\n",
      "Iteration 41, loss = 0.43518234\n",
      "Iteration 42, loss = 0.43448946\n",
      "Iteration 43, loss = 0.43469479\n",
      "Iteration 44, loss = 0.43400142\n",
      "Iteration 45, loss = 0.43387551\n",
      "Iteration 46, loss = 0.43343371\n",
      "Iteration 47, loss = 0.43391421\n",
      "Iteration 48, loss = 0.43427153\n",
      "Iteration 49, loss = 0.43365304\n",
      "Iteration 50, loss = 0.43419367\n",
      "Iteration 51, loss = 0.43273548\n",
      "Iteration 52, loss = 0.43230527\n",
      "Iteration 53, loss = 0.43419217\n",
      "Iteration 54, loss = 0.43181114\n",
      "Iteration 55, loss = 0.43306738\n",
      "Iteration 56, loss = 0.43307194\n",
      "Iteration 57, loss = 0.43441011\n",
      "Iteration 58, loss = 0.43439136\n",
      "Iteration 59, loss = 0.43387560\n",
      "Iteration 60, loss = 0.43320286\n",
      "Iteration 61, loss = 0.43350142\n",
      "Iteration 62, loss = 0.43116714\n",
      "Iteration 63, loss = 0.43231385\n",
      "Iteration 64, loss = 0.43097964\n",
      "Iteration 65, loss = 0.43223701\n",
      "Iteration 66, loss = 0.43236383\n",
      "Iteration 67, loss = 0.43321328\n",
      "Iteration 68, loss = 0.43213303\n",
      "Iteration 69, loss = 0.43291201\n",
      "Iteration 70, loss = 0.43160641\n",
      "Iteration 71, loss = 0.43061810\n",
      "Iteration 72, loss = 0.43054556\n",
      "Iteration 73, loss = 0.43204248\n",
      "Iteration 74, loss = 0.43044860\n",
      "Iteration 75, loss = 0.43016509\n",
      "Iteration 76, loss = 0.43127748\n",
      "Iteration 77, loss = 0.43042768\n",
      "Iteration 78, loss = 0.43101569\n",
      "Iteration 79, loss = 0.43134012\n",
      "Iteration 80, loss = 0.43114127\n",
      "Iteration 81, loss = 0.43171867\n",
      "Iteration 82, loss = 0.43294313\n",
      "Iteration 83, loss = 0.43369966\n",
      "Iteration 84, loss = 0.43069334\n",
      "Iteration 85, loss = 0.43045996\n",
      "Iteration 86, loss = 0.43082173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.55654774\n",
      "Iteration 2, loss = 0.50146315\n",
      "Iteration 3, loss = 0.49130940\n",
      "Iteration 4, loss = 0.48760455\n",
      "Iteration 5, loss = 0.48460576\n",
      "Iteration 6, loss = 0.48217353\n",
      "Iteration 7, loss = 0.48183875\n",
      "Iteration 8, loss = 0.47879298\n",
      "Iteration 9, loss = 0.47594973\n",
      "Iteration 10, loss = 0.47336589\n",
      "Iteration 11, loss = 0.47325937\n",
      "Iteration 12, loss = 0.47105281\n",
      "Iteration 13, loss = 0.46945277\n",
      "Iteration 14, loss = 0.46639885\n",
      "Iteration 15, loss = 0.46422106\n",
      "Iteration 16, loss = 0.45947722\n",
      "Iteration 17, loss = 0.45934676\n",
      "Iteration 18, loss = 0.45444797\n",
      "Iteration 19, loss = 0.45367078\n",
      "Iteration 20, loss = 0.44806051\n",
      "Iteration 21, loss = 0.44601961\n",
      "Iteration 22, loss = 0.44425736\n",
      "Iteration 23, loss = 0.44251368\n",
      "Iteration 24, loss = 0.44079165\n",
      "Iteration 25, loss = 0.43951399\n",
      "Iteration 26, loss = 0.44115088\n",
      "Iteration 27, loss = 0.44048475\n",
      "Iteration 28, loss = 0.44164755\n",
      "Iteration 29, loss = 0.43789033\n",
      "Iteration 30, loss = 0.43518974\n",
      "Iteration 31, loss = 0.43325967\n",
      "Iteration 32, loss = 0.43554208\n",
      "Iteration 33, loss = 0.43718384\n",
      "Iteration 34, loss = 0.43490268\n",
      "Iteration 35, loss = 0.43380660\n",
      "Iteration 36, loss = 0.43535254\n",
      "Iteration 37, loss = 0.43490399\n",
      "Iteration 38, loss = 0.43250903\n",
      "Iteration 39, loss = 0.42964340\n",
      "Iteration 40, loss = 0.43037925\n",
      "Iteration 41, loss = 0.43043884\n",
      "Iteration 42, loss = 0.43184609\n",
      "Iteration 43, loss = 0.43120073\n",
      "Iteration 44, loss = 0.43192804\n",
      "Iteration 45, loss = 0.43225621\n",
      "Iteration 46, loss = 0.42765190\n",
      "Iteration 47, loss = 0.42787710\n",
      "Iteration 48, loss = 0.42801914\n",
      "Iteration 49, loss = 0.42802768\n",
      "Iteration 50, loss = 0.42911780\n",
      "Iteration 51, loss = 0.42924112\n",
      "Iteration 52, loss = 0.42714343\n",
      "Iteration 53, loss = 0.42722331\n",
      "Iteration 54, loss = 0.42894618\n",
      "Iteration 55, loss = 0.42681570\n",
      "Iteration 56, loss = 0.42889000\n",
      "Iteration 57, loss = 0.43042775\n",
      "Iteration 58, loss = 0.43010650\n",
      "Iteration 59, loss = 0.42752771\n",
      "Iteration 60, loss = 0.42702707\n",
      "Iteration 61, loss = 0.42737394\n",
      "Iteration 62, loss = 0.42729230\n",
      "Iteration 63, loss = 0.42693565\n",
      "Iteration 64, loss = 0.42810934\n",
      "Iteration 65, loss = 0.42779466\n",
      "Iteration 66, loss = 0.42740668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60788281\n",
      "Iteration 2, loss = 0.53781315\n",
      "Iteration 3, loss = 0.52733824\n",
      "Iteration 4, loss = 0.52510092\n",
      "Iteration 5, loss = 0.52014537\n",
      "Iteration 6, loss = 0.51463980\n",
      "Iteration 7, loss = 0.51309703\n",
      "Iteration 8, loss = 0.51208338\n",
      "Iteration 9, loss = 0.50829367\n",
      "Iteration 10, loss = 0.50236464\n",
      "Iteration 11, loss = 0.49829614\n",
      "Iteration 12, loss = 0.49651913\n",
      "Iteration 13, loss = 0.49425993\n",
      "Iteration 14, loss = 0.49219454\n",
      "Iteration 15, loss = 0.49392043\n",
      "Iteration 16, loss = 0.49056521\n",
      "Iteration 17, loss = 0.48598372\n",
      "Iteration 18, loss = 0.48325901\n",
      "Iteration 19, loss = 0.47955627\n",
      "Iteration 20, loss = 0.48053615\n",
      "Iteration 21, loss = 0.47367312\n",
      "Iteration 22, loss = 0.46694388\n",
      "Iteration 23, loss = 0.46700452\n",
      "Iteration 24, loss = 0.46361302\n",
      "Iteration 25, loss = 0.46445108\n",
      "Iteration 26, loss = 0.46167720\n",
      "Iteration 27, loss = 0.45722442\n",
      "Iteration 28, loss = 0.46058746\n",
      "Iteration 29, loss = 0.45880914\n",
      "Iteration 30, loss = 0.45919000\n",
      "Iteration 31, loss = 0.45705114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.45729040\n",
      "Iteration 33, loss = 0.45708590\n",
      "Iteration 34, loss = 0.45501849\n",
      "Iteration 35, loss = 0.45433223\n",
      "Iteration 36, loss = 0.45621641\n",
      "Iteration 37, loss = 0.45379267\n",
      "Iteration 38, loss = 0.45508693\n",
      "Iteration 39, loss = 0.45299221\n",
      "Iteration 40, loss = 0.45312890\n",
      "Iteration 41, loss = 0.45636985\n",
      "Iteration 42, loss = 0.45271349\n",
      "Iteration 43, loss = 0.45317735\n",
      "Iteration 44, loss = 0.45242053\n",
      "Iteration 45, loss = 0.45210856\n",
      "Iteration 46, loss = 0.45149431\n",
      "Iteration 47, loss = 0.45019421\n",
      "Iteration 48, loss = 0.45000558\n",
      "Iteration 49, loss = 0.45278682\n",
      "Iteration 50, loss = 0.45513997\n",
      "Iteration 51, loss = 0.45143003\n",
      "Iteration 52, loss = 0.45298003\n",
      "Iteration 53, loss = 0.45472906\n",
      "Iteration 54, loss = 0.45302521\n",
      "Iteration 55, loss = 0.45268456\n",
      "Iteration 56, loss = 0.45035943\n",
      "Iteration 57, loss = 0.45090509\n",
      "Iteration 58, loss = 0.44951176\n",
      "Iteration 59, loss = 0.44956013\n",
      "Iteration 60, loss = 0.44990382\n",
      "Iteration 61, loss = 0.44971328\n",
      "Iteration 62, loss = 0.44843004\n",
      "Iteration 63, loss = 0.44734510\n",
      "Iteration 64, loss = 0.44976174\n",
      "Iteration 65, loss = 0.45022144\n",
      "Iteration 66, loss = 0.44925399\n",
      "Iteration 67, loss = 0.45126228\n",
      "Iteration 68, loss = 0.44963998\n",
      "Iteration 69, loss = 0.45194495\n",
      "Iteration 70, loss = 0.44957679\n",
      "Iteration 71, loss = 0.44928169\n",
      "Iteration 72, loss = 0.45201021\n",
      "Iteration 73, loss = 0.44699107\n",
      "Iteration 74, loss = 0.45093948\n",
      "Iteration 75, loss = 0.45263703\n",
      "Iteration 76, loss = 0.45059171\n",
      "Iteration 77, loss = 0.45049262\n",
      "Iteration 78, loss = 0.44944421\n",
      "Iteration 79, loss = 0.45092187\n",
      "Iteration 80, loss = 0.44966495\n",
      "Iteration 81, loss = 0.44975062\n",
      "Iteration 82, loss = 0.45449520\n",
      "Iteration 83, loss = 0.45241765\n",
      "Iteration 84, loss = 0.45193370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61189807\n",
      "Iteration 2, loss = 0.53843684\n",
      "Iteration 3, loss = 0.52517659\n",
      "Iteration 4, loss = 0.52095751\n",
      "Iteration 5, loss = 0.51528615\n",
      "Iteration 6, loss = 0.51015837\n",
      "Iteration 7, loss = 0.50697834\n",
      "Iteration 8, loss = 0.50464511\n",
      "Iteration 9, loss = 0.50155696\n",
      "Iteration 10, loss = 0.49872891\n",
      "Iteration 11, loss = 0.49592156\n",
      "Iteration 12, loss = 0.49206663\n",
      "Iteration 13, loss = 0.48866590\n",
      "Iteration 14, loss = 0.48338192\n",
      "Iteration 15, loss = 0.47909419\n",
      "Iteration 16, loss = 0.47414905\n",
      "Iteration 17, loss = 0.46290884\n",
      "Iteration 18, loss = 0.45653955\n",
      "Iteration 19, loss = 0.45220253\n",
      "Iteration 20, loss = 0.45115593\n",
      "Iteration 21, loss = 0.44828992\n",
      "Iteration 22, loss = 0.44792294\n",
      "Iteration 23, loss = 0.44484824\n",
      "Iteration 24, loss = 0.44341937\n",
      "Iteration 25, loss = 0.44497581\n",
      "Iteration 26, loss = 0.44319047\n",
      "Iteration 27, loss = 0.44277277\n",
      "Iteration 28, loss = 0.44309670\n",
      "Iteration 29, loss = 0.43799441\n",
      "Iteration 30, loss = 0.43897274\n",
      "Iteration 31, loss = 0.43632709\n",
      "Iteration 32, loss = 0.44079133\n",
      "Iteration 33, loss = 0.43766721\n",
      "Iteration 34, loss = 0.43844701\n",
      "Iteration 35, loss = 0.43696533\n",
      "Iteration 36, loss = 0.43685240\n",
      "Iteration 37, loss = 0.43413068\n",
      "Iteration 38, loss = 0.43793270\n",
      "Iteration 39, loss = 0.43508105\n",
      "Iteration 40, loss = 0.43362307\n",
      "Iteration 41, loss = 0.43485411\n",
      "Iteration 42, loss = 0.43479493\n",
      "Iteration 43, loss = 0.43455712\n",
      "Iteration 44, loss = 0.43411472\n",
      "Iteration 45, loss = 0.43513508\n",
      "Iteration 46, loss = 0.43313425\n",
      "Iteration 47, loss = 0.43434813\n",
      "Iteration 48, loss = 0.43298575\n",
      "Iteration 49, loss = 0.43376533\n",
      "Iteration 50, loss = 0.43426041\n",
      "Iteration 51, loss = 0.43295547\n",
      "Iteration 52, loss = 0.43173201\n",
      "Iteration 53, loss = 0.43523324\n",
      "Iteration 54, loss = 0.43158205\n",
      "Iteration 55, loss = 0.43138434\n",
      "Iteration 56, loss = 0.43177792\n",
      "Iteration 57, loss = 0.43436626\n",
      "Iteration 58, loss = 0.43571985\n",
      "Iteration 59, loss = 0.43505071\n",
      "Iteration 60, loss = 0.43186721\n",
      "Iteration 61, loss = 0.43209440\n",
      "Iteration 62, loss = 0.43101201\n",
      "Iteration 63, loss = 0.43141322\n",
      "Iteration 64, loss = 0.43130596\n",
      "Iteration 65, loss = 0.43188490\n",
      "Iteration 66, loss = 0.43279595\n",
      "Iteration 67, loss = 0.43197444\n",
      "Iteration 68, loss = 0.43122510\n",
      "Iteration 69, loss = 0.43144364\n",
      "Iteration 70, loss = 0.43101933\n",
      "Iteration 71, loss = 0.43167967\n",
      "Iteration 72, loss = 0.43092088\n",
      "Iteration 73, loss = 0.43393313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60326144\n",
      "Iteration 2, loss = 0.53498668\n",
      "Iteration 3, loss = 0.52526211\n",
      "Iteration 4, loss = 0.52131395\n",
      "Iteration 5, loss = 0.51823305\n",
      "Iteration 6, loss = 0.51535788\n",
      "Iteration 7, loss = 0.51310253\n",
      "Iteration 8, loss = 0.50928199\n",
      "Iteration 9, loss = 0.50638821\n",
      "Iteration 10, loss = 0.50361651\n",
      "Iteration 11, loss = 0.50019162\n",
      "Iteration 12, loss = 0.49690031\n",
      "Iteration 13, loss = 0.49482032\n",
      "Iteration 14, loss = 0.49274426\n",
      "Iteration 15, loss = 0.48754756\n",
      "Iteration 16, loss = 0.48251295\n",
      "Iteration 17, loss = 0.47847755\n",
      "Iteration 18, loss = 0.47396119\n",
      "Iteration 19, loss = 0.46937762\n",
      "Iteration 20, loss = 0.46450291\n",
      "Iteration 21, loss = 0.45982617\n",
      "Iteration 22, loss = 0.45825630\n",
      "Iteration 23, loss = 0.45312037\n",
      "Iteration 24, loss = 0.45165361\n",
      "Iteration 25, loss = 0.45057728\n",
      "Iteration 26, loss = 0.44767804\n",
      "Iteration 27, loss = 0.45018469\n",
      "Iteration 28, loss = 0.44952955\n",
      "Iteration 29, loss = 0.44443937\n",
      "Iteration 30, loss = 0.44475918\n",
      "Iteration 31, loss = 0.44291841\n",
      "Iteration 32, loss = 0.44386533\n",
      "Iteration 33, loss = 0.44532713\n",
      "Iteration 34, loss = 0.44278678\n",
      "Iteration 35, loss = 0.44371706\n",
      "Iteration 36, loss = 0.44515918\n",
      "Iteration 37, loss = 0.44409559\n",
      "Iteration 38, loss = 0.43967103\n",
      "Iteration 39, loss = 0.44045680\n",
      "Iteration 40, loss = 0.44152246\n",
      "Iteration 41, loss = 0.44200507\n",
      "Iteration 42, loss = 0.44393837\n",
      "Iteration 43, loss = 0.44700645\n",
      "Iteration 44, loss = 0.44206172\n",
      "Iteration 45, loss = 0.44033238\n",
      "Iteration 46, loss = 0.44014280\n",
      "Iteration 47, loss = 0.43868241\n",
      "Iteration 48, loss = 0.44070027\n",
      "Iteration 49, loss = 0.44006947\n",
      "Iteration 50, loss = 0.43678743\n",
      "Iteration 51, loss = 0.43897533\n",
      "Iteration 52, loss = 0.43547523\n",
      "Iteration 53, loss = 0.43481715\n",
      "Iteration 54, loss = 0.43369967\n",
      "Iteration 55, loss = 0.43419910\n",
      "Iteration 56, loss = 0.43343954\n",
      "Iteration 57, loss = 0.43365664\n",
      "Iteration 58, loss = 0.43622849\n",
      "Iteration 59, loss = 0.43422261\n",
      "Iteration 60, loss = 0.43443995\n",
      "Iteration 61, loss = 0.43140400\n",
      "Iteration 62, loss = 0.43196456\n",
      "Iteration 63, loss = 0.43177556\n",
      "Iteration 64, loss = 0.43406944\n",
      "Iteration 65, loss = 0.43257526\n",
      "Iteration 66, loss = 0.43206082\n",
      "Iteration 67, loss = 0.43323001\n",
      "Iteration 68, loss = 0.43102722\n",
      "Iteration 69, loss = 0.43020127\n",
      "Iteration 70, loss = 0.42832320\n",
      "Iteration 71, loss = 0.43386264\n",
      "Iteration 72, loss = 0.44040778\n",
      "Iteration 73, loss = 0.43222526\n",
      "Iteration 74, loss = 0.43198237\n",
      "Iteration 75, loss = 0.43504002\n",
      "Iteration 76, loss = 0.43232171\n",
      "Iteration 77, loss = 0.43410218\n",
      "Iteration 78, loss = 0.42892392\n",
      "Iteration 79, loss = 0.42983562\n",
      "Iteration 80, loss = 0.43004756\n",
      "Iteration 81, loss = 0.43131127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59341586\n",
      "Iteration 2, loss = 0.49064608\n",
      "Iteration 3, loss = 0.48389224\n",
      "Iteration 4, loss = 0.48209326\n",
      "Iteration 5, loss = 0.48201024\n",
      "Iteration 6, loss = 0.47610467\n",
      "Iteration 7, loss = 0.47421909\n",
      "Iteration 8, loss = 0.47030449\n",
      "Iteration 9, loss = 0.46498888\n",
      "Iteration 10, loss = 0.45715203\n",
      "Iteration 11, loss = 0.45453585\n",
      "Iteration 12, loss = 0.45247556\n",
      "Iteration 13, loss = 0.44806511\n",
      "Iteration 14, loss = 0.44510870\n",
      "Iteration 15, loss = 0.44388421\n",
      "Iteration 16, loss = 0.44179508\n",
      "Iteration 17, loss = 0.43810350\n",
      "Iteration 18, loss = 0.43999091\n",
      "Iteration 19, loss = 0.43583068\n",
      "Iteration 20, loss = 0.43411575\n",
      "Iteration 21, loss = 0.43266462\n",
      "Iteration 22, loss = 0.43062971\n",
      "Iteration 23, loss = 0.43467544\n",
      "Iteration 24, loss = 0.43408796\n",
      "Iteration 25, loss = 0.43213956\n",
      "Iteration 26, loss = 0.43095445\n",
      "Iteration 27, loss = 0.42968281\n",
      "Iteration 28, loss = 0.43084368\n",
      "Iteration 29, loss = 0.42861189\n",
      "Iteration 30, loss = 0.42981315\n",
      "Iteration 31, loss = 0.42850157\n",
      "Iteration 32, loss = 0.43161917\n",
      "Iteration 33, loss = 0.42912778\n",
      "Iteration 34, loss = 0.42819130\n",
      "Iteration 35, loss = 0.42701703\n",
      "Iteration 36, loss = 0.42806666\n",
      "Iteration 37, loss = 0.42826554\n",
      "Iteration 38, loss = 0.42843148\n",
      "Iteration 39, loss = 0.42915812\n",
      "Iteration 40, loss = 0.42679120\n",
      "Iteration 41, loss = 0.42789497\n",
      "Iteration 42, loss = 0.42709972\n",
      "Iteration 43, loss = 0.42702794\n",
      "Iteration 44, loss = 0.42805286\n",
      "Iteration 45, loss = 0.42611454\n",
      "Iteration 46, loss = 0.42560952\n",
      "Iteration 47, loss = 0.42601418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 48, loss = 0.42568165\n",
      "Iteration 49, loss = 0.42785592\n",
      "Iteration 50, loss = 0.42547133\n",
      "Iteration 51, loss = 0.42627157\n",
      "Iteration 52, loss = 0.42482571\n",
      "Iteration 53, loss = 0.42809918\n",
      "Iteration 54, loss = 0.42687619\n",
      "Iteration 55, loss = 0.42548939\n",
      "Iteration 56, loss = 0.42659013\n",
      "Iteration 57, loss = 0.42590681\n",
      "Iteration 58, loss = 0.42458879\n",
      "Iteration 59, loss = 0.42595977\n",
      "Iteration 60, loss = 0.42530286\n",
      "Iteration 61, loss = 0.42556872\n",
      "Iteration 62, loss = 0.42367088\n",
      "Iteration 63, loss = 0.42573020\n",
      "Iteration 64, loss = 0.42590930\n",
      "Iteration 65, loss = 0.42617109\n",
      "Iteration 66, loss = 0.42682915\n",
      "Iteration 67, loss = 0.42482627\n",
      "Iteration 68, loss = 0.42212877\n",
      "Iteration 69, loss = 0.42714626\n",
      "Iteration 70, loss = 0.42666732\n",
      "Iteration 71, loss = 0.42476013\n",
      "Iteration 72, loss = 0.42338250\n",
      "Iteration 73, loss = 0.42733747\n",
      "Iteration 74, loss = 0.42474924\n",
      "Iteration 75, loss = 0.42670108\n",
      "Iteration 76, loss = 0.42405755\n",
      "Iteration 77, loss = 0.42503291\n",
      "Iteration 78, loss = 0.42484077\n",
      "Iteration 79, loss = 0.42570116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.60069002\n",
      "Iteration 2, loss = 0.48547453\n",
      "Iteration 3, loss = 0.47680813\n",
      "Iteration 4, loss = 0.47348223\n",
      "Iteration 5, loss = 0.47186431\n",
      "Iteration 6, loss = 0.46864854\n",
      "Iteration 7, loss = 0.46686164\n",
      "Iteration 8, loss = 0.46346082\n",
      "Iteration 9, loss = 0.45873565\n",
      "Iteration 10, loss = 0.45652726\n",
      "Iteration 11, loss = 0.45425022\n",
      "Iteration 12, loss = 0.45016509\n",
      "Iteration 13, loss = 0.44682814\n",
      "Iteration 14, loss = 0.44315447\n",
      "Iteration 15, loss = 0.43912191\n",
      "Iteration 16, loss = 0.43908316\n",
      "Iteration 17, loss = 0.43443990\n",
      "Iteration 18, loss = 0.43202429\n",
      "Iteration 19, loss = 0.42897847\n",
      "Iteration 20, loss = 0.43265814\n",
      "Iteration 21, loss = 0.42864900\n",
      "Iteration 22, loss = 0.42854116\n",
      "Iteration 23, loss = 0.42632429\n",
      "Iteration 24, loss = 0.42569167\n",
      "Iteration 25, loss = 0.42677155\n",
      "Iteration 26, loss = 0.42490923\n",
      "Iteration 27, loss = 0.42785854\n",
      "Iteration 28, loss = 0.42756855\n",
      "Iteration 29, loss = 0.42402323\n",
      "Iteration 30, loss = 0.42548499\n",
      "Iteration 31, loss = 0.42365283\n",
      "Iteration 32, loss = 0.42438787\n",
      "Iteration 33, loss = 0.42851761\n",
      "Iteration 34, loss = 0.42696935\n",
      "Iteration 35, loss = 0.42513237\n",
      "Iteration 36, loss = 0.42327043\n",
      "Iteration 37, loss = 0.42375384\n",
      "Iteration 38, loss = 0.42343907\n",
      "Iteration 39, loss = 0.42198031\n",
      "Iteration 40, loss = 0.42131828\n",
      "Iteration 41, loss = 0.42192757\n",
      "Iteration 42, loss = 0.42200317\n",
      "Iteration 43, loss = 0.42537893\n",
      "Iteration 44, loss = 0.42558467\n",
      "Iteration 45, loss = 0.42314819\n",
      "Iteration 46, loss = 0.42046417\n",
      "Iteration 47, loss = 0.42189549\n",
      "Iteration 48, loss = 0.42148697\n",
      "Iteration 49, loss = 0.42122638\n",
      "Iteration 50, loss = 0.42165950\n",
      "Iteration 51, loss = 0.42033347\n",
      "Iteration 52, loss = 0.42137605\n",
      "Iteration 53, loss = 0.42147005\n",
      "Iteration 54, loss = 0.41848715\n",
      "Iteration 55, loss = 0.42054944\n",
      "Iteration 56, loss = 0.41887674\n",
      "Iteration 57, loss = 0.42070559\n",
      "Iteration 58, loss = 0.42148712\n",
      "Iteration 59, loss = 0.41960056\n",
      "Iteration 60, loss = 0.41965937\n",
      "Iteration 61, loss = 0.42091180\n",
      "Iteration 62, loss = 0.41930431\n",
      "Iteration 63, loss = 0.41944371\n",
      "Iteration 64, loss = 0.41894038\n",
      "Iteration 65, loss = 0.41896226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.59286750\n",
      "Iteration 2, loss = 0.48900234\n",
      "Iteration 3, loss = 0.47849182\n",
      "Iteration 4, loss = 0.47393698\n",
      "Iteration 5, loss = 0.47335762\n",
      "Iteration 6, loss = 0.47174163\n",
      "Iteration 7, loss = 0.47033850\n",
      "Iteration 8, loss = 0.46669638\n",
      "Iteration 9, loss = 0.46685232\n",
      "Iteration 10, loss = 0.46360248\n",
      "Iteration 11, loss = 0.46144861\n",
      "Iteration 12, loss = 0.45416473\n",
      "Iteration 13, loss = 0.45072820\n",
      "Iteration 14, loss = 0.44702757\n",
      "Iteration 15, loss = 0.44318110\n",
      "Iteration 16, loss = 0.44219133\n",
      "Iteration 17, loss = 0.43976794\n",
      "Iteration 18, loss = 0.44143112\n",
      "Iteration 19, loss = 0.43764581\n",
      "Iteration 20, loss = 0.43813736\n",
      "Iteration 21, loss = 0.43946235\n",
      "Iteration 22, loss = 0.43424358\n",
      "Iteration 23, loss = 0.43317433\n",
      "Iteration 24, loss = 0.43739449\n",
      "Iteration 25, loss = 0.43192015\n",
      "Iteration 26, loss = 0.43052272\n",
      "Iteration 27, loss = 0.43475727\n",
      "Iteration 28, loss = 0.43336457\n",
      "Iteration 29, loss = 0.43085280\n",
      "Iteration 30, loss = 0.42926657\n",
      "Iteration 31, loss = 0.42955418\n",
      "Iteration 32, loss = 0.43017492\n",
      "Iteration 33, loss = 0.43512984\n",
      "Iteration 34, loss = 0.42901408\n",
      "Iteration 35, loss = 0.43294557\n",
      "Iteration 36, loss = 0.43236728\n",
      "Iteration 37, loss = 0.43017947\n",
      "Iteration 38, loss = 0.42794367\n",
      "Iteration 39, loss = 0.42718939\n",
      "Iteration 40, loss = 0.42891526\n",
      "Iteration 41, loss = 0.42692309\n",
      "Iteration 42, loss = 0.42933424\n",
      "Iteration 43, loss = 0.42833534\n",
      "Iteration 44, loss = 0.42786967\n",
      "Iteration 45, loss = 0.42877905\n",
      "Iteration 46, loss = 0.42636848\n",
      "Iteration 47, loss = 0.42754021\n",
      "Iteration 48, loss = 0.42725935\n",
      "Iteration 49, loss = 0.42837185\n",
      "Iteration 50, loss = 0.42565564\n",
      "Iteration 51, loss = 0.42471284\n",
      "Iteration 52, loss = 0.42692955\n",
      "Iteration 53, loss = 0.42677949\n",
      "Iteration 54, loss = 0.42752028\n",
      "Iteration 55, loss = 0.42681451\n",
      "Iteration 56, loss = 0.42611591\n",
      "Iteration 57, loss = 0.42548992\n",
      "Iteration 58, loss = 0.42647024\n",
      "Iteration 59, loss = 0.42807905\n",
      "Iteration 60, loss = 0.42709053\n",
      "Iteration 61, loss = 0.42684919\n",
      "Iteration 62, loss = 0.42590066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62366322\n",
      "Iteration 2, loss = 0.49041569\n",
      "Iteration 3, loss = 0.48378088\n",
      "Iteration 4, loss = 0.48163160\n",
      "Iteration 5, loss = 0.48093342\n",
      "Iteration 6, loss = 0.47667665\n",
      "Iteration 7, loss = 0.47595364\n",
      "Iteration 8, loss = 0.47500314\n",
      "Iteration 9, loss = 0.47555379\n",
      "Iteration 10, loss = 0.47362743\n",
      "Iteration 11, loss = 0.47322056\n",
      "Iteration 12, loss = 0.47216300\n",
      "Iteration 13, loss = 0.46951399\n",
      "Iteration 14, loss = 0.46965914\n",
      "Iteration 15, loss = 0.46848010\n",
      "Iteration 16, loss = 0.46568646\n",
      "Iteration 17, loss = 0.46430156\n",
      "Iteration 18, loss = 0.46424740\n",
      "Iteration 19, loss = 0.46209368\n",
      "Iteration 20, loss = 0.45842487\n",
      "Iteration 21, loss = 0.45503597\n",
      "Iteration 22, loss = 0.44926546\n",
      "Iteration 23, loss = 0.45056616\n",
      "Iteration 24, loss = 0.44365307\n",
      "Iteration 25, loss = 0.44379312\n",
      "Iteration 26, loss = 0.44347441\n",
      "Iteration 27, loss = 0.43734791\n",
      "Iteration 28, loss = 0.43423890\n",
      "Iteration 29, loss = 0.43368138\n",
      "Iteration 30, loss = 0.43333889\n",
      "Iteration 31, loss = 0.43367783\n",
      "Iteration 32, loss = 0.43400734\n",
      "Iteration 33, loss = 0.42982651\n",
      "Iteration 34, loss = 0.42539248\n",
      "Iteration 35, loss = 0.42664045\n",
      "Iteration 36, loss = 0.42819270\n",
      "Iteration 37, loss = 0.42522170\n",
      "Iteration 38, loss = 0.42638905\n",
      "Iteration 39, loss = 0.42397374\n",
      "Iteration 40, loss = 0.42283797\n",
      "Iteration 41, loss = 0.42193457\n",
      "Iteration 42, loss = 0.42159351\n",
      "Iteration 43, loss = 0.42149246\n",
      "Iteration 44, loss = 0.42407087\n",
      "Iteration 45, loss = 0.42113104\n",
      "Iteration 46, loss = 0.42030476\n",
      "Iteration 47, loss = 0.42022071\n",
      "Iteration 48, loss = 0.42106322\n",
      "Iteration 49, loss = 0.42058487\n",
      "Iteration 50, loss = 0.41987678\n",
      "Iteration 51, loss = 0.42059575\n",
      "Iteration 52, loss = 0.41971487\n",
      "Iteration 53, loss = 0.42301953\n",
      "Iteration 54, loss = 0.41851380\n",
      "Iteration 55, loss = 0.42066087\n",
      "Iteration 56, loss = 0.42003456\n",
      "Iteration 57, loss = 0.42018178\n",
      "Iteration 58, loss = 0.41766105\n",
      "Iteration 59, loss = 0.42009623\n",
      "Iteration 60, loss = 0.41766599\n",
      "Iteration 61, loss = 0.41711843\n",
      "Iteration 62, loss = 0.41631821\n",
      "Iteration 63, loss = 0.41564916\n",
      "Iteration 64, loss = 0.41617505\n",
      "Iteration 65, loss = 0.41548691\n",
      "Iteration 66, loss = 0.41620447\n",
      "Iteration 67, loss = 0.41604810\n",
      "Iteration 68, loss = 0.41369372\n",
      "Iteration 69, loss = 0.42181128\n",
      "Iteration 70, loss = 0.41870317\n",
      "Iteration 71, loss = 0.41665120\n",
      "Iteration 72, loss = 0.41658116\n",
      "Iteration 73, loss = 0.41742239\n",
      "Iteration 74, loss = 0.41502322\n",
      "Iteration 75, loss = 0.41642192\n",
      "Iteration 76, loss = 0.41464462\n",
      "Iteration 77, loss = 0.41423594\n",
      "Iteration 78, loss = 0.41924188\n",
      "Iteration 79, loss = 0.41793260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63045779\n",
      "Iteration 2, loss = 0.48534147\n",
      "Iteration 3, loss = 0.47628851\n",
      "Iteration 4, loss = 0.47402556\n",
      "Iteration 5, loss = 0.47315210\n",
      "Iteration 6, loss = 0.46842839\n",
      "Iteration 7, loss = 0.46853247\n",
      "Iteration 8, loss = 0.46608184\n",
      "Iteration 9, loss = 0.46170697\n",
      "Iteration 10, loss = 0.45815680\n",
      "Iteration 11, loss = 0.45674917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.45477770\n",
      "Iteration 13, loss = 0.45308045\n",
      "Iteration 14, loss = 0.44941887\n",
      "Iteration 15, loss = 0.44771301\n",
      "Iteration 16, loss = 0.44736153\n",
      "Iteration 17, loss = 0.44420308\n",
      "Iteration 18, loss = 0.44404483\n",
      "Iteration 19, loss = 0.44178691\n",
      "Iteration 20, loss = 0.44140936\n",
      "Iteration 21, loss = 0.43855421\n",
      "Iteration 22, loss = 0.43799417\n",
      "Iteration 23, loss = 0.43455826\n",
      "Iteration 24, loss = 0.43407605\n",
      "Iteration 25, loss = 0.43431372\n",
      "Iteration 26, loss = 0.43099668\n",
      "Iteration 27, loss = 0.43220389\n",
      "Iteration 28, loss = 0.42704254\n",
      "Iteration 29, loss = 0.42670791\n",
      "Iteration 30, loss = 0.42538834\n",
      "Iteration 31, loss = 0.42381634\n",
      "Iteration 32, loss = 0.42402333\n",
      "Iteration 33, loss = 0.42526808\n",
      "Iteration 34, loss = 0.42707875\n",
      "Iteration 35, loss = 0.42544555\n",
      "Iteration 36, loss = 0.42263606\n",
      "Iteration 37, loss = 0.42224706\n",
      "Iteration 38, loss = 0.42275902\n",
      "Iteration 39, loss = 0.42056096\n",
      "Iteration 40, loss = 0.41945396\n",
      "Iteration 41, loss = 0.41932360\n",
      "Iteration 42, loss = 0.42064882\n",
      "Iteration 43, loss = 0.42062540\n",
      "Iteration 44, loss = 0.42491465\n",
      "Iteration 45, loss = 0.42636807\n",
      "Iteration 46, loss = 0.42335300\n",
      "Iteration 47, loss = 0.41874181\n",
      "Iteration 48, loss = 0.41943677\n",
      "Iteration 49, loss = 0.41694416\n",
      "Iteration 50, loss = 0.41939305\n",
      "Iteration 51, loss = 0.41700877\n",
      "Iteration 52, loss = 0.41558674\n",
      "Iteration 53, loss = 0.41571816\n",
      "Iteration 54, loss = 0.41417970\n",
      "Iteration 55, loss = 0.41642637\n",
      "Iteration 56, loss = 0.41432177\n",
      "Iteration 57, loss = 0.41561838\n",
      "Iteration 58, loss = 0.41942732\n",
      "Iteration 59, loss = 0.41692123\n",
      "Iteration 60, loss = 0.41382411\n",
      "Iteration 61, loss = 0.41603550\n",
      "Iteration 62, loss = 0.41435424\n",
      "Iteration 63, loss = 0.41426879\n",
      "Iteration 64, loss = 0.41456924\n",
      "Iteration 65, loss = 0.41341825\n",
      "Iteration 66, loss = 0.41426237\n",
      "Iteration 67, loss = 0.41438380\n",
      "Iteration 68, loss = 0.41305868\n",
      "Iteration 69, loss = 0.41463140\n",
      "Iteration 70, loss = 0.41412038\n",
      "Iteration 71, loss = 0.41354907\n",
      "Iteration 72, loss = 0.41442107\n",
      "Iteration 73, loss = 0.41301664\n",
      "Iteration 74, loss = 0.41443258\n",
      "Iteration 75, loss = 0.41594549\n",
      "Iteration 76, loss = 0.41620796\n",
      "Iteration 77, loss = 0.41291109\n",
      "Iteration 78, loss = 0.41399097\n",
      "Iteration 79, loss = 0.41489272\n",
      "Iteration 80, loss = 0.41335817\n",
      "Iteration 81, loss = 0.41328343\n",
      "Iteration 82, loss = 0.41420255\n",
      "Iteration 83, loss = 0.41307315\n",
      "Iteration 84, loss = 0.41558954\n",
      "Iteration 85, loss = 0.41866393\n",
      "Iteration 86, loss = 0.41754084\n",
      "Iteration 87, loss = 0.41489530\n",
      "Iteration 88, loss = 0.41203717\n",
      "Iteration 89, loss = 0.41372040\n",
      "Iteration 90, loss = 0.41454976\n",
      "Iteration 91, loss = 0.41279341\n",
      "Iteration 92, loss = 0.41293748\n",
      "Iteration 93, loss = 0.41174851\n",
      "Iteration 94, loss = 0.41294263\n",
      "Iteration 95, loss = 0.41201925\n",
      "Iteration 96, loss = 0.41145079\n",
      "Iteration 97, loss = 0.41150341\n",
      "Iteration 98, loss = 0.41174385\n",
      "Iteration 99, loss = 0.41184109\n",
      "Iteration 100, loss = 0.41205227\n",
      "Iteration 101, loss = 0.41172559\n",
      "Iteration 102, loss = 0.41300076\n",
      "Iteration 103, loss = 0.41162359\n",
      "Iteration 104, loss = 0.41194372\n",
      "Iteration 105, loss = 0.41149982\n",
      "Iteration 106, loss = 0.41448810\n",
      "Iteration 107, loss = 0.41297572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61974503\n",
      "Iteration 2, loss = 0.48860269\n",
      "Iteration 3, loss = 0.47715259\n",
      "Iteration 4, loss = 0.47402147\n",
      "Iteration 5, loss = 0.47198562\n",
      "Iteration 6, loss = 0.47047856\n",
      "Iteration 7, loss = 0.47016681\n",
      "Iteration 8, loss = 0.46829156\n",
      "Iteration 9, loss = 0.46672189\n",
      "Iteration 10, loss = 0.46469612\n",
      "Iteration 11, loss = 0.46159746\n",
      "Iteration 12, loss = 0.45897902\n",
      "Iteration 13, loss = 0.45604405\n",
      "Iteration 14, loss = 0.45370274\n",
      "Iteration 15, loss = 0.45049507\n",
      "Iteration 16, loss = 0.44581177\n",
      "Iteration 17, loss = 0.44501958\n",
      "Iteration 18, loss = 0.44223639\n",
      "Iteration 19, loss = 0.43921713\n",
      "Iteration 20, loss = 0.43631673\n",
      "Iteration 21, loss = 0.43455261\n",
      "Iteration 22, loss = 0.43273034\n",
      "Iteration 23, loss = 0.43401029\n",
      "Iteration 24, loss = 0.43148357\n",
      "Iteration 25, loss = 0.43038313\n",
      "Iteration 26, loss = 0.43044633\n",
      "Iteration 27, loss = 0.42975290\n",
      "Iteration 28, loss = 0.43523402\n",
      "Iteration 29, loss = 0.42983758\n",
      "Iteration 30, loss = 0.42863250\n",
      "Iteration 31, loss = 0.42930962\n",
      "Iteration 32, loss = 0.42622281\n",
      "Iteration 33, loss = 0.43201065\n",
      "Iteration 34, loss = 0.42840019\n",
      "Iteration 35, loss = 0.42848569\n",
      "Iteration 36, loss = 0.42763102\n",
      "Iteration 37, loss = 0.42761488\n",
      "Iteration 38, loss = 0.42651807\n",
      "Iteration 39, loss = 0.42391172\n",
      "Iteration 40, loss = 0.42508208\n",
      "Iteration 41, loss = 0.42429796\n",
      "Iteration 42, loss = 0.42614586\n",
      "Iteration 43, loss = 0.42494077\n",
      "Iteration 44, loss = 0.42579313\n",
      "Iteration 45, loss = 0.42537551\n",
      "Iteration 46, loss = 0.42382387\n",
      "Iteration 47, loss = 0.42350812\n",
      "Iteration 48, loss = 0.42298103\n",
      "Iteration 49, loss = 0.42719033\n",
      "Iteration 50, loss = 0.42300850\n",
      "Iteration 51, loss = 0.42135763\n",
      "Iteration 52, loss = 0.42185027\n",
      "Iteration 53, loss = 0.42288033\n",
      "Iteration 54, loss = 0.42298117\n",
      "Iteration 55, loss = 0.42267933\n",
      "Iteration 56, loss = 0.42076353\n",
      "Iteration 57, loss = 0.42012618\n",
      "Iteration 58, loss = 0.42213439\n",
      "Iteration 59, loss = 0.42157042\n",
      "Iteration 60, loss = 0.41910720\n",
      "Iteration 61, loss = 0.41897497\n",
      "Iteration 62, loss = 0.42040215\n",
      "Iteration 63, loss = 0.42064389\n",
      "Iteration 64, loss = 0.42101821\n",
      "Iteration 65, loss = 0.42118564\n",
      "Iteration 66, loss = 0.41980492\n",
      "Iteration 67, loss = 0.42200532\n",
      "Iteration 68, loss = 0.41924794\n",
      "Iteration 69, loss = 0.41895671\n",
      "Iteration 70, loss = 0.41912263\n",
      "Iteration 71, loss = 0.41805989\n",
      "Iteration 72, loss = 0.41843286\n",
      "Iteration 73, loss = 0.41863193\n",
      "Iteration 74, loss = 0.41970853\n",
      "Iteration 75, loss = 0.42136347\n",
      "Iteration 76, loss = 0.42010502\n",
      "Iteration 77, loss = 0.42061858\n",
      "Iteration 78, loss = 0.41829364\n",
      "Iteration 79, loss = 0.41924678\n",
      "Iteration 80, loss = 0.41776200\n",
      "Iteration 81, loss = 0.41756410\n",
      "Iteration 82, loss = 0.41762928\n",
      "Iteration 83, loss = 0.41754912\n",
      "Iteration 84, loss = 0.41976938\n",
      "Iteration 85, loss = 0.41944013\n",
      "Iteration 86, loss = 0.41893624\n",
      "Iteration 87, loss = 0.41891970\n",
      "Iteration 88, loss = 0.41689282\n",
      "Iteration 89, loss = 0.42065779\n",
      "Iteration 90, loss = 0.41722387\n",
      "Iteration 91, loss = 0.41801361\n",
      "Iteration 92, loss = 0.41691627\n",
      "Iteration 93, loss = 0.41633203\n",
      "Iteration 94, loss = 0.41701787\n",
      "Iteration 95, loss = 0.41565640\n",
      "Iteration 96, loss = 0.41656196\n",
      "Iteration 97, loss = 0.41663461\n",
      "Iteration 98, loss = 0.41876842\n",
      "Iteration 99, loss = 0.41829626\n",
      "Iteration 100, loss = 0.41451283\n",
      "Iteration 101, loss = 0.41674395\n",
      "Iteration 102, loss = 0.41425360\n",
      "Iteration 103, loss = 0.41564411\n",
      "Iteration 104, loss = 0.41422548\n",
      "Iteration 105, loss = 0.41534802\n",
      "Iteration 106, loss = 0.41562793\n",
      "Iteration 107, loss = 0.41628519\n",
      "Iteration 108, loss = 0.41696742\n",
      "Iteration 109, loss = 0.41288865\n",
      "Iteration 110, loss = 0.41571886\n",
      "Iteration 111, loss = 0.41491081\n",
      "Iteration 112, loss = 0.41452513\n",
      "Iteration 113, loss = 0.41440934\n",
      "Iteration 114, loss = 0.41301819\n",
      "Iteration 115, loss = 0.41419346\n",
      "Iteration 116, loss = 0.41261476\n",
      "Iteration 117, loss = 0.41210266\n",
      "Iteration 118, loss = 0.41201130\n",
      "Iteration 119, loss = 0.41312289\n",
      "Iteration 120, loss = 0.41302707\n",
      "Iteration 121, loss = 0.41237316\n",
      "Iteration 122, loss = 0.41259220\n",
      "Iteration 123, loss = 0.41220496\n",
      "Iteration 124, loss = 0.41246298\n",
      "Iteration 125, loss = 0.41280218\n",
      "Iteration 126, loss = 0.41304331\n",
      "Iteration 127, loss = 0.41244747\n",
      "Iteration 128, loss = 0.41438214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 0 min. and 49.0564181804657 sec.\n",
      "\n",
      "1 total combinations for subset length 7.\n",
      "Iteration 1, loss = 0.52508546\n",
      "Iteration 2, loss = 0.48647238\n",
      "Iteration 3, loss = 0.47931638\n",
      "Iteration 4, loss = 0.47325209\n",
      "Iteration 5, loss = 0.47027314\n",
      "Iteration 6, loss = 0.47033199\n",
      "Iteration 7, loss = 0.46866815\n",
      "Iteration 8, loss = 0.46722022\n",
      "Iteration 9, loss = 0.46406895\n",
      "Iteration 10, loss = 0.46244543\n",
      "Iteration 11, loss = 0.46217406\n",
      "Iteration 12, loss = 0.46435462\n",
      "Iteration 13, loss = 0.46027048\n",
      "Iteration 14, loss = 0.46070680\n",
      "Iteration 15, loss = 0.46043884\n",
      "Iteration 16, loss = 0.46061296\n",
      "Iteration 17, loss = 0.46021718\n",
      "Iteration 18, loss = 0.46106189\n",
      "Iteration 19, loss = 0.45715101\n",
      "Iteration 20, loss = 0.45587890\n",
      "Iteration 21, loss = 0.45610347\n",
      "Iteration 22, loss = 0.45570070\n",
      "Iteration 23, loss = 0.45588621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 0.45515992\n",
      "Iteration 25, loss = 0.45593561\n",
      "Iteration 26, loss = 0.45179763\n",
      "Iteration 27, loss = 0.44966072\n",
      "Iteration 28, loss = 0.44989997\n",
      "Iteration 29, loss = 0.44997830\n",
      "Iteration 30, loss = 0.44726420\n",
      "Iteration 31, loss = 0.44780745\n",
      "Iteration 32, loss = 0.44728302\n",
      "Iteration 33, loss = 0.44784535\n",
      "Iteration 34, loss = 0.44386811\n",
      "Iteration 35, loss = 0.44515781\n",
      "Iteration 36, loss = 0.44598356\n",
      "Iteration 37, loss = 0.44334529\n",
      "Iteration 38, loss = 0.44232228\n",
      "Iteration 39, loss = 0.44233365\n",
      "Iteration 40, loss = 0.44381359\n",
      "Iteration 41, loss = 0.44176956\n",
      "Iteration 42, loss = 0.44958615\n",
      "Iteration 43, loss = 0.44405273\n",
      "Iteration 44, loss = 0.44171288\n",
      "Iteration 45, loss = 0.44236813\n",
      "Iteration 46, loss = 0.43910270\n",
      "Iteration 47, loss = 0.44262593\n",
      "Iteration 48, loss = 0.44068933\n",
      "Iteration 49, loss = 0.43947774\n",
      "Iteration 50, loss = 0.43930958\n",
      "Iteration 51, loss = 0.43874376\n",
      "Iteration 52, loss = 0.43908583\n",
      "Iteration 53, loss = 0.44471837\n",
      "Iteration 54, loss = 0.43928415\n",
      "Iteration 55, loss = 0.43989159\n",
      "Iteration 56, loss = 0.43697937\n",
      "Iteration 57, loss = 0.43812057\n",
      "Iteration 58, loss = 0.44099787\n",
      "Iteration 59, loss = 0.43741417\n",
      "Iteration 60, loss = 0.43531728\n",
      "Iteration 61, loss = 0.43497578\n",
      "Iteration 62, loss = 0.43675397\n",
      "Iteration 63, loss = 0.43552800\n",
      "Iteration 64, loss = 0.43535008\n",
      "Iteration 65, loss = 0.43368788\n",
      "Iteration 66, loss = 0.43225087\n",
      "Iteration 67, loss = 0.43746414\n",
      "Iteration 68, loss = 0.43123698\n",
      "Iteration 69, loss = 0.43239723\n",
      "Iteration 70, loss = 0.43185532\n",
      "Iteration 71, loss = 0.43273022\n",
      "Iteration 72, loss = 0.43194446\n",
      "Iteration 73, loss = 0.43254007\n",
      "Iteration 74, loss = 0.43099070\n",
      "Iteration 75, loss = 0.43580855\n",
      "Iteration 76, loss = 0.43206255\n",
      "Iteration 77, loss = 0.43091179\n",
      "Iteration 78, loss = 0.43131014\n",
      "Iteration 79, loss = 0.43023916\n",
      "Iteration 80, loss = 0.43120526\n",
      "Iteration 81, loss = 0.43408902\n",
      "Iteration 82, loss = 0.43237352\n",
      "Iteration 83, loss = 0.43185879\n",
      "Iteration 84, loss = 0.43045631\n",
      "Iteration 85, loss = 0.43383028\n",
      "Iteration 86, loss = 0.42886304\n",
      "Iteration 87, loss = 0.42991969\n",
      "Iteration 88, loss = 0.42902054\n",
      "Iteration 89, loss = 0.42739074\n",
      "Iteration 90, loss = 0.42969734\n",
      "Iteration 91, loss = 0.42927801\n",
      "Iteration 92, loss = 0.43022711\n",
      "Iteration 93, loss = 0.42848774\n",
      "Iteration 94, loss = 0.42857053\n",
      "Iteration 95, loss = 0.43002197\n",
      "Iteration 96, loss = 0.43092873\n",
      "Iteration 97, loss = 0.43184220\n",
      "Iteration 98, loss = 0.42864187\n",
      "Iteration 99, loss = 0.42667514\n",
      "Iteration 100, loss = 0.42808951\n",
      "Iteration 101, loss = 0.43203403\n",
      "Iteration 102, loss = 0.42844867\n",
      "Iteration 103, loss = 0.42926325\n",
      "Iteration 104, loss = 0.43103339\n",
      "Iteration 105, loss = 0.43121641\n",
      "Iteration 106, loss = 0.42886415\n",
      "Iteration 107, loss = 0.42588737\n",
      "Iteration 108, loss = 0.42740999\n",
      "Iteration 109, loss = 0.42735265\n",
      "Iteration 110, loss = 0.42820494\n",
      "Iteration 111, loss = 0.42516383\n",
      "Iteration 112, loss = 0.42663753\n",
      "Iteration 113, loss = 0.42583775\n",
      "Iteration 114, loss = 0.42756563\n",
      "Iteration 115, loss = 0.42646201\n",
      "Iteration 116, loss = 0.42535358\n",
      "Iteration 117, loss = 0.42512329\n",
      "Iteration 118, loss = 0.42702262\n",
      "Iteration 119, loss = 0.42723696\n",
      "Iteration 120, loss = 0.42606463\n",
      "Iteration 121, loss = 0.42574436\n",
      "Iteration 122, loss = 0.42760491\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51549418\n",
      "Iteration 2, loss = 0.48588213\n",
      "Iteration 3, loss = 0.47484492\n",
      "Iteration 4, loss = 0.47042811\n",
      "Iteration 5, loss = 0.46484103\n",
      "Iteration 6, loss = 0.46241872\n",
      "Iteration 7, loss = 0.46023023\n",
      "Iteration 8, loss = 0.45687510\n",
      "Iteration 9, loss = 0.45568637\n",
      "Iteration 10, loss = 0.45720684\n",
      "Iteration 11, loss = 0.45534087\n",
      "Iteration 12, loss = 0.45483684\n",
      "Iteration 13, loss = 0.45401948\n",
      "Iteration 14, loss = 0.45351547\n",
      "Iteration 15, loss = 0.45414623\n",
      "Iteration 16, loss = 0.45170838\n",
      "Iteration 17, loss = 0.45333545\n",
      "Iteration 18, loss = 0.44998771\n",
      "Iteration 19, loss = 0.45278202\n",
      "Iteration 20, loss = 0.44887878\n",
      "Iteration 21, loss = 0.44960423\n",
      "Iteration 22, loss = 0.44826216\n",
      "Iteration 23, loss = 0.44774291\n",
      "Iteration 24, loss = 0.44704162\n",
      "Iteration 25, loss = 0.44673666\n",
      "Iteration 26, loss = 0.44774636\n",
      "Iteration 27, loss = 0.44265360\n",
      "Iteration 28, loss = 0.44140989\n",
      "Iteration 29, loss = 0.44137198\n",
      "Iteration 30, loss = 0.43955897\n",
      "Iteration 31, loss = 0.44038175\n",
      "Iteration 32, loss = 0.44283346\n",
      "Iteration 33, loss = 0.43779181\n",
      "Iteration 34, loss = 0.43538037\n",
      "Iteration 35, loss = 0.43635860\n",
      "Iteration 36, loss = 0.43441831\n",
      "Iteration 37, loss = 0.43492723\n",
      "Iteration 38, loss = 0.43288642\n",
      "Iteration 39, loss = 0.43146367\n",
      "Iteration 40, loss = 0.43086997\n",
      "Iteration 41, loss = 0.43193293\n",
      "Iteration 42, loss = 0.43227249\n",
      "Iteration 43, loss = 0.43010119\n",
      "Iteration 44, loss = 0.42951640\n",
      "Iteration 45, loss = 0.42919358\n",
      "Iteration 46, loss = 0.43001332\n",
      "Iteration 47, loss = 0.42907376\n",
      "Iteration 48, loss = 0.42809217\n",
      "Iteration 49, loss = 0.42661024\n",
      "Iteration 50, loss = 0.42746889\n",
      "Iteration 51, loss = 0.42723082\n",
      "Iteration 52, loss = 0.42748226\n",
      "Iteration 53, loss = 0.42703773\n",
      "Iteration 54, loss = 0.42756262\n",
      "Iteration 55, loss = 0.42589255\n",
      "Iteration 56, loss = 0.42587808\n",
      "Iteration 57, loss = 0.42618424\n",
      "Iteration 58, loss = 0.42550217\n",
      "Iteration 59, loss = 0.42404595\n",
      "Iteration 60, loss = 0.42512175\n",
      "Iteration 61, loss = 0.42370086\n",
      "Iteration 62, loss = 0.42596238\n",
      "Iteration 63, loss = 0.42335218\n",
      "Iteration 64, loss = 0.42363211\n",
      "Iteration 65, loss = 0.42195173\n",
      "Iteration 66, loss = 0.42182026\n",
      "Iteration 67, loss = 0.42384361\n",
      "Iteration 68, loss = 0.42176095\n",
      "Iteration 69, loss = 0.42269817\n",
      "Iteration 70, loss = 0.42171156\n",
      "Iteration 71, loss = 0.42327839\n",
      "Iteration 72, loss = 0.42149600\n",
      "Iteration 73, loss = 0.42023695\n",
      "Iteration 74, loss = 0.41910770\n",
      "Iteration 75, loss = 0.42154819\n",
      "Iteration 76, loss = 0.42021662\n",
      "Iteration 77, loss = 0.42221331\n",
      "Iteration 78, loss = 0.42201661\n",
      "Iteration 79, loss = 0.42360957\n",
      "Iteration 80, loss = 0.41902557\n",
      "Iteration 81, loss = 0.42236005\n",
      "Iteration 82, loss = 0.41895342\n",
      "Iteration 83, loss = 0.41967735\n",
      "Iteration 84, loss = 0.41947060\n",
      "Iteration 85, loss = 0.42046192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52409668\n",
      "Iteration 2, loss = 0.49005911\n",
      "Iteration 3, loss = 0.47855894\n",
      "Iteration 4, loss = 0.47315935\n",
      "Iteration 5, loss = 0.46871479\n",
      "Iteration 6, loss = 0.46756330\n",
      "Iteration 7, loss = 0.46454303\n",
      "Iteration 8, loss = 0.46383065\n",
      "Iteration 9, loss = 0.46515402\n",
      "Iteration 10, loss = 0.46254557\n",
      "Iteration 11, loss = 0.46554505\n",
      "Iteration 12, loss = 0.46229072\n",
      "Iteration 13, loss = 0.45899522\n",
      "Iteration 14, loss = 0.46048006\n",
      "Iteration 15, loss = 0.45849294\n",
      "Iteration 16, loss = 0.45670448\n",
      "Iteration 17, loss = 0.45726970\n",
      "Iteration 18, loss = 0.45615587\n",
      "Iteration 19, loss = 0.45806477\n",
      "Iteration 20, loss = 0.45678997\n",
      "Iteration 21, loss = 0.45582265\n",
      "Iteration 22, loss = 0.45810505\n",
      "Iteration 23, loss = 0.45613768\n",
      "Iteration 24, loss = 0.45543301\n",
      "Iteration 25, loss = 0.45477019\n",
      "Iteration 26, loss = 0.45334528\n",
      "Iteration 27, loss = 0.45349857\n",
      "Iteration 28, loss = 0.45426995\n",
      "Iteration 29, loss = 0.45273531\n",
      "Iteration 30, loss = 0.45141794\n",
      "Iteration 31, loss = 0.45121094\n",
      "Iteration 32, loss = 0.44987731\n",
      "Iteration 33, loss = 0.44690489\n",
      "Iteration 34, loss = 0.44443083\n",
      "Iteration 35, loss = 0.44544859\n",
      "Iteration 36, loss = 0.44510526\n",
      "Iteration 37, loss = 0.44427854\n",
      "Iteration 38, loss = 0.44207026\n",
      "Iteration 39, loss = 0.44039303\n",
      "Iteration 40, loss = 0.44117982\n",
      "Iteration 41, loss = 0.43592700\n",
      "Iteration 42, loss = 0.43937873\n",
      "Iteration 43, loss = 0.43609597\n",
      "Iteration 44, loss = 0.43656177\n",
      "Iteration 45, loss = 0.43451259\n",
      "Iteration 46, loss = 0.43239608\n",
      "Iteration 47, loss = 0.43440276\n",
      "Iteration 48, loss = 0.43102088\n",
      "Iteration 49, loss = 0.43059114\n",
      "Iteration 50, loss = 0.42981221\n",
      "Iteration 51, loss = 0.42987273\n",
      "Iteration 52, loss = 0.43063360\n",
      "Iteration 53, loss = 0.42962792\n",
      "Iteration 54, loss = 0.42721223\n",
      "Iteration 55, loss = 0.42681052\n",
      "Iteration 56, loss = 0.42356909\n",
      "Iteration 57, loss = 0.42410374\n",
      "Iteration 58, loss = 0.42777013\n",
      "Iteration 59, loss = 0.42522031\n",
      "Iteration 60, loss = 0.42795514\n",
      "Iteration 61, loss = 0.42270374\n",
      "Iteration 62, loss = 0.42225325\n",
      "Iteration 63, loss = 0.42307582\n",
      "Iteration 64, loss = 0.42188060\n",
      "Iteration 65, loss = 0.42490046\n",
      "Iteration 66, loss = 0.42556616\n",
      "Iteration 67, loss = 0.42041256\n",
      "Iteration 68, loss = 0.42229193\n",
      "Iteration 69, loss = 0.42178081\n",
      "Iteration 70, loss = 0.42067439\n",
      "Iteration 71, loss = 0.42515168\n",
      "Iteration 72, loss = 0.42206172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 73, loss = 0.41978667\n",
      "Iteration 74, loss = 0.42095369\n",
      "Iteration 75, loss = 0.42125578\n",
      "Iteration 76, loss = 0.42367287\n",
      "Iteration 77, loss = 0.41910966\n",
      "Iteration 78, loss = 0.41940324\n",
      "Iteration 79, loss = 0.41851443\n",
      "Iteration 80, loss = 0.41990678\n",
      "Iteration 81, loss = 0.42004297\n",
      "Iteration 82, loss = 0.41910226\n",
      "Iteration 83, loss = 0.41893927\n",
      "Iteration 84, loss = 0.41966040\n",
      "Iteration 85, loss = 0.41825796\n",
      "Iteration 86, loss = 0.42072860\n",
      "Iteration 87, loss = 0.41826595\n",
      "Iteration 88, loss = 0.41692543\n",
      "Iteration 89, loss = 0.41886772\n",
      "Iteration 90, loss = 0.41717371\n",
      "Iteration 91, loss = 0.41539762\n",
      "Iteration 92, loss = 0.41871243\n",
      "Iteration 93, loss = 0.41797800\n",
      "Iteration 94, loss = 0.41606472\n",
      "Iteration 95, loss = 0.41704826\n",
      "Iteration 96, loss = 0.41668049\n",
      "Iteration 97, loss = 0.41563592\n",
      "Iteration 98, loss = 0.41657468\n",
      "Iteration 99, loss = 0.41909803\n",
      "Iteration 100, loss = 0.42126339\n",
      "Iteration 101, loss = 0.41897543\n",
      "Iteration 102, loss = 0.41499482\n",
      "Iteration 103, loss = 0.41851372\n",
      "Iteration 104, loss = 0.41750168\n",
      "Iteration 105, loss = 0.41540994\n",
      "Iteration 106, loss = 0.41503380\n",
      "Iteration 107, loss = 0.41436822\n",
      "Iteration 108, loss = 0.41248635\n",
      "Iteration 109, loss = 0.41503000\n",
      "Iteration 110, loss = 0.41372192\n",
      "Iteration 111, loss = 0.41516949\n",
      "Iteration 112, loss = 0.41429536\n",
      "Iteration 113, loss = 0.41286633\n",
      "Iteration 114, loss = 0.41335747\n",
      "Iteration 115, loss = 0.41327043\n",
      "Iteration 116, loss = 0.41515895\n",
      "Iteration 117, loss = 0.41404628\n",
      "Iteration 118, loss = 0.41187650\n",
      "Iteration 119, loss = 0.41298528\n",
      "Iteration 120, loss = 0.41140937\n",
      "Iteration 121, loss = 0.41432979\n",
      "Iteration 122, loss = 0.41480592\n",
      "Iteration 123, loss = 0.41405192\n",
      "Iteration 124, loss = 0.41325029\n",
      "Iteration 125, loss = 0.41584238\n",
      "Iteration 126, loss = 0.41245605\n",
      "Iteration 127, loss = 0.41168983\n",
      "Iteration 128, loss = 0.41203873\n",
      "Iteration 129, loss = 0.41613714\n",
      "Iteration 130, loss = 0.41130339\n",
      "Iteration 131, loss = 0.41182309\n",
      "Iteration 132, loss = 0.41128631\n",
      "Iteration 133, loss = 0.41191468\n",
      "Iteration 134, loss = 0.41370362\n",
      "Iteration 135, loss = 0.41398957\n",
      "Iteration 136, loss = 0.41464441\n",
      "Iteration 137, loss = 0.41376877\n",
      "Iteration 138, loss = 0.40990047\n",
      "Iteration 139, loss = 0.41305308\n",
      "Iteration 140, loss = 0.41140138\n",
      "Iteration 141, loss = 0.41268707\n",
      "Iteration 142, loss = 0.41210241\n",
      "Iteration 143, loss = 0.41299916\n",
      "Iteration 144, loss = 0.41159436\n",
      "Iteration 145, loss = 0.41073205\n",
      "Iteration 146, loss = 0.41179553\n",
      "Iteration 147, loss = 0.41154844\n",
      "Iteration 148, loss = 0.40924798\n",
      "Iteration 149, loss = 0.41103575\n",
      "Iteration 150, loss = 0.41268718\n",
      "Iteration 151, loss = 0.41034373\n",
      "Iteration 152, loss = 0.41038831\n",
      "Iteration 153, loss = 0.41121156\n",
      "Iteration 154, loss = 0.41176705\n",
      "Iteration 155, loss = 0.41060919\n",
      "Iteration 156, loss = 0.41025497\n",
      "Iteration 157, loss = 0.41412450\n",
      "Iteration 158, loss = 0.41053425\n",
      "Iteration 159, loss = 0.41059250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Elapsed time: 0 min. and 10.748347997665405 sec.\n",
      "\n",
      "0 total combinations for subset length 8.\n",
      "Elapsed time: 0 min. and 0.0 sec.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# next, we will search for the best subset between 1 feature and 8 features.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m subsets_df \u001b[38;5;241m=\u001b[39m \u001b[43mbest_subsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_subset_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_subset_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop three feature subsets:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(subsets_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_score\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m))\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mbest_subsets\u001b[1;34m(X, y, model, min_subset_len, max_subset_len, cv, verbose)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# iterating through desired subset lengths\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_subset_len, max_subset_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 69\u001b[0m     best \u001b[38;5;241m=\u001b[39m \u001b[43mbest_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# adding to lists to create df with later\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     subset_length\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36mbest_subset\u001b[1;34m(X, y, model, subset_len, cv, verbose)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# printing and returning metrics\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElapsed time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m((end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min. and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [[X\u001b[38;5;241m.\u001b[39mcolumns[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m best_subset], best_score]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# next, we will search for the best subset between 1 feature and 8 features.\n",
    "\n",
    "subsets_df = best_subsets(X, y, mlp, min_subset_len=1, max_subset_len=8, cv=3)\n",
    "print('Top three feature subsets:')\n",
    "print(subsets_df.sort_values(by='cv_score', ascending=False).head(3))\n",
    "best_subset_ftrs = subsets_df[subsets_df.cv_score == max(subsets_df.cv_score)]\n",
    "print('Best feature subset:')\n",
    "for f in best_subset_ftrs.features:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c7dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
